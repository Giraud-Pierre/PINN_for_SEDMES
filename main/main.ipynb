{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVy6Z9dcD+s5utgmqEvfMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giraud-Pierre/PINN_for_SEDMES/blob/Burgers_equation/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook aims to solve burger's equation with a PINN. Data from a matlab simulation are loaded and can be used to add data points or data for the initial conditions and boundary conditions for example, as well as to compare the results of the neural network afterwards."
      ],
      "metadata": {
        "id": "1Yc9SwXAMIhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data and libraries\n"
      ],
      "metadata": {
        "id": "htuo0z7ySVn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2lAKAYcjJzt5",
        "outputId": "5def6ee4-6f91-42b3-b758-b73580ebdae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PINN_for_SEDMES'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (202/202), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 206 (delta 77), reused 135 (delta 54), pack-reused 4\u001b[K\n",
            "Receiving objects: 100% (206/206), 19.11 MiB | 14.25 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "/content/PINN_for_SEDMES/main\n"
          ]
        }
      ],
      "source": [
        "#if runing on colab, use this to get the data\n",
        "!git clone -b Burgers_equation https://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@github.com/Giraud-Pierre/PINN_for_SEDMES.git\n",
        "%cd PINN_for_SEDMES/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "import time\n",
        "import scipy.io\n",
        "from scipy.stats import qmc\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "Y8izWMEWQRDu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "iD9sUZN-QTBH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# architecture of the feedforward network with 2 inputs being space (x)\n",
        "# and time and 2 outputs being Cg and Cs\n",
        "layers = [2, 100, 100, 100, 1]\n",
        "\n",
        "#get data from matlab workspace\n",
        "data = scipy.io.loadmat(\"../data/burgers_data.mat\") #load the simulation data from matlab\n",
        "\n",
        "t = data['t'].flatten()[:,None] # time from simulation\n",
        "x = data['x'].flatten()[:,None] # x from simulation\n",
        "exact_u = data['usol'] #Calculated speed from simulation, function of x and time\n",
        "\n",
        "#Domain bounds\n",
        "lb = np.array([x[0], t[0]]).flatten() #lower bondaries [space (m), time (s)]\n",
        "ub = np.array([x[-1], t[-1]]).flatten() #upper boundaries\n",
        "\n",
        "########## Boundary and initial conditions ################\n",
        "'''Boundary and initial conditions are enforced EITHER points from the simulation\n",
        "at t=0, x=0 and x=max(x), OR on theoretical values, ie: on x = 0 and x = max(x),\n",
        "u = 0 whereas on t = 0, u = -sin(pi*x)'''\n",
        "#Boundary conditions\n",
        "u_x0 = np.expand_dims(exact_u[0,:],axis =1) #u where x=0\n",
        "u_xmax = np.expand_dims(exact_u[-1,:],axis=1) #u where x=max(x)\n",
        "n_bound = 50 #number of points for the training\n",
        "\n",
        "#initial conditions\n",
        "u_t0 = np.expand_dims(exact_u[:,0],axis=1) #u where t=0\n",
        "n_0 = 100 #number of points for the training\n",
        "\n",
        "#Combining the\n",
        "########## Equations: #####################################\n",
        "'''To enforce the PDEs (here the gas phase balance and the particulate phase\n",
        "balance), the PINN generates at each epochs of training a number of random\n",
        "points. On these points, it will calculate the residues of each PDEs. It will\n",
        "then try to make them equal to zero by introducing them in the loss function.'''\n",
        "nf = 1000\n"
      ],
      "metadata": {
        "id": "IVParo-4ShXO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PINN model"
      ],
      "metadata": {
        "id": "pCj52mYzyrPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BurgersPINN:\n",
        "  '''PINN model tailored to solve burgers equation with the matlab\n",
        "  simulation data for the initial and boundary conditions'''\n",
        "  def __init__(self, x, t, u_x0, u_xmax, n_bound, u_t0, n_0, nf, layers, lb, ub, keep_best_only = True):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    x = array of the values for x (shapes = (n,1)) in the matlab simulation\n",
        "    t = array of the values for t (shapes = (n,1)) in the matlab simulation\n",
        "    u_x0 = the solutions from the matlab function of time at x=0\n",
        "    u_xmax = the solution function of time at x =max(x)\n",
        "    n_bound = the number of points to enforce the boundary conditions\n",
        "    u_t0 = the solutions from the matlab function of space at t=0\n",
        "    n_0 = the number of points to enforce the initial conditions\n",
        "    nf = the number of collocation points to enforce the PDE\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    lb = the lower boundary [space, time]\n",
        "    ub = the upper boundary [space,time]\n",
        "    keep_best_only = True to only keep the model with the best loss, False will update the model no matter the loss'''\n",
        "\n",
        "    '''Initialization'''\n",
        "    #constant\n",
        "    self.nu = 0.01/np.pi #viscosity coefficient\n",
        "\n",
        "    #data from matlab\n",
        "    self.x = x\n",
        "    self.t = t\n",
        "    self.u_x0 = u_x0\n",
        "    self.u_xmax = u_xmax\n",
        "    self.n_bound = n_bound\n",
        "    self.u_t0 = u_t0\n",
        "    self.n_0 = n_0\n",
        "    self.nf = nf\n",
        "    self.lb =lb\n",
        "    self.ub = ub\n",
        "\n",
        "    #initialization of the network\n",
        "    self.keep_best_only = keep_best_only\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "\n",
        "    '''creating tensorflow placeholder (one for each array)'''\n",
        "    #placeholder for initial and boundary conditions\n",
        "    self.x_bounds_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    self.t_bounds_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    self.u_bounds_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    #placeholder for residues\n",
        "    self.x_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #Network graph (also used for initial and boundary condition)\n",
        "    self.u_pred = self.Net_u(self.x_bounds_tf, self.t_bounds_tf)\n",
        "    #residues graph and exact solutions for debug\n",
        "    self.f_pred = self.net_f_u(self.x_f_tf, self.t_f_tf)\n",
        "\n",
        "    '''loss function'''\n",
        "    #Global loss function\n",
        "    self.loss = tf.reduce_mean(input_tensor=tf.square(self.u_pred - self.u_bounds_tf)) + \\\n",
        "                tf.reduce_mean(input_tensor=tf.square(self.f_pred))\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_log = []\n",
        "\n",
        "    #log for the weights and biases\n",
        "    self.weights_log = []\n",
        "    self.biases_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 5\n",
        "    self.reduce_LR_cooldown = 2\n",
        "    self.count_cooldown = 15\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Save_Best_Weights_Biases_losses(self, loss):\n",
        "    '''Check if the current loss is better than the previous recorded loss.\n",
        "    Then save the best losses, weights and biases between the current model and\n",
        "    the previous one (and reverse to the previous model if it was better).'''\n",
        "    if(len(self.loss_log) < 1 or loss < self.loss_log[-1] or not self.keep_best_only):\n",
        "      #if it is the first epoch of training or if the loss is better than\n",
        "      #the previous one, keep the current model.\n",
        "      self.weights_log.append(self.weights)\n",
        "      self.biases_log.append(self.biases)\n",
        "      self.loss_log.append(loss)\n",
        "\n",
        "      return None\n",
        "    else:\n",
        "      #if the loss is worse than the previous one, return to the previous model.\n",
        "      self.weights = self.weights_log[-1]\n",
        "      self.biases = self.biases_log[-1]\n",
        "      self.weights_log.append(self.weights)\n",
        "      self.biases_log.append(self.biases)\n",
        "      self.loss_log.append(self.loss_log[-1])\n",
        "      return None\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_data_and_collocation_points(self):\n",
        "    #initial conditions\n",
        "    idx_t0 = np.random.randint(self.u_x0.shape[0],size=self.n_0)\n",
        "    x_t0 = self.x[idx_t0]\n",
        "    t_t0 = x_t0 *0 + self.lb[1]\n",
        "    u_t0 = self.u_t0[idx_t0]\n",
        "\n",
        "    #Boundary conditions\n",
        "    idt_bounds = np.random.randint(self.u_x0.shape[0],size=self.n_bound)\n",
        "    t_bounds = t[idt_bounds]\n",
        "    x_x0 = t_bounds * 0 + self.lb[0]\n",
        "    u_x0 = self.u_x0[idt_bounds]\n",
        "    x_xmax = t_bounds * 0 + self.ub[0]\n",
        "    u_xmax = self.u_xmax[idt_bounds]\n",
        "\n",
        "    #Combined boundaries\n",
        "    self.x_bounds=np.concatenate((x_t0,x_x0,x_xmax))\n",
        "    self.t_bounds=np.concatenate((t_t0,t_bounds,t_bounds))\n",
        "    self.u_bounds=np.concatenate((u_t0,u_x0,u_xmax))\n",
        "\n",
        "    #Equations collocation points\n",
        "    X_T_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.nf)\n",
        "    self.x_f = X_T_f[:,0:1]\n",
        "    self.t_f = X_T_f[:,1:2]\n",
        "\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = tf.math.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_u(self, x, t):\n",
        "    '''Calculate Cg, Cs at a given x and t using the neural network'''\n",
        "    X = tf.concat([x,t],1)\n",
        "\n",
        "    u = self.Neural_net(X,self.weights,self.biases)\n",
        "\n",
        "    return u\n",
        "\n",
        "  def net_f_u(self, x, t):\n",
        "    '''Calculate the residues of the PDE'''\n",
        "\n",
        "    u = self.Net_u(x, t)\n",
        "\n",
        "    u_x = tf.gradients(ys=u,xs=x)[0]\n",
        "    u_xx = tf.gradients(ys=u_x,xs=x)[0]\n",
        "    u_t = tf.gradients(ys=u,xs=t)[0]\n",
        "\n",
        "    f= u_t + u*u_x - u_xx * self.nu\n",
        "\n",
        "    return f\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    #take random samples of the data for training\n",
        "    self.Initialize_random_data_and_collocation_points()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.x_bounds_tf: self.x_bounds, self.t_bounds_tf: self.t_bounds,\n",
        "                 self.u_bounds_tf: self.u_bounds,\n",
        "                 self.x_f_tf: self.x_f, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        #record the losses\n",
        "        loss_value = self.sess.run(self.loss, tf_dict)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "        #record the best loss, weights and biases\n",
        "        self.Save_Best_Weights_Biases_losses(loss_value)\n",
        "\n",
        "        if it % 100 == 0:\n",
        "          # Generate new random sample points for the next 100 epochs\n",
        "          self.Initialize_random_data_and_collocation_points()\n",
        "\n",
        "        #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, learning rate: %.5f, Time: %.2f\" \\\n",
        "                % (it, loss_value,  self.learning_rate, elapsed))\n",
        "        start_time = time.time()\n",
        "\n",
        "        if(it%1000 == 0):\n",
        "          #save the gradients every 1000 epochs (skipped because it took too much time)\n",
        "          #self.save_gradients(tf_dict)\n",
        "          print(\"Gradients have not been stored ...\")\n",
        "\n",
        "    return None\n",
        "\n",
        "  def Predict(self, x_t):\n",
        "    '''Use to predict Cg and Cs for a given array x_t of shape (n,2) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(len(x_t.shape) == 2 and x_t.shape[1] == 2):\n",
        "      x = x_t[:,0:1]\n",
        "      t = x_t[:,1:2]\n",
        "    else:\n",
        "      print(\"Error: unexpected shape of x or t, should be (n,) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.x_bounds_tf: x, self.t_bounds_tf: t}\n",
        "\n",
        "    u = self.sess.run([self.u_pred], tf_dict)\n",
        "\n",
        "    return u"
      ],
      "metadata": {
        "id": "ZUyDMgmId5n7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PINN testing"
      ],
      "metadata": {
        "id": "CZqTvhJpyvQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[2,64,64,64,1],[2,128,128,128,1],[2,256,256,256,1],[2,64,64,64,64,1],[2,256,256,256,256,1],[2,128,128,128,128,1],[2,128,128,64,64,1],[2,256,128,64,32,1]]\n",
        "u_predict = []\n",
        "predict_Cs_Sol = []\n",
        "x_t = [[0,0]]\n",
        "for idx_t in range(len(t)):\n",
        "  x_t = np.append(x_t, np.insert(x,1,t[idx_t][0], axis = 1), axis = 0)\n",
        "x_t = x_t[1:]\n",
        "\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  model = BurgersPINN(x, t, u_x0, u_xmax, n_bound, u_t0, n_0, nf, layers, lb, ub)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  model.Train(1000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  u_predict.append(model.Predict(x_t))"
      ],
      "metadata": {
        "id": "m5vFeQMGg6t9",
        "outputId": "0652eeb8-06ec-434c-f1a6-5b514e9b8bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 64, 64, 64, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.718e-01, learning rate: 0.00100, Time: 0.79\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.862e-01, learning rate: 0.00100, Time: 0.31\n",
            "It: 20, Loss: 1.595e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 30, Loss: 1.606e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 40, Loss: 1.579e-01, learning rate: 0.00100, Time: 0.31\n",
            "It: 50, Loss: 1.579e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 60, Loss: 1.573e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 70, Loss: 1.571e-01, learning rate: 0.00100, Time: 0.42\n",
            "It: 80, Loss: 1.567e-01, learning rate: 0.00100, Time: 0.49\n",
            "It: 90, Loss: 1.560e-01, learning rate: 0.00100, Time: 0.53\n",
            "It: 100, Loss: 1.549e-01, learning rate: 0.00100, Time: 0.50\n",
            "It: 110, Loss: 1.480e-01, learning rate: 0.00100, Time: 0.52\n",
            "It: 120, Loss: 1.418e-01, learning rate: 0.00100, Time: 0.51\n",
            "It: 130, Loss: 1.216e-01, learning rate: 0.00100, Time: 0.39\n",
            "It: 140, Loss: 7.186e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 150, Loss: 4.592e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 160, Loss: 4.131e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 170, Loss: 3.929e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 180, Loss: 3.286e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 190, Loss: 2.824e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 200, Loss: 2.474e-02, learning rate: 0.00100, Time: 0.32\n",
            "It: 210, Loss: 2.278e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 220, Loss: 2.035e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 230, Loss: 1.850e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 240, Loss: 1.694e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 250, Loss: 1.557e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 260, Loss: 1.435e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 270, Loss: 1.319e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 280, Loss: 1.209e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 290, Loss: 1.154e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 300, Loss: 1.020e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 310, Loss: 9.699e-03, learning rate: 0.00100, Time: 0.33\n",
            "It: 320, Loss: 8.914e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 330, Loss: 8.356e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 340, Loss: 7.840e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 350, Loss: 7.379e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 360, Loss: 6.981e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 370, Loss: 7.736e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 380, Loss: 6.371e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 390, Loss: 6.091e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 400, Loss: 5.727e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 410, Loss: 6.178e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 420, Loss: 5.832e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 430, Loss: 5.501e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 440, Loss: 5.699e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 450, Loss: 5.249e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 460, Loss: 4.936e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 470, Loss: 4.499e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 480, Loss: 4.186e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 490, Loss: 3.887e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 500, Loss: 3.639e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 510, Loss: 3.240e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 520, Loss: 3.054e-03, learning rate: 0.00100, Time: 0.51\n",
            "It: 530, Loss: 2.890e-03, learning rate: 0.00100, Time: 0.37\n",
            "It: 540, Loss: 3.075e-03, learning rate: 0.00100, Time: 0.39\n",
            "It: 550, Loss: 4.031e-03, learning rate: 0.00100, Time: 0.56\n",
            "It: 560, Loss: 3.187e-03, learning rate: 0.00100, Time: 0.51\n",
            "It: 570, Loss: 2.654e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 580, Loss: 2.454e-03, learning rate: 0.00100, Time: 0.34\n",
            "It: 590, Loss: 2.343e-03, learning rate: 0.00100, Time: 0.64\n",
            "It: 600, Loss: 2.277e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 610, Loss: 2.259e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 620, Loss: 2.199e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 630, Loss: 2.154e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 640, Loss: 2.116e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 650, Loss: 2.083e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 660, Loss: 2.052e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 670, Loss: 2.024e-03, learning rate: 0.00100, Time: 0.56\n",
            "It: 680, Loss: 1.998e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 690, Loss: 2.120e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 700, Loss: 2.668e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 710, Loss: 2.396e-03, learning rate: 0.00100, Time: 0.43\n",
            "It: 720, Loss: 2.124e-03, learning rate: 0.00100, Time: 0.39\n",
            "It: 730, Loss: 2.041e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 740, Loss: 2.008e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 750, Loss: 1.966e-03, learning rate: 0.00090, Time: 0.36\n",
            "It: 760, Loss: 1.942e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 770, Loss: 1.922e-03, learning rate: 0.00090, Time: 0.50\n",
            "It: 780, Loss: 1.902e-03, learning rate: 0.00090, Time: 0.47\n",
            "It: 790, Loss: 1.882e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 800, Loss: 1.864e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 810, Loss: 1.710e-03, learning rate: 0.00090, Time: 0.51\n",
            "It: 820, Loss: 1.674e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 830, Loss: 1.658e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 840, Loss: 1.640e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 850, Loss: 1.624e-03, learning rate: 0.00090, Time: 0.35\n",
            "It: 860, Loss: 1.610e-03, learning rate: 0.00090, Time: 0.41\n",
            "It: 870, Loss: 1.597e-03, learning rate: 0.00090, Time: 0.47\n",
            "It: 880, Loss: 1.583e-03, learning rate: 0.00090, Time: 0.55\n",
            "It: 890, Loss: 1.571e-03, learning rate: 0.00090, Time: 0.64\n",
            "It: 900, Loss: 1.558e-03, learning rate: 0.00090, Time: 0.68\n",
            "It: 910, Loss: 1.530e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 920, Loss: 1.601e-03, learning rate: 0.00090, Time: 0.58\n",
            "It: 930, Loss: 2.403e-03, learning rate: 0.00090, Time: 0.52\n",
            "It: 940, Loss: 1.539e-03, learning rate: 0.00090, Time: 0.58\n",
            "It: 950, Loss: 1.398e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 960, Loss: 1.420e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 970, Loss: 1.384e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 980, Loss: 1.368e-03, learning rate: 0.00090, Time: 0.56\n",
            "It: 990, Loss: 1.358e-03, learning rate: 0.00090, Time: 0.60\n",
            "Training time: 41.5551\n",
            "[2, 128, 128, 128, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.773e-01, learning rate: 0.00100, Time: 1.02\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.858e-01, learning rate: 0.00100, Time: 0.47\n",
            "It: 20, Loss: 1.708e-01, learning rate: 0.00100, Time: 0.46\n",
            "It: 30, Loss: 1.667e-01, learning rate: 0.00100, Time: 0.54\n",
            "It: 40, Loss: 1.668e-01, learning rate: 0.00100, Time: 0.54\n",
            "It: 50, Loss: 1.656e-01, learning rate: 0.00100, Time: 0.58\n",
            "It: 60, Loss: 1.655e-01, learning rate: 0.00100, Time: 0.52\n",
            "It: 70, Loss: 1.650e-01, learning rate: 0.00100, Time: 0.56\n",
            "It: 80, Loss: 1.645e-01, learning rate: 0.00100, Time: 0.57\n",
            "It: 90, Loss: 1.637e-01, learning rate: 0.00100, Time: 0.56\n",
            "It: 100, Loss: 1.626e-01, learning rate: 0.00100, Time: 0.62\n",
            "It: 110, Loss: 1.484e-01, learning rate: 0.00100, Time: 0.73\n",
            "It: 120, Loss: 1.453e-01, learning rate: 0.00100, Time: 0.99\n",
            "It: 130, Loss: 1.374e-01, learning rate: 0.00100, Time: 0.86\n",
            "It: 140, Loss: 1.113e-01, learning rate: 0.00100, Time: 0.81\n",
            "It: 150, Loss: 5.849e-02, learning rate: 0.00100, Time: 0.72\n",
            "It: 160, Loss: 4.969e-02, learning rate: 0.00100, Time: 0.61\n",
            "It: 170, Loss: 3.773e-02, learning rate: 0.00100, Time: 0.57\n",
            "It: 180, Loss: 3.278e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 190, Loss: 2.869e-02, learning rate: 0.00100, Time: 0.58\n",
            "It: 200, Loss: 2.642e-02, learning rate: 0.00100, Time: 0.54\n",
            "It: 210, Loss: 2.577e-02, learning rate: 0.00100, Time: 0.59\n",
            "It: 220, Loss: 2.373e-02, learning rate: 0.00100, Time: 0.60\n",
            "It: 230, Loss: 2.175e-02, learning rate: 0.00100, Time: 0.58\n",
            "It: 240, Loss: 1.969e-02, learning rate: 0.00100, Time: 0.67\n",
            "It: 250, Loss: 1.774e-02, learning rate: 0.00100, Time: 0.56\n",
            "It: 260, Loss: 1.587e-02, learning rate: 0.00100, Time: 0.62\n",
            "It: 270, Loss: 1.416e-02, learning rate: 0.00100, Time: 0.66\n",
            "It: 280, Loss: 1.273e-02, learning rate: 0.00100, Time: 0.63\n",
            "It: 290, Loss: 1.359e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 300, Loss: 1.125e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 310, Loss: 9.633e-03, learning rate: 0.00100, Time: 0.53\n",
            "It: 320, Loss: 8.931e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 330, Loss: 8.324e-03, learning rate: 0.00100, Time: 0.66\n",
            "It: 340, Loss: 7.778e-03, learning rate: 0.00100, Time: 0.65\n",
            "It: 350, Loss: 7.285e-03, learning rate: 0.00100, Time: 0.68\n",
            "It: 360, Loss: 6.826e-03, learning rate: 0.00100, Time: 0.62\n",
            "It: 370, Loss: 7.652e-03, learning rate: 0.00100, Time: 0.65\n",
            "It: 380, Loss: 6.869e-03, learning rate: 0.00100, Time: 0.59\n",
            "It: 390, Loss: 5.797e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 400, Loss: 5.634e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 410, Loss: 5.461e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 420, Loss: 5.234e-03, learning rate: 0.00100, Time: 0.43\n",
            "It: 430, Loss: 5.028e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 440, Loss: 4.854e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 450, Loss: 4.704e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 460, Loss: 4.612e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 470, Loss: 4.963e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 480, Loss: 4.489e-03, learning rate: 0.00100, Time: 0.38\n",
            "It: 490, Loss: 4.307e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 500, Loss: 4.158e-03, learning rate: 0.00100, Time: 0.41\n",
            "It: 510, Loss: 4.436e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 520, Loss: 4.300e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 530, Loss: 4.175e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 540, Loss: 4.046e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 550, Loss: 3.930e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 560, Loss: 5.497e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 570, Loss: 3.878e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 580, Loss: 3.597e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 590, Loss: 3.462e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 600, Loss: 3.333e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 610, Loss: 3.087e-03, learning rate: 0.00100, Time: 0.27\n",
            "It: 620, Loss: 2.935e-03, learning rate: 0.00100, Time: 0.26\n",
            "It: 630, Loss: 2.887e-03, learning rate: 0.00100, Time: 0.27\n",
            "It: 640, Loss: 3.323e-03, learning rate: 0.00100, Time: 0.27\n",
            "It: 650, Loss: 3.028e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 660, Loss: 2.647e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 670, Loss: 2.397e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 680, Loss: 2.280e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 690, Loss: 2.182e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 700, Loss: 2.099e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 710, Loss: 2.962e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 720, Loss: 2.934e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 730, Loss: 2.157e-03, learning rate: 0.00100, Time: 0.27\n",
            "It: 740, Loss: 2.200e-03, learning rate: 0.00100, Time: 0.27\n",
            "It: 750, Loss: 2.053e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 760, Loss: 2.010e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 770, Loss: 1.981e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 780, Loss: 1.943e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 790, Loss: 1.929e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 800, Loss: 7.365e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 810, Loss: 2.837e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 820, Loss: 1.695e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 830, Loss: 1.665e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 840, Loss: 1.505e-03, learning rate: 0.00100, Time: 0.32\n",
            "It: 850, Loss: 1.480e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 860, Loss: 1.463e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 870, Loss: 1.439e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 880, Loss: 1.422e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 890, Loss: 1.405e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 900, Loss: 1.389e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 910, Loss: 1.572e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 920, Loss: 1.528e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 930, Loss: 1.502e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 940, Loss: 1.483e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 950, Loss: 1.467e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 960, Loss: 1.451e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 970, Loss: 1.437e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 980, Loss: 1.424e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 990, Loss: 1.410e-03, learning rate: 0.00081, Time: 0.30\n",
            "Training time: 45.1440\n",
            "[2, 256, 256, 256, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.545e-01, learning rate: 0.00100, Time: 1.17\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.692e-01, learning rate: 0.00100, Time: 0.50\n",
            "It: 20, Loss: 1.593e-01, learning rate: 0.00100, Time: 0.48\n",
            "It: 30, Loss: 1.543e-01, learning rate: 0.00100, Time: 0.33\n",
            "It: 40, Loss: 1.543e-01, learning rate: 0.00100, Time: 0.31\n",
            "It: 50, Loss: 1.530e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 60, Loss: 1.524e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 70, Loss: 1.511e-01, learning rate: 0.00100, Time: 0.31\n",
            "It: 80, Loss: 1.489e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 90, Loss: 1.451e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 100, Loss: 1.331e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 110, Loss: 1.023e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 120, Loss: 4.931e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 130, Loss: 4.953e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 140, Loss: 3.781e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 150, Loss: 3.056e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 160, Loss: 2.606e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 170, Loss: 2.239e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 180, Loss: 1.932e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 190, Loss: 1.722e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 200, Loss: 1.573e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 210, Loss: 1.436e-02, learning rate: 0.00100, Time: 0.33\n",
            "It: 220, Loss: 1.294e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 230, Loss: 1.186e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 240, Loss: 1.094e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 250, Loss: 1.006e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 260, Loss: 9.254e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 270, Loss: 9.098e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 280, Loss: 8.214e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 290, Loss: 7.623e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 300, Loss: 7.017e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 310, Loss: 6.836e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 320, Loss: 6.047e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 330, Loss: 5.759e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 340, Loss: 5.828e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 350, Loss: 5.283e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 360, Loss: 5.138e-03, learning rate: 0.00100, Time: 0.32\n",
            "It: 370, Loss: 4.955e-03, learning rate: 0.00100, Time: 0.35\n",
            "It: 380, Loss: 4.655e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 390, Loss: 4.499e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 400, Loss: 4.373e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 410, Loss: 3.975e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 420, Loss: 3.714e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 430, Loss: 3.529e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 440, Loss: 3.509e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 450, Loss: 3.304e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 460, Loss: 3.207e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 470, Loss: 3.098e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 480, Loss: 2.904e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 490, Loss: 2.752e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 500, Loss: 2.670e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 510, Loss: 3.649e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 520, Loss: 3.538e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 530, Loss: 2.737e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 540, Loss: 2.620e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 550, Loss: 2.516e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 560, Loss: 2.449e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 570, Loss: 2.403e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 580, Loss: 2.466e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 590, Loss: 2.439e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 600, Loss: 2.307e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 610, Loss: 1.944e-03, learning rate: 0.00100, Time: 0.32\n",
            "It: 620, Loss: 1.899e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 630, Loss: 1.837e-03, learning rate: 0.00100, Time: 0.32\n",
            "It: 640, Loss: 1.798e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 650, Loss: 1.767e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 660, Loss: 1.729e-03, learning rate: 0.00100, Time: 0.37\n",
            "It: 670, Loss: 1.698e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 680, Loss: 1.671e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 690, Loss: 1.652e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 700, Loss: 2.734e-03, learning rate: 0.00100, Time: 0.43\n",
            "It: 710, Loss: 2.935e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 720, Loss: 2.244e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 730, Loss: 1.995e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 740, Loss: 1.885e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 750, Loss: 1.835e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 760, Loss: 1.818e-03, learning rate: 0.00090, Time: 0.49\n",
            "It: 770, Loss: 1.798e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 780, Loss: 1.782e-03, learning rate: 0.00081, Time: 0.49\n",
            "It: 790, Loss: 1.768e-03, learning rate: 0.00081, Time: 0.49\n",
            "It: 800, Loss: 1.755e-03, learning rate: 0.00081, Time: 0.42\n",
            "It: 810, Loss: 1.812e-03, learning rate: 0.00073, Time: 0.29\n",
            "It: 820, Loss: 1.793e-03, learning rate: 0.00073, Time: 0.30\n",
            "It: 830, Loss: 1.777e-03, learning rate: 0.00073, Time: 0.30\n",
            "It: 840, Loss: 1.766e-03, learning rate: 0.00066, Time: 0.30\n",
            "It: 850, Loss: 1.757e-03, learning rate: 0.00066, Time: 0.29\n",
            "It: 860, Loss: 1.748e-03, learning rate: 0.00066, Time: 0.29\n",
            "It: 870, Loss: 1.740e-03, learning rate: 0.00059, Time: 0.30\n",
            "It: 880, Loss: 1.733e-03, learning rate: 0.00059, Time: 0.29\n",
            "It: 890, Loss: 1.725e-03, learning rate: 0.00059, Time: 0.30\n",
            "It: 900, Loss: 1.718e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 910, Loss: 1.483e-03, learning rate: 0.00053, Time: 0.31\n",
            "It: 920, Loss: 1.470e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 930, Loss: 1.455e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 940, Loss: 1.447e-03, learning rate: 0.00053, Time: 0.30\n",
            "It: 950, Loss: 1.440e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 960, Loss: 1.434e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 970, Loss: 1.428e-03, learning rate: 0.00053, Time: 0.29\n",
            "It: 980, Loss: 1.423e-03, learning rate: 0.00053, Time: 0.31\n",
            "It: 990, Loss: 1.418e-03, learning rate: 0.00053, Time: 0.30\n",
            "Training time: 34.4114\n",
            "[2, 64, 64, 64, 64, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.601e-01, learning rate: 0.00100, Time: 1.23\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.867e-01, learning rate: 0.00100, Time: 0.49\n",
            "It: 20, Loss: 1.719e-01, learning rate: 0.00100, Time: 0.49\n",
            "It: 30, Loss: 1.706e-01, learning rate: 0.00100, Time: 0.50\n",
            "It: 40, Loss: 1.682e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 50, Loss: 1.667e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 60, Loss: 1.632e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 70, Loss: 1.520e-01, learning rate: 0.00100, Time: 0.32\n",
            "It: 80, Loss: 1.192e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 90, Loss: 6.390e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 100, Loss: 5.322e-02, learning rate: 0.00100, Time: 0.33\n",
            "It: 110, Loss: 3.926e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 120, Loss: 3.887e-02, learning rate: 0.00100, Time: 0.40\n",
            "It: 130, Loss: 3.561e-02, learning rate: 0.00100, Time: 0.46\n",
            "It: 140, Loss: 3.340e-02, learning rate: 0.00100, Time: 0.38\n",
            "It: 150, Loss: 3.066e-02, learning rate: 0.00100, Time: 0.46\n",
            "It: 160, Loss: 2.752e-02, learning rate: 0.00100, Time: 0.46\n",
            "It: 170, Loss: 2.459e-02, learning rate: 0.00100, Time: 0.33\n",
            "It: 180, Loss: 2.258e-02, learning rate: 0.00100, Time: 0.42\n",
            "It: 190, Loss: 2.095e-02, learning rate: 0.00100, Time: 0.40\n",
            "It: 200, Loss: 1.937e-02, learning rate: 0.00100, Time: 0.41\n",
            "It: 210, Loss: 2.101e-02, learning rate: 0.00100, Time: 0.65\n",
            "It: 220, Loss: 1.937e-02, learning rate: 0.00100, Time: 0.63\n",
            "It: 230, Loss: 1.791e-02, learning rate: 0.00100, Time: 0.49\n",
            "It: 240, Loss: 1.664e-02, learning rate: 0.00100, Time: 0.55\n",
            "It: 250, Loss: 1.558e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 260, Loss: 1.447e-02, learning rate: 0.00100, Time: 0.60\n",
            "It: 270, Loss: 1.361e-02, learning rate: 0.00100, Time: 0.61\n",
            "It: 280, Loss: 1.255e-02, learning rate: 0.00100, Time: 0.70\n",
            "It: 290, Loss: 1.228e-02, learning rate: 0.00100, Time: 0.60\n",
            "It: 300, Loss: 1.078e-02, learning rate: 0.00100, Time: 0.65\n",
            "It: 310, Loss: 1.042e-02, learning rate: 0.00100, Time: 0.71\n",
            "It: 320, Loss: 9.431e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 330, Loss: 8.433e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 340, Loss: 7.581e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 350, Loss: 6.821e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 360, Loss: 6.328e-03, learning rate: 0.00100, Time: 0.37\n",
            "It: 370, Loss: 6.017e-03, learning rate: 0.00100, Time: 0.69\n",
            "It: 380, Loss: 5.321e-03, learning rate: 0.00100, Time: 0.73\n",
            "It: 390, Loss: 4.895e-03, learning rate: 0.00100, Time: 0.65\n",
            "It: 400, Loss: 4.577e-03, learning rate: 0.00100, Time: 0.67\n",
            "It: 410, Loss: 3.874e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 420, Loss: 3.618e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 430, Loss: 4.008e-03, learning rate: 0.00100, Time: 0.38\n",
            "It: 440, Loss: 3.782e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 450, Loss: 3.321e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 460, Loss: 3.130e-03, learning rate: 0.00100, Time: 0.41\n",
            "It: 470, Loss: 3.018e-03, learning rate: 0.00100, Time: 0.51\n",
            "It: 480, Loss: 2.902e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 490, Loss: 2.834e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 500, Loss: 2.761e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 510, Loss: 3.217e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 520, Loss: 2.923e-03, learning rate: 0.00100, Time: 0.60\n",
            "It: 530, Loss: 3.004e-03, learning rate: 0.00100, Time: 0.80\n",
            "It: 540, Loss: 2.930e-03, learning rate: 0.00100, Time: 0.79\n",
            "It: 550, Loss: 3.162e-03, learning rate: 0.00100, Time: 0.66\n",
            "It: 560, Loss: 2.677e-03, learning rate: 0.00090, Time: 0.89\n",
            "It: 570, Loss: 2.583e-03, learning rate: 0.00090, Time: 0.55\n",
            "It: 580, Loss: 2.532e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 590, Loss: 2.473e-03, learning rate: 0.00090, Time: 0.56\n",
            "It: 600, Loss: 2.420e-03, learning rate: 0.00090, Time: 0.61\n",
            "It: 610, Loss: 2.597e-03, learning rate: 0.00090, Time: 0.59\n",
            "It: 620, Loss: 2.540e-03, learning rate: 0.00090, Time: 0.56\n",
            "It: 630, Loss: 2.671e-03, learning rate: 0.00090, Time: 0.62\n",
            "It: 640, Loss: 2.409e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 650, Loss: 2.282e-03, learning rate: 0.00090, Time: 0.52\n",
            "It: 660, Loss: 2.220e-03, learning rate: 0.00090, Time: 0.48\n",
            "It: 670, Loss: 2.182e-03, learning rate: 0.00090, Time: 0.46\n",
            "It: 680, Loss: 2.137e-03, learning rate: 0.00090, Time: 0.49\n",
            "It: 690, Loss: 2.101e-03, learning rate: 0.00090, Time: 0.42\n",
            "It: 700, Loss: 2.397e-03, learning rate: 0.00090, Time: 0.44\n",
            "It: 710, Loss: 2.788e-03, learning rate: 0.00090, Time: 0.31\n",
            "It: 720, Loss: 2.171e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 730, Loss: 2.023e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 740, Loss: 1.927e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 750, Loss: 1.877e-03, learning rate: 0.00090, Time: 0.31\n",
            "It: 760, Loss: 1.847e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 770, Loss: 1.814e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 780, Loss: 1.783e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 790, Loss: 1.763e-03, learning rate: 0.00090, Time: 0.37\n",
            "It: 800, Loss: 2.751e-03, learning rate: 0.00090, Time: 0.50\n",
            "It: 810, Loss: 1.940e-03, learning rate: 0.00090, Time: 0.47\n",
            "It: 820, Loss: 1.757e-03, learning rate: 0.00090, Time: 0.49\n",
            "It: 830, Loss: 1.655e-03, learning rate: 0.00090, Time: 0.49\n",
            "It: 840, Loss: 1.618e-03, learning rate: 0.00090, Time: 0.50\n",
            "It: 850, Loss: 1.908e-03, learning rate: 0.00090, Time: 0.49\n",
            "It: 860, Loss: 1.562e-03, learning rate: 0.00090, Time: 0.31\n",
            "It: 870, Loss: 1.736e-03, learning rate: 0.00090, Time: 0.35\n",
            "It: 880, Loss: 1.549e-03, learning rate: 0.00090, Time: 0.42\n",
            "It: 890, Loss: 1.490e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 900, Loss: 1.470e-03, learning rate: 0.00090, Time: 0.53\n",
            "It: 910, Loss: 1.609e-03, learning rate: 0.00090, Time: 0.55\n",
            "It: 920, Loss: 1.765e-03, learning rate: 0.00090, Time: 0.54\n",
            "It: 930, Loss: 1.600e-03, learning rate: 0.00090, Time: 0.55\n",
            "It: 940, Loss: 1.608e-03, learning rate: 0.00090, Time: 0.53\n",
            "It: 950, Loss: 1.496e-03, learning rate: 0.00090, Time: 0.57\n",
            "It: 960, Loss: 1.544e-03, learning rate: 0.00081, Time: 0.52\n",
            "It: 970, Loss: 1.457e-03, learning rate: 0.00081, Time: 0.62\n",
            "It: 980, Loss: 1.410e-03, learning rate: 0.00081, Time: 0.55\n",
            "It: 990, Loss: 1.383e-03, learning rate: 0.00081, Time: 0.58\n",
            "Training time: 50.8133\n",
            "[2, 256, 256, 256, 256, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.767e-01, learning rate: 0.00100, Time: 1.61\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.868e-01, learning rate: 0.00100, Time: 0.38\n",
            "It: 20, Loss: 1.628e-01, learning rate: 0.00100, Time: 0.31\n",
            "It: 30, Loss: 1.625e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 40, Loss: 1.613e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 50, Loss: 1.607e-01, learning rate: 0.00100, Time: 0.33\n",
            "It: 60, Loss: 1.605e-01, learning rate: 0.00100, Time: 0.38\n",
            "It: 70, Loss: 1.602e-01, learning rate: 0.00100, Time: 0.42\n",
            "It: 80, Loss: 1.599e-01, learning rate: 0.00100, Time: 0.43\n",
            "It: 90, Loss: 1.595e-01, learning rate: 0.00100, Time: 0.40\n",
            "It: 100, Loss: 1.590e-01, learning rate: 0.00100, Time: 0.50\n",
            "It: 110, Loss: 1.591e-01, learning rate: 0.00100, Time: 0.47\n",
            "It: 120, Loss: 1.569e-01, learning rate: 0.00100, Time: 0.38\n",
            "It: 130, Loss: 1.507e-01, learning rate: 0.00100, Time: 0.47\n",
            "It: 140, Loss: 1.316e-01, learning rate: 0.00100, Time: 0.38\n",
            "It: 150, Loss: 8.953e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 160, Loss: 4.587e-02, learning rate: 0.00100, Time: 0.51\n",
            "It: 170, Loss: 4.776e-02, learning rate: 0.00100, Time: 0.73\n",
            "It: 180, Loss: 3.936e-02, learning rate: 0.00100, Time: 0.77\n",
            "It: 190, Loss: 3.426e-02, learning rate: 0.00100, Time: 0.60\n",
            "It: 200, Loss: 2.977e-02, learning rate: 0.00100, Time: 0.73\n",
            "It: 210, Loss: 2.594e-02, learning rate: 0.00100, Time: 0.57\n",
            "It: 220, Loss: 2.297e-02, learning rate: 0.00100, Time: 0.38\n",
            "It: 230, Loss: 2.045e-02, learning rate: 0.00100, Time: 0.45\n",
            "It: 240, Loss: 1.866e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 250, Loss: 1.733e-02, learning rate: 0.00100, Time: 0.38\n",
            "It: 260, Loss: 1.677e-02, learning rate: 0.00100, Time: 0.37\n",
            "It: 270, Loss: 1.580e-02, learning rate: 0.00100, Time: 0.50\n",
            "It: 280, Loss: 1.508e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 290, Loss: 1.458e-02, learning rate: 0.00100, Time: 0.47\n",
            "It: 300, Loss: 1.406e-02, learning rate: 0.00100, Time: 0.49\n",
            "It: 310, Loss: 1.411e-02, learning rate: 0.00100, Time: 0.51\n",
            "It: 320, Loss: 1.322e-02, learning rate: 0.00100, Time: 0.42\n",
            "It: 330, Loss: 1.257e-02, learning rate: 0.00100, Time: 0.54\n",
            "It: 340, Loss: 1.214e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 350, Loss: 1.138e-02, learning rate: 0.00100, Time: 0.43\n",
            "It: 360, Loss: 1.092e-02, learning rate: 0.00100, Time: 0.44\n",
            "It: 370, Loss: 1.037e-02, learning rate: 0.00100, Time: 0.34\n",
            "It: 380, Loss: 9.746e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 390, Loss: 9.490e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 400, Loss: 8.786e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 410, Loss: 8.891e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 420, Loss: 8.238e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 430, Loss: 8.187e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 440, Loss: 8.003e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 450, Loss: 7.062e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 460, Loss: 6.556e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 470, Loss: 6.362e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 480, Loss: 6.141e-03, learning rate: 0.00100, Time: 0.94\n",
            "It: 490, Loss: 5.984e-03, learning rate: 0.00100, Time: 0.85\n",
            "It: 500, Loss: 6.150e-03, learning rate: 0.00100, Time: 0.81\n",
            "It: 510, Loss: 6.295e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 520, Loss: 5.896e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 530, Loss: 5.738e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 540, Loss: 5.566e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 550, Loss: 5.645e-03, learning rate: 0.00100, Time: 0.53\n",
            "It: 560, Loss: 5.441e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 570, Loss: 5.373e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 580, Loss: 5.166e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 590, Loss: 5.050e-03, learning rate: 0.00100, Time: 0.41\n",
            "It: 600, Loss: 5.529e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 610, Loss: 4.937e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 620, Loss: 4.691e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 630, Loss: 4.534e-03, learning rate: 0.00100, Time: 0.60\n",
            "It: 640, Loss: 4.341e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 650, Loss: 4.234e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 660, Loss: 4.641e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 670, Loss: 4.062e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 680, Loss: 3.870e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 690, Loss: 3.656e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 700, Loss: 3.477e-03, learning rate: 0.00100, Time: 0.59\n",
            "It: 710, Loss: 3.359e-03, learning rate: 0.00100, Time: 0.78\n",
            "It: 720, Loss: 3.311e-03, learning rate: 0.00100, Time: 0.83\n",
            "It: 730, Loss: 3.596e-03, learning rate: 0.00100, Time: 0.79\n",
            "It: 740, Loss: 3.007e-03, learning rate: 0.00100, Time: 0.81\n",
            "It: 750, Loss: 2.868e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 760, Loss: 2.787e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 770, Loss: 2.681e-03, learning rate: 0.00100, Time: 0.37\n",
            "It: 780, Loss: 2.595e-03, learning rate: 0.00100, Time: 0.34\n",
            "It: 790, Loss: 2.513e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 800, Loss: 2.438e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 810, Loss: 3.119e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 820, Loss: 2.980e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 830, Loss: 2.872e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 840, Loss: 2.944e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 850, Loss: 2.751e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 860, Loss: 3.057e-03, learning rate: 0.00090, Time: 0.60\n",
            "It: 870, Loss: 2.799e-03, learning rate: 0.00090, Time: 0.59\n",
            "It: 880, Loss: 2.600e-03, learning rate: 0.00090, Time: 0.54\n",
            "It: 890, Loss: 2.497e-03, learning rate: 0.00081, Time: 0.53\n",
            "It: 900, Loss: 2.436e-03, learning rate: 0.00081, Time: 0.56\n",
            "It: 910, Loss: 1.992e-03, learning rate: 0.00081, Time: 0.51\n",
            "It: 920, Loss: 1.927e-03, learning rate: 0.00081, Time: 0.54\n",
            "It: 930, Loss: 1.877e-03, learning rate: 0.00081, Time: 0.53\n",
            "It: 940, Loss: 1.844e-03, learning rate: 0.00081, Time: 0.61\n",
            "It: 950, Loss: 1.814e-03, learning rate: 0.00081, Time: 0.60\n",
            "It: 960, Loss: 1.788e-03, learning rate: 0.00081, Time: 0.66\n",
            "It: 970, Loss: 1.765e-03, learning rate: 0.00081, Time: 0.83\n",
            "It: 980, Loss: 1.744e-03, learning rate: 0.00081, Time: 0.82\n",
            "It: 990, Loss: 1.724e-03, learning rate: 0.00081, Time: 0.72\n",
            "Training time: 52.3603\n",
            "[2, 128, 128, 128, 128, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 3.406e-01, learning rate: 0.00100, Time: 1.53\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 2.367e-01, learning rate: 0.00100, Time: 0.59\n",
            "It: 20, Loss: 1.830e-01, learning rate: 0.00100, Time: 0.64\n",
            "It: 30, Loss: 1.728e-01, learning rate: 0.00100, Time: 0.65\n",
            "It: 40, Loss: 1.731e-01, learning rate: 0.00100, Time: 0.54\n",
            "It: 50, Loss: 1.717e-01, learning rate: 0.00100, Time: 0.67\n",
            "It: 60, Loss: 1.716e-01, learning rate: 0.00100, Time: 0.60\n",
            "It: 70, Loss: 1.715e-01, learning rate: 0.00100, Time: 0.42\n",
            "It: 80, Loss: 1.713e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 90, Loss: 1.713e-01, learning rate: 0.00100, Time: 0.41\n",
            "It: 100, Loss: 1.712e-01, learning rate: 0.00100, Time: 0.45\n",
            "It: 110, Loss: 1.477e-01, learning rate: 0.00100, Time: 0.43\n",
            "It: 120, Loss: 1.474e-01, learning rate: 0.00100, Time: 0.37\n",
            "It: 130, Loss: 1.471e-01, learning rate: 0.00100, Time: 0.33\n",
            "It: 140, Loss: 1.470e-01, learning rate: 0.00100, Time: 0.42\n",
            "It: 150, Loss: 1.468e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 160, Loss: 1.466e-01, learning rate: 0.00100, Time: 0.49\n",
            "It: 170, Loss: 1.463e-01, learning rate: 0.00100, Time: 0.43\n",
            "It: 180, Loss: 1.457e-01, learning rate: 0.00100, Time: 0.43\n",
            "It: 190, Loss: 1.446e-01, learning rate: 0.00100, Time: 0.45\n",
            "It: 200, Loss: 1.419e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 210, Loss: 1.383e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 220, Loss: 1.173e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 230, Loss: 7.152e-02, learning rate: 0.00100, Time: 0.32\n",
            "It: 240, Loss: 4.435e-02, learning rate: 0.00100, Time: 0.34\n",
            "It: 250, Loss: 4.047e-02, learning rate: 0.00100, Time: 0.32\n",
            "It: 260, Loss: 3.725e-02, learning rate: 0.00100, Time: 0.34\n",
            "It: 270, Loss: 3.221e-02, learning rate: 0.00100, Time: 0.32\n",
            "It: 280, Loss: 2.831e-02, learning rate: 0.00100, Time: 0.40\n",
            "It: 290, Loss: 2.503e-02, learning rate: 0.00100, Time: 0.50\n",
            "It: 300, Loss: 2.187e-02, learning rate: 0.00100, Time: 0.47\n",
            "It: 310, Loss: 1.778e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 320, Loss: 1.616e-02, learning rate: 0.00100, Time: 0.75\n",
            "It: 330, Loss: 1.501e-02, learning rate: 0.00100, Time: 0.65\n",
            "It: 340, Loss: 1.419e-02, learning rate: 0.00100, Time: 0.73\n",
            "It: 350, Loss: 1.357e-02, learning rate: 0.00100, Time: 0.79\n",
            "It: 360, Loss: 1.307e-02, learning rate: 0.00100, Time: 0.68\n",
            "It: 370, Loss: 1.261e-02, learning rate: 0.00100, Time: 0.61\n",
            "It: 380, Loss: 1.217e-02, learning rate: 0.00100, Time: 0.54\n",
            "It: 390, Loss: 1.173e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 400, Loss: 1.146e-02, learning rate: 0.00100, Time: 0.61\n",
            "It: 410, Loss: 1.185e-02, learning rate: 0.00100, Time: 0.51\n",
            "It: 420, Loss: 1.119e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 430, Loss: 1.061e-02, learning rate: 0.00100, Time: 0.45\n",
            "It: 440, Loss: 1.005e-02, learning rate: 0.00100, Time: 0.47\n",
            "It: 450, Loss: 9.521e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 460, Loss: 9.027e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 470, Loss: 9.258e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 480, Loss: 8.417e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 490, Loss: 7.878e-03, learning rate: 0.00100, Time: 0.53\n",
            "It: 500, Loss: 7.467e-03, learning rate: 0.00100, Time: 0.60\n",
            "It: 510, Loss: 7.278e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 520, Loss: 6.967e-03, learning rate: 0.00100, Time: 0.56\n",
            "It: 530, Loss: 6.706e-03, learning rate: 0.00100, Time: 0.67\n",
            "It: 540, Loss: 6.900e-03, learning rate: 0.00100, Time: 0.59\n",
            "It: 550, Loss: 6.465e-03, learning rate: 0.00100, Time: 0.61\n",
            "It: 560, Loss: 6.000e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 570, Loss: 5.804e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 580, Loss: 5.617e-03, learning rate: 0.00100, Time: 0.72\n",
            "It: 590, Loss: 5.498e-03, learning rate: 0.00100, Time: 0.82\n",
            "It: 600, Loss: 5.760e-03, learning rate: 0.00100, Time: 0.66\n",
            "It: 610, Loss: 4.718e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 620, Loss: 4.445e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 630, Loss: 4.277e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 640, Loss: 4.176e-03, learning rate: 0.00100, Time: 0.43\n",
            "It: 650, Loss: 4.061e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 660, Loss: 3.962e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 670, Loss: 3.965e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 680, Loss: 4.231e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 690, Loss: 4.135e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 700, Loss: 3.591e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 710, Loss: 4.428e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 720, Loss: 4.139e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 730, Loss: 3.960e-03, learning rate: 0.00100, Time: 0.33\n",
            "It: 740, Loss: 3.771e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 750, Loss: 3.587e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 760, Loss: 3.406e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 770, Loss: 5.537e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 780, Loss: 3.466e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 790, Loss: 3.067e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 800, Loss: 2.846e-03, learning rate: 0.00100, Time: 0.32\n",
            "It: 810, Loss: 2.510e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 820, Loss: 2.393e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 830, Loss: 2.263e-03, learning rate: 0.00100, Time: 0.64\n",
            "It: 840, Loss: 2.156e-03, learning rate: 0.00100, Time: 0.68\n",
            "It: 850, Loss: 2.058e-03, learning rate: 0.00100, Time: 0.81\n",
            "It: 860, Loss: 1.971e-03, learning rate: 0.00100, Time: 0.77\n",
            "It: 870, Loss: 1.962e-03, learning rate: 0.00100, Time: 0.83\n",
            "It: 880, Loss: 1.967e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 890, Loss: 2.568e-03, learning rate: 0.00100, Time: 0.53\n",
            "It: 900, Loss: 2.018e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 910, Loss: 1.801e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 920, Loss: 1.739e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 930, Loss: 1.705e-03, learning rate: 0.00100, Time: 0.63\n",
            "It: 940, Loss: 1.656e-03, learning rate: 0.00100, Time: 0.63\n",
            "It: 950, Loss: 1.626e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 960, Loss: 1.598e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 970, Loss: 1.575e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 980, Loss: 1.555e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 990, Loss: 1.535e-03, learning rate: 0.00100, Time: 0.49\n",
            "Training time: 52.7907\n",
            "[2, 128, 128, 64, 64, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.444e-01, learning rate: 0.00100, Time: 1.54\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.570e-01, learning rate: 0.00100, Time: 0.42\n",
            "It: 20, Loss: 1.485e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 30, Loss: 1.420e-01, learning rate: 0.00100, Time: 0.29\n",
            "It: 40, Loss: 1.425e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 50, Loss: 1.412e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 60, Loss: 1.402e-01, learning rate: 0.00100, Time: 0.30\n",
            "It: 70, Loss: 1.389e-01, learning rate: 0.00100, Time: 0.41\n",
            "It: 80, Loss: 1.359e-01, learning rate: 0.00100, Time: 0.45\n",
            "It: 90, Loss: 1.249e-01, learning rate: 0.00100, Time: 0.36\n",
            "It: 100, Loss: 9.362e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 110, Loss: 4.481e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 120, Loss: 3.515e-02, learning rate: 0.00100, Time: 0.56\n",
            "It: 130, Loss: 2.948e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 140, Loss: 2.498e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 150, Loss: 2.223e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 160, Loss: 1.989e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 170, Loss: 1.804e-02, learning rate: 0.00100, Time: 0.45\n",
            "It: 180, Loss: 1.663e-02, learning rate: 0.00100, Time: 0.56\n",
            "It: 190, Loss: 1.546e-02, learning rate: 0.00100, Time: 0.54\n",
            "It: 200, Loss: 1.448e-02, learning rate: 0.00100, Time: 0.52\n",
            "It: 210, Loss: 1.578e-02, learning rate: 0.00100, Time: 0.53\n",
            "It: 220, Loss: 1.449e-02, learning rate: 0.00100, Time: 0.57\n",
            "It: 230, Loss: 1.320e-02, learning rate: 0.00100, Time: 0.51\n",
            "It: 240, Loss: 1.209e-02, learning rate: 0.00100, Time: 0.61\n",
            "It: 250, Loss: 1.062e-02, learning rate: 0.00100, Time: 0.69\n",
            "It: 260, Loss: 9.423e-03, learning rate: 0.00100, Time: 0.63\n",
            "It: 270, Loss: 8.424e-03, learning rate: 0.00100, Time: 0.73\n",
            "It: 280, Loss: 7.654e-03, learning rate: 0.00100, Time: 0.83\n",
            "It: 290, Loss: 7.128e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 300, Loss: 6.877e-03, learning rate: 0.00100, Time: 0.60\n",
            "It: 310, Loss: 6.649e-03, learning rate: 0.00100, Time: 0.65\n",
            "It: 320, Loss: 5.960e-03, learning rate: 0.00100, Time: 0.64\n",
            "It: 330, Loss: 5.583e-03, learning rate: 0.00100, Time: 0.75\n",
            "It: 340, Loss: 5.296e-03, learning rate: 0.00100, Time: 0.77\n",
            "It: 350, Loss: 5.038e-03, learning rate: 0.00100, Time: 0.84\n",
            "It: 360, Loss: 5.030e-03, learning rate: 0.00100, Time: 0.62\n",
            "It: 370, Loss: 4.838e-03, learning rate: 0.00100, Time: 0.74\n",
            "It: 380, Loss: 4.438e-03, learning rate: 0.00100, Time: 0.65\n",
            "It: 390, Loss: 4.329e-03, learning rate: 0.00100, Time: 0.86\n",
            "It: 400, Loss: 4.096e-03, learning rate: 0.00100, Time: 0.80\n",
            "It: 410, Loss: 4.046e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 420, Loss: 4.335e-03, learning rate: 0.00100, Time: 0.66\n",
            "It: 430, Loss: 3.866e-03, learning rate: 0.00100, Time: 1.13\n",
            "It: 440, Loss: 3.593e-03, learning rate: 0.00100, Time: 1.06\n",
            "It: 450, Loss: 3.326e-03, learning rate: 0.00100, Time: 0.78\n",
            "It: 460, Loss: 3.427e-03, learning rate: 0.00100, Time: 0.80\n",
            "It: 470, Loss: 3.306e-03, learning rate: 0.00100, Time: 0.74\n",
            "It: 480, Loss: 2.972e-03, learning rate: 0.00100, Time: 0.77\n",
            "It: 490, Loss: 2.946e-03, learning rate: 0.00100, Time: 0.56\n",
            "It: 500, Loss: 2.776e-03, learning rate: 0.00100, Time: 0.61\n",
            "It: 510, Loss: 2.740e-03, learning rate: 0.00100, Time: 0.66\n",
            "It: 520, Loss: 2.637e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 530, Loss: 2.581e-03, learning rate: 0.00100, Time: 0.54\n",
            "It: 540, Loss: 4.232e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 550, Loss: 2.745e-03, learning rate: 0.00100, Time: 0.59\n",
            "It: 560, Loss: 2.538e-03, learning rate: 0.00100, Time: 0.64\n",
            "It: 570, Loss: 2.339e-03, learning rate: 0.00100, Time: 0.60\n",
            "It: 580, Loss: 2.242e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 590, Loss: 2.188e-03, learning rate: 0.00100, Time: 0.38\n",
            "It: 600, Loss: 2.133e-03, learning rate: 0.00100, Time: 0.36\n",
            "It: 610, Loss: 4.782e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 620, Loss: 2.675e-03, learning rate: 0.00100, Time: 0.52\n",
            "It: 630, Loss: 1.710e-03, learning rate: 0.00100, Time: 0.71\n",
            "It: 640, Loss: 1.763e-03, learning rate: 0.00100, Time: 0.87\n",
            "It: 650, Loss: 1.616e-03, learning rate: 0.00100, Time: 0.91\n",
            "It: 660, Loss: 1.595e-03, learning rate: 0.00100, Time: 0.82\n",
            "It: 670, Loss: 1.559e-03, learning rate: 0.00100, Time: 0.68\n",
            "It: 680, Loss: 1.524e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 690, Loss: 1.494e-03, learning rate: 0.00100, Time: 0.53\n",
            "It: 700, Loss: 1.465e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 710, Loss: 1.914e-03, learning rate: 0.00100, Time: 0.64\n",
            "It: 720, Loss: 1.868e-03, learning rate: 0.00100, Time: 0.57\n",
            "It: 730, Loss: 1.832e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 740, Loss: 1.801e-03, learning rate: 0.00100, Time: 0.58\n",
            "It: 750, Loss: 1.771e-03, learning rate: 0.00100, Time: 0.55\n",
            "It: 760, Loss: 1.748e-03, learning rate: 0.00090, Time: 0.60\n",
            "It: 770, Loss: 1.719e-03, learning rate: 0.00090, Time: 0.56\n",
            "It: 780, Loss: 1.696e-03, learning rate: 0.00090, Time: 0.58\n",
            "It: 790, Loss: 1.673e-03, learning rate: 0.00081, Time: 0.55\n",
            "It: 800, Loss: 1.653e-03, learning rate: 0.00081, Time: 0.63\n",
            "It: 810, Loss: 1.413e-03, learning rate: 0.00081, Time: 0.55\n",
            "It: 820, Loss: 1.393e-03, learning rate: 0.00081, Time: 0.56\n",
            "It: 830, Loss: 1.376e-03, learning rate: 0.00081, Time: 0.66\n",
            "It: 840, Loss: 1.361e-03, learning rate: 0.00081, Time: 0.84\n",
            "It: 850, Loss: 1.348e-03, learning rate: 0.00081, Time: 1.46\n",
            "It: 860, Loss: 1.337e-03, learning rate: 0.00081, Time: 1.21\n",
            "It: 870, Loss: 1.325e-03, learning rate: 0.00081, Time: 0.35\n",
            "It: 880, Loss: 1.314e-03, learning rate: 0.00081, Time: 0.30\n",
            "It: 890, Loss: 1.304e-03, learning rate: 0.00081, Time: 0.29\n",
            "It: 900, Loss: 1.294e-03, learning rate: 0.00081, Time: 0.30\n",
            "It: 910, Loss: 1.313e-03, learning rate: 0.00081, Time: 0.30\n",
            "It: 920, Loss: 1.304e-03, learning rate: 0.00081, Time: 0.29\n",
            "It: 930, Loss: 1.777e-03, learning rate: 0.00081, Time: 0.28\n",
            "It: 940, Loss: 1.421e-03, learning rate: 0.00081, Time: 0.31\n",
            "It: 950, Loss: 1.377e-03, learning rate: 0.00081, Time: 0.30\n",
            "It: 960, Loss: 1.245e-03, learning rate: 0.00073, Time: 0.28\n",
            "It: 970, Loss: 1.220e-03, learning rate: 0.00073, Time: 0.30\n",
            "It: 980, Loss: 1.212e-03, learning rate: 0.00073, Time: 0.31\n",
            "It: 990, Loss: 1.201e-03, learning rate: 0.00073, Time: 0.29\n",
            "Training time: 59.1068\n",
            "[2, 256, 128, 64, 32, 1]\n",
            "Device mapping: no known devices.\n",
            "It: 0, Loss: 2.814e-01, learning rate: 0.00100, Time: 2.77\n",
            "Gradients have not been stored ...\n",
            "It: 10, Loss: 1.785e-01, learning rate: 0.00100, Time: 0.53\n",
            "It: 20, Loss: 1.508e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 30, Loss: 1.523e-01, learning rate: 0.00100, Time: 0.48\n",
            "It: 40, Loss: 1.498e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 50, Loss: 1.499e-01, learning rate: 0.00100, Time: 0.45\n",
            "It: 60, Loss: 1.494e-01, learning rate: 0.00100, Time: 0.46\n",
            "It: 70, Loss: 1.492e-01, learning rate: 0.00100, Time: 0.50\n",
            "It: 80, Loss: 1.489e-01, learning rate: 0.00100, Time: 0.44\n",
            "It: 90, Loss: 1.485e-01, learning rate: 0.00100, Time: 0.51\n",
            "It: 100, Loss: 1.480e-01, learning rate: 0.00100, Time: 0.38\n",
            "It: 110, Loss: 1.592e-01, learning rate: 0.00100, Time: 0.51\n",
            "It: 120, Loss: 1.580e-01, learning rate: 0.00100, Time: 0.47\n",
            "It: 130, Loss: 1.560e-01, learning rate: 0.00100, Time: 0.33\n",
            "It: 140, Loss: 1.522e-01, learning rate: 0.00100, Time: 0.28\n",
            "It: 150, Loss: 1.423e-01, learning rate: 0.00100, Time: 0.26\n",
            "It: 160, Loss: 1.116e-01, learning rate: 0.00100, Time: 0.27\n",
            "It: 170, Loss: 5.500e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 180, Loss: 5.361e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 190, Loss: 4.240e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 200, Loss: 3.781e-02, learning rate: 0.00100, Time: 0.31\n",
            "It: 210, Loss: 3.100e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 220, Loss: 2.774e-02, learning rate: 0.00100, Time: 0.29\n",
            "It: 230, Loss: 2.436e-02, learning rate: 0.00100, Time: 0.30\n",
            "It: 240, Loss: 2.174e-02, learning rate: 0.00100, Time: 0.41\n",
            "It: 250, Loss: 1.923e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 260, Loss: 1.724e-02, learning rate: 0.00100, Time: 0.49\n",
            "It: 270, Loss: 1.563e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 280, Loss: 1.434e-02, learning rate: 0.00100, Time: 0.49\n",
            "It: 290, Loss: 1.375e-02, learning rate: 0.00100, Time: 0.48\n",
            "It: 300, Loss: 1.289e-02, learning rate: 0.00100, Time: 0.44\n",
            "It: 310, Loss: 1.015e-02, learning rate: 0.00100, Time: 0.28\n",
            "It: 320, Loss: 9.594e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 330, Loss: 9.052e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 340, Loss: 8.531e-03, learning rate: 0.00100, Time: 0.33\n",
            "It: 350, Loss: 8.019e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 360, Loss: 7.688e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 370, Loss: 7.604e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 380, Loss: 7.042e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 390, Loss: 6.658e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 400, Loss: 6.377e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 410, Loss: 7.166e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 420, Loss: 6.718e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 430, Loss: 6.864e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 440, Loss: 6.771e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 450, Loss: 6.019e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 460, Loss: 5.607e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 470, Loss: 5.449e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 480, Loss: 5.240e-03, learning rate: 0.00100, Time: 0.46\n",
            "It: 490, Loss: 5.034e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 500, Loss: 5.131e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 510, Loss: 6.135e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 520, Loss: 4.905e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 530, Loss: 4.531e-03, learning rate: 0.00100, Time: 0.43\n",
            "It: 540, Loss: 4.392e-03, learning rate: 0.00100, Time: 0.50\n",
            "It: 550, Loss: 4.203e-03, learning rate: 0.00100, Time: 0.40\n",
            "It: 560, Loss: 4.018e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 570, Loss: 3.853e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 580, Loss: 3.684e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 590, Loss: 3.515e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 600, Loss: 3.370e-03, learning rate: 0.00100, Time: 0.42\n",
            "It: 610, Loss: 4.947e-03, learning rate: 0.00100, Time: 0.44\n",
            "It: 620, Loss: 3.386e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 630, Loss: 3.121e-03, learning rate: 0.00100, Time: 0.49\n",
            "It: 640, Loss: 2.892e-03, learning rate: 0.00100, Time: 0.48\n",
            "It: 650, Loss: 2.705e-03, learning rate: 0.00100, Time: 0.47\n",
            "It: 660, Loss: 2.586e-03, learning rate: 0.00100, Time: 0.45\n",
            "It: 670, Loss: 2.474e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 680, Loss: 2.375e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 690, Loss: 2.287e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 700, Loss: 2.208e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 710, Loss: 2.140e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 720, Loss: 2.046e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 730, Loss: 2.035e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 740, Loss: 3.390e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 750, Loss: 1.953e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 760, Loss: 2.194e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 770, Loss: 1.843e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 780, Loss: 1.815e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 790, Loss: 1.769e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 800, Loss: 1.723e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 810, Loss: 1.553e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 820, Loss: 1.515e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 830, Loss: 1.481e-03, learning rate: 0.00100, Time: 0.31\n",
            "It: 840, Loss: 1.453e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 850, Loss: 1.428e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 860, Loss: 1.404e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 870, Loss: 1.382e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 880, Loss: 1.360e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 890, Loss: 1.348e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 900, Loss: 3.929e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 910, Loss: 2.028e-03, learning rate: 0.00100, Time: 0.28\n",
            "It: 920, Loss: 1.741e-03, learning rate: 0.00100, Time: 0.30\n",
            "It: 930, Loss: 1.659e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 940, Loss: 1.509e-03, learning rate: 0.00100, Time: 0.29\n",
            "It: 950, Loss: 1.449e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 960, Loss: 1.431e-03, learning rate: 0.00090, Time: 0.30\n",
            "It: 970, Loss: 1.413e-03, learning rate: 0.00090, Time: 0.29\n",
            "It: 980, Loss: 1.395e-03, learning rate: 0.00081, Time: 0.30\n",
            "It: 990, Loss: 1.379e-03, learning rate: 0.00081, Time: 0.31\n",
            "Training time: 37.8227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "u_predict = np.array(u_predict).reshape(np.insert(exact_u.shape,0,len(u_predict)))\n",
        "\n",
        "for idx in range(u_predict.shape[0]):\n",
        "  error = np.linalg.norm(exact_u.flatten()[:,None]-u_predict[idx].flatten()[:,None],2)/np.linalg.norm(exact_u.flatten()[:,None],2)\n",
        "  print('Error on u ' + str(DifferentLayers[idx]) + ' : %e' % (error) )\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "ZJdsXLte1SOH",
        "outputId": "a3b63113-9423-41a9-dea4-5b83ca85b7e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error on u [2, 64, 64, 64, 1] : 1.355966e+00\n",
            "\n",
            "\n",
            "Error on u [2, 128, 128, 128, 1] : 1.349851e+00\n",
            "\n",
            "\n",
            "Error on u [2, 256, 256, 256, 1] : 1.345953e+00\n",
            "\n",
            "\n",
            "Error on u [2, 64, 64, 64, 64, 1] : 1.356281e+00\n",
            "\n",
            "\n",
            "Error on u [2, 256, 256, 256, 256, 1] : 1.345336e+00\n",
            "\n",
            "\n",
            "Error on u [2, 128, 128, 128, 128, 1] : 1.353863e+00\n",
            "\n",
            "\n",
            "Error on u [2, 128, 128, 64, 64, 1] : 1.354152e+00\n",
            "\n",
            "\n",
            "Error on u [2, 256, 128, 64, 32, 1] : 1.351036e+00\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g', 'y', 'c', 'b', 'm', 'r', 'k']\n",
        "line = ['-', '--', '-.']\n",
        "line_idx = 0\n",
        "\n",
        "T = [0,24,49,74,99]\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "for idx_model in [5,6]:\n",
        "  color_idx = 0\n",
        "  for idx_time in T:\n",
        "    plt.plot(x, u_predict[idx_model][idx_time].flatten(), color[color_idx%5], linestyle = line[line_idx], label = str(DifferentLayers[idx_model]) + \"at t= \" + str(t[idx_time]))\n",
        "    color_idx +=1\n",
        "  line_idx +=1\n",
        "\n",
        "for idx_time in T:\n",
        "  plt.plot(x, exact_u[idx_time], color[idx_time], linestyle = ':', label = 'u expected at t= ' + str(t[idx_time]))\n",
        "plt.ylabel(\"Cg (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Eiv252Fg2XAp",
        "outputId": "aeb729fa-8f33-4b0d-98e1-9852b65c57ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-34b5e764acd4>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcolor_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx_time\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolor_idx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDifferentLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"at t= \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcolor_idx\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mline_idx\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (256, 1) and (100,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "RH8kRsxs28CL",
        "outputId": "8d9d4d0b-06d9-4553-ba2a-c44e4e3a1622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.  ],\n",
              "       [0.01],\n",
              "       [0.02],\n",
              "       [0.03],\n",
              "       [0.04],\n",
              "       [0.05],\n",
              "       [0.06],\n",
              "       [0.07],\n",
              "       [0.08],\n",
              "       [0.09],\n",
              "       [0.1 ],\n",
              "       [0.11],\n",
              "       [0.12],\n",
              "       [0.13],\n",
              "       [0.14],\n",
              "       [0.15],\n",
              "       [0.16],\n",
              "       [0.17],\n",
              "       [0.18],\n",
              "       [0.19],\n",
              "       [0.2 ],\n",
              "       [0.21],\n",
              "       [0.22],\n",
              "       [0.23],\n",
              "       [0.24],\n",
              "       [0.25],\n",
              "       [0.26],\n",
              "       [0.27],\n",
              "       [0.28],\n",
              "       [0.29],\n",
              "       [0.3 ],\n",
              "       [0.31],\n",
              "       [0.32],\n",
              "       [0.33],\n",
              "       [0.34],\n",
              "       [0.35],\n",
              "       [0.36],\n",
              "       [0.37],\n",
              "       [0.38],\n",
              "       [0.39],\n",
              "       [0.4 ],\n",
              "       [0.41],\n",
              "       [0.42],\n",
              "       [0.43],\n",
              "       [0.44],\n",
              "       [0.45],\n",
              "       [0.46],\n",
              "       [0.47],\n",
              "       [0.48],\n",
              "       [0.49],\n",
              "       [0.5 ],\n",
              "       [0.51],\n",
              "       [0.52],\n",
              "       [0.53],\n",
              "       [0.54],\n",
              "       [0.55],\n",
              "       [0.56],\n",
              "       [0.57],\n",
              "       [0.58],\n",
              "       [0.59],\n",
              "       [0.6 ],\n",
              "       [0.61],\n",
              "       [0.62],\n",
              "       [0.63],\n",
              "       [0.64],\n",
              "       [0.65],\n",
              "       [0.66],\n",
              "       [0.67],\n",
              "       [0.68],\n",
              "       [0.69],\n",
              "       [0.7 ],\n",
              "       [0.71],\n",
              "       [0.72],\n",
              "       [0.73],\n",
              "       [0.74],\n",
              "       [0.75],\n",
              "       [0.76],\n",
              "       [0.77],\n",
              "       [0.78],\n",
              "       [0.79],\n",
              "       [0.8 ],\n",
              "       [0.81],\n",
              "       [0.82],\n",
              "       [0.83],\n",
              "       [0.84],\n",
              "       [0.85],\n",
              "       [0.86],\n",
              "       [0.87],\n",
              "       [0.88],\n",
              "       [0.89],\n",
              "       [0.9 ],\n",
              "       [0.91],\n",
              "       [0.92],\n",
              "       [0.93],\n",
              "       [0.94],\n",
              "       [0.95],\n",
              "       [0.96],\n",
              "       [0.97],\n",
              "       [0.98],\n",
              "       [0.99]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}