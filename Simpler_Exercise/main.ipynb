{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wHGOJP72k-4L",
        "XJw6Hlcm05ke",
        "JK0dl1zYlMZd",
        "XjeN35wnlV6e",
        "y3aj-N5xzrHd",
        "27BsVukSmGe5",
        "fKbLQQK6uzXG",
        "TZ2Roujtljaf",
        "NNnDSrA2loQX",
        "5xnbiov17XUB",
        "0e92_S5jp333",
        "hzoWWeL3u70i"
      ],
      "authorship_tag": "ABX9TyOpX+KAGDUvuesqWJu1qaxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giraud-Pierre/PINN_for_SEDMES/blob/adsorption_exercise/Simpler_Exercise/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and libraries\n"
      ],
      "metadata": {
        "id": "wHGOJP72k-4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if runing on colab, use this to get the data\n",
        "!git clone -b adsorption_exercise https://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@github.com/Giraud-Pierre/PINN_for_SEDMES.git\n",
        "%cd PINN_for_SEDMES/main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sykRD8SEVssI",
        "outputId": "763b427c-9c08-435a-b305-8dfcd3ebfdbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PINN_for_SEDMES'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 179 (delta 71), reused 134 (delta 54), pack-reused 4\u001b[K\n",
            "Receiving objects: 100% (179/179), 18.27 MiB | 17.74 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/PINN_for_SEDMES/main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "import time\n",
        "import scipy.io\n",
        "from scipy.stats import qmc\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "metadata": {
        "id": "krf8WFiX_m3e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "uEzUvd08_rhv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reaction exercise"
      ],
      "metadata": {
        "id": "XJw6Hlcm05ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part aims to use a PINN to solve a simple reaction exercise. A liquid, containing a molecule with a concentration Cin = 1 mol/L, is introduced into a perfectly stirred reactor. This molecule reacts with a linear reaction speed of r = -Ke C with Ke = 1. At the end of the reactor, the concentration C of the molecule follow a PDE: Q*Cin + r*V = Q*C + V*dC/dt where V is the volume of the reactor.\n",
        "\n",
        "If tau = V/Q, then dC/dt = (Cin - C*(1+Ke*tau))/tau which, given that C = C0 = 0 at t=0, should give a solution of C = (Cin/(1+Ke*tau)) * (1 - exp(-(1+Ke*tau)*t)).\n",
        "\n",
        "Let us try to find this function with a PINN for tau = 1\n",
        "--> dC/dt = Cin - 2*C => C = (1/2) * (1 + exp(-2*t))."
      ],
      "metadata": {
        "id": "kowpPo_QOZOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# architecture of the feedforward network with 1 input being time\n",
        "#and 1 outputs being C\n",
        "layers = [1, 128, 128, 128, 128, 1]\n",
        "\n",
        "#number of collocation points for the ODE\n",
        "Nf = 50\n",
        "\n",
        "#upper and lower boundary of the experiment (here time)\n",
        "lb = np.array([0])\n",
        "ub = np.array([10])"
      ],
      "metadata": {
        "id": "fTXuBVgJ_mUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get data from matlab workspace\n",
        "data = scipy.io.loadmat(\"../data/ODE_reaction_data.mat\") #load the simulation data from matlab\n",
        "\n",
        "t = data['t'].flatten()[:,None] # time from simulation\n",
        "exact_C = data['C'] #C from simulation, function of x"
      ],
      "metadata": {
        "id": "s4aiAjAfJ1jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PINN models"
      ],
      "metadata": {
        "id": "TY8Vf2-blGpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original PINN with ODE"
      ],
      "metadata": {
        "id": "JK0dl1zYlMZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_ODE:\n",
        "  '''PINN model tailored to answer this exercise using the ODE (with differenciatino)'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''Initialize the self.adaptative coefficient, which will be used to make sure\n",
        "    the initial conditions and the boundaries are enforced properly.'''\n",
        "    self.beta = 0.9\n",
        "    self.lambda_0_value = np.array(5.0)\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    self.encoder_weights_1 = self.Xavier_init(1,layers[1])\n",
        "    self.encoder_weights_2 = self.Xavier_init(1,layers[1])\n",
        "    self.encoder_biases_1 = self.Xavier_init(1,layers[1])\n",
        "    self.encoder_biases_2 = self.Xavier_init(1,layers[1])\n",
        "\n",
        "    '''creating tensorflow placeholder (one for each array)'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "\n",
        "    #placeholder for the self-adaptative loss coefficients\n",
        "    self.lambda_0_tf = tf.compat.v1.placeholder(tf.float32, shape = self.lambda_0_value.shape)\n",
        "\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Creating the loss graph by adding the different losses with respect to\n",
        "    the initial conditions and the residues of the ODE. The loss on the\n",
        "    intial conditions is given a self adaptative coefficient lambda to make\n",
        "    sure it get enforced properly.'''\n",
        "    #Initial condition loss\n",
        "    self.loss_0 = self.lambda_0_tf * tf.reduce_mean(input_tensor=tf.square(self.C0_pred - self.C0))\n",
        "\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self.loss_0 + self. loss_r\n",
        "\n",
        "    '''Computing the self adaptative loss coefficient'''\n",
        "    #graph to get the gradients for each losses\n",
        "    self.grad_0, self.grad_r = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.grad_0.append(tf.gradients(self.loss_0, self.weights[weights_idx])[0])\n",
        "      self.grad_r.append(tf.gradients(self.loss_r, self.weights[weights_idx])[0])\n",
        "\n",
        "    #Getting the mean of these gradients for initial condition, and the max for residues\n",
        "    self.mean_grad_0_list, self.max_grad_r_list = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.mean_grad_0_list.append(tf.reduce_mean(tf.abs(self.grad_0[weights_idx])))\n",
        "      self.max_grad_r_list.append(tf.reduce_max(tf.abs(self.grad_r[weights_idx])))\n",
        "\n",
        "    self.mean_grad_0 = tf.reduce_mean(tf.stack(self.mean_grad_0_list))\n",
        "    self.max_grad_r = tf.reduce_max(tf.stack(self.max_grad_r_list))\n",
        "\n",
        "    #computing the loss coefficients\n",
        "    self.lambda_0_graph = self.max_grad_r / self.mean_grad_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    #log for the adaptative coefficients\n",
        "    self.lambda_0_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adapatative learning rate\n",
        "    self.global_step = tf.Variable(0, trainable = False)\n",
        "    starter_learning_rate = 0.001\n",
        "    self.learning_rate = tf.compat.v1.train.exponential_decay(\n",
        "                                starter_learning_rate,\n",
        "                                self.global_step,\n",
        "                                decay_steps = 50,\n",
        "                                decay_rate = 0.8,\n",
        "                                staircase = False)\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(\n",
        "                                self.loss, global_step = self.global_step)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    #Special encoders for the ResNET in PINN\n",
        "    encoder_1 = tf.tanh(tf.add(tf.matmul(H,self.encoder_weights_1), self.encoder_biases_1))\n",
        "    encoder_2 = tf.tanh(tf.add(tf.matmul(H,self.encoder_weights_2), self.encoder_biases_2))\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "      #Applying the encoders to get better results and avoid vanishing and exploding gradients\n",
        "      H = tf.math.multiply(H, encoder_1) + tf.multiply((1-H),encoder_2)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "\n",
        "    C_t = tf.gradients(ys=C, xs=t)[0]\n",
        "\n",
        "    #Residue\n",
        "    r = self.tau * C_t - self.Cin + C * (1 + self.Ke * self.tau)\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "      # Generate random collocation points for this epoch\n",
        "      self.Initialize_random_collocation_points()\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.lambda_0_tf: self.lambda_0_value}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        #record the losses\n",
        "        loss_0, loss_r = self.sess.run([self.loss_0, self.loss_r], tf_dict)\n",
        "        self.loss_0_log.append(loss_0 / self.lambda_0_value)\n",
        "        self.loss_r_log.append(loss_r)\n",
        "\n",
        "        loss_value = self.sess.run(self.loss, tf_dict)\n",
        "        self.loss_log.append(loss_value)\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e,lambda_0: %.2f,Time: %.2f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, self.lambda_0_value, elapsed))\n",
        "        start_time = time.time()\n",
        "\n",
        "                #adapt the loss weights\n",
        "        lambda_0_temp = self.sess.run(self.lambda_0_graph,tf_dict)\n",
        "        self.lambda_0_value = min(10.0, lambda_0_temp *(1 - self.beta) + self.beta * self.lambda_0_value)\n",
        "        self.lambda_0_log.append(self.lambda_0_value)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "8gAuPyGdStKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN with no differenciation"
      ],
      "metadata": {
        "id": "XjeN35wnlV6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_NoDiff:\n",
        "  '''PINN model tailored to answer this exercise using the solution (without differenciation)'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting the learning rate\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 10\n",
        "    self.reduce_LR_cooldown = 10\n",
        "    self.count_cooldown = 200\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    #self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "\n",
        "    #Residue\n",
        "    r = C - (self.Cin/(1 + self.Ke * self.tau)) * (1 - tf.math.exp(-t*((1/self.tau)+self.Ke)))\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      #record the loss\n",
        "      loss_value = self.sess.run(self.loss, tf_dict)\n",
        "      self.loss_log.append(loss_value)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        # Generate random collocation points for the next 10 epochs\n",
        "        #self.Initialize_random_collocation_points()\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "E6dkiJEeOz5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN with differenciation and solution of the differenciation"
      ],
      "metadata": {
        "id": "y3aj-N5xzrHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_DiffSol:\n",
        "  '''PINN model tailored to answer this exercise (comparing differenciation and its direct solution)'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Loss on initial condition\n",
        "    self.loss_0 = tf.reduce_mean(input_tensor=tf.square(self.C0_pred - self.C0))\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r + self.loss_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 10\n",
        "    self.reduce_LR_cooldown = 10\n",
        "    self.count_cooldown = 200\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    #self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "    C_t = tf.gradients(ys=C, xs=t)[0]\n",
        "    #Residue\n",
        "    r = C_t- tf.math.exp(-2*t)\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        # Generate random collocation points for the next 10 epochs\n",
        "        #self.Initialize_random_collocation_points()\n",
        "\n",
        "        #record the loss\n",
        "        loss_value, loss_0, loss_r = self.sess.run([self.loss, self.loss_0, self.loss_r], tf_dict)\n",
        "        self.loss_log.append(loss_value)\n",
        "        self.loss_0_log.append(loss_0)\n",
        "        self.loss_r_log.append(loss_r)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "E1ukkiGcz-bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_DiffSol2:\n",
        "  '''PINN model tailored to answer this exercise using the solution (without differenciatino)'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Loss on initial condition\n",
        "    self.loss_0 = tf.reduce_mean(input_tensor=tf.square(self.C0_pred-self.C0))\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r + self.loss_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 10\n",
        "    self.reduce_LR_cooldown = 10\n",
        "    self.count_cooldown = 200\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    #self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "    C_t = tf.gradients(ys=C, xs=t)[0]\n",
        "    #Residue\n",
        "    r = C_t- (self.Cin/self.tau) * tf.math.exp(-(self.Ke + (1/self.tau))*t)\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        # Generate random collocation points for the next 10 epochs\n",
        "        #self.Initialize_random_collocation_points()\n",
        "\n",
        "        #record the loss\n",
        "        loss_value, loss_0, loss_r = self.sess.run([self.loss, self.loss_0, self.loss_r], tf_dict)\n",
        "        self.loss_log.append(loss_value)\n",
        "        self.loss_0_log.append(loss_0)\n",
        "        self.loss_r_log.append(loss_r)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "v23W-1DAnQue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN with ODE simpler"
      ],
      "metadata": {
        "id": "27BsVukSmGe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_ODESimpler:\n",
        "  '''PINN model tailored to answer this exercise'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Loss on initial condition\n",
        "    self.loss_0 = tf.reduce_mean(input_tensor=tf.square(self.C0_pred - self.C0))\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r + self.loss_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 10\n",
        "    self.reduce_LR_cooldown = 10\n",
        "    self.count_cooldown = 200\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    #self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "    C_t = tf.gradients(ys=C, xs=t)[0]\n",
        "    #Residue\n",
        "    r = C_t + C * (self.Ke + (1/self.tau)) - (self.Cin / self.tau)\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "        # Generate random collocation points for the next 10 epochs\n",
        "        #self.Initialize_random_collocation_points()\n",
        "\n",
        "        #record the loss\n",
        "        loss_value, loss_0, loss_r = self.sess.run([self.loss, self.loss_0, self.loss_r], tf_dict)\n",
        "        self.loss_log.append(loss_value)\n",
        "        self.loss_0_log.append(loss_0)\n",
        "        self.loss_r_log.append(loss_r)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "A3meucSomOYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN with ODE intermediate"
      ],
      "metadata": {
        "id": "fKbLQQK6uzXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReactionPINN_ODEIntermediate:\n",
        "  '''PINN model tailored to answer this exercise'''\n",
        "  def __init__(self, Nf, layers, ub, lb):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.tau = 1 #Volume of the reactor / flow rate inside the ractor (s)\n",
        "    self.C0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 1 #reaction constant (mol/l.s)\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''Initialize the self.adaptative coefficient, which will be used to make sure\n",
        "    the initial conditions and the boundaries are enforced properly.'''\n",
        "    self.beta = 0.9\n",
        "    self.lambda_0_value = np.array(1.0)\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "    #placeholder for the self-adaptative loss coefficients\n",
        "    self.lambda_0_tf = tf.compat.v1.placeholder(tf.float32, shape = self.lambda_0_value.shape)\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.C0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.r_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Loss on initial condition\n",
        "    self.loss_0 = self.lambda_0_tf * tf.reduce_mean(input_tensor=tf.square(self.C0_pred - self.C0))\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.r_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r + self.loss_0\n",
        "\n",
        "    '''Computing the self adaptative loss coefficient'''\n",
        "    #graph to get the gradients for each losses\n",
        "    self.grad_0, self.grad_r = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.grad_0.append(tf.gradients(self.loss_0, self.weights[weights_idx])[0])\n",
        "      self.grad_r.append(tf.gradients(self.loss_r, self.weights[weights_idx])[0])\n",
        "\n",
        "    #Getting the mean of these gradients for initial condition, and the max for residues\n",
        "    self.mean_grad_0_list, self.max_grad_r_list = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.mean_grad_0_list.append(tf.reduce_mean(tf.abs(self.grad_0[weights_idx])))\n",
        "      self.max_grad_r_list.append(tf.reduce_max(tf.abs(self.grad_r[weights_idx])))\n",
        "\n",
        "    self.mean_grad_0 = tf.reduce_mean(tf.stack(self.mean_grad_0_list))\n",
        "    self.max_grad_r = tf.reduce_max(tf.stack(self.max_grad_r_list))\n",
        "\n",
        "    #computing the loss coefficients\n",
        "    self.lambda_0_graph = self.max_grad_r / self.mean_grad_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    #log for the coefficient\n",
        "    self.lambda_0_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 10\n",
        "    self.reduce_LR_cooldown = 10\n",
        "    self.count_cooldown = 200\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    #self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    C = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    return C\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    C = self.Net_initial(t)\n",
        "    C_t = tf.gradients(ys=C, xs=t)[0]\n",
        "    #Residue\n",
        "    r = C_t + C * (self.Ke + (1/self.tau)) - (self.Cin / self.tau)\n",
        "    return r\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate,\n",
        "                 self.lambda_0_tf: self.lambda_0_value}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "\n",
        "        #record the loss\n",
        "        loss_value, loss_0, loss_r = self.sess.run([self.loss, self.loss_0, self.loss_r], tf_dict)\n",
        "        self.loss_log.append(loss_value)\n",
        "        self.loss_0_log.append(loss_0 / self.lambda_0_value)\n",
        "        self.loss_r_log.append(loss_r)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "        #adapt the loss weights\n",
        "        lambda_0_temp = self.sess.run(self.lambda_0_graph,tf_dict)\n",
        "        self.lambda_0_value = min(10.0, lambda_0_temp *(1 - self.beta) + self.beta * self.lambda_0_value)\n",
        "        self.lambda_0_log.append(self.lambda_0_value)\n",
        "\n",
        "        if it % 200 == 0:\n",
        "          # Generate new random collocation points for the next 200 epochs\n",
        "          self.Initialize_random_collocation_points()\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    C = np.array(self.sess.run([self.C0_pred], tf_dict))\n",
        "\n",
        "    return C"
      ],
      "metadata": {
        "id": "XD-M9YOIt93I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the PINN models"
      ],
      "metadata": {
        "id": "XtmuXzDmlfOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the PINN with the ODE"
      ],
      "metadata": {
        "id": "TZ2Roujtljaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PINN model with the differential equation\n",
        "modelDiff = ReactionPINN_ODE(Nf, layers, ub, lb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3iUvTxTd6zZ",
        "outputId": "824b4989-5d9e-4130-e5a7-57f35b72dc78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #Training the model\n",
        "start_time = time.time()\n",
        "modelDiff.Train(1000)\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDDMyNgWBLge",
        "outputId": "1728a94b-ebf3-4b78-c61a-3354eaef4558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It: 0, Loss: 1.821e-03, Loss_0: 9.484e-06, Loss_r: 1.812e-03,lambda_0: 9.37,Time: 3.55\n",
            "It: 10, Loss: 1.892e-03, Loss_0: 8.508e-07, Loss_r: 1.891e-03,lambda_0: 9.23,Time: 0.12\n",
            "It: 20, Loss: 1.429e-03, Loss_0: 3.213e-06, Loss_r: 1.426e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 30, Loss: 1.359e-03, Loss_0: 3.432e-06, Loss_r: 1.356e-03,lambda_0: 9.98,Time: 0.10\n",
            "It: 40, Loss: 2.178e-03, Loss_0: 2.983e-06, Loss_r: 2.175e-03,lambda_0: 9.98,Time: 0.10\n",
            "It: 50, Loss: 1.948e-03, Loss_0: 2.990e-06, Loss_r: 1.945e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 60, Loss: 1.318e-03, Loss_0: 5.030e-06, Loss_r: 1.313e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 70, Loss: 1.367e-03, Loss_0: 5.025e-06, Loss_r: 1.362e-03,lambda_0: 9.58,Time: 0.11\n",
            "It: 80, Loss: 2.143e-03, Loss_0: 4.592e-06, Loss_r: 2.139e-03,lambda_0: 9.27,Time: 0.11\n",
            "It: 90, Loss: 2.236e-03, Loss_0: 5.832e-06, Loss_r: 2.230e-03,lambda_0: 9.92,Time: 0.10\n",
            "It: 100, Loss: 1.145e-03, Loss_0: 1.223e-06, Loss_r: 1.144e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 110, Loss: 1.387e-03, Loss_0: 4.901e-06, Loss_r: 1.382e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 120, Loss: 1.371e-03, Loss_0: 2.781e-06, Loss_r: 1.368e-03,lambda_0: 9.71,Time: 0.10\n",
            "It: 130, Loss: 2.415e-03, Loss_0: 4.407e-06, Loss_r: 2.410e-03,lambda_0: 9.58,Time: 0.10\n",
            "It: 140, Loss: 1.428e-03, Loss_0: 3.688e-06, Loss_r: 1.424e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 150, Loss: 1.571e-03, Loss_0: 1.864e-06, Loss_r: 1.569e-03,lambda_0: 9.85,Time: 0.10\n",
            "It: 160, Loss: 1.820e-03, Loss_0: 5.658e-06, Loss_r: 1.815e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 170, Loss: 2.011e-03, Loss_0: 2.395e-06, Loss_r: 2.009e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 180, Loss: 1.823e-03, Loss_0: 6.121e-06, Loss_r: 1.817e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 190, Loss: 1.879e-03, Loss_0: 2.731e-06, Loss_r: 1.877e-03,lambda_0: 9.79,Time: 0.10\n",
            "It: 200, Loss: 1.225e-03, Loss_0: 3.269e-06, Loss_r: 1.221e-03,lambda_0: 9.97,Time: 0.10\n",
            "It: 210, Loss: 1.810e-03, Loss_0: 5.176e-06, Loss_r: 1.805e-03,lambda_0: 9.79,Time: 0.11\n",
            "It: 220, Loss: 1.678e-03, Loss_0: 4.244e-06, Loss_r: 1.674e-03,lambda_0: 9.71,Time: 0.10\n",
            "It: 230, Loss: 1.249e-03, Loss_0: 4.005e-06, Loss_r: 1.245e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 240, Loss: 1.479e-03, Loss_0: 4.759e-06, Loss_r: 1.474e-03,lambda_0: 9.85,Time: 0.09\n",
            "It: 250, Loss: 1.408e-03, Loss_0: 3.387e-06, Loss_r: 1.404e-03,lambda_0: 9.66,Time: 0.11\n",
            "It: 260, Loss: 1.985e-03, Loss_0: 2.652e-06, Loss_r: 1.983e-03,lambda_0: 9.58,Time: 0.11\n",
            "It: 270, Loss: 1.450e-03, Loss_0: 5.119e-06, Loss_r: 1.445e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 280, Loss: 1.988e-03, Loss_0: 4.172e-06, Loss_r: 1.984e-03,lambda_0: 9.80,Time: 0.10\n",
            "It: 290, Loss: 1.792e-03, Loss_0: 3.597e-06, Loss_r: 1.788e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 300, Loss: 1.738e-03, Loss_0: 2.779e-06, Loss_r: 1.736e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 310, Loss: 2.132e-03, Loss_0: 4.253e-06, Loss_r: 2.128e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 320, Loss: 2.327e-03, Loss_0: 2.957e-06, Loss_r: 2.324e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 330, Loss: 1.996e-03, Loss_0: 3.634e-06, Loss_r: 1.992e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 340, Loss: 1.918e-03, Loss_0: 5.744e-06, Loss_r: 1.913e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 350, Loss: 1.648e-03, Loss_0: 2.066e-06, Loss_r: 1.646e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 360, Loss: 1.122e-03, Loss_0: 3.001e-06, Loss_r: 1.119e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 370, Loss: 1.218e-03, Loss_0: 4.753e-06, Loss_r: 1.213e-03,lambda_0: 9.75,Time: 0.10\n",
            "It: 380, Loss: 1.607e-03, Loss_0: 2.927e-06, Loss_r: 1.604e-03,lambda_0: 9.43,Time: 0.10\n",
            "It: 390, Loss: 1.471e-03, Loss_0: 3.061e-06, Loss_r: 1.468e-03,lambda_0: 9.92,Time: 0.09\n",
            "It: 400, Loss: 2.049e-03, Loss_0: 2.413e-06, Loss_r: 2.046e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 410, Loss: 2.428e-03, Loss_0: 4.173e-06, Loss_r: 2.424e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 420, Loss: 1.231e-03, Loss_0: 5.198e-06, Loss_r: 1.225e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 430, Loss: 1.238e-03, Loss_0: 1.635e-06, Loss_r: 1.236e-03,lambda_0: 9.68,Time: 0.11\n",
            "It: 440, Loss: 2.242e-03, Loss_0: 2.483e-06, Loss_r: 2.239e-03,lambda_0: 9.79,Time: 0.09\n",
            "It: 450, Loss: 1.406e-03, Loss_0: 4.186e-06, Loss_r: 1.401e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 460, Loss: 1.673e-03, Loss_0: 4.306e-06, Loss_r: 1.668e-03,lambda_0: 9.84,Time: 0.13\n",
            "It: 470, Loss: 1.850e-03, Loss_0: 3.507e-06, Loss_r: 1.846e-03,lambda_0: 9.75,Time: 0.11\n",
            "It: 480, Loss: 2.014e-03, Loss_0: 2.786e-06, Loss_r: 2.011e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 490, Loss: 2.221e-03, Loss_0: 2.947e-06, Loss_r: 2.218e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 500, Loss: 1.820e-03, Loss_0: 3.420e-06, Loss_r: 1.816e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 510, Loss: 1.408e-03, Loss_0: 2.562e-06, Loss_r: 1.405e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 520, Loss: 1.274e-03, Loss_0: 2.918e-06, Loss_r: 1.271e-03,lambda_0: 9.99,Time: 0.10\n",
            "It: 530, Loss: 1.347e-03, Loss_0: 4.188e-06, Loss_r: 1.343e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 540, Loss: 1.541e-03, Loss_0: 3.136e-06, Loss_r: 1.538e-03,lambda_0: 9.74,Time: 0.10\n",
            "It: 550, Loss: 2.166e-03, Loss_0: 2.471e-06, Loss_r: 2.164e-03,lambda_0: 9.76,Time: 0.11\n",
            "It: 560, Loss: 1.203e-03, Loss_0: 3.070e-06, Loss_r: 1.200e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 570, Loss: 1.375e-03, Loss_0: 2.930e-06, Loss_r: 1.373e-03,lambda_0: 9.82,Time: 0.10\n",
            "It: 580, Loss: 2.154e-03, Loss_0: 3.208e-06, Loss_r: 2.151e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 590, Loss: 2.238e-03, Loss_0: 2.699e-06, Loss_r: 2.235e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 600, Loss: 1.860e-03, Loss_0: 2.749e-06, Loss_r: 1.857e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 610, Loss: 1.435e-03, Loss_0: 3.225e-06, Loss_r: 1.432e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 620, Loss: 2.244e-03, Loss_0: 3.119e-06, Loss_r: 2.241e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 630, Loss: 1.855e-03, Loss_0: 3.362e-06, Loss_r: 1.852e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 640, Loss: 1.798e-03, Loss_0: 3.532e-06, Loss_r: 1.794e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 650, Loss: 1.748e-03, Loss_0: 3.552e-06, Loss_r: 1.745e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 660, Loss: 1.948e-03, Loss_0: 3.282e-06, Loss_r: 1.945e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 670, Loss: 1.934e-03, Loss_0: 3.083e-06, Loss_r: 1.930e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 680, Loss: 1.361e-03, Loss_0: 3.286e-06, Loss_r: 1.358e-03,lambda_0: 10.00,Time: 0.14\n",
            "It: 690, Loss: 1.037e-03, Loss_0: 2.771e-06, Loss_r: 1.035e-03,lambda_0: 9.94,Time: 0.16\n",
            "It: 700, Loss: 1.306e-03, Loss_0: 2.907e-06, Loss_r: 1.303e-03,lambda_0: 9.80,Time: 0.18\n",
            "It: 710, Loss: 1.502e-03, Loss_0: 3.237e-06, Loss_r: 1.498e-03,lambda_0: 9.75,Time: 0.17\n",
            "It: 720, Loss: 2.174e-03, Loss_0: 3.472e-06, Loss_r: 2.170e-03,lambda_0: 9.68,Time: 0.19\n",
            "It: 730, Loss: 1.790e-03, Loss_0: 3.608e-06, Loss_r: 1.786e-03,lambda_0: 10.00,Time: 0.16\n",
            "It: 740, Loss: 1.234e-03, Loss_0: 3.347e-06, Loss_r: 1.231e-03,lambda_0: 10.00,Time: 0.17\n",
            "It: 750, Loss: 1.920e-03, Loss_0: 4.073e-06, Loss_r: 1.916e-03,lambda_0: 9.96,Time: 0.17\n",
            "It: 760, Loss: 1.426e-03, Loss_0: 4.661e-06, Loss_r: 1.421e-03,lambda_0: 10.00,Time: 0.17\n",
            "It: 770, Loss: 1.463e-03, Loss_0: 3.937e-06, Loss_r: 1.459e-03,lambda_0: 9.82,Time: 0.18\n",
            "It: 780, Loss: 1.482e-03, Loss_0: 3.208e-06, Loss_r: 1.479e-03,lambda_0: 9.82,Time: 0.18\n",
            "It: 790, Loss: 1.991e-03, Loss_0: 2.806e-06, Loss_r: 1.988e-03,lambda_0: 9.95,Time: 0.18\n",
            "It: 800, Loss: 1.372e-03, Loss_0: 2.925e-06, Loss_r: 1.369e-03,lambda_0: 10.00,Time: 0.19\n",
            "It: 810, Loss: 1.412e-03, Loss_0: 2.993e-06, Loss_r: 1.409e-03,lambda_0: 10.00,Time: 0.15\n",
            "It: 820, Loss: 2.275e-03, Loss_0: 3.375e-06, Loss_r: 2.272e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 830, Loss: 1.546e-03, Loss_0: 3.568e-06, Loss_r: 1.542e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 840, Loss: 1.904e-03, Loss_0: 3.568e-06, Loss_r: 1.900e-03,lambda_0: 9.99,Time: 0.10\n",
            "It: 850, Loss: 1.561e-03, Loss_0: 3.612e-06, Loss_r: 1.557e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 860, Loss: 1.778e-03, Loss_0: 3.505e-06, Loss_r: 1.774e-03,lambda_0: 9.88,Time: 0.13\n",
            "It: 870, Loss: 1.461e-03, Loss_0: 3.360e-06, Loss_r: 1.458e-03,lambda_0: 9.94,Time: 0.11\n",
            "It: 880, Loss: 1.642e-03, Loss_0: 3.254e-06, Loss_r: 1.639e-03,lambda_0: 9.92,Time: 0.13\n",
            "It: 890, Loss: 1.719e-03, Loss_0: 3.152e-06, Loss_r: 1.716e-03,lambda_0: 10.00,Time: 0.11\n",
            "It: 900, Loss: 1.291e-03, Loss_0: 2.789e-06, Loss_r: 1.288e-03,lambda_0: 10.00,Time: 0.12\n",
            "It: 910, Loss: 1.785e-03, Loss_0: 2.781e-06, Loss_r: 1.782e-03,lambda_0: 9.90,Time: 0.10\n",
            "It: 920, Loss: 1.287e-03, Loss_0: 2.860e-06, Loss_r: 1.284e-03,lambda_0: 10.00,Time: 0.12\n",
            "It: 930, Loss: 1.392e-03, Loss_0: 2.844e-06, Loss_r: 1.389e-03,lambda_0: 9.92,Time: 0.11\n",
            "It: 940, Loss: 1.221e-03, Loss_0: 2.905e-06, Loss_r: 1.219e-03,lambda_0: 9.93,Time: 0.10\n",
            "It: 950, Loss: 2.533e-03, Loss_0: 3.015e-06, Loss_r: 2.530e-03,lambda_0: 9.76,Time: 0.11\n",
            "It: 960, Loss: 2.257e-03, Loss_0: 3.431e-06, Loss_r: 2.254e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 970, Loss: 1.667e-03, Loss_0: 3.514e-06, Loss_r: 1.664e-03,lambda_0: 10.00,Time: 0.10\n",
            "It: 980, Loss: 1.609e-03, Loss_0: 3.496e-06, Loss_r: 1.605e-03,lambda_0: 10.00,Time: 0.12\n",
            "It: 990, Loss: 1.777e-03, Loss_0: 3.379e-06, Loss_r: 1.773e-03,lambda_0: 10.00,Time: 0.10\n",
            "Training time: 15.0176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "predict_CDiff = modelDiff.Predict(t)\n",
        "\n",
        "predict_CDiff = predict_CDiff.reshape(exact_C.shape)\n",
        "error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CDiff.flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "print('Error C Diff: %e' % (error_C))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I1lGYY-WBEx",
        "outputId": "b766cc20-49f0-4cea-9926-c3361b1df4bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Diff: 2.868058e-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','c','b','m','r']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t, predict_CDiff.flatten(), color[0], label = 'C expected')\n",
        "plt.plot(t, exact_C.flatten(), color[1], label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "62bU49X9jhT4",
        "outputId": "40da2b1f-5575-4ba3-e479-98539ed2b862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb7d1253a00>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the PINN with no differenciation"
      ],
      "metadata": {
        "id": "NNnDSrA2loQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,1],[1,128,128,128,1],[1,256,256,256,1],[1,64,64,64,64,1],[1,256,256,256,256,1],[1,128,128,128,128,1],[1,128,128,64,64,1],[1,256,128,64,32,1]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = ReactionPINN_NoDiff(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZAz74JQSJ5d",
        "outputId": "46c14f15-0f76-4b2d-e0b6-3f68e2c3e120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 9.358e-02, Time: 0.74, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.032e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.864e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.086e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.125e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.022e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.862e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.806e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.752e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.739e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.739e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.736e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.733e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.730e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.728e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.725e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.722e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.719e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.716e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.712e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.708e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.704e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.699e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.694e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.688e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.681e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.674e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.666e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.657e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.647e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.635e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.622e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.607e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.589e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.568e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.543e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.514e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.478e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.435e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.383e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.319e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.241e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.145e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.027e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 4.885e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.715e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.519e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.298e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.059e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.811e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.565e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.334e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.126e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.948e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.798e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.668e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.545e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.423e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.297e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 2.166e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 2.032e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.893e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.749e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.602e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.454e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.308e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.167e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.038e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 9.473e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 8.395e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 7.467e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 6.763e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 6.176e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.697e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.281e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.135e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 4.931e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.414e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.049e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 3.739e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 3.491e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 3.249e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.027e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.849e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.035e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.576e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.266e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.137e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.958e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.813e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.680e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.648e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.529e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.502e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.248e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.161e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.074e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 9.868e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 9.163e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 990, Loss: 8.912e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.703e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 7.335e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 7.878e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 6.400e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 6.112e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 5.614e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 5.224e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 4.863e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 4.611e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 6.085e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.427e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 6.425e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.644e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 3.833e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.306e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.130e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.982e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.843e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.726e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.623e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 3.064e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.594e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.368e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.711e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 2.275e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.271e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.026e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.978e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.908e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.842e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.783e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.732e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.687e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.859e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.604e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 7.155e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 2.857e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.511e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.745e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.418e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.418e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.346e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.308e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.276e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.245e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.215e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.186e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.158e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.132e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.285e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.543e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.648e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.225e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.491e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.228e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.114e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.027e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 9.590e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 9.291e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 9.120e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.901e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 8.720e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 8.538e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 8.362e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 8.192e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 8.026e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.864e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 7.707e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 7.554e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 7.405e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 7.266e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 8.075e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.504e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 5.469e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.125e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 8.372e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.049e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 8.789e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 7.344e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 6.698e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 6.400e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.219e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 6.085e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 5.979e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 5.881e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.783e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 5.689e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 5.597e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 5.508e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 5.421e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.337e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 5.254e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 5.174e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.096e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.020e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 4.946e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 4.874e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 4.805e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.900e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 5.467e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.049e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 5.450e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.431e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 8.610e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 4.644e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 5.172e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 4.910e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 4.599e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 4.434e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.343e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 4.277e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 4.221e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 4.170e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 4.123e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 4.077e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 4.032e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 3.988e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 3.946e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 3.904e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 3.864e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 3.825e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 3.786e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 3.749e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 3.713e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 3.677e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 3.643e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 3.610e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 3.577e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 3.552e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 4.712e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 3.504e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 3.596e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 3.482e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 3.421e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 3.389e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 3.365e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 3.338e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 3.315e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 3.292e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 3.270e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 3.249e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 3.228e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 3.207e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 3.187e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 3.167e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 3.149e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 3.152e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 3.115e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 3.098e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.082e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 3.065e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.050e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.035e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 3.021e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 3.006e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 2.992e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 2.978e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 2.964e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 2.951e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 2.938e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 2.925e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 2.912e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 2.900e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 2.888e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 2.876e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 2.923e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.855e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.849e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.836e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.823e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 2.814e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 2.804e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 2.794e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 2.785e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 2.775e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 2.766e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 2.757e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 2.748e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 2.739e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 2.731e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 2.722e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 2.713e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 2.705e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 2.700e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 3.285e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 2.692e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 2.734e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.694e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 2.663e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 2.656e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 2.647e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 2.639e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 2.632e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 2.626e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 2.619e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 2.612e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 2.605e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 2.599e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 2.592e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 2.586e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 2.579e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 2.573e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 2.590e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 2.564e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 2.556e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 2.551e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 2.544e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 2.539e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 2.533e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 2.528e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 2.522e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 2.517e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 2.511e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 2.506e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 2.500e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 2.495e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 2.490e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 2.485e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 2.479e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 2.474e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 2.469e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 2.479e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 2.461e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 2.454e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 2.450e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 2.445e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 2.440e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 2.435e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 2.431e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 2.426e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 2.421e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 2.417e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 2.412e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 2.407e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 2.403e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 2.398e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 2.393e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 2.389e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 2.384e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 2.380e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 2.376e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 2.409e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 2.371e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.362e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 2.360e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 2.354e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 2.350e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 2.346e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 2.341e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 2.337e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 2.333e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 2.329e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 2.325e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 2.320e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 2.316e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 2.312e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 2.308e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 2.304e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 2.300e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 2.295e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 2.291e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 2.287e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 2.307e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 2.280e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 2.277e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 2.273e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 2.267e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 2.264e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 2.260e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 2.256e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 2.252e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 2.248e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 2.244e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 2.241e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 2.237e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 2.233e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 2.229e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 2.225e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 2.221e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 2.217e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 2.213e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 2.218e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 2.207e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 2.202e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 2.199e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 2.195e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 2.192e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 2.188e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 2.184e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 2.181e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 2.177e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 2.174e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 2.170e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 2.166e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 2.163e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 2.159e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 2.155e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 2.152e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 2.148e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 2.144e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 2.141e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 2.138e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 2.214e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 2.136e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 2.129e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 2.128e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 2.120e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 2.117e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 2.113e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 2.110e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 2.106e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 2.103e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 2.099e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 2.096e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 2.092e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 2.089e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 2.086e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 2.082e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 2.079e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 2.075e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 2.072e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 2.068e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 2.069e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 2.062e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 2.058e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 2.055e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 2.052e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 2.049e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 2.045e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 2.042e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 2.039e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 2.036e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 2.032e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 2.029e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 2.026e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 2.022e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 2.019e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 2.016e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 2.013e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 2.009e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 2.006e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 2.003e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 2.019e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 1.999e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 1.993e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 1.991e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 1.987e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 1.984e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 1.981e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 1.978e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 1.975e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 1.972e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 1.968e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 1.965e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 1.962e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 1.959e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 1.956e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 1.953e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 1.950e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 1.946e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 1.943e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 1.940e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 1.937e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 1.997e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 1.936e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 1.929e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 1.928e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 1.922e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 1.919e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 1.916e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 1.913e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 1.910e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 1.907e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.904e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.901e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.898e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 1.895e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 1.892e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 1.889e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 1.886e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 1.883e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 1.880e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 1.877e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 1.886e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 1.873e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 1.868e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 1.866e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 1.863e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 1.860e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 1.857e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 1.854e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 1.851e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 1.849e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 1.846e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 1.843e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.840e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 1.837e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 1.834e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 1.831e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 1.829e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "Training time: 13.6001\n",
            "[1, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 9.195e-02, Time: 0.76, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.521e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.352e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 8.210e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.395e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.147e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.931e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.820e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.822e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.815e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.807e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.805e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.802e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.799e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.796e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.793e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.789e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.784e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.779e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.773e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.766e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.758e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.749e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.738e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.724e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.709e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.689e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.666e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.637e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.600e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.555e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.497e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.423e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.330e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.212e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.065e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.884e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 4.670e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.425e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 390, Loss: 4.157e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.878e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.604e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.351e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.133e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.955e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.812e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.688e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.569e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.449e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.324e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.194e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.059e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.934e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.699e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.910e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.611e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.408e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.278e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.133e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.008e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 600, Loss: 8.953e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 610, Loss: 7.959e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 620, Loss: 7.126e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 6.435e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 640, Loss: 5.869e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.516e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.202e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 670, Loss: 6.902e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 680, Loss: 4.941e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 4.373e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 700, Loss: 4.081e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.808e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 720, Loss: 3.521e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 3.297e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 740, Loss: 3.078e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.872e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.678e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.495e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.323e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.166e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 6.408e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.085e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.265e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.356e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.934e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.707e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.505e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.365e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.276e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.181e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.096e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.018e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 9.447e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 930, Loss: 8.768e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 8.140e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 950, Loss: 7.557e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 960, Loss: 7.019e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 6.522e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 980, Loss: 6.134e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 990, Loss: 9.504e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.431e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 6.650e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 8.688e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 8.472e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 6.603e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 5.383e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 4.731e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 4.312e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 4.007e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 3.770e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 3.551e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 3.348e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.163e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.993e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.837e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.694e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.563e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.442e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.330e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.228e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 2.133e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.045e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.964e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.889e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.819e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.754e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.693e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.637e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.608e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.153e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.426e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 6.697e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.915e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.851e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.429e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.979e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.716e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.589e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.524e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.478e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.429e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.381e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.339e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.299e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.262e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.226e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.193e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.161e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.131e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.102e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.074e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.048e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.022e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.979e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 9.744e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.517e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 9.298e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 9.087e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 8.882e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 8.684e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.491e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 8.305e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 8.124e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 7.948e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 7.776e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 7.610e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.474e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.450e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 2.203e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 5.707e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 9.010e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.294e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.875e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.739e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.010e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 8.143e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 8.183e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 7.806e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 7.502e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 7.322e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 7.124e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.951e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 6.786e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 6.629e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 6.479e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 6.336e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 6.199e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.068e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 5.941e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 5.819e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.702e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 5.589e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 5.479e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.373e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.271e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 5.171e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 5.075e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 4.982e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.891e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 4.803e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 4.718e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 4.635e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 4.555e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 4.477e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 4.403e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 4.524e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2060, Loss: 4.271e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2070, Loss: 4.214e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2080, Loss: 4.149e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2090, Loss: 4.079e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 4.021e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 3.964e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 3.908e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 3.854e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 3.801e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 3.750e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 3.700e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 3.651e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 3.603e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 3.557e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 3.512e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 3.477e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 5.271e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2230, Loss: 3.405e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2240, Loss: 3.581e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2250, Loss: 3.374e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2260, Loss: 3.305e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2270, Loss: 3.248e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2280, Loss: 3.218e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 3.181e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 3.148e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 3.116e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 3.085e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 3.055e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 3.026e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 2.997e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 2.969e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 2.942e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 2.930e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 5.388e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2400, Loss: 2.921e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2410, Loss: 3.088e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2420, Loss: 2.937e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2430, Loss: 2.816e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2440, Loss: 2.793e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2450, Loss: 2.767e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 2.740e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 2.720e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 2.701e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 2.681e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 2.663e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 2.645e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 2.627e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 2.609e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 2.592e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 2.575e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 2.567e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 4.143e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2580, Loss: 2.554e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2590, Loss: 2.685e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2600, Loss: 2.568e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2610, Loss: 2.499e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2620, Loss: 2.481e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2630, Loss: 2.466e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2640, Loss: 2.448e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 2.436e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 2.423e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 2.411e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 2.399e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 2.387e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 2.376e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 2.364e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 2.353e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.342e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 2.333e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 2.602e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2760, Loss: 2.321e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2770, Loss: 2.322e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2780, Loss: 2.308e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2790, Loss: 2.284e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2800, Loss: 2.277e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2810, Loss: 2.266e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 2.257e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 2.249e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 2.240e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 2.232e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 2.224e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 2.215e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 2.207e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 2.200e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 2.192e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 2.184e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 2.177e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 2.182e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 2940, Loss: 2.163e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2950, Loss: 2.156e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 2960, Loss: 2.150e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2970, Loss: 2.143e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2980, Loss: 2.136e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2990, Loss: 2.130e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3000, Loss: 2.124e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3010, Loss: 2.117e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 2.111e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 2.105e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 2.099e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 2.093e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 2.087e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 2.081e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 2.075e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 2.070e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 2.064e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 2.058e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 2.054e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 2.295e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3140, Loss: 2.050e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3150, Loss: 2.056e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3160, Loss: 2.046e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3170, Loss: 2.028e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3180, Loss: 2.024e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3190, Loss: 2.018e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3200, Loss: 2.013e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 2.008e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 2.004e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 1.999e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.994e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.990e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.985e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.980e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.976e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.971e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.967e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 1.972e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3320, Loss: 1.960e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3330, Loss: 1.954e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3340, Loss: 1.951e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3350, Loss: 1.946e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3360, Loss: 1.942e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3370, Loss: 1.938e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3380, Loss: 1.935e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3390, Loss: 1.931e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3400, Loss: 1.927e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3410, Loss: 1.923e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 1.919e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.915e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.911e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.907e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.904e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.900e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.896e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.892e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.888e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.885e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.882e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 2.154e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3540, Loss: 1.875e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3550, Loss: 1.909e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3560, Loss: 1.875e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3570, Loss: 1.868e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3580, Loss: 1.861e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3590, Loss: 1.858e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3600, Loss: 1.854e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 1.851e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 1.847e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 1.844e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 1.841e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 1.837e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 1.834e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 1.831e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 1.827e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 1.824e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 1.821e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 1.823e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3720, Loss: 1.815e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3730, Loss: 1.812e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3740, Loss: 1.809e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3750, Loss: 1.806e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3760, Loss: 1.803e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3770, Loss: 1.800e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3780, Loss: 1.797e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3790, Loss: 1.794e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 1.791e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 1.788e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 1.785e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 1.782e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 1.779e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 1.776e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 1.773e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 1.770e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 1.767e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 1.764e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 1.761e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 1.759e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 1.774e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3930, Loss: 1.754e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3940, Loss: 1.751e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3950, Loss: 1.749e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3960, Loss: 1.745e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3970, Loss: 1.742e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3980, Loss: 1.739e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3990, Loss: 1.737e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 1.734e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 1.731e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 1.729e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 1.726e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 1.723e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 1.721e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 1.718e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 1.715e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 1.713e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 1.710e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 1.707e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 1.707e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4120, Loss: 1.702e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4130, Loss: 1.700e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4140, Loss: 1.697e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4150, Loss: 1.695e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4160, Loss: 1.692e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4170, Loss: 1.690e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4180, Loss: 1.687e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 1.685e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 1.682e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 1.680e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 1.677e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 1.675e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 1.672e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 1.670e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 1.667e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 1.665e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 1.662e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 1.660e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 1.657e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 1.656e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 1.716e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4330, Loss: 1.656e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4340, Loss: 1.649e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4350, Loss: 1.649e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4360, Loss: 1.644e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 1.641e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 1.639e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 1.636e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 1.634e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 1.632e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 1.630e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 1.627e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 1.625e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 1.623e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 1.620e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 1.618e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 1.616e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 1.613e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 1.611e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 1.609e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 1.638e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4530, Loss: 1.608e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4540, Loss: 1.602e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4550, Loss: 1.601e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4560, Loss: 1.598e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4570, Loss: 1.596e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 1.594e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 1.591e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 1.589e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 1.587e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 1.585e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 1.583e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 1.581e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 1.578e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 1.576e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 1.574e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 1.572e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 1.570e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 1.567e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 1.565e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 1.563e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 1.561e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 1.559e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 1.680e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4760, Loss: 1.554e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4770, Loss: 1.574e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 1.552e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 1.551e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 1.546e-06, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 1.545e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 1.542e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 1.540e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 1.538e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 1.536e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 1.534e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 1.532e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 1.530e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 1.528e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 1.526e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 1.524e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 1.524e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4930, Loss: 1.520e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4940, Loss: 1.518e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "It: 4950, Loss: 1.516e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4960, Loss: 1.514e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 1.512e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 1.510e-06, Time: 0.02, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 1.509e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "Training time: 15.0330\n",
            "[1, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 5.078e-02, Time: 1.17, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.515e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.059e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 7.713e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.699e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.015e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.914e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.924e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.887e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.884e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.883e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.882e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.881e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.881e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.881e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.880e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.880e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.880e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.880e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.879e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.879e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.879e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.879e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.878e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.878e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.878e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.877e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.877e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.877e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.876e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.876e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.875e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.875e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.874e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.874e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.873e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.873e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.872e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.871e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.871e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.870e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.869e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.869e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.868e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.867e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.866e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.865e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.864e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.863e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.861e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.860e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.858e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.857e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.855e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.853e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.851e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.849e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.846e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.843e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.840e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 5.836e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 610, Loss: 5.831e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 620, Loss: 5.826e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 5.820e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 640, Loss: 5.814e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 650, Loss: 5.805e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 660, Loss: 5.795e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 670, Loss: 5.783e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 680, Loss: 5.767e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 690, Loss: 5.746e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 700, Loss: 5.720e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 710, Loss: 5.683e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.632e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.557e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.445e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.270e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 760, Loss: 5.000e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.602e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.097e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 790, Loss: 4.262e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 800, Loss: 7.969e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 5.853e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.929e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.797e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.376e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 850, Loss: 3.187e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 860, Loss: 3.027e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.878e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.726e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.580e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.436e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.290e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.140e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.985e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.822e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.652e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.478e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.311e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.153e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.972e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.745e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.563e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.191e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.087e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 9.420e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 8.197e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 7.176e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 6.339e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 5.652e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 5.087e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.615e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 4.214e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.865e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 3.557e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.279e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.027e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.797e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.586e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.392e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.213e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 2.048e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.896e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.755e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.626e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.252e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.187e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.374e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 4.550e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 4.024e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.538e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.471e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.166e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.936e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.769e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.626e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.498e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.382e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.276e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.179e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.090e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.008e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 9.333e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 8.645e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.014e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 7.433e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 6.899e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 6.408e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 5.957e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 5.543e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 5.162e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 4.812e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 4.490e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 4.194e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 3.922e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 3.672e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 3.443e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 3.232e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 3.039e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.861e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.698e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.548e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.410e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 2.284e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.168e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.061e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.963e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.872e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.789e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.713e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.643e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.578e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.518e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.463e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.412e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.365e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.321e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.280e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.243e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.208e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.176e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.152e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.710e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 3.071e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.310e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 2.855e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 9.314e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 3.830e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 2.488e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.249e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 2.193e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 2.056e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.908e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.813e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.725e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.644e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.572e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.507e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.447e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.392e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.342e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.296e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.253e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.214e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.178e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.145e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.114e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.085e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.059e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.034e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.011e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 9.892e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 9.688e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 9.498e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 9.318e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 9.149e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 8.988e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 8.837e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 8.693e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 8.557e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 8.427e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 8.303e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 8.185e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 8.073e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 7.965e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 7.861e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 7.762e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 7.667e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 7.576e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 7.488e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 7.404e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 7.322e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 7.244e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 7.168e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 7.095e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 7.028e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 8.222e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 6.898e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 7.069e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 6.787e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 6.762e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 6.682e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 6.624e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 6.574e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 6.525e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 6.477e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 6.431e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 6.385e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 6.341e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 6.298e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 6.289e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 1.118e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 6.317e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 6.588e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 6.349e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 6.101e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 6.068e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 6.024e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 5.979e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 5.946e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 5.914e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 5.884e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 5.853e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 5.824e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 5.794e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 5.766e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 5.742e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 5.985e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 5.715e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 5.664e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 5.651e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 5.613e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 5.590e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 5.565e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 5.542e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 5.519e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 5.496e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 5.474e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 5.452e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 5.430e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 5.408e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 5.386e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 5.365e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 5.344e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 5.323e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 5.308e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 6.536e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 5.279e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 5.386e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 5.276e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 5.219e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 5.194e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 5.176e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 5.154e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 5.136e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 5.118e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 5.100e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 5.083e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 5.065e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 5.048e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 5.030e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 5.013e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 4.996e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 5.052e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 4.970e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 4.948e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 4.935e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 4.916e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 4.900e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 4.885e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 4.869e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 4.854e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 4.838e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 4.823e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 4.808e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 4.792e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 4.777e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 4.761e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 4.746e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 4.731e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 4.715e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 4.701e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 4.732e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 4.675e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 4.658e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 4.646e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 4.629e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 4.616e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 4.601e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 4.588e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 4.574e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 4.560e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 4.546e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 4.532e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 4.518e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 4.504e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 4.490e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 4.476e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 4.462e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 4.448e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 4.434e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 4.430e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 6.337e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 4.434e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 4.562e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 4.461e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 4.361e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 4.354e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 4.333e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 4.316e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 4.304e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 4.291e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 4.278e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 4.265e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 4.252e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 4.238e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 4.225e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 4.212e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 4.199e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 4.187e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 4.225e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 4.167e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 4.150e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 4.140e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 4.126e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 4.114e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 4.101e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 4.089e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 4.077e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 4.065e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 4.053e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 4.041e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 4.029e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 4.017e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 4.004e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 3.992e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 3.980e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 3.968e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 3.955e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 3.943e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 3.934e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 4.707e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 3.918e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 3.981e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 3.919e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 3.878e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 3.866e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 3.853e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 3.839e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 3.828e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 3.817e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 3.805e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 3.794e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 3.782e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 3.771e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 3.759e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 3.748e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 3.736e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 3.725e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 3.735e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 3.705e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 3.692e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 3.682e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 3.671e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 3.660e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 3.649e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 3.639e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 3.628e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 3.617e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 3.607e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 3.596e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 3.585e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 3.574e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 3.564e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 3.553e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 3.542e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 3.531e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 3.520e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 3.510e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 3.559e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 3.494e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 3.479e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 3.472e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 3.459e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 3.449e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 3.438e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 3.428e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 3.418e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 3.408e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 3.398e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 3.388e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 3.378e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 3.368e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 3.358e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 3.347e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 3.337e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 3.327e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 3.317e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 3.306e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 3.296e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 3.286e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 3.509e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 3.267e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 3.299e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 3.249e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 3.244e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 3.229e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 3.219e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 3.210e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 3.200e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 3.191e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 3.181e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 3.172e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 3.162e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 3.152e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 3.143e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 3.133e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 3.124e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 3.114e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 3.133e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 3.097e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 3.088e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 3.080e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 3.069e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 3.061e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 3.052e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 3.043e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 3.034e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 3.025e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 3.016e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 3.007e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 2.998e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 2.990e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 2.981e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 2.972e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 2.963e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 2.954e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 2.945e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 2.958e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 2.930e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 2.919e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 2.912e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 2.903e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 2.895e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 2.887e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 2.878e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 2.870e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 2.862e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 2.853e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 2.845e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 2.837e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 2.828e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 2.820e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 2.812e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 2.803e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 2.795e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 2.787e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 2.779e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 2.817e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 2.767e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 2.755e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 14.2955\n",
            "[1, 64, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.423e-01, Time: 0.89, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.308e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.681e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.102e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.655e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.238e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.852e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.774e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.735e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.710e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.702e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.697e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.691e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.683e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.676e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.668e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.658e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.648e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.637e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.624e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.610e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.595e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.577e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.557e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.534e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.508e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.478e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.443e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.403e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.356e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.301e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.236e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.158e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.066e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.955e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.824e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.667e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 4.484e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.274e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 4.040e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.792e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.543e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.310e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.108e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.945e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.816e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.705e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.600e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.491e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.376e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.255e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.129e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.996e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.855e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.707e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.553e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.393e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.232e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.099e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 9.715e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 7.994e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 6.940e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 5.976e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 630, Loss: 5.202e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 640, Loss: 4.618e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 650, Loss: 4.172e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 660, Loss: 5.126e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.485e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.223e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.979e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.694e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.467e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.256e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.455e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.359e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.748e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.709e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.478e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.355e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.224e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.107e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.073e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.396e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 830, Loss: 8.567e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 840, Loss: 9.135e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 850, Loss: 8.044e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 860, Loss: 6.489e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 870, Loss: 5.940e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 5.408e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 4.911e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 4.531e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 6.104e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.289e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.195e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 5.876e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.475e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.888e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.826e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.487e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.334e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.182e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.044e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.928e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.828e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.241e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 4.734e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.684e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.591e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.520e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.485e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.476e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.413e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.291e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.237e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.189e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.149e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.111e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.076e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.043e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.012e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 9.827e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 9.566e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.090e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 4.098e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 8.081e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 9.136e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.778e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.860e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 9.481e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 8.415e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 8.300e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 8.052e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 7.811e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 7.608e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 7.429e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 7.256e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 7.086e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 6.924e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 6.767e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 6.615e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 6.468e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 6.326e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 6.187e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 6.052e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 5.921e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 5.794e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 5.670e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 5.550e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 5.607e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.073e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 4.918e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.592e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.232e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.708e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 6.997e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 8.001e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 5.508e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 5.403e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 5.155e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 5.021e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 4.892e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 4.797e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.696e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 4.602e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 4.512e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 4.424e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 4.339e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 4.256e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 4.176e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 4.098e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 4.021e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.947e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 3.875e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 3.805e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.737e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 3.670e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 3.605e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 3.542e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 3.481e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 3.421e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 3.362e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 3.305e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 3.250e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 3.196e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 3.186e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.519e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 7.863e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.695e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.072e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.827e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.046e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.670e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 4.453e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.276e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.314e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 3.168e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.085e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 3.038e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 2.986e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 2.937e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.890e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.845e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.802e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.760e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.719e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.679e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.641e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.603e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.567e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 2.531e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 2.497e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 2.463e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 2.430e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 2.398e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 2.367e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 2.337e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 2.307e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 2.278e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 2.250e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 2.223e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 2.197e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 2.217e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 2.148e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 2.130e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 2.105e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 2.082e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 2.061e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 2.041e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 2.021e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 2.001e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.982e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.963e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.945e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.927e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.909e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.892e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 1.875e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.863e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 2.425e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 1.853e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.853e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.833e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.788e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.778e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.761e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 1.748e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 1.735e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 1.723e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 1.711e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 1.699e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 1.687e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 1.675e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 1.664e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 1.653e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 1.642e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 1.637e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 2.439e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.643e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.658e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.637e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.584e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.580e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.566e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.556e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.548e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.539e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.531e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.522e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.514e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.506e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.498e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.490e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.482e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.474e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.466e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.477e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.452e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.447e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.439e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.432e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.425e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.419e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.412e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.406e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.399e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.393e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.387e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.380e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.374e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.368e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.362e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.356e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.364e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.347e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.339e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.335e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.329e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.324e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.318e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.313e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.308e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.303e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.298e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.293e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.288e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.283e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.278e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.273e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.268e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.263e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.258e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.266e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.250e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.244e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.240e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.235e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.231e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.227e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.222e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.218e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.214e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.209e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.205e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.201e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.196e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.192e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.188e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.184e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.180e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.175e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.174e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 1.343e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.183e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.161e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 1.165e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 1.153e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 1.149e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.145e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.141e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.137e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.133e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.129e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.126e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 1.122e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.118e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.114e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.111e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 1.107e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 1.103e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 1.100e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 1.097e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 1.156e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.094e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.087e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.086e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.079e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.076e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.072e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.069e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.066e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.062e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.059e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.056e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 1.052e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.049e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.046e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.042e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.039e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.036e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.033e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.034e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 1.027e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 1.023e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 1.020e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 1.017e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.014e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.011e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 1.008e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.005e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.002e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 9.993e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 9.963e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 9.933e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 9.904e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 9.874e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 9.843e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 9.814e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 9.783e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 9.753e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 9.723e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 9.696e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 9.888e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 9.652e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 9.617e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 9.595e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 9.555e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 9.529e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 9.501e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 9.474e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 9.446e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 9.419e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 9.392e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 9.364e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 9.338e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 9.310e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 9.283e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 9.255e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 9.228e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 9.200e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 9.174e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 9.174e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 9.126e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 9.096e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 9.072e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 9.047e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 9.022e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 8.997e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 8.973e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 8.948e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 8.923e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 8.898e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 8.873e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 8.848e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 8.823e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 8.798e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 8.773e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 8.748e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 8.723e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 8.698e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 8.673e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 8.652e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 9.036e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 8.638e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 8.585e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 8.577e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 8.534e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 8.511e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 8.486e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 8.463e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 8.440e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 8.417e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 8.394e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 8.371e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 8.348e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 8.325e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 8.303e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 8.280e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 8.256e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 8.233e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 8.210e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 8.187e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 8.165e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 8.346e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 8.129e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 8.112e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 8.091e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 8.057e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 8.038e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 8.015e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 7.994e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 7.973e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 7.952e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 7.931e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 7.910e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 7.889e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 7.867e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 7.846e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 7.825e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 7.804e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 7.783e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 7.761e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 7.740e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 7.719e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 7.722e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 7.678e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 7.662e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 7.640e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 7.620e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 7.601e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 7.581e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 7.562e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 7.542e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 7.523e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 7.503e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 7.484e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 7.464e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 7.445e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 7.425e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 7.406e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 7.386e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 7.366e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 7.347e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 7.350e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 7.311e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 7.292e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 7.275e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 7.256e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 7.238e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 7.220e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 7.202e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 14.4983\n",
            "[1, 256, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 4.269e-02, Time: 0.99, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.428e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 6.847e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 6.668e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.294e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 5.887e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.942e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.881e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.877e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.876e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.875e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.874e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.873e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.873e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.872e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.872e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.871e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.871e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.870e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.869e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.869e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.868e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.867e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.866e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.864e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.863e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.861e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.860e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.858e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.855e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.852e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.849e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.844e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.839e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.831e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.822e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.808e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.788e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.756e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.700e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.589e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.336e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 420, Loss: 4.730e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 430, Loss: 6.805e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.203e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.044e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.961e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.887e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.912e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.901e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.877e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.867e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.863e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.860e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.856e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.851e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.844e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.836e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.823e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.806e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 600, Loss: 5.779e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 5.735e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 620, Loss: 5.655e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 5.498e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 5.158e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 650, Loss: 4.470e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 660, Loss: 3.470e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.567e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.877e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.203e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.330e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 710, Loss: 8.164e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.805e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.987e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.597e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.318e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 4.744e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.010e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.355e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.825e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.341e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.926e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.544e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.192e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 840, Loss: 8.900e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 850, Loss: 6.612e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 860, Loss: 5.077e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 4.056e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 880, Loss: 9.296e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 890, Loss: 9.318e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.964e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.159e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 7.953e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 930, Loss: 6.224e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 940, Loss: 4.846e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.938e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 3.267e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.748e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.318e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.960e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.662e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.418e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.216e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.049e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 9.100e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 7.945e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 6.980e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 6.172e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 5.494e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 4.923e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.440e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 4.031e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.682e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 3.385e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.131e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.912e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.723e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.559e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.417e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.293e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 2.184e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.088e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.003e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.948e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 4.022e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 4.309e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.255e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.182e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 9.772e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 9.875e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 9.323e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 7.284e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 6.029e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 5.309e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 4.637e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 4.108e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 3.686e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 3.334e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.042e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.798e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 2.593e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 2.419e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 2.272e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 2.145e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.035e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.940e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.856e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.782e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.717e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.658e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.605e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.557e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.513e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.473e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.436e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.402e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.370e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.341e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.313e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.287e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.263e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.239e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.218e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.197e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.177e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.159e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.141e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.124e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.108e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.092e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.077e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.063e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.049e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.036e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.023e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.010e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 9.979e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 9.861e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 9.746e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 9.634e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 9.524e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 9.417e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 9.313e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 9.213e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 9.842e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 4.134e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 2.415e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.291e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.076e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 8.033e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.266e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 2.865e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.432e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 2.222e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.917e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.718e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.570e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.453e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.358e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.282e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.219e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.166e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.120e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.081e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.047e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.016e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 9.880e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 9.627e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 9.395e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 9.179e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 8.977e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 8.788e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 8.610e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 8.441e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 8.280e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 8.127e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 7.980e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 7.839e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 7.704e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 7.574e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 7.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 7.326e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 7.208e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 7.093e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 6.982e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 6.873e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 6.767e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 6.663e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 6.562e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 6.463e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 6.365e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 6.270e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 6.176e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 6.083e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 5.992e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 5.909e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 8.924e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 5.775e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 6.265e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 5.581e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 5.573e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 5.453e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 5.355e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 5.277e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 5.203e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 5.129e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 5.055e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 4.983e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 4.911e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 4.848e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 5.301e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 4.765e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 4.646e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 4.606e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 4.516e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 4.454e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 4.389e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 4.328e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 4.266e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 4.204e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 4.144e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 4.083e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 4.023e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 3.963e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 3.904e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 3.845e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 3.786e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 3.732e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 4.416e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 3.631e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 3.648e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 3.545e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 3.472e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 3.416e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 3.366e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 3.314e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 3.264e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 3.215e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 3.166e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 3.117e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 3.069e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 3.021e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 2.974e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 2.927e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 2.897e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 5.577e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 2.868e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.993e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 2.845e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 2.682e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 2.646e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 2.597e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 2.551e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 2.511e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 2.472e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 2.434e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 2.395e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 2.357e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 2.319e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 2.282e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 2.245e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 2.211e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 2.476e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 2.159e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 2.122e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 2.094e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 2.044e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 2.016e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.982e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.952e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.921e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.891e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 1.861e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 1.832e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 1.803e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 1.774e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 1.745e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 1.717e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 1.689e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 1.661e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 1.634e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 1.608e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 2.453e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.590e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.704e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.515e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.502e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.472e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.443e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.419e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.397e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 1.375e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 1.354e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 1.332e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 1.311e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 1.290e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 1.271e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 1.350e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.244e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.214e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.200e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.179e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.161e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 1.143e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.126e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.109e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.093e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 1.076e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 1.060e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 1.044e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 1.028e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 1.013e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 9.973e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 9.821e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 9.670e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 9.528e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 1.004e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 9.284e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 9.146e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 9.027e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 8.861e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 8.742e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 8.613e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 8.492e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 8.371e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 8.252e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 8.135e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 8.018e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 7.904e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 7.790e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 7.678e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 7.568e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 7.459e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 7.356e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 7.559e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 7.185e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 7.061e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 6.981e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 6.876e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 6.784e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 6.694e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 6.606e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 6.518e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 6.432e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 6.347e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 6.263e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 6.180e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 6.098e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 6.017e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 5.937e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 5.858e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 5.780e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 5.704e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 5.768e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 5.573e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 5.494e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 5.434e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 5.361e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 5.296e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 5.232e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 5.169e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 5.106e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 5.044e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 4.983e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 4.923e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 4.863e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 4.804e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 4.746e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 4.689e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 4.632e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 4.576e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 4.523e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 4.654e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 4.437e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 4.371e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 4.331e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 4.275e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 4.228e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 4.181e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 4.135e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 4.090e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 4.046e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 4.001e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 3.958e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 3.915e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 3.872e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 3.830e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 3.788e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 3.747e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 3.706e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 3.667e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 3.670e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 3.597e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 3.555e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 3.522e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 3.487e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 3.452e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 3.419e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 3.385e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 3.352e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 3.319e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 3.287e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 3.255e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 3.223e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 3.192e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 3.161e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 3.130e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 3.100e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 3.070e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 3.041e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 3.014e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 3.210e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 2.981e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 2.934e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 2.917e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 2.882e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 2.856e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 2.831e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 2.806e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 2.782e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 2.758e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 2.734e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 2.710e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 2.687e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 2.664e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 2.641e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 2.619e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 2.596e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 2.574e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 2.552e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 2.530e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 2.514e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 3.446e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 2.489e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 2.538e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 2.478e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 2.415e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 2.400e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 2.377e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 2.358e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 2.340e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 2.322e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 2.305e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 2.287e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 2.270e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 2.253e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 2.236e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 2.219e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 2.202e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 2.186e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 2.185e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 3.600e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 2.262e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 2.158e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 2.193e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 2.102e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 2.089e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 2.069e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 2.055e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 2.040e-07, Time: 0.02, Learning Rate: 0.00021\n",
            "Training time: 14.9213\n",
            "[1, 128, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 7.587e-02, Time: 0.99, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.377e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.423e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 7.607e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 5.978e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 5.905e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.931e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.875e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.848e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.847e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.845e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.843e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.842e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.841e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.840e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.839e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.838e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.837e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.836e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.835e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.834e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.832e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.831e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.829e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.827e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.825e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.823e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.821e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.818e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.815e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.812e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.808e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.804e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.800e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.794e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.788e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.781e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.773e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.762e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.750e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.734e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.714e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.688e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.653e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.603e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.528e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.411e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.216e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.888e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.381e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.753e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.151e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 520, Loss: 8.429e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.341e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.229e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.538e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.324e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.997e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.685e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.385e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.098e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 610, Loss: 8.496e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 620, Loss: 6.627e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 630, Loss: 7.674e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.139e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 5.161e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 5.578e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 4.480e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.463e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.750e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.350e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.980e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.698e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.465e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.269e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.107e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 760, Loss: 9.705e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 8.561e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 7.596e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 6.780e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 6.086e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 5.503e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.813e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.086e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.688e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 850, Loss: 3.406e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.554e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 870, Loss: 7.993e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 880, Loss: 5.598e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 890, Loss: 5.262e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 4.737e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 4.223e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.894e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 930, Loss: 3.592e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 3.332e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.110e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.916e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.745e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.593e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.459e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.339e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.231e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.134e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.047e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.967e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.894e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.827e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.766e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.709e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.656e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.608e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.562e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.520e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.480e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.443e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.408e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.375e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.344e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.315e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.287e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.261e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.386e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.930e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.002e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 4.535e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.389e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 4.976e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 3.328e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.849e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.942e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.642e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.584e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.502e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.437e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.385e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.339e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.298e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.260e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.227e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.196e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.168e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.142e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.118e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.096e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.076e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.056e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.038e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.021e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.004e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 9.888e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 9.741e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 9.601e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 9.467e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.338e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 9.215e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.097e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 8.984e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 8.875e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 8.770e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 8.668e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.570e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 8.475e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 8.383e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 8.294e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 8.208e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 8.124e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 8.042e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 7.962e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 7.885e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 7.809e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 7.736e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 7.711e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.359e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 3.144e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.356e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.442e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.312e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.257e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.500e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.299e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.097e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 9.871e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 9.270e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 8.886e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 8.609e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 8.392e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 8.210e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 8.047e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 7.902e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 7.772e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 7.653e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 7.543e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 7.441e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 7.345e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 7.255e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 7.169e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 7.087e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 7.009e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 6.934e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 6.861e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 6.791e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 6.723e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 6.656e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 6.592e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 6.529e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 6.468e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 6.408e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 6.350e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 6.292e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 6.236e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 6.181e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 6.127e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 6.074e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 6.022e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 5.970e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 5.920e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 5.870e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 5.821e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 5.841e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 5.731e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 5.695e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 5.648e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 5.604e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 5.563e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 5.522e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 5.481e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 5.441e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 5.402e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 5.362e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 5.324e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 5.285e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 5.247e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 5.209e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 5.171e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 5.134e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 5.109e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 9.357e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 5.037e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 5.765e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 4.986e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 5.042e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 4.906e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 4.870e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 4.840e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 4.806e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 4.774e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 4.742e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 4.711e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 4.680e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 4.649e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 4.619e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 4.614e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 7.041e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 4.703e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 4.602e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 4.627e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 4.453e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 4.445e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 4.399e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 4.374e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 4.347e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 4.320e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 4.294e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 4.268e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 4.242e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 4.217e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 4.191e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 4.166e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 4.140e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 4.116e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 4.146e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 4.071e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 4.048e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 4.026e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 4.000e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 3.979e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 3.956e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 3.934e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 3.912e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 3.891e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 3.869e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 3.847e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 3.826e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 3.804e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 3.783e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 3.761e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 3.740e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 3.719e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 3.698e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 3.677e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 3.662e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 8.465e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 4.093e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 4.458e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 3.712e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 3.592e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 3.593e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 3.545e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 3.514e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 3.493e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 3.473e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 3.454e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 3.436e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 3.418e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 3.400e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 3.393e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 4.056e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 3.427e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 3.341e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 3.354e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 3.306e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 3.288e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 3.270e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 3.255e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 3.239e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 3.223e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 3.208e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 3.192e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 3.177e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 3.161e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 3.146e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 3.131e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 3.116e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 3.101e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 3.086e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 3.082e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 5.495e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 3.068e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 3.311e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 3.112e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 3.023e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 2.999e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 2.984e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 2.964e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 2.950e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 2.937e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 2.924e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 2.911e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 2.898e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 2.885e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 2.872e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 2.859e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 2.848e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.927e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 2.835e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 2.811e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 2.803e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 2.789e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 2.776e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 2.765e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 2.754e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 2.742e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 2.731e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 2.720e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 2.709e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 2.697e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 2.686e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 2.675e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 2.664e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 2.653e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 2.642e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 2.631e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 2.625e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 3.325e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 2.630e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 2.635e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 2.621e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 2.570e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 2.566e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 2.551e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 2.542e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 2.532e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 2.522e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 2.513e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 2.503e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 2.493e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 2.484e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 2.474e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 2.465e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 2.456e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 2.447e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 2.476e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 2.433e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 2.420e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 2.413e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 2.403e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 2.395e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 2.386e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 2.378e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 2.370e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 2.361e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 2.353e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 2.345e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 2.337e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 2.328e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 2.320e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 2.312e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 2.304e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 2.296e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 2.288e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 2.289e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 2.274e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 2.265e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 2.258e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 2.251e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 2.243e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 2.236e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 2.229e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 2.222e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 2.214e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 2.207e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 2.200e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 2.193e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 2.186e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 2.179e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 2.172e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 2.165e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 2.158e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 2.151e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 2.143e-06, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 2.137e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 2.212e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 2.124e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 2.127e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 2.114e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 2.105e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 2.098e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 2.092e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 2.086e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 2.079e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 2.073e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 2.067e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 2.061e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 2.055e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 2.048e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 2.042e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 2.036e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 2.030e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 2.024e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 2.019e-06, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 2.246e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 2.013e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 2.020e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 2.007e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 1.990e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 1.986e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 1.979e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 1.973e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 1.968e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 1.963e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 1.957e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 1.952e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 1.946e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 1.941e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 1.935e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 1.930e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 1.925e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 1.919e-06, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 1.921e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 1.910e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 1.904e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 1.900e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 1.895e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 1.890e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 1.885e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 1.880e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 1.875e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 1.871e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 1.866e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 1.861e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 1.856e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 1.851e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 1.847e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 1.842e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 1.837e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 1.832e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 1.828e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 1.823e-06, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 1.823e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 1.814e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 1.810e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 1.806e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 1.801e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 1.797e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 1.793e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 1.788e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 1.784e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 1.780e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 1.776e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 1.771e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 1.767e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 1.763e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 1.759e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 1.754e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 1.750e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 1.746e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 1.742e-06, Time: 0.02, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 1.739e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 1.904e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 1.744e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "Training time: 14.6436\n",
            "[1, 128, 128, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 6.858e-02, Time: 0.99, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.796e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.351e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 7.983e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.437e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.045e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.876e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.844e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.842e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.831e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.827e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.825e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.822e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.819e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.816e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.813e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.809e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.804e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.799e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.793e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.786e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.777e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.767e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.755e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.740e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.722e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.700e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.671e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.634e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.585e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.519e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.427e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.299e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.114e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.850e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.489e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.053e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.616e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.265e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.026e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.836e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.614e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.337e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.176e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.076e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.552e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 1.173e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 9.357e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 7.142e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.572e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.528e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.760e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.170e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.161e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.859e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.026e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.062e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.490e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.203e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.033e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 8.993e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 7.755e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 6.841e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 6.113e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 6.683e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 3.637e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.499e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 4.898e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 4.431e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 4.247e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.811e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.498e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 3.325e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 3.145e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.986e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.839e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.706e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.582e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.468e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.507e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 800, Loss: 3.735e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.467e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.607e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.124e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.149e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.352e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.993e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.845e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.747e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.662e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.596e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.536e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.479e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.424e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.373e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.324e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.277e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.233e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.191e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.172e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.495e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 6.268e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.207e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 5.889e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 3.248e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.203e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.182e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.117e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.001e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 9.792e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 9.432e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 9.145e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 8.901e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 8.664e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 8.440e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 8.227e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 8.024e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 7.832e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 7.648e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 7.472e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 7.304e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 7.144e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 6.991e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 6.845e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 6.705e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 6.571e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 6.443e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 6.320e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 6.203e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 6.099e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 8.198e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 8.460e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 3.958e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.349e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.353e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.438e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.053e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 6.444e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 6.660e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 6.037e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 5.863e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 5.772e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 5.651e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 5.553e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 5.465e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 5.382e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 5.302e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 5.224e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 5.150e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 5.078e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 5.009e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 4.942e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 4.877e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 4.813e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 4.752e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.692e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.634e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 4.578e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 4.523e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 4.469e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 4.417e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 4.366e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.317e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 4.519e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.352e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 5.831e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 9.357e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 7.548e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.541e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 4.819e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 6.616e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 5.627e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 4.560e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 4.606e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 4.432e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 4.375e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 4.304e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 4.252e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 4.199e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 4.149e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 4.100e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 4.053e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 4.007e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 3.963e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 3.919e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.876e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 3.834e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 3.793e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 3.753e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 3.713e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 3.674e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.636e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 3.598e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.561e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.525e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 3.489e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.453e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 3.418e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 3.384e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 3.350e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 3.316e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 3.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.259e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 5.948e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2040, Loss: 3.189e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2050, Loss: 3.621e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2060, Loss: 3.158e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2070, Loss: 3.171e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2080, Loss: 3.078e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2090, Loss: 3.050e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 3.023e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 2.994e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 2.966e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 2.939e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.912e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 2.886e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 2.859e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 2.833e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.807e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 2.782e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 3.029e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2210, Loss: 2.733e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2220, Loss: 2.756e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2230, Loss: 2.690e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2240, Loss: 2.672e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2250, Loss: 2.643e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2260, Loss: 2.621e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2270, Loss: 2.598e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2280, Loss: 2.576e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 2.554e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 2.533e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 2.511e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 2.489e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 2.468e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 2.447e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 2.431e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 2.971e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2370, Loss: 2.423e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2380, Loss: 2.391e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2390, Loss: 2.382e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2400, Loss: 2.330e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2410, Loss: 2.316e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2420, Loss: 2.293e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2430, Loss: 2.275e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2440, Loss: 2.257e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2450, Loss: 2.238e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 2.220e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 2.202e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 2.184e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 2.167e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 2.149e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 2.131e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 2.114e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 2.100e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 2.482e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2550, Loss: 2.085e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2560, Loss: 2.071e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2570, Loss: 2.056e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2580, Loss: 2.018e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2590, Loss: 2.006e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2600, Loss: 1.987e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2610, Loss: 1.972e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2620, Loss: 1.957e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2630, Loss: 1.942e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2640, Loss: 1.927e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 1.912e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 1.897e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 1.883e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 1.868e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.854e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.839e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 1.825e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.868e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2730, Loss: 1.801e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2740, Loss: 1.787e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2750, Loss: 1.775e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2760, Loss: 1.759e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2770, Loss: 1.747e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2780, Loss: 1.734e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2790, Loss: 1.722e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2800, Loss: 1.709e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2810, Loss: 1.697e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 1.684e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 1.672e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 1.660e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 1.648e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 1.635e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 1.623e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 1.611e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 1.599e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 1.589e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 1.880e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2920, Loss: 1.573e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 2930, Loss: 1.581e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 2940, Loss: 1.559e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 2950, Loss: 1.534e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 2960, Loss: 1.525e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 2970, Loss: 1.513e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 2980, Loss: 1.502e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 2990, Loss: 1.492e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3000, Loss: 1.482e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3010, Loss: 1.471e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 1.461e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 1.451e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 1.441e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 1.431e-06, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 1.421e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.411e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.402e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.480e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3100, Loss: 1.389e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3110, Loss: 1.377e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3120, Loss: 1.370e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3130, Loss: 1.356e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3140, Loss: 1.348e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3150, Loss: 1.339e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3160, Loss: 1.330e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3170, Loss: 1.322e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3180, Loss: 1.313e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3190, Loss: 1.305e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3200, Loss: 1.296e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 1.288e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 1.279e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 1.271e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.263e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.254e-06, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.247e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.257e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3280, Loss: 1.234e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3290, Loss: 1.223e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3300, Loss: 1.217e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3310, Loss: 1.209e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3320, Loss: 1.201e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3330, Loss: 1.194e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3340, Loss: 1.187e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3350, Loss: 1.180e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3360, Loss: 1.173e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3370, Loss: 1.165e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3380, Loss: 1.158e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3390, Loss: 1.151e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3400, Loss: 1.144e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3410, Loss: 1.137e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 1.130e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.123e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.116e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.110e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.104e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.169e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3480, Loss: 1.096e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3490, Loss: 1.086e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3500, Loss: 1.082e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3510, Loss: 1.072e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3520, Loss: 1.066e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3530, Loss: 1.060e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3540, Loss: 1.054e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3550, Loss: 1.048e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3560, Loss: 1.042e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3570, Loss: 1.036e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3580, Loss: 1.030e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3590, Loss: 1.024e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3600, Loss: 1.018e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 1.012e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 1.007e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 1.001e-06, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 9.950e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 9.896e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 1.004e-06, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3670, Loss: 9.807e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3680, Loss: 9.736e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3690, Loss: 9.694e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3700, Loss: 9.634e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3710, Loss: 9.583e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3720, Loss: 9.532e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3730, Loss: 9.482e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3740, Loss: 9.432e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3750, Loss: 9.382e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3760, Loss: 9.332e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3770, Loss: 9.283e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3780, Loss: 9.234e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3790, Loss: 9.185e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 9.136e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 9.088e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 9.039e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 8.991e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 8.943e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 8.896e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 8.923e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3870, Loss: 8.810e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3880, Loss: 8.765e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3890, Loss: 8.724e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3900, Loss: 8.678e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3910, Loss: 8.636e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3920, Loss: 8.593e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3930, Loss: 8.551e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3940, Loss: 8.509e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3950, Loss: 8.468e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3960, Loss: 8.426e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3970, Loss: 8.385e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3980, Loss: 8.344e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 3990, Loss: 8.303e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 8.262e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 8.221e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 8.181e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 8.140e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 8.102e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 8.157e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4060, Loss: 8.036e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4070, Loss: 7.989e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4080, Loss: 7.957e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4090, Loss: 7.917e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4100, Loss: 7.881e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4110, Loss: 7.846e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4120, Loss: 7.810e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4130, Loss: 7.775e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4140, Loss: 7.740e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4150, Loss: 7.705e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4160, Loss: 7.670e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4170, Loss: 7.635e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4180, Loss: 7.600e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 7.566e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 7.531e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 7.497e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 7.463e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 7.429e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 7.399e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 7.702e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4260, Loss: 7.366e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4270, Loss: 7.305e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4280, Loss: 7.288e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4290, Loss: 7.242e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4300, Loss: 7.211e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4310, Loss: 7.180e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4320, Loss: 7.151e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4330, Loss: 7.120e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4340, Loss: 7.091e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4350, Loss: 7.061e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4360, Loss: 7.032e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 7.002e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 6.973e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 6.944e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 6.914e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 6.885e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 6.857e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 6.828e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 6.822e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 8.729e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4460, Loss: 6.935e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4470, Loss: 6.751e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4480, Loss: 6.801e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4490, Loss: 6.682e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4500, Loss: 6.650e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4510, Loss: 6.620e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4520, Loss: 6.593e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4530, Loss: 6.567e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4540, Loss: 6.541e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4550, Loss: 6.516e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4560, Loss: 6.491e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4570, Loss: 6.466e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 6.441e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 6.416e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 6.391e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 6.367e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 6.342e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 6.318e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 6.300e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 6.805e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4660, Loss: 6.302e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4670, Loss: 6.232e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4680, Loss: 6.231e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4690, Loss: 6.185e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4700, Loss: 6.161e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4710, Loss: 6.139e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4720, Loss: 6.116e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4730, Loss: 6.095e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4740, Loss: 6.073e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4750, Loss: 6.052e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4760, Loss: 6.030e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4770, Loss: 6.009e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 5.987e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 5.966e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 5.945e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 5.924e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 5.902e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 5.882e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 5.861e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 5.882e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4860, Loss: 5.825e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4870, Loss: 5.802e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4880, Loss: 5.786e-07, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4890, Loss: 5.765e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4900, Loss: 5.746e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4910, Loss: 5.727e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4920, Loss: 5.709e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4930, Loss: 5.690e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4940, Loss: 5.672e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4950, Loss: 5.653e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4960, Loss: 5.635e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 5.617e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 5.598e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 5.580e-07, Time: 0.03, Learning Rate: 0.00019\n",
            "Training time: 15.3181\n",
            "[1, 256, 128, 64, 32, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 7.768e-02, Time: 1.03, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.962e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.455e-02, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 30, Loss: 8.177e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.030e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 50, Loss: 5.458e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.384e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.208e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.007e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 90, Loss: 4.796e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 4.538e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 110, Loss: 4.253e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 120, Loss: 3.947e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 130, Loss: 3.638e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 140, Loss: 3.348e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 3.094e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 160, Loss: 2.884e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 170, Loss: 2.708e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.543e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 2.362e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 200, Loss: 2.151e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.914e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.667e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.424e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.191e-03, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 250, Loss: 9.826e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 8.088e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 6.708e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.644e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 290, Loss: 4.825e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.179e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 310, Loss: 3.649e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 3.198e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.805e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 340, Loss: 2.459e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 350, Loss: 2.154e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.887e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.654e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.454e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.282e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.137e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.014e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 420, Loss: 9.106e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 430, Loss: 8.236e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 440, Loss: 7.503e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.883e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 6.356e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.903e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.510e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.166e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.862e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.589e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.343e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.117e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.938e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.208e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 560, Loss: 3.704e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 4.601e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.316e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.205e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.028e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 610, Loss: 2.849e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.717e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.598e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.486e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.378e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.276e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.178e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.085e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.997e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.912e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.831e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.754e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.681e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.801e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.037e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.079e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 770, Loss: 3.304e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.872e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.429e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.383e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.271e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.206e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.165e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.120e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.078e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.039e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.002e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 9.667e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 9.331e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 900, Loss: 9.010e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 910, Loss: 8.704e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 920, Loss: 8.412e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 8.134e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 940, Loss: 7.868e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 7.614e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 7.372e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 7.142e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 6.967e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.211e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.680e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.806e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.741e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 7.171e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 7.062e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 6.887e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 6.051e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 5.682e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 5.496e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 5.350e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 5.214e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 5.083e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 4.961e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 4.845e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 4.734e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 4.627e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 4.525e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 4.426e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 4.332e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 4.241e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 4.154e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 4.070e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.989e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.915e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 5.185e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 4.548e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.276e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.205e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.866e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 3.777e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 5.649e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 3.651e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 3.829e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 3.541e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 3.465e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 3.412e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 3.350e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 3.294e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.241e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 3.191e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 3.142e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 3.095e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 3.050e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.005e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.962e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.920e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.879e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.839e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.800e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.762e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.725e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.689e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.654e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.620e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 2.586e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.562e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.176e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 3.685e-04, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.455e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 3.788e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.085e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.347e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.407e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.718e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.474e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.481e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.431e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.384e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 2.349e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 2.319e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.291e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 2.265e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.239e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.215e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 2.190e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 2.166e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.143e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 2.120e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 2.097e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 2.075e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 2.053e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.032e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.011e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.990e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.970e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.950e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.931e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.990e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.463e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 2.617e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.148e-05, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.149e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.062e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.481e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 2.114e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 2.195e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 2.043e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.887e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.868e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.850e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.829e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.810e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.792e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.775e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.759e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.742e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.727e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.711e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.696e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.680e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.666e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.651e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.637e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.623e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.609e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.595e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.582e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.568e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.555e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 1.543e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 1.530e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 1.518e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 1.512e-06, Time: 0.02, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 2.999e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.494e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.664e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.512e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.471e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.445e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.437e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.424e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.413e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.403e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.394e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.385e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.376e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 1.367e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.358e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 1.350e-06, Time: 0.02, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 1.373e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.338e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.325e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.318e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.310e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.302e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 1.295e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 1.288e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 1.281e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 1.274e-06, Time: 0.02, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 1.267e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 1.260e-06, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 1.253e-06, Time: 0.09, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 1.246e-06, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 1.240e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 1.233e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 1.227e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 1.220e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 1.217e-06, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 1.398e-06, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.223e-06, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.199e-06, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.201e-06, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.187e-06, Time: 0.08, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.181e-06, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.175e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.169e-06, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.164e-06, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.159e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.154e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.148e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.143e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.138e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.133e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.128e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.123e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.119e-06, Time: 0.02, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.115e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 1.234e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.117e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.103e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.103e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.093e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.089e-06, Time: 0.02, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.084e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.080e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.076e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.072e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.068e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.063e-06, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.060e-06, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.056e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.052e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.048e-06, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 1.044e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 1.040e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 1.039e-06, Time: 0.08, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 1.199e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.049e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.027e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.030e-06, Time: 0.08, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.021e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.016e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.012e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.009e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.006e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.002e-06, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 9.990e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 9.958e-07, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 9.926e-07, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 9.893e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 9.861e-07, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 9.830e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 9.797e-07, Time: 0.02, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 9.765e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 9.735e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 9.734e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 9.680e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 9.646e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 9.619e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 9.590e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 9.562e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 9.534e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 9.506e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 9.479e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 9.451e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 9.424e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 9.396e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 9.369e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 9.341e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 9.314e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 9.286e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 9.260e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 9.232e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 9.205e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 9.178e-07, Time: 0.02, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 9.186e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 9.132e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 9.103e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 9.080e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 9.055e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 9.030e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 9.006e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 8.982e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 8.958e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 8.934e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 8.910e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 8.886e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 8.862e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 8.838e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 8.815e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 8.791e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 8.767e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 8.743e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 8.719e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 8.696e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 8.693e-07, Time: 0.02, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 1.097e-06, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 8.783e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 8.690e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 8.722e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 8.567e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 8.557e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 8.520e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 8.501e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 8.477e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 8.455e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 8.434e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 8.413e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 8.391e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 8.370e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 8.349e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 8.327e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 8.306e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 8.284e-07, Time: 0.02, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 8.264e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 8.296e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 8.231e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 8.203e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 8.185e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 8.165e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 8.144e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 8.125e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 8.105e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 8.086e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 8.066e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 8.047e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 8.027e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 8.007e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 7.988e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 7.968e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 7.949e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 7.929e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 7.910e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 7.890e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 7.871e-07, Time: 0.02, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 7.877e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 7.838e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 7.815e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 7.798e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 7.780e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 7.762e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 7.744e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 7.726e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 7.707e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 7.690e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 7.671e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 7.653e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 7.635e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 7.617e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 7.599e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 7.580e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 7.562e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 7.544e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 7.526e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 7.508e-07, Time: 0.02, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 7.501e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 8.080e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 7.544e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 7.438e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 7.444e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 7.414e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 7.388e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 7.372e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 7.354e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 7.337e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 7.320e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 7.303e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 7.286e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 7.269e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 7.252e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 7.235e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 7.218e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 7.201e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 7.184e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 7.167e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 7.150e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 7.133e-07, Time: 0.02, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 7.186e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 7.107e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 7.085e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 7.071e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 7.053e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 7.037e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 7.021e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 7.005e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 6.989e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 6.973e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 6.957e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 6.941e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 6.925e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 6.909e-07, Time: 0.02, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 6.893e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 6.877e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 6.861e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 6.845e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 6.828e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 6.812e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 6.799e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 7.065e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 6.786e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 6.758e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 6.751e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 6.721e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 6.707e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 6.690e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 6.676e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 6.660e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 6.646e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 6.630e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 6.615e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 6.600e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 6.584e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 6.569e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 6.555e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 6.539e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 6.523e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 6.508e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 6.493e-07, Time: 0.02, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 6.493e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 6.465e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 6.449e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 6.436e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 6.421e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 6.407e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 6.393e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 6.378e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 6.364e-07, Time: 0.02, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 6.350e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 6.336e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 6.322e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 6.307e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 6.293e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 6.278e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 6.264e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 6.249e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 6.235e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 6.220e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 6.207e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 6.261e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 6.191e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 16.2879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CSol[idx] = predict_CSol[idx].reshape(exact_C.shape)\n",
        "\n",
        "  error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "  print('Error C Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_C) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWk-J4e3TVEu",
        "outputId": "e853a63d-0464-4e4c-9d17-429746cebaec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Sol [1, 64, 64, 64, 1] : 3.236310e-03\n",
            "Error C Sol [1, 128, 128, 128, 1] : 2.970893e-03\n",
            "Error C Sol [1, 256, 256, 256, 1] : 3.996123e-03\n",
            "Error C Sol [1, 64, 64, 64, 64, 1] : 2.077416e-03\n",
            "Error C Sol [1, 256, 256, 256, 256, 1] : 1.554187e-03\n",
            "Error C Sol [1, 128, 128, 128, 128, 1] : 3.143308e-03\n",
            "Error C Sol [1, 128, 128, 64, 64, 1] : 1.841275e-03\n",
            "Error C Sol [1, 256, 128, 64, 32, 1] : 1.855681e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [2,3,6,7]:\n",
        "  plt.plot(t, predict_CSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_C.flatten(), 'c', label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "fcwee-4zWQss",
        "outputId": "4252cf41-0490-4f5c-d68e-35371bba8566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc1fd7bbdf0>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1Te8EttyCn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the PINN with the differenciation and its solution"
      ],
      "metadata": {
        "id": "5xnbiov17XUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,1],[1,128,128,128,1],[1,256,256,256,1],[1,64,64,64,64,1],[1,256,256,256,256,1],[1,128,128,128,128,1],[1,128,128,64,64,1],[1,256,128,64,32,1]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = ReactionPINN_DiffSol(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06287fe-2dca-49b9-9783-94f9d8f6c99b",
        "id": "TMxXy0ND7XUI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.357e-02, Loss_0: 2.143e-04, Loss_r: 3.335e-02, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.211e-02, Loss_0: 4.935e-05, Loss_r: 3.206e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.172e-02, Loss_0: 7.672e-06, Loss_r: 3.171e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.088e-02, Loss_0: 3.657e-06, Loss_r: 3.087e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.853e-02, Loss_0: 2.002e-06, Loss_r: 2.852e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 2.352e-02, Loss_0: 1.249e-05, Loss_r: 2.351e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.706e-02, Loss_0: 1.369e-06, Loss_r: 1.705e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.043e-02, Loss_0: 1.168e-04, Loss_r: 1.032e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.113e-03, Loss_0: 1.270e-05, Loss_r: 5.100e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 2.732e-03, Loss_0: 5.467e-05, Loss_r: 2.678e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.463e-03, Loss_0: 3.554e-06, Loss_r: 1.460e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 6.572e-04, Loss_0: 4.116e-06, Loss_r: 6.531e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 3.450e-04, Loss_0: 1.587e-05, Loss_r: 3.291e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.723e-04, Loss_0: 4.350e-07, Loss_r: 1.718e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 2.099e-04, Loss_0: 9.382e-05, Loss_r: 1.161e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.583e-04, Loss_0: 6.701e-05, Loss_r: 9.127e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 7.227e-05, Loss_0: 1.820e-06, Loss_r: 7.045e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 6.090e-05, Loss_0: 6.130e-06, Loss_r: 5.477e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 4.310e-05, Loss_0: 8.615e-07, Loss_r: 4.224e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 3.436e-05, Loss_0: 1.009e-06, Loss_r: 3.335e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 200, Loss: 2.675e-05, Loss_0: 9.371e-09, Loss_r: 2.674e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 2.205e-05, Loss_0: 3.181e-07, Loss_r: 2.173e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.794e-05, Loss_0: 4.972e-08, Loss_r: 1.789e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.996e-05, Loss_0: 4.902e-06, Loss_r: 1.506e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 2.081e-05, Loss_0: 7.759e-06, Loss_r: 1.305e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.437e-05, Loss_0: 2.504e-06, Loss_r: 1.187e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 2.474e-05, Loss_0: 1.383e-05, Loss_r: 1.091e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.612e-05, Loss_0: 6.301e-06, Loss_r: 9.815e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.138e-05, Loss_0: 2.363e-06, Loss_r: 9.015e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 9.249e-06, Loss_0: 8.624e-07, Loss_r: 8.387e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 7.883e-06, Loss_0: 1.742e-08, Loss_r: 7.866e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 7.533e-06, Loss_0: 6.923e-08, Loss_r: 7.464e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 7.135e-06, Loss_0: 4.904e-09, Loss_r: 7.130e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 7.071e-06, Loss_0: 2.222e-07, Loss_r: 6.849e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.976e-05, Loss_0: 1.288e-05, Loss_r: 6.879e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 7.187e-06, Loss_0: 7.601e-07, Loss_r: 6.427e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 7.150e-06, Loss_0: 7.951e-07, Loss_r: 6.355e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 6.186e-06, Loss_0: 1.752e-08, Loss_r: 6.169e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.124e-06, Loss_0: 1.286e-07, Loss_r: 5.995e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 6.747e-06, Loss_0: 9.041e-07, Loss_r: 5.843e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 6.494e-06, Loss_0: 8.180e-07, Loss_r: 5.676e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.547e-06, Loss_0: 4.951e-08, Loss_r: 5.497e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.363e-06, Loss_0: 1.395e-08, Loss_r: 5.349e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.208e-06, Loss_0: 7.132e-11, Loss_r: 5.208e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.143e-06, Loss_0: 6.778e-08, Loss_r: 5.076e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.396e-06, Loss_0: 4.391e-07, Loss_r: 4.957e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.497e-05, Loss_0: 4.910e-05, Loss_r: 5.866e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 9.725e-05, Loss_0: 9.108e-05, Loss_r: 6.174e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.880e-05, Loss_0: 2.351e-05, Loss_r: 5.287e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.312e-05, Loss_0: 8.251e-06, Loss_r: 4.873e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 6.610e-06, Loss_0: 2.011e-06, Loss_r: 4.599e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.487e-06, Loss_0: 2.571e-08, Loss_r: 4.461e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.824e-06, Loss_0: 4.399e-07, Loss_r: 4.384e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.405e-06, Loss_0: 1.238e-07, Loss_r: 4.281e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.295e-06, Loss_0: 1.141e-07, Loss_r: 4.181e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 4.130e-06, Loss_0: 3.068e-08, Loss_r: 4.099e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 4.037e-06, Loss_0: 2.435e-08, Loss_r: 4.012e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.936e-06, Loss_0: 1.955e-09, Loss_r: 3.934e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.899e-06, Loss_0: 3.993e-08, Loss_r: 3.859e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 6.191e-06, Loss_0: 2.349e-06, Loss_r: 3.842e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 2.739e-04, Loss_0: 2.642e-04, Loss_r: 9.656e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.602e-05, Loss_0: 1.210e-05, Loss_r: 3.920e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.057e-05, Loss_0: 6.753e-06, Loss_r: 3.821e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 8.102e-06, Loss_0: 4.324e-06, Loss_r: 3.778e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 5.291e-06, Loss_0: 1.604e-06, Loss_r: 3.687e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 3.822e-06, Loss_0: 2.221e-07, Loss_r: 3.600e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 3.564e-06, Loss_0: 3.192e-08, Loss_r: 3.532e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.671e-06, Loss_0: 1.947e-07, Loss_r: 3.476e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.437e-06, Loss_0: 1.481e-08, Loss_r: 3.422e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.403e-06, Loss_0: 2.993e-08, Loss_r: 3.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.327e-06, Loss_0: 7.227e-09, Loss_r: 3.320e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.274e-06, Loss_0: 1.912e-09, Loss_r: 3.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 3.228e-06, Loss_0: 2.665e-09, Loss_r: 3.225e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 3.180e-06, Loss_0: 7.385e-12, Loss_r: 3.180e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 3.138e-06, Loss_0: 1.184e-09, Loss_r: 3.137e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 3.149e-06, Loss_0: 5.210e-08, Loss_r: 3.097e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.288e-05, Loss_0: 9.619e-06, Loss_r: 3.261e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 9.545e-05, Loss_0: 9.024e-05, Loss_r: 5.217e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.324e-05, Loss_0: 2.895e-05, Loss_r: 4.293e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 3.309e-06, Loss_0: 1.311e-07, Loss_r: 3.178e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 8.862e-06, Loss_0: 5.745e-06, Loss_r: 3.117e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.858e-06, Loss_0: 3.766e-06, Loss_r: 3.092e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 4.320e-06, Loss_0: 1.285e-06, Loss_r: 3.036e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.421e-06, Loss_0: 4.284e-07, Loss_r: 2.993e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.131e-06, Loss_0: 1.760e-07, Loss_r: 2.955e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.988e-06, Loss_0: 6.889e-08, Loss_r: 2.919e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.894e-06, Loss_0: 9.248e-09, Loss_r: 2.885e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.854e-06, Loss_0: 1.928e-09, Loss_r: 2.852e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.822e-06, Loss_0: 2.759e-09, Loss_r: 2.820e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.788e-06, Loss_0: 8.077e-10, Loss_r: 2.787e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.756e-06, Loss_0: 4.395e-11, Loss_r: 2.756e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.726e-06, Loss_0: 2.730e-11, Loss_r: 2.726e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.696e-06, Loss_0: 6.211e-11, Loss_r: 2.696e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 2.667e-06, Loss_0: 2.228e-11, Loss_r: 2.667e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.638e-06, Loss_0: 5.846e-11, Loss_r: 2.638e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 2.610e-06, Loss_0: 2.782e-10, Loss_r: 2.610e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.592e-06, Loss_0: 8.875e-09, Loss_r: 2.583e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 3.687e-06, Loss_0: 1.106e-06, Loss_r: 2.581e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.770e-04, Loss_0: 2.686e-04, Loss_r: 8.372e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.353e-05, Loss_0: 1.077e-05, Loss_r: 2.757e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 5.290e-06, Loss_0: 2.591e-06, Loss_r: 2.700e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.548e-05, Loss_0: 1.262e-05, Loss_r: 2.863e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 8.703e-06, Loss_0: 5.983e-06, Loss_r: 2.720e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 3.715e-06, Loss_0: 1.106e-06, Loss_r: 2.609e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.681e-06, Loss_0: 1.186e-07, Loss_r: 2.563e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 2.549e-06, Loss_0: 1.657e-08, Loss_r: 2.533e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 2.517e-06, Loss_0: 1.187e-08, Loss_r: 2.505e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.495e-06, Loss_0: 1.604e-08, Loss_r: 2.479e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.466e-06, Loss_0: 1.335e-08, Loss_r: 2.453e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.430e-06, Loss_0: 2.845e-09, Loss_r: 2.427e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 2.402e-06, Loss_0: 3.543e-10, Loss_r: 2.402e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.378e-06, Loss_0: 7.764e-10, Loss_r: 2.377e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.353e-06, Loss_0: 1.130e-10, Loss_r: 2.353e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.329e-06, Loss_0: 7.558e-12, Loss_r: 2.329e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.305e-06, Loss_0: 2.076e-11, Loss_r: 2.305e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.282e-06, Loss_0: 4.597e-13, Loss_r: 2.282e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.259e-06, Loss_0: 2.817e-12, Loss_r: 2.259e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.236e-06, Loss_0: 8.069e-12, Loss_r: 2.236e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.214e-06, Loss_0: 9.560e-14, Loss_r: 2.214e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.191e-06, Loss_0: 2.160e-12, Loss_r: 2.191e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 2.170e-06, Loss_0: 1.415e-10, Loss_r: 2.170e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.169e-06, Loss_0: 2.073e-08, Loss_r: 2.149e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 9.315e-06, Loss_0: 7.042e-06, Loss_r: 2.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 2.874e-04, Loss_0: 2.784e-04, Loss_r: 9.021e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 4.750e-05, Loss_0: 4.320e-05, Loss_r: 4.299e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.775e-05, Loss_0: 1.542e-05, Loss_r: 2.336e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.421e-05, Loss_0: 1.187e-05, Loss_r: 2.341e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.913e-06, Loss_0: 6.396e-07, Loss_r: 2.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 4.195e-06, Loss_0: 1.928e-06, Loss_r: 2.267e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.291e-06, Loss_0: 9.665e-08, Loss_r: 2.195e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.236e-06, Loss_0: 6.848e-08, Loss_r: 2.167e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.228e-06, Loss_0: 8.361e-08, Loss_r: 2.144e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.152e-06, Loss_0: 3.145e-08, Loss_r: 2.121e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.107e-06, Loss_0: 9.020e-09, Loss_r: 2.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.080e-06, Loss_0: 3.041e-09, Loss_r: 2.077e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.057e-06, Loss_0: 1.391e-09, Loss_r: 2.055e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 2.035e-06, Loss_0: 6.751e-10, Loss_r: 2.034e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 2.014e-06, Loss_0: 2.302e-10, Loss_r: 2.014e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.993e-06, Loss_0: 3.011e-11, Loss_r: 1.993e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.973e-06, Loss_0: 1.461e-12, Loss_r: 1.973e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.953e-06, Loss_0: 3.894e-13, Loss_r: 1.953e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.933e-06, Loss_0: 3.151e-12, Loss_r: 1.933e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.913e-06, Loss_0: 7.325e-12, Loss_r: 1.913e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.894e-06, Loss_0: 1.859e-12, Loss_r: 1.894e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.875e-06, Loss_0: 7.942e-12, Loss_r: 1.875e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.856e-06, Loss_0: 2.198e-12, Loss_r: 1.856e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.837e-06, Loss_0: 7.057e-13, Loss_r: 1.837e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.818e-06, Loss_0: 2.237e-11, Loss_r: 1.818e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.800e-06, Loss_0: 2.032e-11, Loss_r: 1.800e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.782e-06, Loss_0: 6.626e-12, Loss_r: 1.782e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.763e-06, Loss_0: 3.486e-13, Loss_r: 1.763e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.746e-06, Loss_0: 2.728e-10, Loss_r: 1.746e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.787e-06, Loss_0: 5.716e-08, Loss_r: 1.730e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.479e-05, Loss_0: 2.262e-05, Loss_r: 2.169e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 3.275e-06, Loss_0: 1.386e-06, Loss_r: 1.889e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.163e-05, Loss_0: 3.823e-05, Loss_r: 3.402e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 3.603e-05, Loss_0: 3.301e-05, Loss_r: 3.026e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 3.158e-06, Loss_0: 1.291e-06, Loss_r: 1.866e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 6.525e-06, Loss_0: 4.604e-06, Loss_r: 1.921e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.822e-06, Loss_0: 9.649e-10, Loss_r: 1.821e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.320e-06, Loss_0: 5.207e-07, Loss_r: 1.800e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.927e-06, Loss_0: 1.500e-07, Loss_r: 1.777e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.759e-06, Loss_0: 1.392e-09, Loss_r: 1.758e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.747e-06, Loss_0: 6.992e-09, Loss_r: 1.740e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.728e-06, Loss_0: 6.873e-09, Loss_r: 1.721e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.706e-06, Loss_0: 3.137e-09, Loss_r: 1.702e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.685e-06, Loss_0: 1.087e-09, Loss_r: 1.684e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.667e-06, Loss_0: 3.310e-10, Loss_r: 1.667e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.649e-06, Loss_0: 6.623e-11, Loss_r: 1.649e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.632e-06, Loss_0: 3.972e-14, Loss_r: 1.632e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.615e-06, Loss_0: 1.824e-11, Loss_r: 1.615e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.598e-06, Loss_0: 4.032e-11, Loss_r: 1.598e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.582e-06, Loss_0: 4.414e-12, Loss_r: 1.582e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.566e-06, Loss_0: 2.052e-12, Loss_r: 1.566e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.549e-06, Loss_0: 2.886e-12, Loss_r: 1.549e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.533e-06, Loss_0: 3.197e-12, Loss_r: 1.533e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.518e-06, Loss_0: 1.679e-15, Loss_r: 1.518e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.502e-06, Loss_0: 2.327e-12, Loss_r: 1.502e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.487e-06, Loss_0: 3.553e-12, Loss_r: 1.487e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.471e-06, Loss_0: 1.498e-12, Loss_r: 1.471e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.456e-06, Loss_0: 2.946e-11, Loss_r: 1.456e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.441e-06, Loss_0: 1.311e-11, Loss_r: 1.441e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.426e-06, Loss_0: 2.873e-14, Loss_r: 1.426e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.412e-06, Loss_0: 5.113e-11, Loss_r: 1.412e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.399e-06, Loss_0: 1.747e-09, Loss_r: 1.397e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.604e-06, Loss_0: 2.160e-07, Loss_r: 1.388e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 5.756e-05, Loss_0: 5.506e-05, Loss_r: 2.498e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 9.746e-05, Loss_0: 9.419e-05, Loss_r: 3.271e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 4.694e-05, Loss_0: 4.434e-05, Loss_r: 2.598e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 3.049e-05, Loss_0: 2.858e-05, Loss_r: 1.909e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.477e-06, Loss_0: 3.972e-06, Loss_r: 1.505e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.558e-06, Loss_0: 6.919e-08, Loss_r: 1.489e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.172e-06, Loss_0: 7.032e-07, Loss_r: 1.469e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.893e-06, Loss_0: 4.447e-07, Loss_r: 1.449e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.593e-06, Loss_0: 1.626e-07, Loss_r: 1.431e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.467e-06, Loss_0: 5.341e-08, Loss_r: 1.413e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.416e-06, Loss_0: 1.966e-08, Loss_r: 1.396e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.388e-06, Loss_0: 7.664e-09, Loss_r: 1.380e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.368e-06, Loss_0: 2.146e-09, Loss_r: 1.365e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.351e-06, Loss_0: 1.845e-10, Loss_r: 1.351e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.337e-06, Loss_0: 7.297e-11, Loss_r: 1.336e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.323e-06, Loss_0: 1.619e-10, Loss_r: 1.322e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.309e-06, Loss_0: 1.559e-11, Loss_r: 1.309e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.295e-06, Loss_0: 2.036e-12, Loss_r: 1.295e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.282e-06, Loss_0: 7.599e-12, Loss_r: 1.282e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.268e-06, Loss_0: 3.053e-12, Loss_r: 1.268e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.255e-06, Loss_0: 1.729e-12, Loss_r: 1.255e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.242e-06, Loss_0: 5.517e-12, Loss_r: 1.242e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.229e-06, Loss_0: 4.399e-12, Loss_r: 1.229e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.217e-06, Loss_0: 3.854e-12, Loss_r: 1.217e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.204e-06, Loss_0: 5.473e-12, Loss_r: 1.204e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.192e-06, Loss_0: 2.720e-15, Loss_r: 1.192e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.180e-06, Loss_0: 2.740e-13, Loss_r: 1.180e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.167e-06, Loss_0: 3.917e-13, Loss_r: 1.167e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.155e-06, Loss_0: 7.785e-12, Loss_r: 1.155e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.144e-06, Loss_0: 2.418e-13, Loss_r: 1.144e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.133e-06, Loss_0: 1.019e-09, Loss_r: 1.132e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.575e-06, Loss_0: 4.444e-07, Loss_r: 1.131e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 1.112e-06, Loss_0: 2.484e-09, Loss_r: 1.110e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 1.186e-06, Loss_0: 8.538e-08, Loss_r: 1.101e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.090e-06, Loss_0: 7.317e-10, Loss_r: 1.090e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 1.091e-06, Loss_0: 1.097e-08, Loss_r: 1.080e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.071e-06, Loss_0: 1.523e-09, Loss_r: 1.070e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.060e-06, Loss_0: 4.817e-11, Loss_r: 1.060e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.050e-06, Loss_0: 1.776e-10, Loss_r: 1.050e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.041e-06, Loss_0: 1.636e-10, Loss_r: 1.041e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.031e-06, Loss_0: 1.276e-11, Loss_r: 1.031e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.022e-06, Loss_0: 3.312e-12, Loss_r: 1.022e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.012e-06, Loss_0: 5.892e-11, Loss_r: 1.012e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.003e-06, Loss_0: 4.549e-12, Loss_r: 1.003e-06, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 9.939e-07, Loss_0: 1.352e-10, Loss_r: 9.938e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 9.874e-07, Loss_0: 2.597e-09, Loss_r: 9.848e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.104e-06, Loss_0: 1.247e-07, Loss_r: 9.789e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 9.859e-07, Loss_0: 1.770e-08, Loss_r: 9.682e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 9.598e-07, Loss_0: 3.573e-10, Loss_r: 9.594e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 9.571e-07, Loss_0: 5.599e-09, Loss_r: 9.515e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 9.451e-07, Loss_0: 1.541e-09, Loss_r: 9.436e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 9.361e-07, Loss_0: 1.769e-10, Loss_r: 9.359e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 9.284e-07, Loss_0: 1.965e-10, Loss_r: 9.282e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 9.205e-07, Loss_0: 9.742e-11, Loss_r: 9.204e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 9.129e-07, Loss_0: 1.061e-12, Loss_r: 9.128e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 9.053e-07, Loss_0: 3.508e-13, Loss_r: 9.053e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 8.978e-07, Loss_0: 1.099e-11, Loss_r: 8.978e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 8.904e-07, Loss_0: 1.652e-12, Loss_r: 8.904e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 8.830e-07, Loss_0: 4.114e-12, Loss_r: 8.830e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 8.758e-07, Loss_0: 3.825e-14, Loss_r: 8.758e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 8.685e-07, Loss_0: 7.569e-12, Loss_r: 8.685e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 8.614e-07, Loss_0: 3.087e-11, Loss_r: 8.613e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 8.543e-07, Loss_0: 6.994e-11, Loss_r: 8.542e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 8.478e-07, Loss_0: 6.293e-10, Loss_r: 8.472e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 8.644e-07, Loss_0: 2.404e-08, Loss_r: 8.403e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.620e-06, Loss_0: 2.734e-06, Loss_r: 8.859e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.005e-06, Loss_0: 1.748e-07, Loss_r: 8.302e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 9.330e-07, Loss_0: 1.084e-07, Loss_r: 8.246e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 9.835e-07, Loss_0: 1.636e-07, Loss_r: 8.199e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 8.121e-07, Loss_0: 2.310e-09, Loss_r: 8.097e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 8.253e-07, Loss_0: 2.153e-08, Loss_r: 8.037e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 7.977e-07, Loss_0: 2.151e-13, Loss_r: 7.977e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 7.943e-07, Loss_0: 2.384e-09, Loss_r: 7.920e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 7.871e-07, Loss_0: 1.206e-09, Loss_r: 7.859e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 7.803e-07, Loss_0: 1.826e-10, Loss_r: 7.801e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 7.744e-07, Loss_0: 6.373e-11, Loss_r: 7.743e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 7.686e-07, Loss_0: 1.924e-11, Loss_r: 7.686e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 7.630e-07, Loss_0: 4.897e-11, Loss_r: 7.629e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 7.573e-07, Loss_0: 3.291e-12, Loss_r: 7.573e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 7.517e-07, Loss_0: 4.039e-12, Loss_r: 7.517e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 7.462e-07, Loss_0: 5.565e-11, Loss_r: 7.462e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 7.410e-07, Loss_0: 3.217e-10, Loss_r: 7.406e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 7.412e-07, Loss_0: 6.012e-09, Loss_r: 7.352e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.102e-06, Loss_0: 3.660e-07, Loss_r: 7.364e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 7.688e-07, Loss_0: 4.329e-08, Loss_r: 7.255e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 7.238e-07, Loss_0: 3.386e-09, Loss_r: 7.204e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 7.348e-07, Loss_0: 1.863e-08, Loss_r: 7.162e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 7.139e-07, Loss_0: 3.010e-09, Loss_r: 7.109e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 7.074e-07, Loss_0: 1.405e-09, Loss_r: 7.060e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 7.019e-07, Loss_0: 5.408e-10, Loss_r: 7.014e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 6.970e-07, Loss_0: 2.211e-10, Loss_r: 6.968e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 6.922e-07, Loss_0: 3.497e-12, Loss_r: 6.922e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 6.876e-07, Loss_0: 2.076e-11, Loss_r: 6.876e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 6.831e-07, Loss_0: 1.965e-11, Loss_r: 6.831e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 6.786e-07, Loss_0: 1.216e-11, Loss_r: 6.786e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 6.741e-07, Loss_0: 1.159e-12, Loss_r: 6.741e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 6.696e-07, Loss_0: 4.749e-14, Loss_r: 6.696e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 6.653e-07, Loss_0: 5.854e-12, Loss_r: 6.653e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 6.609e-07, Loss_0: 7.234e-12, Loss_r: 6.609e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 6.565e-07, Loss_0: 6.156e-12, Loss_r: 6.565e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 6.525e-07, Loss_0: 2.637e-10, Loss_r: 6.522e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 6.580e-07, Loss_0: 9.999e-09, Loss_r: 6.480e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.440e-06, Loss_0: 7.808e-07, Loss_r: 6.594e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 7.183e-07, Loss_0: 7.704e-08, Loss_r: 6.412e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 6.498e-07, Loss_0: 1.305e-08, Loss_r: 6.368e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 6.768e-07, Loss_0: 4.285e-08, Loss_r: 6.339e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 6.336e-07, Loss_0: 4.457e-09, Loss_r: 6.291e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 6.291e-07, Loss_0: 3.903e-09, Loss_r: 6.252e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 6.222e-07, Loss_0: 6.790e-10, Loss_r: 6.215e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 6.188e-07, Loss_0: 8.676e-10, Loss_r: 6.180e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 6.143e-07, Loss_0: 4.260e-11, Loss_r: 6.142e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 6.107e-07, Loss_0: 1.736e-11, Loss_r: 6.106e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 6.071e-07, Loss_0: 3.332e-12, Loss_r: 6.071e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 6.036e-07, Loss_0: 6.927e-12, Loss_r: 6.035e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 6.000e-07, Loss_0: 2.256e-13, Loss_r: 6.000e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 5.965e-07, Loss_0: 7.806e-14, Loss_r: 5.965e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 5.930e-07, Loss_0: 1.458e-11, Loss_r: 5.929e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 5.895e-07, Loss_0: 1.319e-13, Loss_r: 5.895e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 5.861e-07, Loss_0: 1.916e-13, Loss_r: 5.861e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 5.827e-07, Loss_0: 8.971e-12, Loss_r: 5.827e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 5.793e-07, Loss_0: 3.185e-11, Loss_r: 5.793e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 5.779e-07, Loss_0: 1.844e-09, Loss_r: 5.760e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 8.772e-07, Loss_0: 2.970e-07, Loss_r: 5.802e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 5.793e-07, Loss_0: 9.308e-09, Loss_r: 5.700e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 5.904e-07, Loss_0: 2.340e-08, Loss_r: 5.670e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 5.800e-07, Loss_0: 1.616e-08, Loss_r: 5.639e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 5.612e-07, Loss_0: 3.170e-10, Loss_r: 5.608e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 5.601e-07, Loss_0: 2.164e-09, Loss_r: 5.580e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 5.553e-07, Loss_0: 2.973e-10, Loss_r: 5.550e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 5.521e-07, Loss_0: 4.678e-11, Loss_r: 5.521e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 5.493e-07, Loss_0: 7.460e-11, Loss_r: 5.492e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 5.464e-07, Loss_0: 2.985e-11, Loss_r: 5.463e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 5.435e-07, Loss_0: 3.617e-12, Loss_r: 5.435e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 5.407e-07, Loss_0: 4.092e-12, Loss_r: 5.407e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 5.379e-07, Loss_0: 5.756e-12, Loss_r: 5.379e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 5.351e-07, Loss_0: 8.617e-12, Loss_r: 5.351e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 5.323e-07, Loss_0: 2.483e-12, Loss_r: 5.323e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 5.295e-07, Loss_0: 6.717e-13, Loss_r: 5.295e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 5.269e-07, Loss_0: 5.632e-11, Loss_r: 5.268e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 5.251e-07, Loss_0: 9.735e-10, Loss_r: 5.242e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 5.794e-07, Loss_0: 5.631e-08, Loss_r: 5.231e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 5.265e-07, Loss_0: 7.280e-09, Loss_r: 5.192e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 5.168e-07, Loss_0: 2.016e-10, Loss_r: 5.166e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 5.167e-07, Loss_0: 2.582e-09, Loss_r: 5.141e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 5.125e-07, Loss_0: 7.933e-10, Loss_r: 5.117e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 5.095e-07, Loss_0: 6.158e-11, Loss_r: 5.094e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 5.071e-07, Loss_0: 9.617e-11, Loss_r: 5.070e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 5.047e-07, Loss_0: 1.620e-11, Loss_r: 5.047e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 5.024e-07, Loss_0: 7.254e-12, Loss_r: 5.024e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 5.001e-07, Loss_0: 6.455e-12, Loss_r: 5.000e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 4.977e-07, Loss_0: 6.820e-12, Loss_r: 4.977e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 4.954e-07, Loss_0: 4.604e-12, Loss_r: 4.954e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 4.931e-07, Loss_0: 1.377e-12, Loss_r: 4.931e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 4.908e-07, Loss_0: 4.122e-12, Loss_r: 4.908e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 4.886e-07, Loss_0: 1.411e-11, Loss_r: 4.886e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 4.863e-07, Loss_0: 1.850e-11, Loss_r: 4.863e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 4.840e-07, Loss_0: 2.413e-12, Loss_r: 4.840e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 4.819e-07, Loss_0: 2.896e-11, Loss_r: 4.818e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 4.801e-07, Loss_0: 4.840e-10, Loss_r: 4.797e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 4.966e-07, Loss_0: 1.855e-08, Loss_r: 4.780e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 2.067e-06, Loss_0: 1.552e-06, Loss_r: 5.143e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 6.488e-07, Loss_0: 1.709e-07, Loss_r: 4.779e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 4.855e-07, Loss_0: 1.382e-08, Loss_r: 4.717e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 5.489e-07, Loss_0: 7.785e-08, Loss_r: 4.711e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 4.856e-07, Loss_0: 1.767e-08, Loss_r: 4.679e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 4.688e-07, Loss_0: 2.921e-09, Loss_r: 4.659e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 4.672e-07, Loss_0: 3.263e-09, Loss_r: 4.640e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 4.626e-07, Loss_0: 7.333e-10, Loss_r: 4.619e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 4.601e-07, Loss_0: 1.032e-10, Loss_r: 4.600e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 4.583e-07, Loss_0: 1.926e-10, Loss_r: 4.581e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 4.563e-07, Loss_0: 7.389e-11, Loss_r: 4.562e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 4.543e-07, Loss_0: 9.423e-12, Loss_r: 4.543e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 4.525e-07, Loss_0: 7.154e-12, Loss_r: 4.525e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 4.506e-07, Loss_0: 1.338e-12, Loss_r: 4.506e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 4.487e-07, Loss_0: 2.333e-11, Loss_r: 4.487e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 4.469e-07, Loss_0: 6.208e-13, Loss_r: 4.469e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 4.451e-07, Loss_0: 3.584e-11, Loss_r: 4.451e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 4.433e-07, Loss_0: 6.920e-11, Loss_r: 4.432e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 4.422e-07, Loss_0: 7.364e-10, Loss_r: 4.414e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 4.611e-07, Loss_0: 2.085e-08, Loss_r: 4.402e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 4.423e-07, Loss_0: 4.189e-09, Loss_r: 4.381e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 4.364e-07, Loss_0: 7.110e-11, Loss_r: 4.363e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 4.352e-07, Loss_0: 4.543e-10, Loss_r: 4.347e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 4.336e-07, Loss_0: 4.746e-10, Loss_r: 4.331e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 4.315e-07, Loss_0: 1.392e-11, Loss_r: 4.315e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 4.299e-07, Loss_0: 4.607e-11, Loss_r: 4.299e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 4.283e-07, Loss_0: 2.626e-14, Loss_r: 4.283e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 4.267e-07, Loss_0: 1.875e-11, Loss_r: 4.267e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 4.251e-07, Loss_0: 3.377e-13, Loss_r: 4.251e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 4.236e-07, Loss_0: 7.502e-13, Loss_r: 4.236e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 4.220e-07, Loss_0: 1.208e-12, Loss_r: 4.220e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 4.204e-07, Loss_0: 7.395e-14, Loss_r: 4.204e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 4.189e-07, Loss_0: 2.773e-12, Loss_r: 4.188e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 4.173e-07, Loss_0: 1.252e-13, Loss_r: 4.173e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 4.158e-07, Loss_0: 2.238e-13, Loss_r: 4.158e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 4.142e-07, Loss_0: 4.099e-12, Loss_r: 4.142e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 4.127e-07, Loss_0: 5.587e-12, Loss_r: 4.127e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 4.112e-07, Loss_0: 3.898e-11, Loss_r: 4.111e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 4.102e-07, Loss_0: 5.816e-10, Loss_r: 4.096e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 4.286e-07, Loss_0: 2.013e-08, Loss_r: 4.085e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 4.105e-07, Loss_0: 3.727e-09, Loss_r: 4.068e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 4.054e-07, Loss_0: 2.416e-11, Loss_r: 4.054e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 4.045e-07, Loss_0: 4.727e-10, Loss_r: 4.041e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 4.031e-07, Loss_0: 3.533e-10, Loss_r: 4.027e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 4.013e-07, Loss_0: 4.011e-13, Loss_r: 4.013e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 4.000e-07, Loss_0: 4.681e-11, Loss_r: 4.000e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 3.986e-07, Loss_0: 3.870e-13, Loss_r: 3.986e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 3.973e-07, Loss_0: 6.267e-12, Loss_r: 3.973e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 3.960e-07, Loss_0: 3.638e-12, Loss_r: 3.960e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 3.946e-07, Loss_0: 2.720e-13, Loss_r: 3.946e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 3.933e-07, Loss_0: 7.335e-12, Loss_r: 3.933e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 3.920e-07, Loss_0: 1.809e-12, Loss_r: 3.920e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 3.907e-07, Loss_0: 2.779e-12, Loss_r: 3.907e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 3.894e-07, Loss_0: 4.750e-13, Loss_r: 3.894e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 3.881e-07, Loss_0: 2.761e-12, Loss_r: 3.881e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 3.868e-07, Loss_0: 2.958e-13, Loss_r: 3.868e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 3.854e-07, Loss_0: 2.556e-11, Loss_r: 3.854e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 3.843e-07, Loss_0: 1.247e-10, Loss_r: 3.841e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 3.841e-07, Loss_0: 1.146e-09, Loss_r: 3.829e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 4.253e-07, Loss_0: 4.250e-08, Loss_r: 3.828e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 3.884e-07, Loss_0: 7.718e-09, Loss_r: 3.806e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 3.793e-07, Loss_0: 2.962e-11, Loss_r: 3.793e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 3.792e-07, Loss_0: 1.098e-09, Loss_r: 3.781e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 3.778e-07, Loss_0: 8.157e-10, Loss_r: 3.770e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 3.758e-07, Loss_0: 2.226e-11, Loss_r: 3.758e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 3.748e-07, Loss_0: 9.643e-11, Loss_r: 3.747e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 3.735e-07, Loss_0: 9.676e-14, Loss_r: 3.735e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 3.724e-07, Loss_0: 1.348e-11, Loss_r: 3.724e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 3.712e-07, Loss_0: 1.754e-12, Loss_r: 3.712e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 3.700e-07, Loss_0: 4.931e-13, Loss_r: 3.700e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 3.690e-07, Loss_0: 1.137e-13, Loss_r: 3.690e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 3.678e-07, Loss_0: 4.509e-14, Loss_r: 3.678e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 3.667e-07, Loss_0: 6.809e-13, Loss_r: 3.667e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 3.655e-07, Loss_0: 3.224e-12, Loss_r: 3.655e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 3.644e-07, Loss_0: 4.352e-14, Loss_r: 3.644e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 3.633e-07, Loss_0: 1.226e-13, Loss_r: 3.633e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 3.622e-07, Loss_0: 7.713e-12, Loss_r: 3.622e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 3.611e-07, Loss_0: 8.535e-11, Loss_r: 3.611e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 3.608e-07, Loss_0: 8.164e-10, Loss_r: 3.600e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 3.837e-07, Loss_0: 2.441e-08, Loss_r: 3.593e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 3.627e-07, Loss_0: 4.784e-09, Loss_r: 3.579e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 3.569e-07, Loss_0: 5.643e-11, Loss_r: 3.568e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 3.563e-07, Loss_0: 4.590e-10, Loss_r: 3.559e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 3.553e-07, Loss_0: 4.187e-10, Loss_r: 3.549e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 3.539e-07, Loss_0: 1.777e-11, Loss_r: 3.538e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 3.529e-07, Loss_0: 3.699e-11, Loss_r: 3.528e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 3.519e-07, Loss_0: 5.684e-12, Loss_r: 3.519e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 3.509e-07, Loss_0: 1.104e-11, Loss_r: 3.509e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 3.499e-07, Loss_0: 4.062e-12, Loss_r: 3.499e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 3.489e-07, Loss_0: 2.567e-13, Loss_r: 3.489e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 3.479e-07, Loss_0: 3.040e-13, Loss_r: 3.479e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 3.469e-07, Loss_0: 3.356e-13, Loss_r: 3.469e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 3.459e-07, Loss_0: 5.927e-12, Loss_r: 3.459e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 3.450e-07, Loss_0: 7.035e-12, Loss_r: 3.449e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 3.440e-07, Loss_0: 9.381e-15, Loss_r: 3.440e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 3.430e-07, Loss_0: 6.559e-12, Loss_r: 3.430e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 3.420e-07, Loss_0: 1.127e-11, Loss_r: 3.420e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 3.411e-07, Loss_0: 1.741e-13, Loss_r: 3.411e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 3.401e-07, Loss_0: 5.578e-12, Loss_r: 3.401e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 3.392e-07, Loss_0: 4.223e-11, Loss_r: 3.391e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 3.413e-07, Loss_0: 3.029e-09, Loss_r: 3.382e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 7.673e-07, Loss_0: 4.186e-07, Loss_r: 3.487e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 3.601e-07, Loss_0: 2.300e-08, Loss_r: 3.371e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 3.543e-07, Loss_0: 1.852e-08, Loss_r: 3.358e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 3.605e-07, Loss_0: 2.539e-08, Loss_r: 3.351e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 3.343e-07, Loss_0: 5.844e-10, Loss_r: 3.337e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 3.358e-07, Loss_0: 2.788e-09, Loss_r: 3.330e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 3.321e-07, Loss_0: 7.482e-11, Loss_r: 3.320e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 3.316e-07, Loss_0: 4.563e-10, Loss_r: 3.311e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 3.304e-07, Loss_0: 9.628e-11, Loss_r: 3.303e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 3.294e-07, Loss_0: 1.493e-12, Loss_r: 3.294e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 3.285e-07, Loss_0: 1.113e-11, Loss_r: 3.285e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 3.277e-07, Loss_0: 3.291e-12, Loss_r: 3.277e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 3.268e-07, Loss_0: 3.224e-12, Loss_r: 3.268e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 3.259e-07, Loss_0: 7.120e-13, Loss_r: 3.259e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 3.251e-07, Loss_0: 1.664e-13, Loss_r: 3.251e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 3.242e-07, Loss_0: 2.069e-11, Loss_r: 3.242e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 3.234e-07, Loss_0: 4.717e-12, Loss_r: 3.233e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 3.225e-07, Loss_0: 4.294e-11, Loss_r: 3.225e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 3.224e-07, Loss_0: 7.385e-10, Loss_r: 3.217e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 3.528e-07, Loss_0: 3.109e-08, Loss_r: 3.218e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 3.258e-07, Loss_0: 5.582e-09, Loss_r: 3.202e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 3.193e-07, Loss_0: 3.298e-11, Loss_r: 3.193e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 3.193e-07, Loss_0: 8.019e-10, Loss_r: 3.185e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 3.182e-07, Loss_0: 5.565e-10, Loss_r: 3.177e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 3.169e-07, Loss_0: 9.595e-12, Loss_r: 3.169e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 3.162e-07, Loss_0: 4.999e-11, Loss_r: 3.162e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 3.154e-07, Loss_0: 5.667e-12, Loss_r: 3.154e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 3.146e-07, Loss_0: 1.763e-11, Loss_r: 3.146e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 3.138e-07, Loss_0: 3.197e-14, Loss_r: 3.138e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 3.130e-07, Loss_0: 7.088e-13, Loss_r: 3.130e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 3.123e-07, Loss_0: 3.033e-12, Loss_r: 3.123e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 3.115e-07, Loss_0: 4.275e-12, Loss_r: 3.115e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 3.107e-07, Loss_0: 1.123e-12, Loss_r: 3.107e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 3.099e-07, Loss_0: 6.286e-12, Loss_r: 3.099e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 3.092e-07, Loss_0: 1.283e-12, Loss_r: 3.092e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 3.084e-07, Loss_0: 4.062e-12, Loss_r: 3.084e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 3.077e-07, Loss_0: 3.801e-13, Loss_r: 3.077e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 3.069e-07, Loss_0: 3.803e-12, Loss_r: 3.069e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 3.061e-07, Loss_0: 5.036e-13, Loss_r: 3.061e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 3.054e-07, Loss_0: 7.183e-13, Loss_r: 3.054e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 3.046e-07, Loss_0: 1.388e-11, Loss_r: 3.046e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 3.047e-07, Loss_0: 8.768e-10, Loss_r: 3.038e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 5.301e-07, Loss_0: 2.216e-07, Loss_r: 3.085e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 3.039e-07, Loss_0: 1.565e-09, Loss_r: 3.024e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 3.288e-07, Loss_0: 2.627e-08, Loss_r: 3.025e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 3.102e-07, Loss_0: 8.891e-09, Loss_r: 3.013e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 3.019e-07, Loss_0: 1.602e-09, Loss_r: 3.003e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 3.008e-07, Loss_0: 1.163e-09, Loss_r: 2.996e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 2.995e-07, Loss_0: 5.608e-10, Loss_r: 2.989e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 2.982e-07, Loss_0: 4.084e-12, Loss_r: 2.982e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 2.976e-07, Loss_0: 4.461e-11, Loss_r: 2.975e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 2.968e-07, Loss_0: 1.954e-11, Loss_r: 2.968e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 2.961e-07, Loss_0: 1.839e-11, Loss_r: 2.961e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 18.8584\n",
            "[1, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.401e-02, Loss_0: 3.431e-04, Loss_r: 3.367e-02, Time: 0.88, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.219e-02, Loss_0: 7.658e-05, Loss_r: 3.212e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.210e-02, Loss_0: 2.928e-05, Loss_r: 3.207e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.202e-02, Loss_0: 8.917e-06, Loss_r: 3.201e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 3.186e-02, Loss_0: 3.924e-06, Loss_r: 3.186e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 3.128e-02, Loss_0: 9.189e-07, Loss_r: 3.128e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.792e-02, Loss_0: 6.457e-06, Loss_r: 2.791e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.947e-02, Loss_0: 6.025e-06, Loss_r: 1.946e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 9.285e-03, Loss_0: 4.490e-04, Loss_r: 8.836e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 3.277e-03, Loss_0: 2.484e-04, Loss_r: 3.029e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.399e-03, Loss_0: 6.769e-05, Loss_r: 1.331e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.604e-04, Loss_0: 1.006e-06, Loss_r: 3.594e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.524e-04, Loss_0: 4.823e-05, Loss_r: 1.042e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.022e-04, Loss_0: 2.282e-05, Loss_r: 7.936e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.797e-05, Loss_0: 7.983e-06, Loss_r: 4.999e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.600e-04, Loss_0: 2.224e-04, Loss_r: 3.758e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 3.863e-05, Loss_0: 1.389e-05, Loss_r: 2.474e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 2.125e-05, Loss_0: 2.344e-06, Loss_r: 1.891e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.751e-05, Loss_0: 1.615e-06, Loss_r: 1.589e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 2.130e-05, Loss_0: 7.234e-06, Loss_r: 1.407e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.206e-05, Loss_0: 8.521e-10, Loss_r: 1.206e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.112e-05, Loss_0: 2.706e-07, Loss_r: 1.085e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.001e-05, Loss_0: 1.747e-09, Loss_r: 1.000e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 9.592e-06, Loss_0: 2.145e-07, Loss_r: 9.378e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 8.954e-06, Loss_0: 1.137e-07, Loss_r: 8.840e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 8.463e-06, Loss_0: 3.930e-08, Loss_r: 8.423e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.198e-05, Loss_0: 3.727e-06, Loss_r: 8.253e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 6.222e-04, Loss_0: 5.819e-04, Loss_r: 4.025e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.098e-05, Loss_0: 3.990e-05, Loss_r: 1.108e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 7.109e-05, Loss_0: 6.046e-05, Loss_r: 1.063e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 7.773e-06, Loss_0: 5.895e-08, Loss_r: 7.714e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.650e-05, Loss_0: 8.744e-06, Loss_r: 7.752e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 8.321e-06, Loss_0: 1.170e-06, Loss_r: 7.151e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 6.823e-06, Loss_0: 9.368e-09, Loss_r: 6.814e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 6.547e-06, Loss_0: 5.646e-09, Loss_r: 6.542e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 6.383e-06, Loss_0: 8.713e-08, Loss_r: 6.296e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 6.112e-06, Loss_0: 5.584e-08, Loss_r: 6.056e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.851e-06, Loss_0: 2.761e-08, Loss_r: 5.823e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.620e-06, Loss_0: 9.992e-09, Loss_r: 5.611e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.412e-06, Loss_0: 9.906e-09, Loss_r: 5.402e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.272e-06, Loss_0: 6.787e-08, Loss_r: 5.205e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.288e-05, Loss_0: 7.566e-06, Loss_r: 5.315e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.825e-04, Loss_0: 5.598e-04, Loss_r: 2.267e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 8.675e-05, Loss_0: 8.029e-05, Loss_r: 6.463e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.903e-05, Loss_0: 3.156e-05, Loss_r: 7.477e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.904e-05, Loss_0: 1.292e-05, Loss_r: 6.120e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 7.391e-06, Loss_0: 2.366e-06, Loss_r: 5.025e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 8.187e-06, Loss_0: 3.325e-06, Loss_r: 4.861e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.363e-06, Loss_0: 7.565e-07, Loss_r: 4.606e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.591e-06, Loss_0: 1.681e-07, Loss_r: 4.423e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.362e-06, Loss_0: 9.750e-08, Loss_r: 4.265e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.190e-06, Loss_0: 6.889e-08, Loss_r: 4.121e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.994e-06, Loss_0: 6.937e-09, Loss_r: 3.987e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.871e-06, Loss_0: 8.125e-09, Loss_r: 3.862e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.742e-06, Loss_0: 8.229e-10, Loss_r: 3.742e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 3.628e-06, Loss_0: 7.680e-11, Loss_r: 3.628e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 3.521e-06, Loss_0: 1.244e-09, Loss_r: 3.520e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.417e-06, Loss_0: 3.869e-12, Loss_r: 3.417e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.320e-06, Loss_0: 7.354e-10, Loss_r: 3.319e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.373e-06, Loss_0: 1.383e-07, Loss_r: 3.235e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 9.712e-05, Loss_0: 8.974e-05, Loss_r: 7.376e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 6.073e-04, Loss_0: 5.837e-04, Loss_r: 2.361e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 4.480e-06, Loss_0: 6.059e-08, Loss_r: 4.419e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 3.045e-05, Loss_0: 2.422e-05, Loss_r: 6.235e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.488e-05, Loss_0: 2.055e-05, Loss_r: 4.332e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.138e-05, Loss_0: 7.094e-06, Loss_r: 4.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 3.954e-06, Loss_0: 3.834e-07, Loss_r: 3.571e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.964e-06, Loss_0: 5.413e-07, Loss_r: 3.422e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.547e-06, Loss_0: 2.459e-07, Loss_r: 3.301e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.248e-06, Loss_0: 6.330e-08, Loss_r: 3.185e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.102e-06, Loss_0: 8.523e-09, Loss_r: 3.093e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.033e-06, Loss_0: 1.780e-08, Loss_r: 3.015e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.949e-06, Loss_0: 6.924e-09, Loss_r: 2.943e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.878e-06, Loss_0: 2.230e-09, Loss_r: 2.875e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.813e-06, Loss_0: 9.560e-10, Loss_r: 2.812e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.752e-06, Loss_0: 4.499e-10, Loss_r: 2.752e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.695e-06, Loss_0: 1.302e-10, Loss_r: 2.694e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.640e-06, Loss_0: 3.258e-12, Loss_r: 2.640e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.588e-06, Loss_0: 2.339e-12, Loss_r: 2.588e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.538e-06, Loss_0: 1.960e-11, Loss_r: 2.538e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.490e-06, Loss_0: 5.614e-12, Loss_r: 2.490e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.445e-06, Loss_0: 1.007e-11, Loss_r: 2.445e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.401e-06, Loss_0: 5.508e-12, Loss_r: 2.401e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.360e-06, Loss_0: 2.567e-11, Loss_r: 2.360e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.320e-06, Loss_0: 2.930e-11, Loss_r: 2.320e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.282e-06, Loss_0: 1.211e-10, Loss_r: 2.282e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.253e-06, Loss_0: 7.199e-09, Loss_r: 2.245e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 4.750e-06, Loss_0: 2.438e-06, Loss_r: 2.312e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.231e-03, Loss_0: 1.186e-03, Loss_r: 4.490e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 4.001e-04, Loss_0: 3.796e-04, Loss_r: 2.050e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.834e-05, Loss_0: 2.228e-05, Loss_r: 6.055e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 3.591e-06, Loss_0: 4.944e-08, Loss_r: 3.541e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 6.798e-06, Loss_0: 3.467e-06, Loss_r: 3.330e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 7.304e-06, Loss_0: 4.248e-06, Loss_r: 3.056e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 4.694e-06, Loss_0: 1.854e-06, Loss_r: 2.841e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 2.748e-06, Loss_0: 9.265e-08, Loss_r: 2.655e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.687e-06, Loss_0: 1.179e-07, Loss_r: 2.569e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.564e-06, Loss_0: 6.373e-08, Loss_r: 2.501e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.461e-06, Loss_0: 1.139e-08, Loss_r: 2.450e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.412e-06, Loss_0: 4.261e-09, Loss_r: 2.407e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.371e-06, Loss_0: 3.780e-09, Loss_r: 2.367e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.330e-06, Loss_0: 3.095e-10, Loss_r: 2.329e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.293e-06, Loss_0: 2.595e-11, Loss_r: 2.293e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.259e-06, Loss_0: 7.835e-11, Loss_r: 2.259e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.226e-06, Loss_0: 4.309e-11, Loss_r: 2.226e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 2.194e-06, Loss_0: 1.810e-11, Loss_r: 2.194e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 2.163e-06, Loss_0: 4.229e-12, Loss_r: 2.163e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.133e-06, Loss_0: 1.388e-15, Loss_r: 2.133e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.104e-06, Loss_0: 3.421e-13, Loss_r: 2.104e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.077e-06, Loss_0: 2.111e-12, Loss_r: 2.077e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 2.050e-06, Loss_0: 1.562e-11, Loss_r: 2.050e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.023e-06, Loss_0: 3.854e-12, Loss_r: 2.023e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.998e-06, Loss_0: 4.462e-12, Loss_r: 1.998e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.973e-06, Loss_0: 3.312e-12, Loss_r: 1.973e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.949e-06, Loss_0: 5.569e-12, Loss_r: 1.949e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.926e-06, Loss_0: 1.037e-11, Loss_r: 1.926e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.903e-06, Loss_0: 1.036e-11, Loss_r: 1.903e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.880e-06, Loss_0: 1.515e-11, Loss_r: 1.880e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.859e-06, Loss_0: 7.399e-11, Loss_r: 1.858e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.839e-06, Loss_0: 2.441e-09, Loss_r: 1.837e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 2.260e-06, Loss_0: 4.267e-07, Loss_r: 1.833e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.016e-04, Loss_0: 1.916e-04, Loss_r: 9.923e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 5.845e-04, Loss_0: 5.485e-04, Loss_r: 3.597e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 5.271e-06, Loss_0: 2.133e-06, Loss_r: 3.139e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 5.136e-05, Loss_0: 4.526e-05, Loss_r: 6.102e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.139e-05, Loss_0: 1.735e-05, Loss_r: 4.042e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.470e-06, Loss_0: 9.221e-07, Loss_r: 2.547e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 3.294e-06, Loss_0: 9.019e-07, Loss_r: 2.392e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 3.129e-06, Loss_0: 8.187e-07, Loss_r: 2.311e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.178e-06, Loss_0: 1.477e-09, Loss_r: 2.177e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.254e-06, Loss_0: 1.206e-07, Loss_r: 2.134e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.101e-06, Loss_0: 2.785e-09, Loss_r: 2.098e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.077e-06, Loss_0: 9.173e-09, Loss_r: 2.068e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.043e-06, Loss_0: 4.357e-09, Loss_r: 2.039e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.010e-06, Loss_0: 2.797e-10, Loss_r: 2.010e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.982e-06, Loss_0: 3.091e-11, Loss_r: 1.982e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.956e-06, Loss_0: 8.405e-11, Loss_r: 1.956e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.930e-06, Loss_0: 5.946e-11, Loss_r: 1.930e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.905e-06, Loss_0: 2.247e-11, Loss_r: 1.905e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.881e-06, Loss_0: 6.351e-12, Loss_r: 1.881e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.858e-06, Loss_0: 2.220e-12, Loss_r: 1.858e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.835e-06, Loss_0: 7.894e-13, Loss_r: 1.835e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.812e-06, Loss_0: 7.806e-14, Loss_r: 1.812e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.790e-06, Loss_0: 1.967e-12, Loss_r: 1.790e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.769e-06, Loss_0: 5.543e-12, Loss_r: 1.769e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.748e-06, Loss_0: 1.488e-12, Loss_r: 1.748e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.727e-06, Loss_0: 4.921e-12, Loss_r: 1.727e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.707e-06, Loss_0: 1.633e-12, Loss_r: 1.707e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.688e-06, Loss_0: 4.580e-12, Loss_r: 1.688e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.668e-06, Loss_0: 6.535e-13, Loss_r: 1.668e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.649e-06, Loss_0: 3.825e-12, Loss_r: 1.649e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.630e-06, Loss_0: 4.337e-12, Loss_r: 1.630e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.612e-06, Loss_0: 1.631e-11, Loss_r: 1.612e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.594e-06, Loss_0: 1.779e-10, Loss_r: 1.593e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.584e-06, Loss_0: 8.093e-09, Loss_r: 1.576e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.874e-06, Loss_0: 1.261e-06, Loss_r: 1.613e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.088e-04, Loss_0: 3.909e-04, Loss_r: 1.784e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.532e-04, Loss_0: 2.379e-04, Loss_r: 1.529e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.427e-05, Loss_0: 1.047e-05, Loss_r: 3.798e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 3.617e-05, Loss_0: 3.304e-05, Loss_r: 3.136e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.191e-05, Loss_0: 9.439e-06, Loss_r: 2.471e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.026e-06, Loss_0: 7.851e-07, Loss_r: 2.241e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.890e-06, Loss_0: 1.786e-06, Loss_r: 2.104e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.991e-06, Loss_0: 6.618e-08, Loss_r: 1.924e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.082e-06, Loss_0: 1.839e-07, Loss_r: 1.898e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.915e-06, Loss_0: 5.807e-08, Loss_r: 1.857e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.828e-06, Loss_0: 2.788e-10, Loss_r: 1.828e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.808e-06, Loss_0: 6.767e-09, Loss_r: 1.801e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.780e-06, Loss_0: 4.383e-09, Loss_r: 1.775e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.752e-06, Loss_0: 1.532e-09, Loss_r: 1.750e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.727e-06, Loss_0: 4.469e-10, Loss_r: 1.726e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.703e-06, Loss_0: 1.832e-10, Loss_r: 1.703e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.681e-06, Loss_0: 9.077e-11, Loss_r: 1.680e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.659e-06, Loss_0: 4.547e-11, Loss_r: 1.658e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.637e-06, Loss_0: 2.167e-11, Loss_r: 1.637e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.616e-06, Loss_0: 4.407e-12, Loss_r: 1.616e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.596e-06, Loss_0: 5.918e-13, Loss_r: 1.596e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.576e-06, Loss_0: 5.607e-13, Loss_r: 1.576e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.556e-06, Loss_0: 5.605e-12, Loss_r: 1.556e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.537e-06, Loss_0: 3.483e-12, Loss_r: 1.537e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.519e-06, Loss_0: 3.987e-13, Loss_r: 1.519e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.500e-06, Loss_0: 4.137e-12, Loss_r: 1.500e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.482e-06, Loss_0: 1.111e-12, Loss_r: 1.482e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.464e-06, Loss_0: 4.880e-12, Loss_r: 1.464e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.447e-06, Loss_0: 6.626e-13, Loss_r: 1.447e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.430e-06, Loss_0: 1.704e-11, Loss_r: 1.430e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.413e-06, Loss_0: 3.708e-11, Loss_r: 1.413e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.396e-06, Loss_0: 1.507e-10, Loss_r: 1.396e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.383e-06, Loss_0: 2.951e-09, Loss_r: 1.380e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.602e-06, Loss_0: 2.287e-07, Loss_r: 1.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.925e-05, Loss_0: 4.576e-05, Loss_r: 3.491e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.294e-05, Loss_0: 3.981e-05, Loss_r: 3.139e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.587e-06, Loss_0: 4.890e-07, Loss_r: 2.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.757e-05, Loss_0: 5.274e-05, Loss_r: 4.830e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.192e-06, Loss_0: 1.095e-06, Loss_r: 2.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 8.421e-06, Loss_0: 6.320e-06, Loss_r: 2.101e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 2.237e-06, Loss_0: 4.588e-07, Loss_r: 1.778e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 2.166e-06, Loss_0: 4.386e-07, Loss_r: 1.727e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.976e-06, Loss_0: 2.833e-07, Loss_r: 1.693e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.668e-06, Loss_0: 1.639e-08, Loss_r: 1.652e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.629e-06, Loss_0: 3.770e-09, Loss_r: 1.625e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.608e-06, Loss_0: 7.996e-09, Loss_r: 1.600e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.580e-06, Loss_0: 4.478e-09, Loss_r: 1.576e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.554e-06, Loss_0: 1.858e-09, Loss_r: 1.552e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.531e-06, Loss_0: 6.831e-10, Loss_r: 1.530e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.509e-06, Loss_0: 2.063e-10, Loss_r: 1.508e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.487e-06, Loss_0: 3.660e-11, Loss_r: 1.487e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.467e-06, Loss_0: 1.965e-13, Loss_r: 1.467e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.447e-06, Loss_0: 2.151e-13, Loss_r: 1.447e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.428e-06, Loss_0: 1.421e-14, Loss_r: 1.428e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.409e-06, Loss_0: 4.652e-12, Loss_r: 1.409e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.391e-06, Loss_0: 4.024e-12, Loss_r: 1.391e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.373e-06, Loss_0: 3.014e-12, Loss_r: 1.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.355e-06, Loss_0: 2.924e-12, Loss_r: 1.355e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.338e-06, Loss_0: 1.329e-12, Loss_r: 1.338e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.321e-06, Loss_0: 5.667e-12, Loss_r: 1.321e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.305e-06, Loss_0: 4.202e-13, Loss_r: 1.305e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.289e-06, Loss_0: 2.842e-12, Loss_r: 1.289e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.273e-06, Loss_0: 5.181e-12, Loss_r: 1.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 1.257e-06, Loss_0: 2.524e-12, Loss_r: 1.257e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 1.242e-06, Loss_0: 2.176e-12, Loss_r: 1.242e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 1.226e-06, Loss_0: 1.115e-10, Loss_r: 1.226e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 1.218e-06, Loss_0: 6.337e-09, Loss_r: 1.212e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 2.152e-06, Loss_0: 9.043e-07, Loss_r: 1.248e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.214e-06, Loss_0: 2.842e-08, Loss_r: 1.186e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.251e-06, Loss_0: 7.605e-08, Loss_r: 1.175e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.207e-06, Loss_0: 4.581e-08, Loss_r: 1.161e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.150e-06, Loss_0: 3.156e-09, Loss_r: 1.147e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.140e-06, Loss_0: 4.968e-09, Loss_r: 1.135e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.125e-06, Loss_0: 2.442e-09, Loss_r: 1.122e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.110e-06, Loss_0: 8.570e-11, Loss_r: 1.110e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.098e-06, Loss_0: 2.127e-12, Loss_r: 1.098e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.087e-06, Loss_0: 1.004e-11, Loss_r: 1.087e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.075e-06, Loss_0: 1.475e-12, Loss_r: 1.075e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.064e-06, Loss_0: 4.588e-12, Loss_r: 1.064e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.053e-06, Loss_0: 1.662e-11, Loss_r: 1.053e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 1.042e-06, Loss_0: 6.655e-12, Loss_r: 1.042e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.031e-06, Loss_0: 1.341e-11, Loss_r: 1.031e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 1.020e-06, Loss_0: 1.654e-10, Loss_r: 1.020e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 1.017e-06, Loss_0: 7.645e-09, Loss_r: 1.010e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 1.978e-06, Loss_0: 9.288e-07, Loss_r: 1.049e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.034e-06, Loss_0: 4.260e-08, Loss_r: 9.917e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.044e-06, Loss_0: 5.909e-08, Loss_r: 9.847e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.027e-06, Loss_0: 5.173e-08, Loss_r: 9.752e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 9.635e-07, Loss_0: 6.875e-10, Loss_r: 9.628e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 9.615e-07, Loss_0: 7.240e-09, Loss_r: 9.542e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 9.465e-07, Loss_0: 1.105e-09, Loss_r: 9.454e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 9.366e-07, Loss_0: 2.516e-11, Loss_r: 9.366e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 9.283e-07, Loss_0: 2.254e-10, Loss_r: 9.280e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 9.197e-07, Loss_0: 1.077e-10, Loss_r: 9.196e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 9.113e-07, Loss_0: 6.803e-11, Loss_r: 9.112e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 9.030e-07, Loss_0: 1.614e-12, Loss_r: 9.030e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 8.948e-07, Loss_0: 1.038e-13, Loss_r: 8.948e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 8.868e-07, Loss_0: 1.080e-11, Loss_r: 8.868e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 8.788e-07, Loss_0: 1.595e-11, Loss_r: 8.788e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 8.709e-07, Loss_0: 2.804e-11, Loss_r: 8.709e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 8.636e-07, Loss_0: 4.012e-10, Loss_r: 8.632e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 8.814e-07, Loss_0: 2.470e-08, Loss_r: 8.567e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 5.013e-06, Loss_0: 3.938e-06, Loss_r: 1.075e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 9.681e-07, Loss_0: 1.199e-07, Loss_r: 8.483e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.165e-06, Loss_0: 3.100e-07, Loss_r: 8.554e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.052e-06, Loss_0: 2.090e-07, Loss_r: 8.427e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 8.296e-07, Loss_0: 6.422e-09, Loss_r: 8.232e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 8.468e-07, Loss_0: 2.889e-08, Loss_r: 8.179e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 8.162e-07, Loss_0: 5.625e-09, Loss_r: 8.106e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 8.039e-07, Loss_0: 1.093e-10, Loss_r: 8.038e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 7.985e-07, Loss_0: 9.740e-10, Loss_r: 7.976e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 7.919e-07, Loss_0: 5.526e-10, Loss_r: 7.914e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 7.854e-07, Loss_0: 3.051e-10, Loss_r: 7.851e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 7.791e-07, Loss_0: 5.635e-11, Loss_r: 7.791e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 7.730e-07, Loss_0: 3.652e-12, Loss_r: 7.730e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 7.671e-07, Loss_0: 3.653e-11, Loss_r: 7.671e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 7.612e-07, Loss_0: 1.321e-11, Loss_r: 7.612e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 7.554e-07, Loss_0: 1.370e-11, Loss_r: 7.554e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 7.496e-07, Loss_0: 1.296e-10, Loss_r: 7.495e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 7.504e-07, Loss_0: 6.232e-09, Loss_r: 7.441e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.538e-06, Loss_0: 7.548e-07, Loss_r: 7.831e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 7.722e-07, Loss_0: 3.694e-08, Loss_r: 7.353e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 7.751e-07, Loss_0: 4.364e-08, Loss_r: 7.315e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 7.699e-07, Loss_0: 4.333e-08, Loss_r: 7.265e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 7.187e-07, Loss_0: 8.618e-11, Loss_r: 7.186e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 7.202e-07, Loss_0: 6.159e-09, Loss_r: 7.140e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 7.094e-07, Loss_0: 3.707e-10, Loss_r: 7.090e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 7.044e-07, Loss_0: 2.249e-10, Loss_r: 7.042e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 6.998e-07, Loss_0: 2.803e-10, Loss_r: 6.995e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 6.949e-07, Loss_0: 9.248e-11, Loss_r: 6.948e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 6.902e-07, Loss_0: 5.568e-11, Loss_r: 6.902e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 6.856e-07, Loss_0: 1.167e-11, Loss_r: 6.856e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 6.810e-07, Loss_0: 5.845e-12, Loss_r: 6.810e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 6.765e-07, Loss_0: 3.270e-13, Loss_r: 6.765e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 6.720e-07, Loss_0: 8.443e-14, Loss_r: 6.720e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 6.676e-07, Loss_0: 3.469e-14, Loss_r: 6.676e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 6.632e-07, Loss_0: 3.490e-12, Loss_r: 6.632e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 6.589e-07, Loss_0: 4.521e-13, Loss_r: 6.589e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 6.545e-07, Loss_0: 1.360e-13, Loss_r: 6.545e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 6.502e-07, Loss_0: 4.414e-12, Loss_r: 6.502e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 6.465e-07, Loss_0: 4.687e-10, Loss_r: 6.460e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 9.784e-07, Loss_0: 3.171e-07, Loss_r: 6.613e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 6.607e-07, Loss_0: 2.102e-08, Loss_r: 6.396e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 6.983e-07, Loss_0: 5.967e-08, Loss_r: 6.386e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 6.361e-07, Loss_0: 5.105e-09, Loss_r: 6.310e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 6.309e-07, Loss_0: 3.615e-09, Loss_r: 6.272e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 6.271e-07, Loss_0: 3.388e-09, Loss_r: 6.238e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 6.208e-07, Loss_0: 9.685e-10, Loss_r: 6.199e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 6.165e-07, Loss_0: 1.979e-10, Loss_r: 6.163e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 6.129e-07, Loss_0: 1.440e-10, Loss_r: 6.127e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 6.093e-07, Loss_0: 5.326e-11, Loss_r: 6.092e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 6.057e-07, Loss_0: 5.127e-11, Loss_r: 6.057e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 6.022e-07, Loss_0: 5.605e-12, Loss_r: 6.022e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 5.987e-07, Loss_0: 2.382e-13, Loss_r: 5.987e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 5.953e-07, Loss_0: 4.267e-12, Loss_r: 5.953e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 5.919e-07, Loss_0: 2.351e-11, Loss_r: 5.919e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 5.895e-07, Loss_0: 9.222e-10, Loss_r: 5.885e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 6.833e-07, Loss_0: 9.237e-08, Loss_r: 5.909e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 5.887e-07, Loss_0: 6.177e-09, Loss_r: 5.825e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 5.832e-07, Loss_0: 3.608e-09, Loss_r: 5.796e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 5.821e-07, Loss_0: 5.414e-09, Loss_r: 5.767e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 5.734e-07, Loss_0: 5.175e-11, Loss_r: 5.733e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 5.712e-07, Loss_0: 7.494e-10, Loss_r: 5.704e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 5.675e-07, Loss_0: 5.143e-13, Loss_r: 5.675e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 5.647e-07, Loss_0: 6.100e-11, Loss_r: 5.646e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 5.617e-07, Loss_0: 2.277e-11, Loss_r: 5.617e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 5.589e-07, Loss_0: 9.381e-15, Loss_r: 5.589e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 5.560e-07, Loss_0: 6.004e-13, Loss_r: 5.560e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 5.532e-07, Loss_0: 4.775e-13, Loss_r: 5.532e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 5.504e-07, Loss_0: 8.918e-13, Loss_r: 5.504e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 5.476e-07, Loss_0: 2.232e-12, Loss_r: 5.476e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 5.448e-07, Loss_0: 1.444e-13, Loss_r: 5.448e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 5.421e-07, Loss_0: 4.957e-13, Loss_r: 5.421e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 5.393e-07, Loss_0: 1.604e-13, Loss_r: 5.393e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 5.367e-07, Loss_0: 5.958e-11, Loss_r: 5.366e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 5.369e-07, Loss_0: 2.774e-09, Loss_r: 5.341e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 8.332e-07, Loss_0: 2.822e-07, Loss_r: 5.509e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 5.498e-07, Loss_0: 1.957e-08, Loss_r: 5.303e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 5.376e-07, Loss_0: 1.059e-08, Loss_r: 5.270e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 5.423e-07, Loss_0: 1.720e-08, Loss_r: 5.251e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 5.220e-07, Loss_0: 3.617e-10, Loss_r: 5.217e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 5.216e-07, Loss_0: 2.096e-09, Loss_r: 5.195e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 5.169e-07, Loss_0: 3.231e-12, Loss_r: 5.169e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 5.149e-07, Loss_0: 3.037e-10, Loss_r: 5.146e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 5.123e-07, Loss_0: 6.433e-11, Loss_r: 5.123e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 5.099e-07, Loss_0: 1.156e-11, Loss_r: 5.099e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 5.076e-07, Loss_0: 6.963e-13, Loss_r: 5.076e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 5.053e-07, Loss_0: 1.498e-12, Loss_r: 5.053e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 5.029e-07, Loss_0: 1.187e-13, Loss_r: 5.029e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 5.007e-07, Loss_0: 9.893e-13, Loss_r: 5.007e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 4.984e-07, Loss_0: 3.227e-13, Loss_r: 4.984e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 4.961e-07, Loss_0: 1.576e-12, Loss_r: 4.961e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 4.938e-07, Loss_0: 5.640e-12, Loss_r: 4.938e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 4.918e-07, Loss_0: 1.473e-10, Loss_r: 4.916e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 4.944e-07, Loss_0: 4.747e-09, Loss_r: 4.896e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 8.866e-07, Loss_0: 3.744e-07, Loss_r: 5.122e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 5.231e-07, Loss_0: 3.571e-08, Loss_r: 4.874e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 4.908e-07, Loss_0: 7.062e-09, Loss_r: 4.837e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 5.038e-07, Loss_0: 2.098e-08, Loss_r: 4.828e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 4.814e-07, Loss_0: 1.970e-09, Loss_r: 4.794e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 4.792e-07, Loss_0: 1.893e-09, Loss_r: 4.773e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 4.756e-07, Loss_0: 3.079e-10, Loss_r: 4.753e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 4.737e-07, Loss_0: 3.530e-10, Loss_r: 4.734e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 4.714e-07, Loss_0: 2.182e-11, Loss_r: 4.713e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 4.694e-07, Loss_0: 8.893e-12, Loss_r: 4.694e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 4.675e-07, Loss_0: 3.539e-12, Loss_r: 4.674e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 4.655e-07, Loss_0: 1.567e-12, Loss_r: 4.655e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 4.636e-07, Loss_0: 3.733e-13, Loss_r: 4.636e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 4.616e-07, Loss_0: 4.750e-13, Loss_r: 4.616e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 4.597e-07, Loss_0: 6.436e-12, Loss_r: 4.597e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 4.578e-07, Loss_0: 2.693e-12, Loss_r: 4.578e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 4.559e-07, Loss_0: 6.314e-12, Loss_r: 4.559e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 4.540e-07, Loss_0: 4.644e-12, Loss_r: 4.539e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 4.521e-07, Loss_0: 6.714e-11, Loss_r: 4.520e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 4.538e-07, Loss_0: 3.478e-09, Loss_r: 4.503e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 8.918e-07, Loss_0: 4.151e-07, Loss_r: 4.767e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 4.717e-07, Loss_0: 2.360e-08, Loss_r: 4.481e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 4.659e-07, Loss_0: 1.944e-08, Loss_r: 4.464e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 4.700e-07, Loss_0: 2.492e-08, Loss_r: 4.451e-07, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 4.417e-07, Loss_0: 1.422e-10, Loss_r: 4.416e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 4.434e-07, Loss_0: 3.369e-09, Loss_r: 4.401e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 4.382e-07, Loss_0: 1.108e-12, Loss_r: 4.382e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 4.369e-07, Loss_0: 3.940e-10, Loss_r: 4.365e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 4.350e-07, Loss_0: 1.841e-10, Loss_r: 4.348e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 4.331e-07, Loss_0: 2.629e-11, Loss_r: 4.331e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 4.315e-07, Loss_0: 6.996e-12, Loss_r: 4.315e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 4.298e-07, Loss_0: 3.001e-12, Loss_r: 4.298e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 4.282e-07, Loss_0: 1.158e-11, Loss_r: 4.281e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 4.265e-07, Loss_0: 8.883e-13, Loss_r: 4.265e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 4.248e-07, Loss_0: 9.563e-13, Loss_r: 4.248e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 4.231e-07, Loss_0: 8.661e-14, Loss_r: 4.231e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 4.215e-07, Loss_0: 6.146e-12, Loss_r: 4.215e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 4.200e-07, Loss_0: 1.313e-10, Loss_r: 4.199e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 4.219e-07, Loss_0: 3.423e-09, Loss_r: 4.185e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 6.748e-07, Loss_0: 2.406e-07, Loss_r: 4.342e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 4.441e-07, Loss_0: 2.691e-08, Loss_r: 4.171e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 4.163e-07, Loss_0: 2.514e-09, Loss_r: 4.138e-07, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 4.253e-07, Loss_0: 1.235e-08, Loss_r: 4.130e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 4.131e-07, Loss_0: 2.282e-09, Loss_r: 4.108e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 4.100e-07, Loss_0: 6.540e-10, Loss_r: 4.093e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 4.082e-07, Loss_0: 3.917e-10, Loss_r: 4.078e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 4.065e-07, Loss_0: 1.642e-10, Loss_r: 4.063e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 4.048e-07, Loss_0: 2.785e-12, Loss_r: 4.048e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 4.034e-07, Loss_0: 1.681e-11, Loss_r: 4.034e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 4.019e-07, Loss_0: 1.874e-11, Loss_r: 4.019e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 4.004e-07, Loss_0: 4.533e-12, Loss_r: 4.004e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 3.989e-07, Loss_0: 1.023e-12, Loss_r: 3.989e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 3.975e-07, Loss_0: 1.890e-12, Loss_r: 3.975e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 3.961e-07, Loss_0: 3.840e-12, Loss_r: 3.961e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 3.946e-07, Loss_0: 6.502e-12, Loss_r: 3.946e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 3.931e-07, Loss_0: 2.143e-12, Loss_r: 3.931e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 3.917e-07, Loss_0: 3.359e-12, Loss_r: 3.917e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 3.903e-07, Loss_0: 3.803e-11, Loss_r: 3.902e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 3.901e-07, Loss_0: 1.276e-09, Loss_r: 3.888e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 5.000e-07, Loss_0: 1.053e-07, Loss_r: 3.946e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 3.968e-07, Loss_0: 1.013e-08, Loss_r: 3.866e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 3.868e-07, Loss_0: 1.892e-09, Loss_r: 3.849e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 3.896e-07, Loss_0: 5.753e-09, Loss_r: 3.839e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 3.828e-07, Loss_0: 6.383e-10, Loss_r: 3.822e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 3.813e-07, Loss_0: 4.944e-10, Loss_r: 3.808e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 3.796e-07, Loss_0: 1.448e-10, Loss_r: 3.795e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 3.783e-07, Loss_0: 1.076e-10, Loss_r: 3.782e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 3.769e-07, Loss_0: 5.854e-12, Loss_r: 3.769e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 3.756e-07, Loss_0: 4.313e-12, Loss_r: 3.756e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 3.743e-07, Loss_0: 4.888e-12, Loss_r: 3.743e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 3.730e-07, Loss_0: 8.122e-12, Loss_r: 3.730e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 3.717e-07, Loss_0: 5.257e-12, Loss_r: 3.717e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 3.704e-07, Loss_0: 4.206e-12, Loss_r: 3.704e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 3.690e-07, Loss_0: 2.930e-12, Loss_r: 3.690e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 3.678e-07, Loss_0: 2.701e-13, Loss_r: 3.678e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 3.665e-07, Loss_0: 2.316e-12, Loss_r: 3.665e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 3.652e-07, Loss_0: 1.148e-11, Loss_r: 3.652e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 3.641e-07, Loss_0: 2.266e-10, Loss_r: 3.639e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 3.705e-07, Loss_0: 7.314e-09, Loss_r: 3.632e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 1.047e-06, Loss_0: 6.386e-07, Loss_r: 4.088e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 4.260e-07, Loss_0: 6.116e-08, Loss_r: 3.648e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 3.701e-07, Loss_0: 1.045e-08, Loss_r: 3.597e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 3.950e-07, Loss_0: 3.474e-08, Loss_r: 3.603e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 3.618e-07, Loss_0: 4.767e-09, Loss_r: 3.570e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 3.581e-07, Loss_0: 2.363e-09, Loss_r: 3.557e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 3.553e-07, Loss_0: 8.201e-10, Loss_r: 3.544e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 3.537e-07, Loss_0: 5.251e-10, Loss_r: 3.532e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 3.520e-07, Loss_0: 5.010e-15, Loss_r: 3.520e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 3.509e-07, Loss_0: 3.351e-11, Loss_r: 3.509e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 3.497e-07, Loss_0: 3.586e-11, Loss_r: 3.497e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 3.485e-07, Loss_0: 8.175e-12, Loss_r: 3.485e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 3.473e-07, Loss_0: 6.202e-12, Loss_r: 3.473e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 3.462e-07, Loss_0: 4.202e-13, Loss_r: 3.462e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 3.450e-07, Loss_0: 1.127e-12, Loss_r: 3.450e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 3.438e-07, Loss_0: 6.417e-14, Loss_r: 3.438e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 3.426e-07, Loss_0: 4.905e-12, Loss_r: 3.426e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 3.415e-07, Loss_0: 9.229e-12, Loss_r: 3.415e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 3.404e-07, Loss_0: 1.099e-10, Loss_r: 3.403e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 3.430e-07, Loss_0: 3.566e-09, Loss_r: 3.394e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 6.265e-07, Loss_0: 2.683e-07, Loss_r: 3.582e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 3.689e-07, Loss_0: 2.969e-08, Loss_r: 3.392e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 3.388e-07, Loss_0: 2.796e-09, Loss_r: 3.360e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 3.494e-07, Loss_0: 1.366e-08, Loss_r: 3.357e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 3.366e-07, Loss_0: 2.665e-09, Loss_r: 3.339e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 3.334e-07, Loss_0: 6.448e-10, Loss_r: 3.327e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 3.322e-07, Loss_0: 4.905e-10, Loss_r: 3.317e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 3.308e-07, Loss_0: 2.077e-10, Loss_r: 3.305e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 3.295e-07, Loss_0: 2.419e-12, Loss_r: 3.295e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 3.284e-07, Loss_0: 2.127e-11, Loss_r: 3.284e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 3.274e-07, Loss_0: 1.288e-11, Loss_r: 3.273e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 3.263e-07, Loss_0: 1.859e-12, Loss_r: 3.263e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 3.252e-07, Loss_0: 5.147e-12, Loss_r: 3.252e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 3.242e-07, Loss_0: 1.484e-12, Loss_r: 3.242e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 3.231e-07, Loss_0: 3.553e-12, Loss_r: 3.231e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 3.220e-07, Loss_0: 4.750e-13, Loss_r: 3.220e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 3.210e-07, Loss_0: 5.775e-13, Loss_r: 3.210e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 3.199e-07, Loss_0: 7.341e-13, Loss_r: 3.199e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 3.188e-07, Loss_0: 4.541e-12, Loss_r: 3.188e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 3.179e-07, Loss_0: 1.656e-10, Loss_r: 3.177e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 3.261e-07, Loss_0: 8.862e-09, Loss_r: 3.173e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 1.570e-06, Loss_0: 1.169e-06, Loss_r: 4.017e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 3.964e-07, Loss_0: 7.620e-08, Loss_r: 3.202e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 3.552e-07, Loss_0: 3.835e-08, Loss_r: 3.168e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 3.874e-07, Loss_0: 6.922e-08, Loss_r: 3.182e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 3.166e-07, Loss_0: 4.376e-09, Loss_r: 3.123e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 3.179e-07, Loss_0: 6.529e-09, Loss_r: 3.114e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 3.109e-07, Loss_0: 8.527e-10, Loss_r: 3.100e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 3.102e-07, Loss_0: 1.125e-09, Loss_r: 3.091e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 3.081e-07, Loss_0: 5.543e-11, Loss_r: 3.081e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 3.071e-07, Loss_0: 3.595e-11, Loss_r: 3.071e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 3.061e-07, Loss_0: 3.081e-11, Loss_r: 3.061e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 3.052e-07, Loss_0: 2.292e-11, Loss_r: 3.052e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 3.042e-07, Loss_0: 7.664e-13, Loss_r: 3.042e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 3.032e-07, Loss_0: 5.250e-13, Loss_r: 3.032e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 3.022e-07, Loss_0: 2.345e-15, Loss_r: 3.022e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 3.012e-07, Loss_0: 1.998e-13, Loss_r: 3.012e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 3.003e-07, Loss_0: 3.387e-12, Loss_r: 3.003e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 2.993e-07, Loss_0: 1.330e-11, Loss_r: 2.993e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 2.985e-07, Loss_0: 2.068e-10, Loss_r: 2.983e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 3.046e-07, Loss_0: 6.879e-09, Loss_r: 2.978e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 8.425e-07, Loss_0: 5.087e-07, Loss_r: 3.338e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 3.612e-07, Loss_0: 6.127e-08, Loss_r: 2.999e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 2.979e-07, Loss_0: 2.970e-09, Loss_r: 2.949e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 3.190e-07, Loss_0: 2.345e-08, Loss_r: 2.956e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 2.999e-07, Loss_0: 6.461e-09, Loss_r: 2.934e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 2.926e-07, Loss_0: 6.028e-10, Loss_r: 2.920e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 2.924e-07, Loss_0: 1.255e-09, Loss_r: 2.912e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 2.903e-07, Loss_0: 1.263e-10, Loss_r: 2.902e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 2.894e-07, Loss_0: 7.104e-11, Loss_r: 2.893e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 17.9663\n",
            "[1, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.270e-02, Loss_0: 3.789e-04, Loss_r: 3.232e-02, Time: 0.76, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.232e-02, Loss_0: 1.284e-06, Loss_r: 3.232e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.221e-02, Loss_0: 2.012e-05, Loss_r: 3.219e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.215e-02, Loss_0: 5.649e-06, Loss_r: 3.215e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 3.183e-02, Loss_0: 3.411e-08, Loss_r: 3.183e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 2.937e-02, Loss_0: 1.552e-05, Loss_r: 2.935e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.829e-02, Loss_0: 2.417e-04, Loss_r: 1.804e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.625e-03, Loss_0: 7.176e-04, Loss_r: 4.907e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 2.250e-03, Loss_0: 3.683e-04, Loss_r: 1.882e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 4.648e-04, Loss_0: 2.605e-04, Loss_r: 2.043e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.089e-04, Loss_0: 4.532e-10, Loss_r: 1.089e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 7.975e-05, Loss_0: 1.396e-05, Loss_r: 6.579e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.728e-05, Loss_0: 1.957e-06, Loss_r: 2.532e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.381e-05, Loss_0: 3.684e-07, Loss_r: 1.344e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.238e-05, Loss_0: 2.652e-06, Loss_r: 9.732e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 8.631e-06, Loss_0: 4.592e-07, Loss_r: 8.172e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.150e-05, Loss_0: 2.202e-06, Loss_r: 9.302e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 7.955e-04, Loss_0: 5.619e-04, Loss_r: 2.337e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.533e-04, Loss_0: 1.763e-04, Loss_r: 7.692e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.674e-05, Loss_0: 6.123e-06, Loss_r: 1.062e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 3.046e-05, Loss_0: 1.728e-05, Loss_r: 1.318e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 8.412e-06, Loss_0: 7.736e-07, Loss_r: 7.638e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.000e-05, Loss_0: 2.263e-06, Loss_r: 7.740e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 7.475e-06, Loss_0: 7.073e-07, Loss_r: 6.768e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 6.193e-06, Loss_0: 4.328e-08, Loss_r: 6.149e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.997e-06, Loss_0: 1.179e-07, Loss_r: 5.879e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.587e-06, Loss_0: 5.361e-09, Loss_r: 5.582e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 6.281e-06, Loss_0: 6.821e-07, Loss_r: 5.599e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 2.052e-04, Loss_0: 1.457e-04, Loss_r: 5.953e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.199e-04, Loss_0: 2.376e-04, Loss_r: 8.225e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.919e-05, Loss_0: 3.988e-05, Loss_r: 1.931e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.002e-05, Loss_0: 2.279e-06, Loss_r: 7.736e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.242e-05, Loss_0: 1.269e-05, Loss_r: 9.726e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 6.746e-06, Loss_0: 8.346e-07, Loss_r: 5.911e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 7.712e-06, Loss_0: 1.834e-06, Loss_r: 5.878e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.447e-06, Loss_0: 3.171e-07, Loss_r: 5.130e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.904e-06, Loss_0: 8.172e-08, Loss_r: 4.823e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 4.699e-06, Loss_0: 7.740e-08, Loss_r: 4.622e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.444e-06, Loss_0: 2.339e-08, Loss_r: 4.420e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 4.257e-06, Loss_0: 1.323e-08, Loss_r: 4.243e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.084e-06, Loss_0: 1.852e-09, Loss_r: 4.082e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.936e-06, Loss_0: 5.262e-09, Loss_r: 3.931e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.789e-06, Loss_0: 1.321e-09, Loss_r: 3.788e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.690e-06, Loss_0: 2.887e-08, Loss_r: 3.661e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.155e-05, Loss_0: 1.342e-05, Loss_r: 8.128e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.670e-05, Loss_0: 4.920e-05, Loss_r: 1.750e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.347e-04, Loss_0: 1.660e-04, Loss_r: 6.871e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.479e-05, Loss_0: 5.105e-06, Loss_r: 9.690e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 3.174e-05, Loss_0: 1.924e-05, Loss_r: 1.250e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.198e-05, Loss_0: 1.210e-05, Loss_r: 9.878e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 6.960e-06, Loss_0: 1.561e-06, Loss_r: 5.399e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.694e-06, Loss_0: 3.627e-08, Loss_r: 4.658e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.421e-06, Loss_0: 8.195e-13, Loss_r: 4.421e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.249e-06, Loss_0: 3.792e-09, Loss_r: 4.245e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.114e-06, Loss_0: 3.161e-08, Loss_r: 4.083e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 3.960e-06, Loss_0: 2.510e-08, Loss_r: 3.935e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 3.782e-06, Loss_0: 5.426e-09, Loss_r: 3.776e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.639e-06, Loss_0: 2.092e-09, Loss_r: 3.637e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.506e-06, Loss_0: 3.754e-10, Loss_r: 3.506e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.381e-06, Loss_0: 2.545e-10, Loss_r: 3.381e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.262e-06, Loss_0: 1.368e-12, Loss_r: 3.262e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.149e-06, Loss_0: 1.847e-11, Loss_r: 3.149e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 3.042e-06, Loss_0: 2.229e-11, Loss_r: 3.042e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.940e-06, Loss_0: 1.343e-11, Loss_r: 2.940e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.842e-06, Loss_0: 1.046e-11, Loss_r: 2.842e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.748e-06, Loss_0: 2.195e-11, Loss_r: 2.748e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.659e-06, Loss_0: 3.068e-11, Loss_r: 2.659e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.573e-06, Loss_0: 2.522e-11, Loss_r: 2.573e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.491e-06, Loss_0: 1.841e-11, Loss_r: 2.491e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.412e-06, Loss_0: 1.421e-11, Loss_r: 2.412e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.336e-06, Loss_0: 2.643e-11, Loss_r: 2.336e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.263e-06, Loss_0: 6.104e-11, Loss_r: 2.263e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.194e-06, Loss_0: 9.618e-10, Loss_r: 2.193e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.471e-06, Loss_0: 2.606e-07, Loss_r: 2.210e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 3.757e-04, Loss_0: 2.796e-04, Loss_r: 9.611e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.152e-05, Loss_0: 1.447e-05, Loss_r: 7.056e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.893e-05, Loss_0: 7.271e-06, Loss_r: 1.166e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 8.807e-05, Loss_0: 7.163e-05, Loss_r: 1.644e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 5.742e-06, Loss_0: 1.868e-08, Loss_r: 5.723e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.329e-05, Loss_0: 6.919e-06, Loss_r: 6.372e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 4.219e-06, Loss_0: 5.139e-07, Loss_r: 3.705e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 3.905e-06, Loss_0: 3.868e-07, Loss_r: 3.518e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.737e-06, Loss_0: 3.349e-07, Loss_r: 3.402e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.262e-06, Loss_0: 7.947e-08, Loss_r: 3.183e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.038e-06, Loss_0: 6.426e-09, Loss_r: 3.031e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.901e-06, Loss_0: 6.464e-10, Loss_r: 2.901e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.783e-06, Loss_0: 1.505e-11, Loss_r: 2.783e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.672e-06, Loss_0: 2.356e-10, Loss_r: 2.672e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.567e-06, Loss_0: 2.876e-11, Loss_r: 2.567e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.468e-06, Loss_0: 3.773e-10, Loss_r: 2.468e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.375e-06, Loss_0: 9.497e-12, Loss_r: 2.375e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.286e-06, Loss_0: 5.197e-11, Loss_r: 2.286e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.202e-06, Loss_0: 5.576e-11, Loss_r: 2.202e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 2.122e-06, Loss_0: 1.109e-11, Loss_r: 2.122e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.047e-06, Loss_0: 1.496e-11, Loss_r: 2.047e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.975e-06, Loss_0: 2.699e-11, Loss_r: 1.975e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.906e-06, Loss_0: 2.739e-11, Loss_r: 1.906e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.841e-06, Loss_0: 2.087e-11, Loss_r: 1.841e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.779e-06, Loss_0: 1.491e-11, Loss_r: 1.779e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.720e-06, Loss_0: 1.362e-11, Loss_r: 1.720e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.663e-06, Loss_0: 1.410e-11, Loss_r: 1.663e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.609e-06, Loss_0: 1.397e-11, Loss_r: 1.609e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.557e-06, Loss_0: 1.605e-11, Loss_r: 1.557e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.508e-06, Loss_0: 1.584e-11, Loss_r: 1.508e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.461e-06, Loss_0: 1.166e-11, Loss_r: 1.461e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.416e-06, Loss_0: 1.235e-11, Loss_r: 1.416e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.373e-06, Loss_0: 8.058e-12, Loss_r: 1.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.332e-06, Loss_0: 9.892e-12, Loss_r: 1.332e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.293e-06, Loss_0: 9.956e-12, Loss_r: 1.293e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.255e-06, Loss_0: 1.534e-11, Loss_r: 1.255e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.219e-06, Loss_0: 1.038e-11, Loss_r: 1.219e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.184e-06, Loss_0: 9.618e-12, Loss_r: 1.184e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.151e-06, Loss_0: 3.449e-11, Loss_r: 1.151e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.120e-06, Loss_0: 1.143e-09, Loss_r: 1.119e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.471e-06, Loss_0: 2.805e-07, Loss_r: 1.191e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.727e-04, Loss_0: 1.985e-04, Loss_r: 7.421e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 4.078e-04, Loss_0: 3.357e-04, Loss_r: 7.212e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.497e-04, Loss_0: 1.381e-04, Loss_r: 1.158e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 6.271e-06, Loss_0: 3.344e-07, Loss_r: 5.936e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.884e-05, Loss_0: 1.521e-05, Loss_r: 3.636e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 9.020e-06, Loss_0: 3.273e-06, Loss_r: 5.747e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.859e-06, Loss_0: 1.161e-07, Loss_r: 2.743e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.255e-06, Loss_0: 8.000e-12, Loss_r: 2.255e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 2.174e-06, Loss_0: 6.722e-08, Loss_r: 2.107e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 2.021e-06, Loss_0: 9.182e-08, Loss_r: 1.929e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.874e-06, Loss_0: 3.911e-08, Loss_r: 1.835e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.771e-06, Loss_0: 6.124e-09, Loss_r: 1.765e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.691e-06, Loss_0: 2.102e-09, Loss_r: 1.689e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.620e-06, Loss_0: 3.580e-10, Loss_r: 1.619e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.556e-06, Loss_0: 3.342e-10, Loss_r: 1.555e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.497e-06, Loss_0: 9.933e-12, Loss_r: 1.497e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.442e-06, Loss_0: 7.167e-13, Loss_r: 1.442e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.390e-06, Loss_0: 6.620e-11, Loss_r: 1.389e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.341e-06, Loss_0: 2.329e-11, Loss_r: 1.340e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.294e-06, Loss_0: 7.666e-12, Loss_r: 1.294e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.251e-06, Loss_0: 1.676e-11, Loss_r: 1.251e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.209e-06, Loss_0: 2.151e-11, Loss_r: 1.209e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.170e-06, Loss_0: 1.698e-11, Loss_r: 1.170e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.133e-06, Loss_0: 1.508e-11, Loss_r: 1.133e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.098e-06, Loss_0: 1.484e-11, Loss_r: 1.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.065e-06, Loss_0: 1.235e-11, Loss_r: 1.065e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.033e-06, Loss_0: 1.331e-11, Loss_r: 1.033e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.003e-06, Loss_0: 1.226e-11, Loss_r: 1.003e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 9.744e-07, Loss_0: 1.023e-11, Loss_r: 9.744e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 9.471e-07, Loss_0: 1.061e-11, Loss_r: 9.471e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 9.210e-07, Loss_0: 1.075e-11, Loss_r: 9.210e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 8.962e-07, Loss_0: 1.133e-11, Loss_r: 8.961e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 8.724e-07, Loss_0: 9.659e-12, Loss_r: 8.723e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 8.496e-07, Loss_0: 7.416e-12, Loss_r: 8.496e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 8.278e-07, Loss_0: 7.708e-12, Loss_r: 8.278e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 8.070e-07, Loss_0: 6.234e-12, Loss_r: 8.070e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.870e-07, Loss_0: 7.411e-12, Loss_r: 7.870e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 7.677e-07, Loss_0: 8.116e-12, Loss_r: 7.677e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 7.492e-07, Loss_0: 4.963e-12, Loss_r: 7.492e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 7.315e-07, Loss_0: 5.671e-12, Loss_r: 7.315e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 7.144e-07, Loss_0: 9.411e-12, Loss_r: 7.144e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 6.979e-07, Loss_0: 2.611e-11, Loss_r: 6.979e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 6.826e-07, Loss_0: 4.167e-10, Loss_r: 6.822e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 7.268e-07, Loss_0: 3.593e-08, Loss_r: 6.908e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.754e-05, Loss_0: 9.884e-06, Loss_r: 7.659e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 6.715e-04, Loss_0: 5.510e-04, Loss_r: 1.206e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.590e-05, Loss_0: 2.783e-05, Loss_r: 8.066e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 6.437e-05, Loss_0: 5.947e-05, Loss_r: 4.903e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.627e-05, Loss_0: 2.378e-05, Loss_r: 2.497e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 7.722e-06, Loss_0: 6.110e-06, Loss_r: 1.612e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.465e-06, Loss_0: 8.362e-07, Loss_r: 1.629e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.766e-06, Loss_0: 7.473e-10, Loss_r: 1.765e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.594e-06, Loss_0: 3.684e-08, Loss_r: 1.558e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.252e-06, Loss_0: 2.542e-09, Loss_r: 1.250e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.150e-06, Loss_0: 1.675e-08, Loss_r: 1.133e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.107e-06, Loss_0: 8.748e-09, Loss_r: 1.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.060e-06, Loss_0: 3.046e-10, Loss_r: 1.059e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.027e-06, Loss_0: 1.928e-09, Loss_r: 1.025e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 9.943e-07, Loss_0: 8.535e-11, Loss_r: 9.942e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 9.639e-07, Loss_0: 1.982e-11, Loss_r: 9.639e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 9.358e-07, Loss_0: 2.439e-12, Loss_r: 9.358e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 9.091e-07, Loss_0: 3.536e-12, Loss_r: 9.091e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 8.839e-07, Loss_0: 6.505e-13, Loss_r: 8.839e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 8.600e-07, Loss_0: 7.703e-12, Loss_r: 8.599e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 8.372e-07, Loss_0: 8.053e-12, Loss_r: 8.372e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.155e-07, Loss_0: 6.844e-12, Loss_r: 8.155e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 7.949e-07, Loss_0: 5.530e-12, Loss_r: 7.949e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 7.751e-07, Loss_0: 1.046e-11, Loss_r: 7.751e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 7.562e-07, Loss_0: 1.164e-11, Loss_r: 7.562e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 7.381e-07, Loss_0: 1.149e-11, Loss_r: 7.381e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 7.207e-07, Loss_0: 1.445e-11, Loss_r: 7.207e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 7.040e-07, Loss_0: 6.258e-12, Loss_r: 7.039e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.879e-07, Loss_0: 1.043e-11, Loss_r: 6.879e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 6.723e-07, Loss_0: 8.716e-12, Loss_r: 6.723e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 6.573e-07, Loss_0: 7.885e-12, Loss_r: 6.573e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 6.428e-07, Loss_0: 8.552e-12, Loss_r: 6.428e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 6.288e-07, Loss_0: 7.441e-12, Loss_r: 6.288e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 6.152e-07, Loss_0: 6.863e-12, Loss_r: 6.152e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 6.020e-07, Loss_0: 1.671e-12, Loss_r: 6.020e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.893e-07, Loss_0: 5.600e-12, Loss_r: 5.893e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 5.787e-07, Loss_0: 8.824e-10, Loss_r: 5.778e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 7.784e-07, Loss_0: 1.146e-07, Loss_r: 6.638e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 5.773e-05, Loss_0: 3.133e-05, Loss_r: 2.641e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 2.825e-05, Loss_0: 3.251e-06, Loss_r: 2.500e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 5.442e-05, Loss_0: 4.075e-05, Loss_r: 1.367e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.676e-05, Loss_0: 1.723e-05, Loss_r: 9.536e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.710e-05, Loss_0: 1.279e-05, Loss_r: 4.313e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 4.026e-06, Loss_0: 1.104e-06, Loss_r: 2.921e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.291e-06, Loss_0: 2.005e-07, Loss_r: 2.091e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.339e-06, Loss_0: 1.605e-07, Loss_r: 1.179e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.040e-06, Loss_0: 7.780e-09, Loss_r: 1.032e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.042e-06, Loss_0: 1.337e-08, Loss_r: 1.029e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 9.525e-07, Loss_0: 1.426e-08, Loss_r: 9.382e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 9.068e-07, Loss_0: 2.371e-09, Loss_r: 9.044e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 8.793e-07, Loss_0: 2.744e-09, Loss_r: 8.765e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 8.505e-07, Loss_0: 1.630e-12, Loss_r: 8.505e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 8.251e-07, Loss_0: 6.061e-11, Loss_r: 8.250e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 8.018e-07, Loss_0: 1.644e-11, Loss_r: 8.018e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 7.797e-07, Loss_0: 5.061e-11, Loss_r: 7.797e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 7.588e-07, Loss_0: 5.720e-11, Loss_r: 7.588e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 7.389e-07, Loss_0: 3.302e-11, Loss_r: 7.388e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 7.199e-07, Loss_0: 1.663e-11, Loss_r: 7.198e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 7.017e-07, Loss_0: 7.864e-12, Loss_r: 7.016e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 6.843e-07, Loss_0: 7.801e-12, Loss_r: 6.843e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 6.675e-07, Loss_0: 7.204e-12, Loss_r: 6.675e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 6.515e-07, Loss_0: 9.043e-12, Loss_r: 6.514e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 6.360e-07, Loss_0: 9.190e-12, Loss_r: 6.360e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 6.210e-07, Loss_0: 8.793e-12, Loss_r: 6.210e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 6.066e-07, Loss_0: 7.025e-12, Loss_r: 6.066e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 5.925e-07, Loss_0: 1.117e-11, Loss_r: 5.925e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 5.790e-07, Loss_0: 1.006e-11, Loss_r: 5.790e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 5.658e-07, Loss_0: 6.937e-12, Loss_r: 5.658e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 5.531e-07, Loss_0: 5.236e-12, Loss_r: 5.531e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 5.406e-07, Loss_0: 3.652e-12, Loss_r: 5.406e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 5.286e-07, Loss_0: 5.382e-12, Loss_r: 5.286e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 5.168e-07, Loss_0: 6.106e-13, Loss_r: 5.168e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 5.055e-07, Loss_0: 5.770e-11, Loss_r: 5.054e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 5.049e-07, Loss_0: 6.564e-09, Loss_r: 4.983e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 2.377e-06, Loss_0: 1.220e-06, Loss_r: 1.157e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 5.027e-07, Loss_0: 1.783e-08, Loss_r: 4.849e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 6.771e-07, Loss_0: 1.387e-07, Loss_r: 5.383e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 5.283e-07, Loss_0: 4.755e-08, Loss_r: 4.808e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 4.675e-07, Loss_0: 1.244e-08, Loss_r: 4.550e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 4.430e-07, Loss_0: 2.619e-09, Loss_r: 4.404e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 4.362e-07, Loss_0: 4.311e-09, Loss_r: 4.319e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 4.231e-07, Loss_0: 9.239e-10, Loss_r: 4.222e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 4.134e-07, Loss_0: 2.943e-10, Loss_r: 4.131e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 4.050e-07, Loss_0: 6.780e-11, Loss_r: 4.050e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 3.970e-07, Loss_0: 1.372e-10, Loss_r: 3.969e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 3.890e-07, Loss_0: 7.754e-12, Loss_r: 3.890e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 3.813e-07, Loss_0: 4.171e-12, Loss_r: 3.813e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 3.737e-07, Loss_0: 4.084e-14, Loss_r: 3.737e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 3.663e-07, Loss_0: 2.810e-16, Loss_r: 3.663e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 3.592e-07, Loss_0: 9.216e-11, Loss_r: 3.591e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 3.719e-07, Loss_0: 1.223e-08, Loss_r: 3.597e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 4.819e-06, Loss_0: 2.815e-06, Loss_r: 2.004e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.755e-07, Loss_0: 2.157e-08, Loss_r: 3.539e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 8.782e-07, Loss_0: 3.482e-07, Loss_r: 5.301e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 4.903e-07, Loss_0: 1.035e-07, Loss_r: 3.868e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 3.704e-07, Loss_0: 2.939e-08, Loss_r: 3.410e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 3.298e-07, Loss_0: 7.505e-09, Loss_r: 3.223e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 3.266e-07, Loss_0: 9.709e-09, Loss_r: 3.169e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 3.090e-07, Loss_0: 1.629e-09, Loss_r: 3.074e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 3.010e-07, Loss_0: 2.359e-10, Loss_r: 3.007e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 2.953e-07, Loss_0: 3.757e-11, Loss_r: 2.953e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 2.900e-07, Loss_0: 1.181e-10, Loss_r: 2.899e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 2.847e-07, Loss_0: 6.174e-11, Loss_r: 2.847e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 2.794e-07, Loss_0: 1.522e-11, Loss_r: 2.794e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 2.744e-07, Loss_0: 6.278e-11, Loss_r: 2.743e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 2.696e-07, Loss_0: 1.775e-10, Loss_r: 2.694e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 2.680e-07, Loss_0: 2.361e-09, Loss_r: 2.656e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 4.519e-07, Loss_0: 1.216e-07, Loss_r: 3.303e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.793e-07, Loss_0: 1.534e-08, Loss_r: 2.639e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.528e-07, Loss_0: 8.985e-10, Loss_r: 2.519e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.571e-07, Loss_0: 5.989e-09, Loss_r: 2.511e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 2.447e-07, Loss_0: 8.873e-10, Loss_r: 2.438e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 2.399e-07, Loss_0: 5.528e-10, Loss_r: 2.393e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 2.354e-07, Loss_0: 1.400e-10, Loss_r: 2.352e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 2.315e-07, Loss_0: 1.019e-10, Loss_r: 2.314e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 2.275e-07, Loss_0: 2.185e-11, Loss_r: 2.275e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 2.238e-07, Loss_0: 2.864e-12, Loss_r: 2.238e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 2.201e-07, Loss_0: 1.430e-13, Loss_r: 2.201e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 2.165e-07, Loss_0: 4.344e-12, Loss_r: 2.165e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 2.129e-07, Loss_0: 3.244e-12, Loss_r: 2.129e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 2.094e-07, Loss_0: 9.059e-13, Loss_r: 2.094e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 2.060e-07, Loss_0: 4.103e-12, Loss_r: 2.060e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 2.026e-07, Loss_0: 7.214e-12, Loss_r: 2.026e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 1.993e-07, Loss_0: 2.350e-11, Loss_r: 1.993e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 1.965e-07, Loss_0: 3.384e-10, Loss_r: 1.962e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 2.286e-07, Loss_0: 2.254e-08, Loss_r: 2.061e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.935e-07, Loss_0: 2.256e-09, Loss_r: 1.912e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.880e-07, Loss_0: 4.138e-10, Loss_r: 1.876e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.866e-07, Loss_0: 1.219e-09, Loss_r: 1.854e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.819e-07, Loss_0: 6.442e-11, Loss_r: 1.819e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.794e-07, Loss_0: 1.687e-10, Loss_r: 1.792e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.765e-07, Loss_0: 1.146e-11, Loss_r: 1.765e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.740e-07, Loss_0: 1.197e-11, Loss_r: 1.740e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.714e-07, Loss_0: 1.379e-11, Loss_r: 1.714e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 1.689e-07, Loss_0: 1.194e-13, Loss_r: 1.689e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 1.665e-07, Loss_0: 1.299e-12, Loss_r: 1.665e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 1.640e-07, Loss_0: 1.908e-13, Loss_r: 1.640e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 1.617e-07, Loss_0: 1.520e-12, Loss_r: 1.617e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 1.593e-07, Loss_0: 3.095e-12, Loss_r: 1.593e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 1.570e-07, Loss_0: 4.946e-12, Loss_r: 1.570e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 1.548e-07, Loss_0: 2.168e-13, Loss_r: 1.548e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 1.525e-07, Loss_0: 1.804e-13, Loss_r: 1.525e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 1.504e-07, Loss_0: 2.844e-11, Loss_r: 1.504e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 1.527e-07, Loss_0: 2.669e-09, Loss_r: 1.500e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 8.467e-07, Loss_0: 4.284e-07, Loss_r: 4.183e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.662e-07, Loss_0: 1.321e-08, Loss_r: 1.530e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.981e-07, Loss_0: 3.445e-08, Loss_r: 1.637e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.776e-07, Loss_0: 2.297e-08, Loss_r: 1.547e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.402e-07, Loss_0: 6.579e-10, Loss_r: 1.395e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.423e-07, Loss_0: 3.017e-09, Loss_r: 1.393e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.365e-07, Loss_0: 6.548e-10, Loss_r: 1.359e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.338e-07, Loss_0: 3.162e-11, Loss_r: 1.338e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.323e-07, Loss_0: 9.410e-11, Loss_r: 1.322e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 1.306e-07, Loss_0: 8.320e-11, Loss_r: 1.305e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 1.289e-07, Loss_0: 1.988e-11, Loss_r: 1.288e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 1.272e-07, Loss_0: 1.276e-11, Loss_r: 1.272e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 1.256e-07, Loss_0: 3.345e-13, Loss_r: 1.256e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 1.241e-07, Loss_0: 4.622e-13, Loss_r: 1.241e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 1.225e-07, Loss_0: 5.705e-13, Loss_r: 1.225e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 1.210e-07, Loss_0: 1.990e-13, Loss_r: 1.210e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 1.195e-07, Loss_0: 2.933e-12, Loss_r: 1.195e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 1.185e-07, Loss_0: 2.228e-10, Loss_r: 1.183e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 1.535e-07, Loss_0: 2.229e-08, Loss_r: 1.312e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.182e-07, Loss_0: 1.653e-09, Loss_r: 1.165e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.153e-07, Loss_0: 7.636e-10, Loss_r: 1.145e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.150e-07, Loss_0: 1.410e-09, Loss_r: 1.136e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.117e-07, Loss_0: 5.091e-11, Loss_r: 1.116e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.107e-07, Loss_0: 1.428e-10, Loss_r: 1.105e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.092e-07, Loss_0: 2.256e-13, Loss_r: 1.092e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.081e-07, Loss_0: 3.014e-11, Loss_r: 1.081e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.069e-07, Loss_0: 3.536e-12, Loss_r: 1.069e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.057e-07, Loss_0: 3.363e-12, Loss_r: 1.057e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 1.046e-07, Loss_0: 1.458e-15, Loss_r: 1.046e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 1.035e-07, Loss_0: 1.414e-12, Loss_r: 1.035e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 1.024e-07, Loss_0: 1.149e-13, Loss_r: 1.024e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 1.013e-07, Loss_0: 4.610e-13, Loss_r: 1.013e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 1.003e-07, Loss_0: 9.930e-13, Loss_r: 1.003e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 9.924e-08, Loss_0: 1.410e-12, Loss_r: 9.924e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 9.821e-08, Loss_0: 4.588e-14, Loss_r: 9.821e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 9.720e-08, Loss_0: 1.200e-13, Loss_r: 9.720e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 9.625e-08, Loss_0: 2.479e-11, Loss_r: 9.623e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 9.993e-08, Loss_0: 2.817e-09, Loss_r: 9.711e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 1.119e-06, Loss_0: 6.239e-07, Loss_r: 4.950e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.058e-07, Loss_0: 7.347e-09, Loss_r: 9.841e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.058e-07, Loss_0: 6.946e-08, Loss_r: 1.363e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.353e-07, Loss_0: 2.680e-08, Loss_r: 1.085e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 9.756e-08, Loss_0: 3.865e-09, Loss_r: 9.369e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 9.551e-08, Loss_0: 3.103e-09, Loss_r: 9.241e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 9.207e-08, Loss_0: 1.620e-09, Loss_r: 9.045e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 8.875e-08, Loss_0: 1.705e-11, Loss_r: 8.873e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 8.801e-08, Loss_0: 4.529e-11, Loss_r: 8.796e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 8.723e-08, Loss_0: 6.430e-11, Loss_r: 8.717e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 8.641e-08, Loss_0: 1.544e-11, Loss_r: 8.639e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 8.563e-08, Loss_0: 9.004e-12, Loss_r: 8.562e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 8.486e-08, Loss_0: 2.466e-12, Loss_r: 8.486e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 8.412e-08, Loss_0: 2.265e-12, Loss_r: 8.412e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 8.339e-08, Loss_0: 2.177e-13, Loss_r: 8.339e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 8.267e-08, Loss_0: 4.709e-14, Loss_r: 8.267e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 8.196e-08, Loss_0: 2.319e-12, Loss_r: 8.196e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 8.146e-08, Loss_0: 1.192e-10, Loss_r: 8.134e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 9.580e-08, Loss_0: 9.195e-09, Loss_r: 8.660e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 8.148e-08, Loss_0: 9.119e-10, Loss_r: 8.057e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 7.956e-08, Loss_0: 1.507e-10, Loss_r: 7.941e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 7.954e-08, Loss_0: 5.313e-10, Loss_r: 7.901e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 7.824e-08, Loss_0: 7.389e-11, Loss_r: 7.817e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 7.761e-08, Loss_0: 3.135e-11, Loss_r: 7.758e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 7.698e-08, Loss_0: 5.283e-12, Loss_r: 7.698e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 7.640e-08, Loss_0: 1.109e-11, Loss_r: 7.639e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 7.581e-08, Loss_0: 7.144e-14, Loss_r: 7.581e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 7.526e-08, Loss_0: 9.024e-15, Loss_r: 7.526e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 7.470e-08, Loss_0: 1.657e-12, Loss_r: 7.470e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 7.415e-08, Loss_0: 4.236e-14, Loss_r: 7.415e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 7.360e-08, Loss_0: 2.730e-13, Loss_r: 7.360e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 7.307e-08, Loss_0: 1.973e-13, Loss_r: 7.307e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 7.255e-08, Loss_0: 5.999e-14, Loss_r: 7.254e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 7.202e-08, Loss_0: 7.395e-14, Loss_r: 7.202e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 7.150e-08, Loss_0: 1.031e-12, Loss_r: 7.150e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 7.099e-08, Loss_0: 5.539e-12, Loss_r: 7.099e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 7.069e-08, Loss_0: 1.217e-10, Loss_r: 7.057e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 8.047e-08, Loss_0: 6.336e-09, Loss_r: 7.413e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 7.107e-08, Loss_0: 9.130e-10, Loss_r: 7.015e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 6.911e-08, Loss_0: 1.457e-11, Loss_r: 6.910e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 6.910e-08, Loss_0: 2.927e-10, Loss_r: 6.880e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 6.836e-08, Loss_0: 1.015e-10, Loss_r: 6.826e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 6.778e-08, Loss_0: 2.782e-12, Loss_r: 6.778e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 6.739e-08, Loss_0: 1.330e-11, Loss_r: 6.738e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 6.694e-08, Loss_0: 1.654e-12, Loss_r: 6.694e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 6.652e-08, Loss_0: 2.293e-12, Loss_r: 6.652e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 6.611e-08, Loss_0: 3.185e-13, Loss_r: 6.611e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 6.571e-08, Loss_0: 8.330e-13, Loss_r: 6.571e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 6.530e-08, Loss_0: 8.606e-14, Loss_r: 6.530e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 6.489e-08, Loss_0: 2.074e-13, Loss_r: 6.489e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 6.449e-08, Loss_0: 1.552e-13, Loss_r: 6.449e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 6.411e-08, Loss_0: 2.978e-13, Loss_r: 6.411e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 6.372e-08, Loss_0: 7.729e-13, Loss_r: 6.372e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 6.334e-08, Loss_0: 8.639e-13, Loss_r: 6.334e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 6.295e-08, Loss_0: 1.050e-13, Loss_r: 6.295e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 6.258e-08, Loss_0: 5.318e-13, Loss_r: 6.257e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 6.224e-08, Loss_0: 2.130e-11, Loss_r: 6.222e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 6.412e-08, Loss_0: 1.385e-09, Loss_r: 6.274e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 3.920e-07, Loss_0: 2.022e-07, Loss_r: 1.898e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 7.492e-08, Loss_0: 8.362e-09, Loss_r: 6.656e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 8.154e-08, Loss_0: 1.281e-08, Loss_r: 6.873e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 7.968e-08, Loss_0: 1.186e-08, Loss_r: 6.781e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 6.023e-08, Loss_0: 2.754e-12, Loss_r: 6.023e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 6.258e-08, Loss_0: 1.609e-09, Loss_r: 6.097e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 5.966e-08, Loss_0: 4.473e-11, Loss_r: 5.961e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 5.947e-08, Loss_0: 1.317e-10, Loss_r: 5.934e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 5.910e-08, Loss_0: 7.499e-11, Loss_r: 5.903e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 5.869e-08, Loss_0: 2.462e-11, Loss_r: 5.866e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 5.836e-08, Loss_0: 3.522e-12, Loss_r: 5.836e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 5.805e-08, Loss_0: 3.264e-12, Loss_r: 5.805e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 5.776e-08, Loss_0: 3.323e-13, Loss_r: 5.776e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 5.747e-08, Loss_0: 4.190e-12, Loss_r: 5.746e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 5.715e-08, Loss_0: 2.400e-13, Loss_r: 5.715e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 5.687e-08, Loss_0: 3.443e-15, Loss_r: 5.687e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 5.657e-08, Loss_0: 2.193e-14, Loss_r: 5.657e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 5.629e-08, Loss_0: 2.310e-12, Loss_r: 5.629e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 5.621e-08, Loss_0: 1.240e-10, Loss_r: 5.609e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 6.989e-08, Loss_0: 8.691e-09, Loss_r: 6.120e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 5.707e-08, Loss_0: 9.512e-10, Loss_r: 5.612e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 5.533e-08, Loss_0: 8.419e-11, Loss_r: 5.524e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 5.565e-08, Loss_0: 4.473e-10, Loss_r: 5.520e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 5.484e-08, Loss_0: 8.764e-11, Loss_r: 5.475e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 5.449e-08, Loss_0: 2.044e-11, Loss_r: 5.447e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 5.423e-08, Loss_0: 1.280e-11, Loss_r: 5.422e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 5.397e-08, Loss_0: 8.470e-12, Loss_r: 5.396e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 5.371e-08, Loss_0: 1.040e-12, Loss_r: 5.371e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 5.348e-08, Loss_0: 5.832e-13, Loss_r: 5.348e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 5.324e-08, Loss_0: 1.304e-12, Loss_r: 5.324e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 5.299e-08, Loss_0: 1.494e-13, Loss_r: 5.299e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 5.275e-08, Loss_0: 3.302e-13, Loss_r: 5.275e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 5.252e-08, Loss_0: 7.437e-13, Loss_r: 5.252e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 5.229e-08, Loss_0: 6.475e-13, Loss_r: 5.229e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 5.206e-08, Loss_0: 2.810e-16, Loss_r: 5.206e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 5.182e-08, Loss_0: 2.276e-14, Loss_r: 5.182e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 5.158e-08, Loss_0: 1.312e-14, Loss_r: 5.158e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 5.136e-08, Loss_0: 1.367e-13, Loss_r: 5.136e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 5.114e-08, Loss_0: 3.030e-12, Loss_r: 5.113e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 5.116e-08, Loss_0: 1.622e-10, Loss_r: 5.100e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 8.626e-08, Loss_0: 2.201e-08, Loss_r: 6.425e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 5.193e-08, Loss_0: 9.198e-10, Loss_r: 5.101e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 5.255e-08, Loss_0: 1.375e-09, Loss_r: 5.117e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.209e-08, Loss_0: 1.212e-09, Loss_r: 5.088e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 4.989e-08, Loss_0: 2.004e-12, Loss_r: 4.989e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 4.999e-08, Loss_0: 2.008e-10, Loss_r: 4.979e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 4.950e-08, Loss_0: 4.975e-12, Loss_r: 4.950e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 4.931e-08, Loss_0: 1.357e-11, Loss_r: 4.930e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 4.912e-08, Loss_0: 1.568e-11, Loss_r: 4.911e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 4.891e-08, Loss_0: 3.613e-12, Loss_r: 4.891e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 4.871e-08, Loss_0: 2.629e-12, Loss_r: 4.871e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 4.852e-08, Loss_0: 4.496e-13, Loss_r: 4.852e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 4.832e-08, Loss_0: 4.801e-13, Loss_r: 4.832e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 4.813e-08, Loss_0: 3.610e-14, Loss_r: 4.813e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 4.795e-08, Loss_0: 2.883e-12, Loss_r: 4.795e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 4.775e-08, Loss_0: 5.636e-12, Loss_r: 4.775e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 4.756e-08, Loss_0: 6.300e-12, Loss_r: 4.756e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 4.741e-08, Loss_0: 2.366e-11, Loss_r: 4.739e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 4.808e-08, Loss_0: 5.627e-10, Loss_r: 4.751e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 9.819e-08, Loss_0: 3.179e-08, Loss_r: 6.640e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 5.363e-08, Loss_0: 4.255e-09, Loss_r: 4.938e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 4.686e-08, Loss_0: 1.132e-10, Loss_r: 4.675e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 4.875e-08, Loss_0: 1.364e-09, Loss_r: 4.738e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 4.703e-08, Loss_0: 4.094e-10, Loss_r: 4.662e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 4.623e-08, Loss_0: 3.258e-11, Loss_r: 4.620e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 4.614e-08, Loss_0: 8.079e-11, Loss_r: 4.606e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 4.587e-08, Loss_0: 8.330e-12, Loss_r: 4.587e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 4.569e-08, Loss_0: 3.171e-12, Loss_r: 4.569e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 4.554e-08, Loss_0: 7.040e-12, Loss_r: 4.553e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 4.537e-08, Loss_0: 9.256e-13, Loss_r: 4.537e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 4.521e-08, Loss_0: 4.471e-13, Loss_r: 4.521e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 4.504e-08, Loss_0: 1.259e-13, Loss_r: 4.504e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 4.488e-08, Loss_0: 1.402e-13, Loss_r: 4.488e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 4.473e-08, Loss_0: 2.779e-13, Loss_r: 4.473e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 4.457e-08, Loss_0: 3.207e-12, Loss_r: 4.456e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 4.441e-08, Loss_0: 3.030e-12, Loss_r: 4.440e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 4.424e-08, Loss_0: 2.020e-12, Loss_r: 4.423e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 4.409e-08, Loss_0: 1.596e-13, Loss_r: 4.409e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 4.394e-08, Loss_0: 8.982e-12, Loss_r: 4.393e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 4.443e-08, Loss_0: 4.053e-10, Loss_r: 4.402e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 1.114e-07, Loss_0: 4.197e-08, Loss_r: 6.939e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 4.852e-08, Loss_0: 3.109e-09, Loss_r: 4.541e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 4.540e-08, Loss_0: 1.307e-09, Loss_r: 4.410e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 4.719e-08, Loss_0: 2.521e-09, Loss_r: 4.467e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 4.326e-08, Loss_0: 1.327e-10, Loss_r: 4.313e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 4.335e-08, Loss_0: 2.542e-10, Loss_r: 4.309e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 4.281e-08, Loss_0: 1.447e-11, Loss_r: 4.279e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 4.271e-08, Loss_0: 4.545e-11, Loss_r: 4.266e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 4.250e-08, Loss_0: 2.838e-12, Loss_r: 4.250e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 4.237e-08, Loss_0: 1.604e-15, Loss_r: 4.237e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 4.223e-08, Loss_0: 3.778e-13, Loss_r: 4.223e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 4.208e-08, Loss_0: 5.508e-14, Loss_r: 4.208e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 4.195e-08, Loss_0: 9.275e-14, Loss_r: 4.195e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 4.181e-08, Loss_0: 5.691e-15, Loss_r: 4.181e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 4.167e-08, Loss_0: 1.472e-13, Loss_r: 4.167e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 4.154e-08, Loss_0: 3.578e-12, Loss_r: 4.154e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 4.140e-08, Loss_0: 8.330e-13, Loss_r: 4.140e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 4.127e-08, Loss_0: 3.905e-13, Loss_r: 4.127e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 4.114e-08, Loss_0: 6.123e-12, Loss_r: 4.113e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 4.136e-08, Loss_0: 2.245e-10, Loss_r: 4.114e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 6.909e-08, Loss_0: 1.762e-08, Loss_r: 5.148e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 18.2834\n",
            "[1, 64, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.522e-02, Loss_0: 4.926e-04, Loss_r: 3.473e-02, Time: 0.77, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.202e-02, Loss_0: 9.986e-05, Loss_r: 3.192e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.121e-02, Loss_0: 4.823e-06, Loss_r: 3.121e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 2.857e-02, Loss_0: 5.533e-08, Loss_r: 2.857e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.013e-02, Loss_0: 3.976e-05, Loss_r: 2.009e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 8.361e-03, Loss_0: 5.987e-06, Loss_r: 8.355e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 9.576e-04, Loss_0: 1.690e-05, Loss_r: 9.407e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 4.684e-04, Loss_0: 4.941e-05, Loss_r: 4.189e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.138e-04, Loss_0: 3.395e-05, Loss_r: 7.982e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 8.207e-05, Loss_0: 3.687e-06, Loss_r: 7.839e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 2.878e-05, Loss_0: 2.348e-06, Loss_r: 2.643e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.398e-05, Loss_0: 6.882e-07, Loss_r: 1.330e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.283e-05, Loss_0: 8.223e-08, Loss_r: 1.275e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.093e-05, Loss_0: 7.066e-07, Loss_r: 1.022e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 9.578e-06, Loss_0: 1.182e-10, Loss_r: 9.578e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.828e-05, Loss_0: 9.219e-06, Loss_r: 9.059e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.625e-04, Loss_0: 1.517e-04, Loss_r: 1.078e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 6.834e-05, Loss_0: 5.888e-05, Loss_r: 9.465e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.903e-05, Loss_0: 2.088e-05, Loss_r: 8.149e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.578e-05, Loss_0: 7.776e-06, Loss_r: 8.007e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 8.925e-06, Loss_0: 1.475e-06, Loss_r: 7.449e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 7.480e-06, Loss_0: 3.672e-07, Loss_r: 7.113e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 6.908e-06, Loss_0: 9.530e-08, Loss_r: 6.812e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 6.583e-06, Loss_0: 8.818e-08, Loss_r: 6.495e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 6.191e-06, Loss_0: 4.982e-09, Loss_r: 6.186e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.925e-06, Loss_0: 3.123e-08, Loss_r: 5.894e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.634e-06, Loss_0: 4.829e-09, Loss_r: 5.630e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.387e-06, Loss_0: 1.846e-08, Loss_r: 5.369e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.468e-06, Loss_0: 3.475e-07, Loss_r: 5.121e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.646e-04, Loss_0: 1.579e-04, Loss_r: 6.732e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.763e-05, Loss_0: 1.269e-05, Loss_r: 4.942e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.435e-05, Loss_0: 3.922e-05, Loss_r: 5.135e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 8.531e-06, Loss_0: 3.566e-06, Loss_r: 4.965e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 9.382e-06, Loss_0: 4.610e-06, Loss_r: 4.772e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.490e-06, Loss_0: 5.776e-10, Loss_r: 4.489e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.619e-06, Loss_0: 3.097e-07, Loss_r: 4.309e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.296e-06, Loss_0: 1.533e-07, Loss_r: 4.143e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.999e-06, Loss_0: 9.628e-09, Loss_r: 3.990e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.874e-06, Loss_0: 2.694e-08, Loss_r: 3.847e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.707e-06, Loss_0: 1.621e-09, Loss_r: 3.705e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.578e-06, Loss_0: 5.670e-09, Loss_r: 3.572e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.450e-06, Loss_0: 1.150e-10, Loss_r: 3.450e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.343e-06, Loss_0: 6.602e-09, Loss_r: 3.336e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.684e-06, Loss_0: 4.415e-07, Loss_r: 3.242e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.872e-04, Loss_0: 1.809e-04, Loss_r: 6.374e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.983e-06, Loss_0: 1.717e-06, Loss_r: 3.266e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.633e-05, Loss_0: 4.190e-05, Loss_r: 4.431e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 6.449e-06, Loss_0: 2.890e-06, Loss_r: 3.559e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 8.042e-06, Loss_0: 4.826e-06, Loss_r: 3.216e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.666e-06, Loss_0: 1.544e-06, Loss_r: 3.122e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.043e-06, Loss_0: 8.225e-09, Loss_r: 3.035e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.987e-06, Loss_0: 3.882e-08, Loss_r: 2.948e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.877e-06, Loss_0: 1.724e-08, Loss_r: 2.860e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.777e-06, Loss_0: 4.429e-11, Loss_r: 2.777e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.711e-06, Loss_0: 1.191e-08, Loss_r: 2.699e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.631e-06, Loss_0: 4.665e-09, Loss_r: 2.626e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.561e-06, Loss_0: 1.782e-09, Loss_r: 2.559e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.493e-06, Loss_0: 6.550e-10, Loss_r: 2.492e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.431e-06, Loss_0: 1.317e-10, Loss_r: 2.430e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 2.371e-06, Loss_0: 4.003e-10, Loss_r: 2.370e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 2.315e-06, Loss_0: 1.431e-09, Loss_r: 2.313e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 2.303e-06, Loss_0: 4.602e-08, Loss_r: 2.257e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.426e-05, Loss_0: 1.193e-05, Loss_r: 2.336e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.422e-05, Loss_0: 1.179e-05, Loss_r: 2.424e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.726e-05, Loss_0: 2.447e-05, Loss_r: 2.791e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.881e-05, Loss_0: 2.587e-05, Loss_r: 2.939e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 6.552e-06, Loss_0: 4.120e-06, Loss_r: 2.432e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 5.265e-06, Loss_0: 2.882e-06, Loss_r: 2.383e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.563e-06, Loss_0: 2.221e-07, Loss_r: 2.341e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.830e-06, Loss_0: 5.466e-07, Loss_r: 2.284e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.328e-06, Loss_0: 1.178e-07, Loss_r: 2.211e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.161e-06, Loss_0: 1.073e-08, Loss_r: 2.151e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.100e-06, Loss_0: 1.427e-09, Loss_r: 2.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.051e-06, Loss_0: 1.635e-09, Loss_r: 2.049e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.005e-06, Loss_0: 2.325e-09, Loss_r: 2.003e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.959e-06, Loss_0: 9.632e-10, Loss_r: 1.958e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.914e-06, Loss_0: 6.126e-11, Loss_r: 1.914e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.873e-06, Loss_0: 3.340e-10, Loss_r: 1.873e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.832e-06, Loss_0: 1.631e-11, Loss_r: 1.832e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.793e-06, Loss_0: 6.135e-11, Loss_r: 1.793e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.756e-06, Loss_0: 4.496e-13, Loss_r: 1.756e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.719e-06, Loss_0: 4.274e-11, Loss_r: 1.719e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.683e-06, Loss_0: 5.658e-12, Loss_r: 1.683e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.648e-06, Loss_0: 2.732e-11, Loss_r: 1.648e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.621e-06, Loss_0: 6.216e-09, Loss_r: 1.615e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 3.687e-06, Loss_0: 2.068e-06, Loss_r: 1.620e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 6.090e-04, Loss_0: 5.956e-04, Loss_r: 1.343e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.722e-04, Loss_0: 1.682e-04, Loss_r: 4.009e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 9.051e-06, Loss_0: 7.147e-06, Loss_r: 1.905e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 8.557e-06, Loss_0: 6.634e-06, Loss_r: 1.923e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 8.833e-06, Loss_0: 6.768e-06, Loss_r: 2.065e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.295e-06, Loss_0: 4.751e-07, Loss_r: 1.820e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.270e-06, Loss_0: 5.053e-07, Loss_r: 1.765e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.885e-06, Loss_0: 1.539e-07, Loss_r: 1.732e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.753e-06, Loss_0: 7.077e-08, Loss_r: 1.682e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.640e-06, Loss_0: 2.760e-09, Loss_r: 1.637e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.612e-06, Loss_0: 1.349e-08, Loss_r: 1.599e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.568e-06, Loss_0: 5.562e-09, Loss_r: 1.563e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.529e-06, Loss_0: 1.421e-09, Loss_r: 1.528e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.495e-06, Loss_0: 4.469e-10, Loss_r: 1.494e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.462e-06, Loss_0: 2.394e-10, Loss_r: 1.462e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.430e-06, Loss_0: 1.592e-10, Loss_r: 1.430e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.399e-06, Loss_0: 8.642e-11, Loss_r: 1.399e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.369e-06, Loss_0: 2.633e-11, Loss_r: 1.369e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.340e-06, Loss_0: 1.637e-12, Loss_r: 1.340e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.312e-06, Loss_0: 8.422e-12, Loss_r: 1.312e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.284e-06, Loss_0: 1.998e-11, Loss_r: 1.284e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.257e-06, Loss_0: 1.200e-11, Loss_r: 1.257e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.231e-06, Loss_0: 6.521e-12, Loss_r: 1.231e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.205e-06, Loss_0: 1.356e-11, Loss_r: 1.205e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.180e-06, Loss_0: 2.590e-11, Loss_r: 1.179e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.155e-06, Loss_0: 3.204e-11, Loss_r: 1.155e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.131e-06, Loss_0: 8.663e-11, Loss_r: 1.131e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.109e-06, Loss_0: 2.231e-09, Loss_r: 1.107e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.354e-06, Loss_0: 2.696e-07, Loss_r: 1.084e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 9.632e-05, Loss_0: 9.417e-05, Loss_r: 2.155e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.579e-04, Loss_0: 2.517e-04, Loss_r: 6.176e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 3.881e-05, Loss_0: 3.564e-05, Loss_r: 3.167e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.722e-05, Loss_0: 1.575e-05, Loss_r: 1.463e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 4.340e-06, Loss_0: 2.921e-06, Loss_r: 1.419e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 4.071e-06, Loss_0: 2.709e-06, Loss_r: 1.362e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.637e-06, Loss_0: 3.516e-07, Loss_r: 1.286e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.457e-06, Loss_0: 2.175e-07, Loss_r: 1.240e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.357e-06, Loss_0: 1.489e-07, Loss_r: 1.208e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.187e-06, Loss_0: 7.210e-09, Loss_r: 1.180e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.154e-06, Loss_0: 2.111e-09, Loss_r: 1.152e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.128e-06, Loss_0: 3.526e-09, Loss_r: 1.125e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.100e-06, Loss_0: 1.716e-09, Loss_r: 1.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.073e-06, Loss_0: 5.740e-10, Loss_r: 1.073e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.048e-06, Loss_0: 1.095e-10, Loss_r: 1.048e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.024e-06, Loss_0: 3.123e-13, Loss_r: 1.024e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.001e-06, Loss_0: 3.815e-11, Loss_r: 1.001e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 9.788e-07, Loss_0: 5.474e-11, Loss_r: 9.787e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 9.570e-07, Loss_0: 1.936e-11, Loss_r: 9.569e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 9.358e-07, Loss_0: 6.091e-13, Loss_r: 9.358e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 9.152e-07, Loss_0: 2.193e-11, Loss_r: 9.152e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 8.951e-07, Loss_0: 4.099e-12, Loss_r: 8.951e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 8.755e-07, Loss_0: 7.548e-12, Loss_r: 8.755e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 8.564e-07, Loss_0: 6.483e-12, Loss_r: 8.564e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 8.378e-07, Loss_0: 6.427e-12, Loss_r: 8.378e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 8.197e-07, Loss_0: 3.177e-12, Loss_r: 8.197e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.019e-07, Loss_0: 8.335e-14, Loss_r: 8.019e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 7.846e-07, Loss_0: 6.956e-12, Loss_r: 7.845e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 7.676e-07, Loss_0: 3.765e-11, Loss_r: 7.676e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 7.527e-07, Loss_0: 1.872e-09, Loss_r: 7.509e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.209e-06, Loss_0: 4.712e-07, Loss_r: 7.379e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.523e-04, Loss_0: 2.488e-04, Loss_r: 3.518e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 3.868e-05, Loss_0: 3.694e-05, Loss_r: 1.738e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 7.482e-05, Loss_0: 7.094e-05, Loss_r: 3.887e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.934e-06, Loss_0: 1.790e-06, Loss_r: 1.144e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 3.494e-06, Loss_0: 2.178e-06, Loss_r: 1.316e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 4.311e-06, Loss_0: 3.231e-06, Loss_r: 1.080e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.799e-06, Loss_0: 7.659e-07, Loss_r: 1.033e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.956e-07, Loss_0: 1.113e-08, Loss_r: 9.845e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.109e-06, Loss_0: 1.561e-07, Loss_r: 9.527e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.349e-07, Loss_0: 5.450e-09, Loss_r: 9.294e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 9.222e-07, Loss_0: 1.710e-08, Loss_r: 9.051e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 8.807e-07, Loss_0: 2.025e-10, Loss_r: 8.805e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 8.605e-07, Loss_0: 2.667e-09, Loss_r: 8.579e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 8.372e-07, Loss_0: 6.121e-10, Loss_r: 8.366e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.162e-07, Loss_0: 8.552e-12, Loss_r: 8.162e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 7.966e-07, Loss_0: 1.275e-11, Loss_r: 7.966e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 7.776e-07, Loss_0: 5.756e-12, Loss_r: 7.776e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 7.594e-07, Loss_0: 1.053e-12, Loss_r: 7.594e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 7.417e-07, Loss_0: 4.130e-13, Loss_r: 7.417e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 7.247e-07, Loss_0: 2.507e-12, Loss_r: 7.247e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.083e-07, Loss_0: 9.105e-12, Loss_r: 7.083e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 6.924e-07, Loss_0: 9.688e-12, Loss_r: 6.924e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 6.770e-07, Loss_0: 4.145e-12, Loss_r: 6.770e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 6.620e-07, Loss_0: 4.938e-12, Loss_r: 6.620e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 6.476e-07, Loss_0: 4.229e-12, Loss_r: 6.475e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 6.335e-07, Loss_0: 3.987e-12, Loss_r: 6.334e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 6.198e-07, Loss_0: 7.055e-12, Loss_r: 6.198e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 6.064e-07, Loss_0: 5.029e-12, Loss_r: 6.064e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 5.935e-07, Loss_0: 2.004e-12, Loss_r: 5.935e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 5.809e-07, Loss_0: 5.774e-12, Loss_r: 5.809e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 5.687e-07, Loss_0: 3.066e-12, Loss_r: 5.687e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 5.567e-07, Loss_0: 2.524e-12, Loss_r: 5.567e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 5.451e-07, Loss_0: 2.189e-11, Loss_r: 5.451e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 5.338e-07, Loss_0: 2.880e-11, Loss_r: 5.338e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 5.229e-07, Loss_0: 7.586e-11, Loss_r: 5.228e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 5.140e-07, Loss_0: 2.045e-09, Loss_r: 5.119e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 7.552e-07, Loss_0: 2.521e-07, Loss_r: 5.031e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 7.514e-05, Loss_0: 7.375e-05, Loss_r: 1.395e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.948e-04, Loss_0: 1.912e-04, Loss_r: 3.625e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 8.025e-05, Loss_0: 7.743e-05, Loss_r: 2.819e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 9.360e-07, Loss_0: 1.293e-07, Loss_r: 8.067e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.054e-05, Loss_0: 9.679e-06, Loss_r: 8.613e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 8.682e-07, Loss_0: 1.071e-07, Loss_r: 7.611e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.996e-06, Loss_0: 1.257e-06, Loss_r: 7.392e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 7.088e-07, Loss_0: 2.949e-09, Loss_r: 7.059e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 8.130e-07, Loss_0: 1.297e-07, Loss_r: 6.833e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 7.034e-07, Loss_0: 4.325e-08, Loss_r: 6.602e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 6.441e-07, Loss_0: 5.446e-10, Loss_r: 6.436e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 6.289e-07, Loss_0: 1.610e-09, Loss_r: 6.273e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 6.132e-07, Loss_0: 1.691e-09, Loss_r: 6.115e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 5.973e-07, Loss_0: 7.126e-10, Loss_r: 5.966e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 5.825e-07, Loss_0: 2.626e-10, Loss_r: 5.822e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 5.687e-07, Loss_0: 7.214e-11, Loss_r: 5.686e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 5.556e-07, Loss_0: 9.004e-12, Loss_r: 5.556e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 5.430e-07, Loss_0: 1.879e-12, Loss_r: 5.430e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 5.310e-07, Loss_0: 1.294e-11, Loss_r: 5.310e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 5.194e-07, Loss_0: 1.274e-11, Loss_r: 5.194e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 5.082e-07, Loss_0: 4.298e-12, Loss_r: 5.082e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 4.974e-07, Loss_0: 5.946e-13, Loss_r: 4.974e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 4.871e-07, Loss_0: 7.817e-12, Loss_r: 4.870e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 4.770e-07, Loss_0: 5.711e-12, Loss_r: 4.770e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 4.672e-07, Loss_0: 3.511e-12, Loss_r: 4.672e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 4.578e-07, Loss_0: 5.275e-12, Loss_r: 4.578e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.487e-07, Loss_0: 2.958e-13, Loss_r: 4.487e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 4.398e-07, Loss_0: 3.158e-12, Loss_r: 4.398e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 4.312e-07, Loss_0: 4.122e-12, Loss_r: 4.312e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 4.229e-07, Loss_0: 6.342e-12, Loss_r: 4.229e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 4.148e-07, Loss_0: 2.602e-12, Loss_r: 4.148e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 4.070e-07, Loss_0: 4.002e-12, Loss_r: 4.070e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 3.994e-07, Loss_0: 3.469e-18, Loss_r: 3.994e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 3.921e-07, Loss_0: 9.797e-11, Loss_r: 3.920e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 4.079e-07, Loss_0: 2.224e-08, Loss_r: 3.856e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 9.648e-06, Loss_0: 9.126e-06, Loss_r: 5.216e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 3.814e-07, Loss_0: 8.027e-09, Loss_r: 3.734e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.919e-06, Loss_0: 1.532e-06, Loss_r: 3.868e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 5.108e-07, Loss_0: 1.461e-07, Loss_r: 3.646e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 5.534e-07, Loss_0: 1.907e-07, Loss_r: 3.627e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 3.539e-07, Loss_0: 7.130e-10, Loss_r: 3.532e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 3.726e-07, Loss_0: 2.504e-08, Loss_r: 3.475e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 3.546e-07, Loss_0: 1.175e-08, Loss_r: 3.429e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 3.411e-07, Loss_0: 4.022e-09, Loss_r: 3.370e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 3.338e-07, Loss_0: 1.588e-09, Loss_r: 3.322e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 3.279e-07, Loss_0: 8.195e-10, Loss_r: 3.271e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 3.224e-07, Loss_0: 5.869e-11, Loss_r: 3.223e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 3.177e-07, Loss_0: 1.119e-10, Loss_r: 3.176e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 3.130e-07, Loss_0: 1.329e-12, Loss_r: 3.129e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 3.084e-07, Loss_0: 6.947e-12, Loss_r: 3.084e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 3.041e-07, Loss_0: 3.957e-11, Loss_r: 3.040e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 3.017e-07, Loss_0: 2.071e-09, Loss_r: 2.997e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 6.290e-07, Loss_0: 3.299e-07, Loss_r: 2.991e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 2.990e-07, Loss_0: 7.128e-09, Loss_r: 2.919e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 3.215e-07, Loss_0: 3.211e-08, Loss_r: 2.894e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 2.997e-07, Loss_0: 1.426e-08, Loss_r: 2.854e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 2.835e-07, Loss_0: 2.099e-09, Loss_r: 2.814e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 2.796e-07, Loss_0: 1.593e-09, Loss_r: 2.780e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 2.757e-07, Loss_0: 8.622e-10, Loss_r: 2.748e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 2.716e-07, Loss_0: 9.141e-11, Loss_r: 2.715e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 2.683e-07, Loss_0: 6.800e-12, Loss_r: 2.683e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 2.652e-07, Loss_0: 7.341e-15, Loss_r: 2.652e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 2.621e-07, Loss_0: 9.381e-13, Loss_r: 2.621e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 2.591e-07, Loss_0: 1.364e-11, Loss_r: 2.591e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 2.562e-07, Loss_0: 4.921e-12, Loss_r: 2.562e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 2.533e-07, Loss_0: 3.574e-12, Loss_r: 2.533e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.505e-07, Loss_0: 1.872e-11, Loss_r: 2.504e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 2.477e-07, Loss_0: 8.057e-11, Loss_r: 2.477e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 2.466e-07, Loss_0: 1.708e-09, Loss_r: 2.449e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.846e-07, Loss_0: 1.408e-07, Loss_r: 2.437e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 2.517e-07, Loss_0: 1.179e-08, Loss_r: 2.399e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 2.417e-07, Loss_0: 3.927e-09, Loss_r: 2.378e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 2.438e-07, Loss_0: 8.178e-09, Loss_r: 2.357e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 2.333e-07, Loss_0: 2.396e-10, Loss_r: 2.331e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 2.319e-07, Loss_0: 1.124e-09, Loss_r: 2.308e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 2.287e-07, Loss_0: 9.711e-12, Loss_r: 2.287e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 2.267e-07, Loss_0: 1.227e-10, Loss_r: 2.266e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 2.245e-07, Loss_0: 7.246e-11, Loss_r: 2.244e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 2.223e-07, Loss_0: 4.190e-12, Loss_r: 2.223e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 2.203e-07, Loss_0: 4.806e-12, Loss_r: 2.203e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.182e-07, Loss_0: 4.122e-14, Loss_r: 2.182e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.162e-07, Loss_0: 3.400e-12, Loss_r: 2.162e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.143e-07, Loss_0: 2.810e-16, Loss_r: 2.143e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.123e-07, Loss_0: 2.687e-12, Loss_r: 2.123e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.104e-07, Loss_0: 1.013e-11, Loss_r: 2.104e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.086e-07, Loss_0: 1.244e-10, Loss_r: 2.085e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.095e-07, Loss_0: 2.919e-09, Loss_r: 2.066e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 3.892e-07, Loss_0: 1.820e-07, Loss_r: 2.072e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 2.241e-07, Loss_0: 2.079e-08, Loss_r: 2.033e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 2.038e-07, Loss_0: 2.033e-09, Loss_r: 2.017e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.101e-07, Loss_0: 9.707e-09, Loss_r: 2.004e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.999e-07, Loss_0: 1.321e-09, Loss_r: 1.985e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.976e-07, Loss_0: 7.764e-10, Loss_r: 1.969e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.956e-07, Loss_0: 2.306e-10, Loss_r: 1.953e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.940e-07, Loss_0: 1.467e-10, Loss_r: 1.938e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.923e-07, Loss_0: 1.078e-11, Loss_r: 1.923e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.908e-07, Loss_0: 1.029e-11, Loss_r: 1.908e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.894e-07, Loss_0: 7.682e-12, Loss_r: 1.894e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.879e-07, Loss_0: 6.474e-12, Loss_r: 1.879e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.865e-07, Loss_0: 1.545e-13, Loss_r: 1.865e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.850e-07, Loss_0: 2.662e-13, Loss_r: 1.850e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.836e-07, Loss_0: 1.916e-13, Loss_r: 1.836e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.822e-07, Loss_0: 8.993e-14, Loss_r: 1.822e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.808e-07, Loss_0: 8.639e-13, Loss_r: 1.808e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.794e-07, Loss_0: 1.749e-12, Loss_r: 1.794e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.780e-07, Loss_0: 1.150e-11, Loss_r: 1.780e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.771e-07, Loss_0: 4.480e-10, Loss_r: 1.767e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 2.563e-07, Loss_0: 7.992e-08, Loss_r: 1.764e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 1.756e-07, Loss_0: 1.529e-09, Loss_r: 1.741e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.811e-07, Loss_0: 7.841e-09, Loss_r: 1.733e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.755e-07, Loss_0: 3.536e-09, Loss_r: 1.720e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.711e-07, Loss_0: 4.640e-10, Loss_r: 1.706e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.699e-07, Loss_0: 4.492e-10, Loss_r: 1.695e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.686e-07, Loss_0: 1.925e-10, Loss_r: 1.684e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.672e-07, Loss_0: 1.915e-12, Loss_r: 1.672e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.661e-07, Loss_0: 1.617e-11, Loss_r: 1.661e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.650e-07, Loss_0: 3.092e-12, Loss_r: 1.650e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.638e-07, Loss_0: 8.058e-12, Loss_r: 1.638e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.627e-07, Loss_0: 9.746e-15, Loss_r: 1.627e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.616e-07, Loss_0: 6.137e-14, Loss_r: 1.616e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.605e-07, Loss_0: 2.100e-12, Loss_r: 1.605e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.594e-07, Loss_0: 4.660e-12, Loss_r: 1.594e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.584e-07, Loss_0: 6.790e-12, Loss_r: 1.584e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.573e-07, Loss_0: 5.356e-11, Loss_r: 1.573e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.574e-07, Loss_0: 1.200e-09, Loss_r: 1.562e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 2.482e-07, Loss_0: 9.168e-08, Loss_r: 1.565e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.628e-07, Loss_0: 8.516e-09, Loss_r: 1.543e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.552e-07, Loss_0: 1.862e-09, Loss_r: 1.534e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.575e-07, Loss_0: 4.994e-09, Loss_r: 1.525e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.519e-07, Loss_0: 3.719e-10, Loss_r: 1.515e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.510e-07, Loss_0: 5.325e-10, Loss_r: 1.505e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.496e-07, Loss_0: 5.307e-11, Loss_r: 1.496e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.488e-07, Loss_0: 7.729e-11, Loss_r: 1.487e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.478e-07, Loss_0: 9.607e-12, Loss_r: 1.478e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.469e-07, Loss_0: 3.238e-12, Loss_r: 1.469e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.460e-07, Loss_0: 3.059e-12, Loss_r: 1.460e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.451e-07, Loss_0: 8.047e-12, Loss_r: 1.451e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.442e-07, Loss_0: 4.775e-13, Loss_r: 1.442e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.433e-07, Loss_0: 1.515e-13, Loss_r: 1.433e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.424e-07, Loss_0: 1.444e-13, Loss_r: 1.424e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.415e-07, Loss_0: 7.697e-13, Loss_r: 1.415e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.407e-07, Loss_0: 7.456e-12, Loss_r: 1.406e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.399e-07, Loss_0: 1.327e-10, Loss_r: 1.398e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.429e-07, Loss_0: 3.829e-09, Loss_r: 1.391e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 3.875e-07, Loss_0: 2.440e-07, Loss_r: 1.435e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.682e-07, Loss_0: 3.006e-08, Loss_r: 1.381e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.382e-07, Loss_0: 1.688e-09, Loss_r: 1.365e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.479e-07, Loss_0: 1.200e-08, Loss_r: 1.359e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 1.378e-07, Loss_0: 2.781e-09, Loss_r: 1.350e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.348e-07, Loss_0: 5.014e-10, Loss_r: 1.343e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.340e-07, Loss_0: 4.668e-10, Loss_r: 1.335e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 1.329e-07, Loss_0: 1.343e-10, Loss_r: 1.327e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 1.320e-07, Loss_0: 1.193e-11, Loss_r: 1.320e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 1.313e-07, Loss_0: 2.852e-11, Loss_r: 1.313e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.305e-07, Loss_0: 2.008e-11, Loss_r: 1.305e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.298e-07, Loss_0: 1.710e-12, Loss_r: 1.298e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.290e-07, Loss_0: 2.133e-12, Loss_r: 1.290e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.283e-07, Loss_0: 6.120e-15, Loss_r: 1.283e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.276e-07, Loss_0: 1.574e-13, Loss_r: 1.276e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.268e-07, Loss_0: 2.759e-13, Loss_r: 1.268e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 1.261e-07, Loss_0: 4.897e-12, Loss_r: 1.261e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.254e-07, Loss_0: 4.412e-11, Loss_r: 1.254e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.254e-07, Loss_0: 7.393e-10, Loss_r: 1.247e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.491e-07, Loss_0: 2.448e-08, Loss_r: 1.246e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.278e-07, Loss_0: 4.371e-09, Loss_r: 1.234e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.227e-07, Loss_0: 7.589e-12, Loss_r: 1.227e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.227e-07, Loss_0: 7.214e-10, Loss_r: 1.220e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.218e-07, Loss_0: 4.895e-10, Loss_r: 1.214e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.207e-07, Loss_0: 2.711e-12, Loss_r: 1.207e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.202e-07, Loss_0: 6.249e-11, Loss_r: 1.201e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.195e-07, Loss_0: 9.909e-14, Loss_r: 1.195e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.188e-07, Loss_0: 1.346e-11, Loss_r: 1.188e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.182e-07, Loss_0: 4.724e-13, Loss_r: 1.182e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.176e-07, Loss_0: 7.828e-13, Loss_r: 1.176e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.170e-07, Loss_0: 8.674e-17, Loss_r: 1.170e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.163e-07, Loss_0: 1.143e-12, Loss_r: 1.163e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.157e-07, Loss_0: 4.011e-15, Loss_r: 1.157e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.151e-07, Loss_0: 2.176e-12, Loss_r: 1.151e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.145e-07, Loss_0: 6.870e-13, Loss_r: 1.145e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.139e-07, Loss_0: 2.057e-14, Loss_r: 1.139e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 1.133e-07, Loss_0: 1.373e-12, Loss_r: 1.133e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.126e-07, Loss_0: 3.125e-12, Loss_r: 1.126e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.121e-07, Loss_0: 4.939e-11, Loss_r: 1.120e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.136e-07, Loss_0: 2.193e-09, Loss_r: 1.114e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 3.433e-07, Loss_0: 2.280e-07, Loss_r: 1.153e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 1.265e-07, Loss_0: 1.599e-08, Loss_r: 1.105e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 1.179e-07, Loss_0: 7.890e-09, Loss_r: 1.100e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 1.230e-07, Loss_0: 1.334e-08, Loss_r: 1.096e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 1.092e-07, Loss_0: 4.388e-10, Loss_r: 1.087e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 1.097e-07, Loss_0: 1.519e-09, Loss_r: 1.082e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 1.077e-07, Loss_0: 5.722e-11, Loss_r: 1.076e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 1.073e-07, Loss_0: 2.137e-10, Loss_r: 1.071e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.066e-07, Loss_0: 3.978e-11, Loss_r: 1.066e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.060e-07, Loss_0: 1.998e-13, Loss_r: 1.060e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 1.055e-07, Loss_0: 6.230e-14, Loss_r: 1.055e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.050e-07, Loss_0: 1.884e-12, Loss_r: 1.050e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.044e-07, Loss_0: 5.036e-13, Loss_r: 1.044e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 1.039e-07, Loss_0: 8.674e-13, Loss_r: 1.039e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.034e-07, Loss_0: 3.518e-12, Loss_r: 1.034e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 1.029e-07, Loss_0: 5.143e-13, Loss_r: 1.029e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 1.023e-07, Loss_0: 2.720e-15, Loss_r: 1.023e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.019e-07, Loss_0: 2.884e-11, Loss_r: 1.018e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.019e-07, Loss_0: 5.512e-10, Loss_r: 1.013e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 1.274e-07, Loss_0: 2.593e-08, Loss_r: 1.015e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 1.043e-07, Loss_0: 3.871e-09, Loss_r: 1.005e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 9.989e-08, Loss_0: 3.162e-11, Loss_r: 9.985e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 1.004e-07, Loss_0: 1.003e-09, Loss_r: 9.941e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 9.938e-08, Loss_0: 4.434e-10, Loss_r: 9.894e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 9.850e-08, Loss_0: 7.204e-12, Loss_r: 9.849e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 9.811e-08, Loss_0: 6.010e-11, Loss_r: 9.804e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 9.757e-08, Loss_0: 3.674e-12, Loss_r: 9.757e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 9.713e-08, Loss_0: 6.569e-12, Loss_r: 9.712e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 9.667e-08, Loss_0: 8.127e-13, Loss_r: 9.667e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 9.622e-08, Loss_0: 1.657e-12, Loss_r: 9.622e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 9.576e-08, Loss_0: 3.164e-13, Loss_r: 9.576e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 9.531e-08, Loss_0: 3.733e-13, Loss_r: 9.531e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 9.485e-08, Loss_0: 7.599e-13, Loss_r: 9.485e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 9.441e-08, Loss_0: 3.206e-13, Loss_r: 9.441e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 9.397e-08, Loss_0: 5.223e-13, Loss_r: 9.397e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 9.352e-08, Loss_0: 4.145e-12, Loss_r: 9.351e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 9.308e-08, Loss_0: 5.029e-12, Loss_r: 9.307e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 9.262e-08, Loss_0: 6.898e-12, Loss_r: 9.261e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 9.220e-08, Loss_0: 1.599e-11, Loss_r: 9.218e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 9.241e-08, Loss_0: 6.679e-10, Loss_r: 9.174e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 1.611e-07, Loss_0: 6.839e-08, Loss_r: 9.268e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 9.582e-08, Loss_0: 4.864e-09, Loss_r: 9.096e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 9.297e-08, Loss_0: 2.358e-09, Loss_r: 9.061e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 9.426e-08, Loss_0: 4.001e-09, Loss_r: 9.026e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 8.987e-08, Loss_0: 1.269e-10, Loss_r: 8.974e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 8.986e-08, Loss_0: 5.257e-10, Loss_r: 8.934e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 8.897e-08, Loss_0: 1.520e-11, Loss_r: 8.895e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 8.864e-08, Loss_0: 7.278e-11, Loss_r: 8.857e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 8.819e-08, Loss_0: 1.614e-11, Loss_r: 8.818e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 8.778e-08, Loss_0: 2.345e-15, Loss_r: 8.778e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 8.741e-08, Loss_0: 8.989e-13, Loss_r: 8.741e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 8.701e-08, Loss_0: 6.109e-12, Loss_r: 8.700e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 8.662e-08, Loss_0: 9.345e-13, Loss_r: 8.662e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 8.623e-08, Loss_0: 2.089e-12, Loss_r: 8.623e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 8.586e-08, Loss_0: 2.221e-13, Loss_r: 8.586e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 8.548e-08, Loss_0: 1.099e-13, Loss_r: 8.548e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 8.510e-08, Loss_0: 5.249e-14, Loss_r: 8.510e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 8.472e-08, Loss_0: 9.411e-12, Loss_r: 8.471e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 8.448e-08, Loss_0: 1.368e-10, Loss_r: 8.435e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 9.008e-08, Loss_0: 5.928e-09, Loss_r: 8.415e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 8.467e-08, Loss_0: 1.004e-09, Loss_r: 8.367e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 8.328e-08, Loss_0: 5.832e-15, Loss_r: 8.328e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 8.312e-08, Loss_0: 1.940e-10, Loss_r: 8.293e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 8.269e-08, Loss_0: 9.676e-11, Loss_r: 8.259e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 8.226e-08, Loss_0: 6.512e-14, Loss_r: 8.226e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 8.194e-08, Loss_0: 1.144e-11, Loss_r: 8.193e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 8.159e-08, Loss_0: 7.534e-13, Loss_r: 8.159e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 8.125e-08, Loss_0: 4.372e-13, Loss_r: 8.125e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 8.092e-08, Loss_0: 4.931e-13, Loss_r: 8.092e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 8.059e-08, Loss_0: 1.952e-14, Loss_r: 8.059e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 8.024e-08, Loss_0: 1.452e-12, Loss_r: 8.024e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 7.992e-08, Loss_0: 1.695e-12, Loss_r: 7.991e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 7.959e-08, Loss_0: 7.477e-12, Loss_r: 7.959e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 7.925e-08, Loss_0: 1.530e-13, Loss_r: 7.925e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 7.892e-08, Loss_0: 3.687e-13, Loss_r: 7.892e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 7.859e-08, Loss_0: 3.227e-13, Loss_r: 7.859e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 7.828e-08, Loss_0: 7.697e-13, Loss_r: 7.828e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 7.795e-08, Loss_0: 8.674e-13, Loss_r: 7.794e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 7.761e-08, Loss_0: 9.722e-12, Loss_r: 7.760e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 7.781e-08, Loss_0: 4.984e-10, Loss_r: 7.731e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 1.169e-07, Loss_0: 3.890e-08, Loss_r: 7.804e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 8.074e-08, Loss_0: 3.940e-09, Loss_r: 7.680e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 7.696e-08, Loss_0: 5.770e-10, Loss_r: 7.638e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 7.830e-08, Loss_0: 2.185e-09, Loss_r: 7.611e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 7.609e-08, Loss_0: 2.886e-10, Loss_r: 7.580e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 7.566e-08, Loss_0: 1.410e-10, Loss_r: 7.552e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 7.527e-08, Loss_0: 5.773e-11, Loss_r: 7.522e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 7.497e-08, Loss_0: 4.139e-11, Loss_r: 7.493e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 7.463e-08, Loss_0: 1.679e-13, Loss_r: 7.463e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 7.436e-08, Loss_0: 4.880e-12, Loss_r: 7.435e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 7.406e-08, Loss_0: 4.701e-12, Loss_r: 7.405e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 7.378e-08, Loss_0: 3.778e-13, Loss_r: 7.378e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 7.349e-08, Loss_0: 1.972e-12, Loss_r: 7.349e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 7.321e-08, Loss_0: 3.332e-14, Loss_r: 7.321e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 7.292e-08, Loss_0: 3.081e-13, Loss_r: 7.292e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 7.264e-08, Loss_0: 2.586e-13, Loss_r: 7.264e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 7.235e-08, Loss_0: 8.674e-17, Loss_r: 7.235e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 7.207e-08, Loss_0: 7.806e-16, Loss_r: 7.207e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 7.184e-08, Loss_0: 4.360e-11, Loss_r: 7.179e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 7.306e-08, Loss_0: 1.492e-09, Loss_r: 7.157e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 1.685e-07, Loss_0: 9.469e-08, Loss_r: 7.379e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 8.382e-08, Loss_0: 1.246e-08, Loss_r: 7.136e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 7.106e-08, Loss_0: 3.318e-10, Loss_r: 7.072e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 7.460e-08, Loss_0: 4.062e-09, Loss_r: 7.054e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 7.157e-08, Loss_0: 1.331e-09, Loss_r: 7.024e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 7.005e-08, Loss_0: 6.723e-11, Loss_r: 6.998e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 6.998e-08, Loss_0: 2.331e-10, Loss_r: 6.975e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 6.949e-08, Loss_0: 1.864e-11, Loss_r: 6.947e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 6.925e-08, Loss_0: 2.032e-11, Loss_r: 6.923e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 6.898e-08, Loss_0: 8.837e-12, Loss_r: 6.897e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 6.874e-08, Loss_0: 2.507e-12, Loss_r: 6.873e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 6.849e-08, Loss_0: 1.900e-13, Loss_r: 6.849e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 6.824e-08, Loss_0: 1.890e-12, Loss_r: 6.824e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 6.798e-08, Loss_0: 4.034e-13, Loss_r: 6.798e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 6.775e-08, Loss_0: 4.979e-12, Loss_r: 6.774e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 6.749e-08, Loss_0: 7.395e-14, Loss_r: 6.749e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 6.724e-08, Loss_0: 8.953e-13, Loss_r: 6.724e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 6.702e-08, Loss_0: 2.934e-11, Loss_r: 6.699e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 6.685e-08, Loss_0: 1.014e-10, Loss_r: 6.675e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 6.807e-08, Loss_0: 1.547e-09, Loss_r: 6.652e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 1.303e-07, Loss_0: 6.260e-08, Loss_r: 6.771e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 7.714e-08, Loss_0: 1.088e-08, Loss_r: 6.626e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 6.585e-08, Loss_0: 2.944e-11, Loss_r: 6.582e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 6.732e-08, Loss_0: 1.645e-09, Loss_r: 6.568e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 6.655e-08, Loss_0: 1.099e-09, Loss_r: 6.545e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 6.519e-08, Loss_0: 9.734e-12, Loss_r: 6.518e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 6.512e-08, Loss_0: 1.505e-10, Loss_r: 6.497e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 6.475e-08, Loss_0: 7.045e-12, Loss_r: 6.475e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 6.456e-08, Loss_0: 2.205e-11, Loss_r: 6.454e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 6.432e-08, Loss_0: 6.009e-12, Loss_r: 6.431e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 6.409e-08, Loss_0: 6.898e-14, Loss_r: 6.409e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 6.388e-08, Loss_0: 2.057e-14, Loss_r: 6.388e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 6.367e-08, Loss_0: 3.007e-12, Loss_r: 6.367e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 20.0065\n",
            "[1, 256, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.272e-02, Loss_0: 4.885e-04, Loss_r: 3.223e-02, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.232e-02, Loss_0: 4.966e-07, Loss_r: 3.232e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.211e-02, Loss_0: 2.569e-05, Loss_r: 3.208e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.140e-02, Loss_0: 2.875e-07, Loss_r: 3.140e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.831e-02, Loss_0: 2.874e-05, Loss_r: 1.828e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 5.343e-03, Loss_0: 2.207e-03, Loss_r: 3.136e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.093e-03, Loss_0: 1.460e-03, Loss_r: 6.327e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 2.899e-04, Loss_0: 1.321e-07, Loss_r: 2.898e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 2.104e-04, Loss_0: 5.039e-06, Loss_r: 2.054e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.735e-05, Loss_0: 5.117e-05, Loss_r: 6.177e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.906e-05, Loss_0: 5.578e-09, Loss_r: 1.906e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 4.383e-06, Loss_0: 3.411e-08, Loss_r: 4.349e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.085e-06, Loss_0: 1.628e-08, Loss_r: 5.069e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 3.051e-06, Loss_0: 3.451e-08, Loss_r: 3.017e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 2.782e-06, Loss_0: 2.268e-11, Loss_r: 2.782e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.443e-06, Loss_0: 1.946e-08, Loss_r: 2.424e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 2.192e-06, Loss_0: 1.633e-08, Loss_r: 2.176e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.999e-06, Loss_0: 6.139e-10, Loss_r: 1.998e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.865e-06, Loss_0: 2.078e-10, Loss_r: 1.865e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.741e-06, Loss_0: 3.352e-10, Loss_r: 1.741e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.634e-06, Loss_0: 9.197e-10, Loss_r: 1.633e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.538e-06, Loss_0: 8.962e-13, Loss_r: 1.538e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.451e-06, Loss_0: 5.543e-10, Loss_r: 1.450e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.372e-06, Loss_0: 7.064e-11, Loss_r: 1.371e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.299e-06, Loss_0: 1.521e-10, Loss_r: 1.299e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.233e-06, Loss_0: 8.238e-11, Loss_r: 1.233e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.172e-06, Loss_0: 1.269e-10, Loss_r: 1.172e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.117e-06, Loss_0: 9.308e-11, Loss_r: 1.117e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.066e-06, Loss_0: 1.035e-10, Loss_r: 1.066e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.019e-06, Loss_0: 9.341e-11, Loss_r: 1.019e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 9.763e-07, Loss_0: 8.838e-11, Loss_r: 9.762e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 9.368e-07, Loss_0: 1.627e-10, Loss_r: 9.367e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 9.402e-07, Loss_0: 1.640e-08, Loss_r: 9.238e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.282e-04, Loss_0: 8.122e-05, Loss_r: 1.470e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 6.223e-04, Loss_0: 3.970e-04, Loss_r: 2.253e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 6.330e-05, Loss_0: 1.002e-05, Loss_r: 5.328e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 9.097e-05, Loss_0: 5.193e-05, Loss_r: 3.904e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.512e-05, Loss_0: 2.614e-05, Loss_r: 8.988e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.448e-06, Loss_0: 2.926e-07, Loss_r: 6.156e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 4.793e-06, Loss_0: 2.761e-07, Loss_r: 4.517e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.286e-06, Loss_0: 9.120e-08, Loss_r: 4.194e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.573e-06, Loss_0: 4.607e-08, Loss_r: 3.527e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.172e-06, Loss_0: 9.405e-09, Loss_r: 3.162e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.934e-06, Loss_0: 5.779e-09, Loss_r: 2.928e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.748e-06, Loss_0: 2.353e-08, Loss_r: 2.725e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.559e-06, Loss_0: 8.323e-09, Loss_r: 2.551e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.403e-06, Loss_0: 2.450e-13, Loss_r: 2.403e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.265e-06, Loss_0: 1.927e-11, Loss_r: 2.265e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.141e-06, Loss_0: 1.387e-09, Loss_r: 2.140e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.029e-06, Loss_0: 2.149e-10, Loss_r: 2.029e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.927e-06, Loss_0: 3.078e-10, Loss_r: 1.926e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.833e-06, Loss_0: 3.356e-10, Loss_r: 1.833e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.748e-06, Loss_0: 2.608e-10, Loss_r: 1.747e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.668e-06, Loss_0: 2.412e-10, Loss_r: 1.668e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.595e-06, Loss_0: 2.522e-10, Loss_r: 1.595e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.527e-06, Loss_0: 1.999e-10, Loss_r: 1.527e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.463e-06, Loss_0: 1.890e-10, Loss_r: 1.463e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.404e-06, Loss_0: 1.820e-10, Loss_r: 1.403e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.348e-06, Loss_0: 1.693e-10, Loss_r: 1.347e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.295e-06, Loss_0: 1.545e-10, Loss_r: 1.295e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.245e-06, Loss_0: 1.435e-10, Loss_r: 1.245e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.198e-06, Loss_0: 1.335e-10, Loss_r: 1.198e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.153e-06, Loss_0: 1.246e-10, Loss_r: 1.153e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.111e-06, Loss_0: 1.100e-10, Loss_r: 1.111e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.071e-06, Loss_0: 1.041e-10, Loss_r: 1.070e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.032e-06, Loss_0: 1.002e-10, Loss_r: 1.032e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 9.956e-07, Loss_0: 9.072e-11, Loss_r: 9.955e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 9.606e-07, Loss_0: 9.259e-11, Loss_r: 9.605e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 9.273e-07, Loss_0: 8.376e-11, Loss_r: 9.272e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 8.954e-07, Loss_0: 7.587e-11, Loss_r: 8.953e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 8.649e-07, Loss_0: 9.644e-11, Loss_r: 8.648e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 8.375e-07, Loss_0: 1.929e-09, Loss_r: 8.356e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.191e-06, Loss_0: 3.039e-06, Loss_r: 2.153e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.176e-03, Loss_0: 8.001e-04, Loss_r: 3.756e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.423e-05, Loss_0: 3.008e-05, Loss_r: 4.415e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 4.929e-05, Loss_0: 2.161e-05, Loss_r: 2.768e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.189e-05, Loss_0: 2.361e-07, Loss_r: 1.166e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 9.135e-06, Loss_0: 1.747e-06, Loss_r: 7.388e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 7.102e-06, Loss_0: 1.364e-06, Loss_r: 5.738e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 5.083e-06, Loss_0: 4.373e-08, Loss_r: 5.039e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 4.695e-06, Loss_0: 1.410e-07, Loss_r: 4.554e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 4.128e-06, Loss_0: 3.743e-08, Loss_r: 4.090e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.708e-06, Loss_0: 1.746e-08, Loss_r: 3.690e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.350e-06, Loss_0: 7.155e-09, Loss_r: 3.343e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.069e-06, Loss_0: 3.760e-10, Loss_r: 3.069e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.834e-06, Loss_0: 1.945e-09, Loss_r: 2.833e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.629e-06, Loss_0: 8.837e-10, Loss_r: 2.628e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.451e-06, Loss_0: 4.448e-10, Loss_r: 2.450e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.292e-06, Loss_0: 9.783e-10, Loss_r: 2.291e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.151e-06, Loss_0: 3.619e-10, Loss_r: 2.151e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.024e-06, Loss_0: 5.406e-10, Loss_r: 2.024e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.909e-06, Loss_0: 4.340e-10, Loss_r: 1.909e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.805e-06, Loss_0: 3.561e-10, Loss_r: 1.805e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.710e-06, Loss_0: 3.576e-10, Loss_r: 1.709e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.622e-06, Loss_0: 3.034e-10, Loss_r: 1.622e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.541e-06, Loss_0: 2.731e-10, Loss_r: 1.541e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.466e-06, Loss_0: 2.559e-10, Loss_r: 1.466e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.397e-06, Loss_0: 2.277e-10, Loss_r: 1.397e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.332e-06, Loss_0: 2.088e-10, Loss_r: 1.332e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.272e-06, Loss_0: 1.884e-10, Loss_r: 1.272e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.216e-06, Loss_0: 1.681e-10, Loss_r: 1.215e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.163e-06, Loss_0: 1.601e-10, Loss_r: 1.163e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.113e-06, Loss_0: 1.448e-10, Loss_r: 1.113e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.067e-06, Loss_0: 1.292e-10, Loss_r: 1.067e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.023e-06, Loss_0: 1.270e-10, Loss_r: 1.023e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 9.820e-07, Loss_0: 1.147e-10, Loss_r: 9.818e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 9.432e-07, Loss_0: 1.152e-10, Loss_r: 9.431e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 9.067e-07, Loss_0: 9.592e-11, Loss_r: 9.066e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 8.722e-07, Loss_0: 9.762e-11, Loss_r: 8.721e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 8.396e-07, Loss_0: 9.208e-11, Loss_r: 8.395e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 8.088e-07, Loss_0: 7.407e-11, Loss_r: 8.088e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 7.798e-07, Loss_0: 6.966e-11, Loss_r: 7.797e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 7.523e-07, Loss_0: 2.751e-11, Loss_r: 7.523e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 7.284e-07, Loss_0: 1.153e-09, Loss_r: 7.273e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.793e-06, Loss_0: 8.788e-07, Loss_r: 9.147e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.109e-03, Loss_0: 8.766e-04, Loss_r: 2.322e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 5.464e-04, Loss_0: 3.753e-04, Loss_r: 1.711e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.174e-05, Loss_0: 1.365e-05, Loss_r: 8.082e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 7.620e-05, Loss_0: 3.217e-05, Loss_r: 4.404e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 8.905e-06, Loss_0: 1.965e-06, Loss_r: 6.940e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 9.440e-06, Loss_0: 2.158e-06, Loss_r: 7.282e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 6.198e-06, Loss_0: 1.080e-06, Loss_r: 5.118e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 4.149e-06, Loss_0: 3.645e-07, Loss_r: 3.784e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.527e-06, Loss_0: 3.941e-08, Loss_r: 3.487e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.116e-06, Loss_0: 1.695e-08, Loss_r: 3.099e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.703e-06, Loss_0: 2.963e-09, Loss_r: 2.700e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.451e-06, Loss_0: 4.007e-09, Loss_r: 2.447e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.245e-06, Loss_0: 4.546e-11, Loss_r: 2.245e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.068e-06, Loss_0: 2.018e-09, Loss_r: 2.066e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.914e-06, Loss_0: 2.641e-10, Loss_r: 1.914e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.780e-06, Loss_0: 4.093e-10, Loss_r: 1.780e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.662e-06, Loss_0: 6.136e-10, Loss_r: 1.662e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.558e-06, Loss_0: 2.308e-10, Loss_r: 1.557e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.464e-06, Loss_0: 3.152e-10, Loss_r: 1.464e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.379e-06, Loss_0: 3.193e-10, Loss_r: 1.379e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.303e-06, Loss_0: 2.335e-10, Loss_r: 1.303e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.233e-06, Loss_0: 2.060e-10, Loss_r: 1.233e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.169e-06, Loss_0: 1.905e-10, Loss_r: 1.169e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.111e-06, Loss_0: 1.725e-10, Loss_r: 1.111e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.057e-06, Loss_0: 1.598e-10, Loss_r: 1.057e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.008e-06, Loss_0: 1.451e-10, Loss_r: 1.008e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 9.621e-07, Loss_0: 1.315e-10, Loss_r: 9.620e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 9.197e-07, Loss_0: 1.196e-10, Loss_r: 9.196e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.803e-07, Loss_0: 1.058e-10, Loss_r: 8.802e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 8.438e-07, Loss_0: 9.344e-11, Loss_r: 8.437e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 8.097e-07, Loss_0: 8.852e-11, Loss_r: 8.096e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 7.780e-07, Loss_0: 8.529e-11, Loss_r: 7.779e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 7.484e-07, Loss_0: 7.361e-11, Loss_r: 7.484e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 7.207e-07, Loss_0: 7.349e-11, Loss_r: 7.207e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 6.949e-07, Loss_0: 6.443e-11, Loss_r: 6.948e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 6.707e-07, Loss_0: 6.131e-11, Loss_r: 6.706e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 6.479e-07, Loss_0: 6.329e-11, Loss_r: 6.479e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.267e-07, Loss_0: 5.946e-11, Loss_r: 6.266e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 6.067e-07, Loss_0: 9.679e-11, Loss_r: 6.066e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 5.895e-07, Loss_0: 1.763e-09, Loss_r: 5.877e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.250e-06, Loss_0: 5.313e-07, Loss_r: 7.190e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 5.557e-04, Loss_0: 4.249e-04, Loss_r: 1.308e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.080e-04, Loss_0: 1.309e-04, Loss_r: 7.717e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.461e-04, Loss_0: 6.962e-05, Loss_r: 7.650e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 5.942e-06, Loss_0: 2.178e-07, Loss_r: 5.724e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.677e-05, Loss_0: 1.069e-05, Loss_r: 1.608e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 4.499e-06, Loss_0: 3.260e-08, Loss_r: 4.467e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 5.941e-06, Loss_0: 1.061e-06, Loss_r: 4.880e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.553e-06, Loss_0: 1.748e-07, Loss_r: 3.378e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.812e-06, Loss_0: 1.911e-08, Loss_r: 2.793e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.541e-06, Loss_0: 2.367e-08, Loss_r: 2.517e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.240e-06, Loss_0: 2.960e-08, Loss_r: 2.210e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.000e-06, Loss_0: 4.299e-09, Loss_r: 1.996e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.814e-06, Loss_0: 4.949e-09, Loss_r: 1.809e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.662e-06, Loss_0: 5.349e-11, Loss_r: 1.661e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.533e-06, Loss_0: 1.421e-09, Loss_r: 1.531e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.422e-06, Loss_0: 1.818e-10, Loss_r: 1.422e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.325e-06, Loss_0: 2.069e-10, Loss_r: 1.325e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.240e-06, Loss_0: 3.679e-10, Loss_r: 1.240e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.164e-06, Loss_0: 1.966e-10, Loss_r: 1.164e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.097e-06, Loss_0: 1.517e-10, Loss_r: 1.096e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.036e-06, Loss_0: 1.589e-10, Loss_r: 1.036e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 9.806e-07, Loss_0: 1.538e-10, Loss_r: 9.805e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 9.306e-07, Loss_0: 1.341e-10, Loss_r: 9.305e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 8.852e-07, Loss_0: 1.188e-10, Loss_r: 8.850e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.436e-07, Loss_0: 1.071e-10, Loss_r: 8.435e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 8.056e-07, Loss_0: 9.781e-11, Loss_r: 8.055e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 7.707e-07, Loss_0: 8.717e-11, Loss_r: 7.706e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 7.386e-07, Loss_0: 7.767e-11, Loss_r: 7.385e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 7.090e-07, Loss_0: 6.575e-11, Loss_r: 7.089e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 6.817e-07, Loss_0: 6.672e-11, Loss_r: 6.816e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 6.565e-07, Loss_0: 5.954e-11, Loss_r: 6.564e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.331e-07, Loss_0: 5.784e-11, Loss_r: 6.330e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 6.113e-07, Loss_0: 4.904e-11, Loss_r: 6.113e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 5.912e-07, Loss_0: 5.078e-11, Loss_r: 5.911e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.724e-07, Loss_0: 3.652e-11, Loss_r: 5.723e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 5.549e-07, Loss_0: 4.131e-11, Loss_r: 5.548e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 5.385e-07, Loss_0: 5.152e-11, Loss_r: 5.384e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.231e-07, Loss_0: 1.101e-10, Loss_r: 5.230e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.096e-07, Loss_0: 1.190e-09, Loss_r: 5.084e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 6.072e-07, Loss_0: 1.115e-07, Loss_r: 4.957e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.723e-05, Loss_0: 3.511e-05, Loss_r: 2.113e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.451e-04, Loss_0: 2.066e-05, Loss_r: 1.244e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.176e-04, Loss_0: 1.095e-04, Loss_r: 8.062e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 3.749e-05, Loss_0: 3.346e-05, Loss_r: 4.027e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.751e-05, Loss_0: 1.137e-05, Loss_r: 6.145e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 7.412e-06, Loss_0: 4.267e-06, Loss_r: 3.145e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.188e-06, Loss_0: 1.390e-06, Loss_r: 1.798e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.979e-06, Loss_0: 5.090e-07, Loss_r: 1.470e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.497e-06, Loss_0: 8.544e-08, Loss_r: 1.411e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.228e-06, Loss_0: 1.069e-08, Loss_r: 1.217e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.131e-06, Loss_0: 3.358e-09, Loss_r: 1.128e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.057e-06, Loss_0: 3.835e-09, Loss_r: 1.053e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 9.815e-07, Loss_0: 2.922e-09, Loss_r: 9.786e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 9.200e-07, Loss_0: 5.244e-10, Loss_r: 9.195e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 8.683e-07, Loss_0: 9.545e-11, Loss_r: 8.682e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 8.215e-07, Loss_0: 1.604e-10, Loss_r: 8.213e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 7.799e-07, Loss_0: 3.332e-10, Loss_r: 7.796e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 7.424e-07, Loss_0: 1.428e-10, Loss_r: 7.422e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 7.084e-07, Loss_0: 5.119e-11, Loss_r: 7.083e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 6.781e-07, Loss_0: 1.253e-12, Loss_r: 6.781e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 7.716e-07, Loss_0: 5.143e-09, Loss_r: 7.665e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 4.436e-05, Loss_0: 2.461e-06, Loss_r: 4.190e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.976e-06, Loss_0: 1.338e-07, Loss_r: 2.842e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 8.024e-07, Loss_0: 1.554e-08, Loss_r: 7.869e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.662e-06, Loss_0: 8.153e-08, Loss_r: 1.581e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 1.472e-06, Loss_0: 7.054e-08, Loss_r: 1.402e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 7.621e-07, Loss_0: 1.803e-08, Loss_r: 7.441e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 5.555e-07, Loss_0: 3.618e-10, Loss_r: 5.551e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 5.773e-07, Loss_0: 3.424e-09, Loss_r: 5.739e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 5.169e-07, Loss_0: 1.490e-10, Loss_r: 5.167e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 5.068e-07, Loss_0: 8.055e-10, Loss_r: 5.060e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 4.903e-07, Loss_0: 8.501e-11, Loss_r: 4.902e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 4.757e-07, Loss_0: 1.577e-10, Loss_r: 4.756e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 4.633e-07, Loss_0: 3.955e-12, Loss_r: 4.633e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 4.517e-07, Loss_0: 7.099e-11, Loss_r: 4.517e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 4.408e-07, Loss_0: 5.094e-12, Loss_r: 4.408e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 4.305e-07, Loss_0: 2.224e-11, Loss_r: 4.305e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 4.208e-07, Loss_0: 4.684e-11, Loss_r: 4.208e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 4.118e-07, Loss_0: 1.010e-10, Loss_r: 4.117e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 4.080e-07, Loss_0: 1.056e-09, Loss_r: 4.070e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 8.542e-07, Loss_0: 7.104e-08, Loss_r: 7.832e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 4.194e-07, Loss_0: 5.464e-09, Loss_r: 4.140e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 4.003e-07, Loss_0: 2.653e-09, Loss_r: 3.976e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 4.014e-07, Loss_0: 3.907e-09, Loss_r: 3.975e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 3.677e-07, Loss_0: 1.680e-11, Loss_r: 3.677e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 3.653e-07, Loss_0: 8.422e-10, Loss_r: 3.644e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 3.560e-07, Loss_0: 1.849e-11, Loss_r: 3.560e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 3.500e-07, Loss_0: 3.133e-13, Loss_r: 3.500e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 3.445e-07, Loss_0: 6.774e-11, Loss_r: 3.445e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 3.391e-07, Loss_0: 8.579e-14, Loss_r: 3.391e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 3.339e-07, Loss_0: 2.268e-11, Loss_r: 3.339e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 3.289e-07, Loss_0: 1.162e-11, Loss_r: 3.289e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 3.241e-07, Loss_0: 5.614e-12, Loss_r: 3.241e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.193e-07, Loss_0: 1.082e-11, Loss_r: 3.193e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 3.148e-07, Loss_0: 1.159e-11, Loss_r: 3.148e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.103e-07, Loss_0: 2.465e-11, Loss_r: 3.103e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.070e-07, Loss_0: 2.935e-10, Loss_r: 3.067e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 4.431e-07, Loss_0: 2.972e-08, Loss_r: 4.134e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 3.025e-07, Loss_0: 1.077e-09, Loss_r: 3.014e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 3.065e-07, Loss_0: 2.219e-09, Loss_r: 3.042e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 2.979e-07, Loss_0: 1.265e-09, Loss_r: 2.967e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 2.881e-07, Loss_0: 1.952e-10, Loss_r: 2.879e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 2.848e-07, Loss_0: 2.444e-10, Loss_r: 2.846e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 2.811e-07, Loss_0: 4.356e-11, Loss_r: 2.811e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 2.775e-07, Loss_0: 2.071e-11, Loss_r: 2.774e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 2.742e-07, Loss_0: 1.072e-11, Loss_r: 2.742e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 2.710e-07, Loss_0: 2.734e-12, Loss_r: 2.710e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.679e-07, Loss_0: 5.506e-12, Loss_r: 2.679e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.649e-07, Loss_0: 1.112e-11, Loss_r: 2.649e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.619e-07, Loss_0: 2.929e-12, Loss_r: 2.619e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.589e-07, Loss_0: 2.269e-12, Loss_r: 2.589e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.560e-07, Loss_0: 8.578e-13, Loss_r: 2.560e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.532e-07, Loss_0: 3.482e-12, Loss_r: 2.532e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.537e-07, Loss_0: 6.847e-10, Loss_r: 2.530e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 6.611e-07, Loss_0: 9.699e-08, Loss_r: 5.641e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 2.646e-07, Loss_0: 4.360e-09, Loss_r: 2.602e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 2.682e-07, Loss_0: 6.490e-09, Loss_r: 2.617e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.639e-07, Loss_0: 6.016e-09, Loss_r: 2.579e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 2.382e-07, Loss_0: 1.929e-11, Loss_r: 2.382e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 2.389e-07, Loss_0: 6.830e-10, Loss_r: 2.383e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.338e-07, Loss_0: 1.545e-10, Loss_r: 2.337e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.311e-07, Loss_0: 2.811e-11, Loss_r: 2.311e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 2.289e-07, Loss_0: 1.034e-11, Loss_r: 2.289e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 2.266e-07, Loss_0: 3.389e-11, Loss_r: 2.266e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 2.244e-07, Loss_0: 1.698e-13, Loss_r: 2.244e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 2.222e-07, Loss_0: 8.074e-12, Loss_r: 2.222e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 2.200e-07, Loss_0: 5.464e-12, Loss_r: 2.200e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 2.179e-07, Loss_0: 1.726e-12, Loss_r: 2.179e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 2.157e-07, Loss_0: 1.283e-12, Loss_r: 2.157e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 2.136e-07, Loss_0: 1.048e-12, Loss_r: 2.136e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 2.116e-07, Loss_0: 9.381e-13, Loss_r: 2.116e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.116e-07, Loss_0: 4.643e-10, Loss_r: 2.111e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 5.179e-07, Loss_0: 8.061e-08, Loss_r: 4.372e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 2.155e-07, Loss_0: 2.405e-09, Loss_r: 2.131e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 2.290e-07, Loss_0: 6.910e-09, Loss_r: 2.221e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 2.184e-07, Loss_0: 4.555e-09, Loss_r: 2.139e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 2.012e-07, Loss_0: 1.495e-10, Loss_r: 2.010e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 2.008e-07, Loss_0: 4.866e-10, Loss_r: 2.003e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.976e-07, Loss_0: 1.972e-10, Loss_r: 1.974e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.953e-07, Loss_0: 3.726e-12, Loss_r: 1.953e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.936e-07, Loss_0: 3.787e-12, Loss_r: 1.936e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.919e-07, Loss_0: 2.234e-11, Loss_r: 1.919e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.902e-07, Loss_0: 2.596e-14, Loss_r: 1.902e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.886e-07, Loss_0: 4.407e-12, Loss_r: 1.886e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.869e-07, Loss_0: 5.206e-12, Loss_r: 1.869e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.853e-07, Loss_0: 7.575e-13, Loss_r: 1.853e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.837e-07, Loss_0: 1.297e-12, Loss_r: 1.837e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.821e-07, Loss_0: 7.246e-13, Loss_r: 1.821e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.805e-07, Loss_0: 2.951e-12, Loss_r: 1.805e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.808e-07, Loss_0: 4.435e-10, Loss_r: 1.803e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 4.163e-07, Loss_0: 6.635e-08, Loss_r: 3.499e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.863e-07, Loss_0: 2.724e-09, Loss_r: 1.836e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.902e-07, Loss_0: 4.559e-09, Loss_r: 1.856e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.869e-07, Loss_0: 4.017e-09, Loss_r: 1.829e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.722e-07, Loss_0: 1.208e-11, Loss_r: 1.721e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.726e-07, Loss_0: 4.793e-10, Loss_r: 1.721e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.696e-07, Loss_0: 8.131e-11, Loss_r: 1.695e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.681e-07, Loss_0: 2.524e-11, Loss_r: 1.681e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.668e-07, Loss_0: 1.157e-11, Loss_r: 1.668e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.655e-07, Loss_0: 2.242e-11, Loss_r: 1.655e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.642e-07, Loss_0: 5.489e-13, Loss_r: 1.642e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.629e-07, Loss_0: 8.298e-12, Loss_r: 1.629e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.616e-07, Loss_0: 1.003e-12, Loss_r: 1.616e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.604e-07, Loss_0: 1.386e-12, Loss_r: 1.604e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.591e-07, Loss_0: 2.313e-12, Loss_r: 1.591e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.579e-07, Loss_0: 1.323e-12, Loss_r: 1.579e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.567e-07, Loss_0: 1.519e-12, Loss_r: 1.567e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.555e-07, Loss_0: 2.028e-13, Loss_r: 1.555e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.546e-07, Loss_0: 6.047e-11, Loss_r: 1.545e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 2.090e-07, Loss_0: 1.622e-08, Loss_r: 1.928e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.528e-07, Loss_0: 1.889e-10, Loss_r: 1.526e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.571e-07, Loss_0: 1.929e-09, Loss_r: 1.552e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.522e-07, Loss_0: 7.536e-10, Loss_r: 1.515e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.493e-07, Loss_0: 1.026e-10, Loss_r: 1.492e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.481e-07, Loss_0: 5.001e-11, Loss_r: 1.480e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 1.470e-07, Loss_0: 6.776e-11, Loss_r: 1.469e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.458e-07, Loss_0: 3.569e-13, Loss_r: 1.458e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.448e-07, Loss_0: 4.232e-13, Loss_r: 1.448e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 1.438e-07, Loss_0: 3.189e-12, Loss_r: 1.438e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 1.428e-07, Loss_0: 5.304e-13, Loss_r: 1.428e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 1.418e-07, Loss_0: 1.711e-12, Loss_r: 1.418e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.409e-07, Loss_0: 2.074e-12, Loss_r: 1.409e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.399e-07, Loss_0: 9.490e-13, Loss_r: 1.399e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.389e-07, Loss_0: 4.385e-13, Loss_r: 1.389e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.380e-07, Loss_0: 1.863e-13, Loss_r: 1.380e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.372e-07, Loss_0: 2.927e-11, Loss_r: 1.371e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.427e-07, Loss_0: 1.933e-09, Loss_r: 1.408e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 8.817e-07, Loss_0: 2.283e-07, Loss_r: 6.534e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.821e-07, Loss_0: 1.440e-08, Loss_r: 1.677e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.636e-07, Loss_0: 9.466e-09, Loss_r: 1.541e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.776e-07, Loss_0: 1.406e-08, Loss_r: 1.635e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.329e-07, Loss_0: 2.794e-10, Loss_r: 1.326e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.369e-07, Loss_0: 1.649e-09, Loss_r: 1.353e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.305e-07, Loss_0: 4.106e-13, Loss_r: 1.305e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.305e-07, Loss_0: 2.612e-10, Loss_r: 1.302e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.292e-07, Loss_0: 5.973e-11, Loss_r: 1.291e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.282e-07, Loss_0: 1.646e-11, Loss_r: 1.281e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.274e-07, Loss_0: 6.717e-15, Loss_r: 1.274e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.266e-07, Loss_0: 3.406e-12, Loss_r: 1.266e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.258e-07, Loss_0: 2.656e-14, Loss_r: 1.258e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.251e-07, Loss_0: 4.948e-12, Loss_r: 1.250e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.243e-07, Loss_0: 1.524e-12, Loss_r: 1.243e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.235e-07, Loss_0: 7.902e-13, Loss_r: 1.235e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.228e-07, Loss_0: 1.133e-13, Loss_r: 1.228e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.221e-07, Loss_0: 1.234e-12, Loss_r: 1.221e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.219e-07, Loss_0: 1.484e-10, Loss_r: 1.217e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.736e-07, Loss_0: 1.660e-08, Loss_r: 1.570e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 1.238e-07, Loss_0: 1.162e-09, Loss_r: 1.227e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 1.211e-07, Loss_0: 6.199e-10, Loss_r: 1.205e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 1.218e-07, Loss_0: 1.088e-09, Loss_r: 1.208e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 1.181e-07, Loss_0: 3.975e-11, Loss_r: 1.181e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 1.178e-07, Loss_0: 1.037e-10, Loss_r: 1.177e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 1.167e-07, Loss_0: 2.968e-15, Loss_r: 1.167e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 1.162e-07, Loss_0: 2.805e-11, Loss_r: 1.161e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 1.155e-07, Loss_0: 1.791e-12, Loss_r: 1.155e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 1.149e-07, Loss_0: 2.956e-12, Loss_r: 1.149e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 1.143e-07, Loss_0: 7.623e-13, Loss_r: 1.143e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 1.136e-07, Loss_0: 6.971e-13, Loss_r: 1.136e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 1.130e-07, Loss_0: 7.869e-13, Loss_r: 1.130e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.124e-07, Loss_0: 1.084e-12, Loss_r: 1.124e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.118e-07, Loss_0: 7.518e-13, Loss_r: 1.118e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 1.112e-07, Loss_0: 1.451e-13, Loss_r: 1.112e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.106e-07, Loss_0: 1.395e-13, Loss_r: 1.106e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.101e-07, Loss_0: 1.761e-14, Loss_r: 1.101e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 1.095e-07, Loss_0: 1.653e-11, Loss_r: 1.095e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.150e-07, Loss_0: 1.888e-09, Loss_r: 1.131e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 1.089e-07, Loss_0: 1.531e-10, Loss_r: 1.087e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 1.080e-07, Loss_0: 5.875e-11, Loss_r: 1.079e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 1.077e-07, Loss_0: 1.342e-10, Loss_r: 1.076e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 1.069e-07, Loss_0: 1.333e-11, Loss_r: 1.068e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 1.064e-07, Loss_0: 6.450e-12, Loss_r: 1.063e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 1.058e-07, Loss_0: 1.423e-15, Loss_r: 1.058e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 1.053e-07, Loss_0: 5.378e-12, Loss_r: 1.053e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 1.048e-07, Loss_0: 9.303e-14, Loss_r: 1.048e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 1.043e-07, Loss_0: 8.561e-13, Loss_r: 1.043e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 1.038e-07, Loss_0: 6.809e-13, Loss_r: 1.038e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 1.033e-07, Loss_0: 5.544e-13, Loss_r: 1.033e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 1.028e-07, Loss_0: 5.896e-13, Loss_r: 1.028e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 1.024e-07, Loss_0: 1.016e-12, Loss_r: 1.024e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 1.019e-07, Loss_0: 6.149e-13, Loss_r: 1.019e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 1.014e-07, Loss_0: 7.373e-13, Loss_r: 1.014e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 1.009e-07, Loss_0: 4.944e-13, Loss_r: 1.009e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 1.004e-07, Loss_0: 8.587e-15, Loss_r: 1.004e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 1.000e-07, Loss_0: 8.694e-12, Loss_r: 1.000e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 1.019e-07, Loss_0: 7.594e-10, Loss_r: 1.012e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 4.242e-07, Loss_0: 1.088e-07, Loss_r: 3.155e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 1.140e-07, Loss_0: 5.064e-09, Loss_r: 1.090e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 1.165e-07, Loss_0: 6.270e-09, Loss_r: 1.102e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 1.170e-07, Loss_0: 6.593e-09, Loss_r: 1.104e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 9.748e-08, Loss_0: 6.873e-12, Loss_r: 9.747e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 9.970e-08, Loss_0: 8.449e-10, Loss_r: 9.886e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 9.669e-08, Loss_0: 1.765e-11, Loss_r: 9.667e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 9.648e-08, Loss_0: 9.190e-11, Loss_r: 9.638e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 9.598e-08, Loss_0: 3.221e-11, Loss_r: 9.595e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 9.547e-08, Loss_0: 1.394e-11, Loss_r: 9.546e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 9.505e-08, Loss_0: 4.471e-13, Loss_r: 9.505e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 9.465e-08, Loss_0: 1.967e-12, Loss_r: 9.465e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 9.425e-08, Loss_0: 1.002e-14, Loss_r: 9.425e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 9.385e-08, Loss_0: 2.037e-12, Loss_r: 9.385e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 9.347e-08, Loss_0: 1.928e-13, Loss_r: 9.347e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 9.308e-08, Loss_0: 2.507e-16, Loss_r: 9.308e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 9.269e-08, Loss_0: 3.761e-13, Loss_r: 9.269e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 9.236e-08, Loss_0: 1.490e-11, Loss_r: 9.235e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 9.400e-08, Loss_0: 6.683e-10, Loss_r: 9.333e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 2.540e-07, Loss_0: 5.561e-08, Loss_r: 1.984e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 1.076e-07, Loss_0: 5.546e-09, Loss_r: 1.021e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 9.333e-08, Loss_0: 8.921e-10, Loss_r: 9.244e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 9.936e-08, Loss_0: 3.124e-09, Loss_r: 9.624e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 9.136e-08, Loss_0: 4.315e-10, Loss_r: 9.093e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 9.050e-08, Loss_0: 1.961e-10, Loss_r: 9.031e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 8.972e-08, Loss_0: 5.726e-11, Loss_r: 8.966e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 8.933e-08, Loss_0: 5.888e-11, Loss_r: 8.927e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 8.886e-08, Loss_0: 7.026e-17, Loss_r: 8.886e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 8.853e-08, Loss_0: 1.098e-12, Loss_r: 8.853e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 8.820e-08, Loss_0: 6.505e-12, Loss_r: 8.819e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 8.787e-08, Loss_0: 5.318e-13, Loss_r: 8.787e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 8.752e-08, Loss_0: 1.975e-12, Loss_r: 8.752e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 8.720e-08, Loss_0: 1.432e-14, Loss_r: 8.720e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 8.687e-08, Loss_0: 9.273e-13, Loss_r: 8.687e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 8.655e-08, Loss_0: 1.271e-12, Loss_r: 8.655e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 8.623e-08, Loss_0: 1.274e-12, Loss_r: 8.622e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 8.591e-08, Loss_0: 4.879e-13, Loss_r: 8.591e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 8.558e-08, Loss_0: 1.784e-13, Loss_r: 8.558e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 8.527e-08, Loss_0: 2.845e-12, Loss_r: 8.527e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 8.660e-08, Loss_0: 5.541e-10, Loss_r: 8.604e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 4.046e-07, Loss_0: 1.123e-07, Loss_r: 2.923e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 9.103e-08, Loss_0: 2.294e-09, Loss_r: 8.874e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 1.133e-07, Loss_0: 1.048e-08, Loss_r: 1.029e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 9.988e-08, Loss_0: 5.795e-09, Loss_r: 9.409e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 8.429e-08, Loss_0: 2.461e-10, Loss_r: 8.404e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 8.555e-08, Loss_0: 7.675e-10, Loss_r: 8.478e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 8.341e-08, Loss_0: 1.671e-10, Loss_r: 8.325e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 8.274e-08, Loss_0: 1.673e-11, Loss_r: 8.272e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 8.252e-08, Loss_0: 2.675e-11, Loss_r: 8.250e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 8.218e-08, Loss_0: 2.247e-11, Loss_r: 8.216e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 8.188e-08, Loss_0: 3.364e-12, Loss_r: 8.187e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 8.158e-08, Loss_0: 4.139e-12, Loss_r: 8.158e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 8.130e-08, Loss_0: 4.009e-16, Loss_r: 8.130e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 8.103e-08, Loss_0: 7.462e-13, Loss_r: 8.103e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 8.075e-08, Loss_0: 9.986e-13, Loss_r: 8.075e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 8.047e-08, Loss_0: 3.275e-13, Loss_r: 8.047e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 8.020e-08, Loss_0: 4.178e-13, Loss_r: 8.020e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 7.998e-08, Loss_0: 1.567e-11, Loss_r: 7.996e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 8.183e-08, Loss_0: 7.516e-10, Loss_r: 8.108e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 2.744e-07, Loss_0: 6.988e-08, Loss_r: 2.046e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 9.669e-08, Loss_0: 6.216e-09, Loss_r: 9.047e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 8.288e-08, Loss_0: 1.471e-09, Loss_r: 8.141e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 8.967e-08, Loss_0: 4.073e-09, Loss_r: 8.560e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 7.954e-08, Loss_0: 4.253e-10, Loss_r: 7.911e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 7.908e-08, Loss_0: 3.064e-10, Loss_r: 7.878e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 7.813e-08, Loss_0: 5.333e-11, Loss_r: 7.807e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 7.787e-08, Loss_0: 7.250e-11, Loss_r: 7.780e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 7.746e-08, Loss_0: 4.833e-13, Loss_r: 7.746e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 7.723e-08, Loss_0: 8.271e-13, Loss_r: 7.723e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 7.699e-08, Loss_0: 4.731e-12, Loss_r: 7.699e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 7.674e-08, Loss_0: 2.748e-14, Loss_r: 7.674e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 7.649e-08, Loss_0: 1.506e-12, Loss_r: 7.649e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 7.626e-08, Loss_0: 9.077e-14, Loss_r: 7.626e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 7.602e-08, Loss_0: 1.645e-13, Loss_r: 7.602e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 7.578e-08, Loss_0: 2.350e-13, Loss_r: 7.578e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 7.554e-08, Loss_0: 1.069e-14, Loss_r: 7.554e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 7.531e-08, Loss_0: 4.046e-13, Loss_r: 7.531e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 7.513e-08, Loss_0: 1.969e-11, Loss_r: 7.511e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 7.720e-08, Loss_0: 8.267e-10, Loss_r: 7.637e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 2.687e-07, Loss_0: 7.099e-08, Loss_r: 1.977e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 9.369e-08, Loss_0: 7.007e-09, Loss_r: 8.668e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 7.708e-08, Loss_0: 1.106e-09, Loss_r: 7.597e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 8.444e-08, Loss_0: 3.926e-09, Loss_r: 8.052e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 7.529e-08, Loss_0: 5.883e-10, Loss_r: 7.470e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 7.424e-08, Loss_0: 2.384e-10, Loss_r: 7.400e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 7.363e-08, Loss_0: 9.039e-11, Loss_r: 7.354e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 7.328e-08, Loss_0: 6.221e-11, Loss_r: 7.322e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 7.292e-08, Loss_0: 4.686e-13, Loss_r: 7.292e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 7.273e-08, Loss_0: 2.900e-12, Loss_r: 7.273e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 7.251e-08, Loss_0: 5.760e-12, Loss_r: 7.251e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 7.230e-08, Loss_0: 3.603e-13, Loss_r: 7.230e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 7.208e-08, Loss_0: 1.306e-12, Loss_r: 7.208e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 7.187e-08, Loss_0: 1.862e-14, Loss_r: 7.187e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 7.166e-08, Loss_0: 2.100e-13, Loss_r: 7.166e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 7.144e-08, Loss_0: 2.984e-13, Loss_r: 7.144e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 7.123e-08, Loss_0: 2.359e-13, Loss_r: 7.123e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 7.103e-08, Loss_0: 7.421e-14, Loss_r: 7.103e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 7.082e-08, Loss_0: 2.450e-13, Loss_r: 7.082e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 7.072e-08, Loss_0: 3.467e-11, Loss_r: 7.069e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "Training time: 20.6361\n",
            "[1, 128, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.414e-02, Loss_0: 7.344e-04, Loss_r: 3.341e-02, Time: 0.84, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.261e-02, Loss_0: 1.412e-04, Loss_r: 3.247e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.214e-02, Loss_0: 4.291e-05, Loss_r: 3.210e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.172e-02, Loss_0: 1.597e-05, Loss_r: 3.170e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.883e-02, Loss_0: 4.321e-05, Loss_r: 2.878e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.344e-02, Loss_0: 4.065e-05, Loss_r: 1.340e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.300e-03, Loss_0: 1.079e-03, Loss_r: 1.221e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.036e-03, Loss_0: 2.599e-04, Loss_r: 7.758e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 2.363e-04, Loss_0: 2.042e-05, Loss_r: 2.159e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.070e-04, Loss_0: 1.323e-05, Loss_r: 9.379e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 6.965e-05, Loss_0: 2.032e-05, Loss_r: 4.933e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.099e-05, Loss_0: 2.665e-06, Loss_r: 2.832e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.385e-05, Loss_0: 2.305e-06, Loss_r: 2.154e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.672e-05, Loss_0: 5.079e-09, Loss_r: 1.671e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.410e-05, Loss_0: 2.747e-08, Loss_r: 1.407e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.263e-05, Loss_0: 1.859e-07, Loss_r: 1.244e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.098e-05, Loss_0: 6.600e-08, Loss_r: 1.091e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 9.667e-06, Loss_0: 2.843e-08, Loss_r: 9.639e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 8.588e-06, Loss_0: 2.912e-08, Loss_r: 8.558e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 7.634e-06, Loss_0: 3.663e-12, Loss_r: 7.634e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.240e-06, Loss_0: 3.508e-07, Loss_r: 6.890e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.752e-03, Loss_0: 1.602e-03, Loss_r: 1.502e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 4.077e-04, Loss_0: 3.754e-04, Loss_r: 3.237e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 2.378e-05, Loss_0: 1.247e-05, Loss_r: 1.131e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.587e-05, Loss_0: 5.220e-06, Loss_r: 1.065e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 2.278e-05, Loss_0: 1.266e-05, Loss_r: 1.011e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.026e-05, Loss_0: 2.214e-06, Loss_r: 8.041e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 8.134e-06, Loss_0: 1.383e-06, Loss_r: 6.752e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.954e-06, Loss_0: 8.330e-09, Loss_r: 5.946e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.415e-06, Loss_0: 7.314e-08, Loss_r: 5.342e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.852e-06, Loss_0: 4.408e-08, Loss_r: 4.808e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.389e-06, Loss_0: 6.597e-10, Loss_r: 4.388e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 4.032e-06, Loss_0: 3.974e-09, Loss_r: 4.028e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 3.720e-06, Loss_0: 4.969e-09, Loss_r: 3.715e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 3.451e-06, Loss_0: 2.254e-09, Loss_r: 3.449e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 3.220e-06, Loss_0: 4.086e-10, Loss_r: 3.220e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.021e-06, Loss_0: 2.466e-10, Loss_r: 3.021e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.848e-06, Loss_0: 2.929e-10, Loss_r: 2.848e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 2.696e-06, Loss_0: 1.473e-10, Loss_r: 2.696e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.562e-06, Loss_0: 9.868e-12, Loss_r: 2.562e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.442e-06, Loss_0: 7.836e-11, Loss_r: 2.442e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.335e-06, Loss_0: 1.637e-11, Loss_r: 2.335e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.238e-06, Loss_0: 3.486e-13, Loss_r: 2.238e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.153e-06, Loss_0: 2.372e-09, Loss_r: 2.150e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.200e-06, Loss_0: 2.897e-06, Loss_r: 2.303e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.378e-03, Loss_0: 1.263e-03, Loss_r: 1.147e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 3.307e-04, Loss_0: 3.152e-04, Loss_r: 1.557e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.666e-06, Loss_0: 3.434e-08, Loss_r: 5.632e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.869e-05, Loss_0: 2.164e-05, Loss_r: 7.051e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.326e-05, Loss_0: 8.174e-06, Loss_r: 5.090e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.861e-06, Loss_0: 1.140e-07, Loss_r: 3.747e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.794e-06, Loss_0: 1.433e-06, Loss_r: 3.360e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.033e-06, Loss_0: 3.445e-08, Loss_r: 2.999e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.880e-06, Loss_0: 9.762e-08, Loss_r: 2.783e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.663e-06, Loss_0: 6.916e-08, Loss_r: 2.594e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.460e-06, Loss_0: 1.144e-08, Loss_r: 2.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.319e-06, Loss_0: 3.341e-09, Loss_r: 2.316e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.207e-06, Loss_0: 3.634e-10, Loss_r: 2.206e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.109e-06, Loss_0: 8.318e-10, Loss_r: 2.108e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 2.023e-06, Loss_0: 1.445e-10, Loss_r: 2.023e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.946e-06, Loss_0: 3.040e-10, Loss_r: 1.945e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.875e-06, Loss_0: 2.707e-11, Loss_r: 1.875e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.811e-06, Loss_0: 1.507e-12, Loss_r: 1.811e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.750e-06, Loss_0: 3.910e-11, Loss_r: 1.750e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.694e-06, Loss_0: 4.724e-11, Loss_r: 1.694e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.641e-06, Loss_0: 3.309e-11, Loss_r: 1.641e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.590e-06, Loss_0: 2.047e-11, Loss_r: 1.590e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.541e-06, Loss_0: 1.974e-11, Loss_r: 1.541e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.495e-06, Loss_0: 1.716e-11, Loss_r: 1.495e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.450e-06, Loss_0: 1.669e-11, Loss_r: 1.450e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.407e-06, Loss_0: 1.867e-11, Loss_r: 1.407e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.365e-06, Loss_0: 1.349e-11, Loss_r: 1.365e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.324e-06, Loss_0: 1.710e-11, Loss_r: 1.324e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.285e-06, Loss_0: 1.274e-11, Loss_r: 1.285e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.247e-06, Loss_0: 1.253e-11, Loss_r: 1.247e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.210e-06, Loss_0: 1.484e-11, Loss_r: 1.210e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.175e-06, Loss_0: 1.893e-11, Loss_r: 1.175e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.140e-06, Loss_0: 3.347e-11, Loss_r: 1.140e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.107e-06, Loss_0: 2.651e-10, Loss_r: 1.106e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.101e-06, Loss_0: 2.621e-08, Loss_r: 1.075e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.516e-05, Loss_0: 1.325e-05, Loss_r: 1.906e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 3.230e-04, Loss_0: 3.054e-04, Loss_r: 1.758e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.867e-04, Loss_0: 2.753e-04, Loss_r: 1.144e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 4.430e-05, Loss_0: 3.993e-05, Loss_r: 4.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 8.499e-06, Loss_0: 5.292e-06, Loss_r: 3.207e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.163e-05, Loss_0: 8.583e-06, Loss_r: 3.047e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.578e-06, Loss_0: 2.135e-07, Loss_r: 2.364e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.690e-06, Loss_0: 5.132e-07, Loss_r: 2.177e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.476e-06, Loss_0: 4.039e-07, Loss_r: 2.072e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.058e-06, Loss_0: 1.311e-07, Loss_r: 1.927e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.857e-06, Loss_0: 2.461e-08, Loss_r: 1.832e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.749e-06, Loss_0: 8.721e-09, Loss_r: 1.740e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.669e-06, Loss_0: 2.170e-09, Loss_r: 1.667e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.602e-06, Loss_0: 2.388e-09, Loss_r: 1.599e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.541e-06, Loss_0: 4.564e-10, Loss_r: 1.540e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.485e-06, Loss_0: 4.084e-10, Loss_r: 1.485e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.434e-06, Loss_0: 5.184e-11, Loss_r: 1.434e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.386e-06, Loss_0: 2.007e-13, Loss_r: 1.386e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.341e-06, Loss_0: 6.439e-11, Loss_r: 1.341e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.298e-06, Loss_0: 5.794e-11, Loss_r: 1.298e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.257e-06, Loss_0: 2.899e-11, Loss_r: 1.257e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.218e-06, Loss_0: 2.001e-11, Loss_r: 1.218e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.181e-06, Loss_0: 2.418e-11, Loss_r: 1.181e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.145e-06, Loss_0: 2.603e-11, Loss_r: 1.145e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.111e-06, Loss_0: 2.479e-11, Loss_r: 1.111e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.078e-06, Loss_0: 1.998e-11, Loss_r: 1.078e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.047e-06, Loss_0: 1.813e-11, Loss_r: 1.047e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.016e-06, Loss_0: 1.993e-11, Loss_r: 1.016e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 9.873e-07, Loss_0: 2.247e-11, Loss_r: 9.873e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 9.594e-07, Loss_0: 1.771e-11, Loss_r: 9.593e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 9.325e-07, Loss_0: 1.336e-11, Loss_r: 9.325e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 9.066e-07, Loss_0: 1.823e-11, Loss_r: 9.065e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 8.817e-07, Loss_0: 1.104e-11, Loss_r: 8.817e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 8.578e-07, Loss_0: 1.721e-11, Loss_r: 8.578e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 8.349e-07, Loss_0: 8.876e-12, Loss_r: 8.348e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 8.127e-07, Loss_0: 1.325e-11, Loss_r: 8.127e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 7.915e-07, Loss_0: 3.453e-11, Loss_r: 7.915e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 7.711e-07, Loss_0: 9.680e-11, Loss_r: 7.710e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 7.532e-07, Loss_0: 2.009e-09, Loss_r: 7.512e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.018e-06, Loss_0: 2.704e-07, Loss_r: 7.476e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.210e-04, Loss_0: 1.128e-04, Loss_r: 8.166e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 5.082e-04, Loss_0: 4.707e-04, Loss_r: 3.751e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.418e-05, Loss_0: 2.923e-05, Loss_r: 4.948e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 2.306e-05, Loss_0: 1.760e-05, Loss_r: 5.459e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.636e-05, Loss_0: 1.280e-05, Loss_r: 3.566e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 7.654e-06, Loss_0: 4.991e-06, Loss_r: 2.663e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.336e-06, Loss_0: 1.324e-06, Loss_r: 2.012e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.159e-06, Loss_0: 3.191e-07, Loss_r: 1.840e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.820e-06, Loss_0: 1.203e-07, Loss_r: 1.699e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.666e-06, Loss_0: 4.975e-08, Loss_r: 1.617e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.562e-06, Loss_0: 3.081e-08, Loss_r: 1.531e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.476e-06, Loss_0: 8.785e-09, Loss_r: 1.467e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.405e-06, Loss_0: 2.177e-09, Loss_r: 1.403e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.347e-06, Loss_0: 3.080e-10, Loss_r: 1.346e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.295e-06, Loss_0: 2.099e-10, Loss_r: 1.295e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.246e-06, Loss_0: 1.572e-10, Loss_r: 1.246e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.200e-06, Loss_0: 1.975e-10, Loss_r: 1.200e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.157e-06, Loss_0: 3.128e-11, Loss_r: 1.157e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.116e-06, Loss_0: 1.265e-11, Loss_r: 1.116e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.078e-06, Loss_0: 2.105e-11, Loss_r: 1.078e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.041e-06, Loss_0: 3.545e-11, Loss_r: 1.041e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.006e-06, Loss_0: 3.566e-11, Loss_r: 1.006e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 9.731e-07, Loss_0: 3.739e-11, Loss_r: 9.731e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 9.416e-07, Loss_0: 3.316e-11, Loss_r: 9.416e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 9.116e-07, Loss_0: 2.819e-11, Loss_r: 9.115e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 8.829e-07, Loss_0: 2.538e-11, Loss_r: 8.829e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 8.557e-07, Loss_0: 1.934e-11, Loss_r: 8.557e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 8.296e-07, Loss_0: 2.024e-11, Loss_r: 8.296e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 8.048e-07, Loss_0: 2.565e-11, Loss_r: 8.047e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 7.810e-07, Loss_0: 1.926e-11, Loss_r: 7.810e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 7.583e-07, Loss_0: 1.617e-11, Loss_r: 7.583e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.365e-07, Loss_0: 1.480e-11, Loss_r: 7.365e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 7.157e-07, Loss_0: 2.051e-11, Loss_r: 7.157e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 6.957e-07, Loss_0: 1.689e-11, Loss_r: 6.957e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 6.766e-07, Loss_0: 2.973e-11, Loss_r: 6.766e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 6.582e-07, Loss_0: 1.688e-11, Loss_r: 6.582e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 6.406e-07, Loss_0: 3.789e-11, Loss_r: 6.406e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 6.239e-07, Loss_0: 3.297e-10, Loss_r: 6.235e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 6.304e-07, Loss_0: 2.245e-08, Loss_r: 6.080e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 7.318e-06, Loss_0: 6.245e-06, Loss_r: 1.073e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 9.752e-04, Loss_0: 9.106e-04, Loss_r: 6.455e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 6.696e-05, Loss_0: 6.247e-05, Loss_r: 4.486e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.187e-05, Loss_0: 2.523e-05, Loss_r: 6.647e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 4.211e-06, Loss_0: 1.919e-06, Loss_r: 2.292e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.972e-06, Loss_0: 7.589e-07, Loss_r: 2.213e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 3.499e-06, Loss_0: 1.524e-06, Loss_r: 1.975e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.668e-06, Loss_0: 8.697e-07, Loss_r: 1.798e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.021e-06, Loss_0: 3.152e-07, Loss_r: 1.706e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.675e-06, Loss_0: 1.347e-07, Loss_r: 1.540e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.494e-06, Loss_0: 3.420e-08, Loss_r: 1.460e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.382e-06, Loss_0: 9.691e-09, Loss_r: 1.372e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.302e-06, Loss_0: 1.139e-10, Loss_r: 1.302e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.237e-06, Loss_0: 2.092e-10, Loss_r: 1.236e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.177e-06, Loss_0: 1.208e-09, Loss_r: 1.176e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.121e-06, Loss_0: 5.134e-12, Loss_r: 1.121e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.070e-06, Loss_0: 1.448e-12, Loss_r: 1.070e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.022e-06, Loss_0: 1.434e-10, Loss_r: 1.022e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 9.781e-07, Loss_0: 7.965e-11, Loss_r: 9.780e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 9.366e-07, Loss_0: 2.869e-11, Loss_r: 9.365e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 8.977e-07, Loss_0: 2.586e-11, Loss_r: 8.977e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.613e-07, Loss_0: 3.170e-11, Loss_r: 8.613e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 8.271e-07, Loss_0: 3.311e-11, Loss_r: 8.270e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 7.949e-07, Loss_0: 2.946e-11, Loss_r: 7.949e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 7.646e-07, Loss_0: 2.682e-11, Loss_r: 7.646e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 7.361e-07, Loss_0: 2.478e-11, Loss_r: 7.361e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 7.091e-07, Loss_0: 2.429e-11, Loss_r: 7.091e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 6.837e-07, Loss_0: 2.071e-11, Loss_r: 6.836e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.595e-07, Loss_0: 1.952e-11, Loss_r: 6.595e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 6.367e-07, Loss_0: 1.532e-11, Loss_r: 6.367e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 6.150e-07, Loss_0: 2.108e-11, Loss_r: 6.149e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.943e-07, Loss_0: 1.727e-11, Loss_r: 5.943e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 5.747e-07, Loss_0: 1.254e-11, Loss_r: 5.747e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 5.560e-07, Loss_0: 1.447e-11, Loss_r: 5.560e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.382e-07, Loss_0: 1.687e-11, Loss_r: 5.382e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.212e-07, Loss_0: 1.039e-11, Loss_r: 5.212e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 5.049e-07, Loss_0: 6.723e-12, Loss_r: 5.049e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 4.894e-07, Loss_0: 1.207e-11, Loss_r: 4.894e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 4.745e-07, Loss_0: 1.941e-11, Loss_r: 4.745e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.602e-07, Loss_0: 3.091e-11, Loss_r: 4.602e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 4.466e-07, Loss_0: 2.169e-10, Loss_r: 4.464e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 4.492e-07, Loss_0: 1.503e-08, Loss_r: 4.342e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 4.702e-06, Loss_0: 3.877e-06, Loss_r: 8.248e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 8.962e-04, Loss_0: 8.193e-04, Loss_r: 7.686e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.893e-04, Loss_0: 1.825e-04, Loss_r: 6.811e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 7.532e-05, Loss_0: 6.794e-05, Loss_r: 7.383e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.269e-05, Loss_0: 2.062e-05, Loss_r: 2.070e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 6.235e-06, Loss_0: 3.785e-06, Loss_r: 2.450e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.201e-06, Loss_0: 5.907e-07, Loss_r: 1.610e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.496e-06, Loss_0: 1.125e-08, Loss_r: 1.485e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.311e-06, Loss_0: 2.397e-09, Loss_r: 1.308e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.204e-06, Loss_0: 1.633e-10, Loss_r: 1.204e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.122e-06, Loss_0: 4.024e-09, Loss_r: 1.118e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.050e-06, Loss_0: 5.588e-09, Loss_r: 1.045e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 9.830e-07, Loss_0: 5.852e-09, Loss_r: 9.771e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 9.208e-07, Loss_0: 1.238e-09, Loss_r: 9.195e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 8.659e-07, Loss_0: 3.013e-10, Loss_r: 8.656e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 8.179e-07, Loss_0: 3.187e-10, Loss_r: 8.175e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 7.740e-07, Loss_0: 8.530e-12, Loss_r: 7.740e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 7.338e-07, Loss_0: 1.745e-11, Loss_r: 7.338e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 6.970e-07, Loss_0: 8.294e-11, Loss_r: 6.969e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 6.631e-07, Loss_0: 4.165e-11, Loss_r: 6.631e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 6.319e-07, Loss_0: 1.818e-11, Loss_r: 6.319e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 6.030e-07, Loss_0: 1.553e-11, Loss_r: 6.029e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 5.762e-07, Loss_0: 1.770e-11, Loss_r: 5.762e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 5.513e-07, Loss_0: 1.745e-11, Loss_r: 5.513e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 5.281e-07, Loss_0: 1.932e-11, Loss_r: 5.281e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 5.064e-07, Loss_0: 1.807e-11, Loss_r: 5.064e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 4.862e-07, Loss_0: 1.559e-11, Loss_r: 4.862e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 4.672e-07, Loss_0: 1.876e-11, Loss_r: 4.672e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 4.493e-07, Loss_0: 1.595e-11, Loss_r: 4.493e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 4.325e-07, Loss_0: 1.233e-11, Loss_r: 4.325e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 4.167e-07, Loss_0: 7.734e-12, Loss_r: 4.167e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 4.017e-07, Loss_0: 1.236e-11, Loss_r: 4.017e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 3.876e-07, Loss_0: 1.124e-11, Loss_r: 3.875e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 3.741e-07, Loss_0: 8.138e-12, Loss_r: 3.741e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 3.614e-07, Loss_0: 8.760e-12, Loss_r: 3.614e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 3.493e-07, Loss_0: 8.079e-12, Loss_r: 3.492e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2370, Loss: 3.377e-07, Loss_0: 6.569e-12, Loss_r: 3.377e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2380, Loss: 3.268e-07, Loss_0: 5.787e-12, Loss_r: 3.268e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2390, Loss: 3.163e-07, Loss_0: 7.269e-12, Loss_r: 3.163e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2400, Loss: 3.063e-07, Loss_0: 3.781e-12, Loss_r: 3.063e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2410, Loss: 2.967e-07, Loss_0: 1.012e-12, Loss_r: 2.967e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2420, Loss: 2.876e-07, Loss_0: 2.240e-12, Loss_r: 2.876e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2430, Loss: 2.792e-07, Loss_0: 2.491e-10, Loss_r: 2.790e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2440, Loss: 2.973e-07, Loss_0: 2.238e-08, Loss_r: 2.749e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2450, Loss: 5.069e-06, Loss_0: 4.112e-06, Loss_r: 9.576e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 3.460e-07, Loss_0: 7.516e-08, Loss_r: 2.708e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 7.269e-07, Loss_0: 4.104e-07, Loss_r: 3.165e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 4.641e-07, Loss_0: 1.888e-07, Loss_r: 2.752e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 2.672e-07, Loss_0: 2.316e-08, Loss_r: 2.441e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 2.562e-07, Loss_0: 1.894e-08, Loss_r: 2.373e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 2.417e-07, Loss_0: 1.253e-08, Loss_r: 2.292e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 2.231e-07, Loss_0: 1.022e-09, Loss_r: 2.221e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 2.162e-07, Loss_0: 2.246e-11, Loss_r: 2.162e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 2.109e-07, Loss_0: 1.707e-12, Loss_r: 2.109e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 2.057e-07, Loss_0: 7.515e-11, Loss_r: 2.056e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2560, Loss: 2.008e-07, Loss_0: 8.491e-11, Loss_r: 2.007e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2570, Loss: 1.958e-07, Loss_0: 3.543e-11, Loss_r: 1.958e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2580, Loss: 1.912e-07, Loss_0: 1.113e-10, Loss_r: 1.911e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2590, Loss: 1.870e-07, Loss_0: 3.851e-10, Loss_r: 1.866e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2600, Loss: 1.898e-07, Loss_0: 6.675e-09, Loss_r: 1.831e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2610, Loss: 6.703e-07, Loss_0: 4.167e-07, Loss_r: 2.536e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 2.259e-07, Loss_0: 4.403e-08, Loss_r: 1.819e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 1.791e-07, Loss_0: 6.683e-09, Loss_r: 1.724e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 1.953e-07, Loss_0: 2.298e-08, Loss_r: 1.723e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 1.662e-07, Loss_0: 1.683e-09, Loss_r: 1.645e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 1.638e-07, Loss_0: 2.707e-09, Loss_r: 1.611e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 1.576e-07, Loss_0: 1.131e-10, Loss_r: 1.575e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 1.550e-07, Loss_0: 3.758e-10, Loss_r: 1.546e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 1.516e-07, Loss_0: 1.493e-10, Loss_r: 1.514e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 1.485e-07, Loss_0: 9.630e-12, Loss_r: 1.485e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 1.456e-07, Loss_0: 1.400e-11, Loss_r: 1.456e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 1.429e-07, Loss_0: 3.847e-13, Loss_r: 1.429e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 1.402e-07, Loss_0: 1.150e-11, Loss_r: 1.402e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 1.375e-07, Loss_0: 2.168e-13, Loss_r: 1.375e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2750, Loss: 1.350e-07, Loss_0: 5.761e-13, Loss_r: 1.350e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2760, Loss: 1.325e-07, Loss_0: 2.152e-12, Loss_r: 1.325e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2770, Loss: 1.302e-07, Loss_0: 5.253e-11, Loss_r: 1.302e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2780, Loss: 1.311e-07, Loss_0: 2.573e-09, Loss_r: 1.285e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2790, Loss: 4.608e-07, Loss_0: 2.767e-07, Loss_r: 1.841e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 1.444e-07, Loss_0: 1.694e-08, Loss_r: 1.275e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 1.370e-07, Loss_0: 1.290e-08, Loss_r: 1.241e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 1.398e-07, Loss_0: 1.681e-08, Loss_r: 1.230e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 1.181e-07, Loss_0: 4.333e-11, Loss_r: 1.180e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 1.190e-07, Loss_0: 2.152e-09, Loss_r: 1.168e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 1.146e-07, Loss_0: 7.594e-11, Loss_r: 1.145e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 1.129e-07, Loss_0: 1.613e-10, Loss_r: 1.128e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 1.113e-07, Loss_0: 9.599e-11, Loss_r: 1.112e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 1.095e-07, Loss_0: 5.077e-11, Loss_r: 1.095e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 1.079e-07, Loss_0: 8.228e-12, Loss_r: 1.079e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 1.064e-07, Loss_0: 1.137e-11, Loss_r: 1.064e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 1.049e-07, Loss_0: 1.159e-12, Loss_r: 1.049e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 1.034e-07, Loss_0: 8.674e-17, Loss_r: 1.034e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 1.020e-07, Loss_0: 1.545e-13, Loss_r: 1.020e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 1.006e-07, Loss_0: 2.888e-13, Loss_r: 1.006e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2950, Loss: 9.921e-08, Loss_0: 8.161e-15, Loss_r: 9.921e-08, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2960, Loss: 9.790e-08, Loss_0: 9.286e-12, Loss_r: 9.789e-08, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2970, Loss: 9.849e-08, Loss_0: 1.493e-09, Loss_r: 9.699e-08, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2980, Loss: 4.936e-07, Loss_0: 3.252e-07, Loss_r: 1.684e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 9.862e-08, Loss_0: 3.468e-09, Loss_r: 9.515e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 1.397e-07, Loss_0: 3.842e-08, Loss_r: 1.013e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 1.073e-07, Loss_0: 1.256e-08, Loss_r: 9.477e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 9.485e-08, Loss_0: 2.927e-09, Loss_r: 9.192e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 9.151e-08, Loss_0: 1.058e-09, Loss_r: 9.045e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 9.038e-08, Loss_0: 1.057e-09, Loss_r: 8.932e-08, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 8.830e-08, Loss_0: 8.599e-11, Loss_r: 8.821e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 8.723e-08, Loss_0: 8.518e-13, Loss_r: 8.723e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 8.629e-08, Loss_0: 5.827e-12, Loss_r: 8.629e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 8.537e-08, Loss_0: 3.631e-13, Loss_r: 8.537e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 8.448e-08, Loss_0: 1.027e-12, Loss_r: 8.448e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 8.360e-08, Loss_0: 4.835e-12, Loss_r: 8.359e-08, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 8.274e-08, Loss_0: 3.313e-13, Loss_r: 8.274e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3120, Loss: 8.192e-08, Loss_0: 5.071e-12, Loss_r: 8.192e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3130, Loss: 8.114e-08, Loss_0: 4.127e-11, Loss_r: 8.110e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3140, Loss: 8.137e-08, Loss_0: 8.568e-10, Loss_r: 8.052e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3150, Loss: 1.371e-07, Loss_0: 4.659e-08, Loss_r: 9.056e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 8.646e-08, Loss_0: 6.113e-09, Loss_r: 8.035e-08, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 7.837e-08, Loss_0: 2.247e-10, Loss_r: 7.815e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 8.012e-08, Loss_0: 2.240e-09, Loss_r: 7.788e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 7.746e-08, Loss_0: 5.790e-10, Loss_r: 7.688e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 7.625e-08, Loss_0: 7.800e-11, Loss_r: 7.617e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 7.562e-08, Loss_0: 7.329e-11, Loss_r: 7.554e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 7.491e-08, Loss_0: 3.449e-11, Loss_r: 7.487e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 7.428e-08, Loss_0: 1.544e-12, Loss_r: 7.427e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 7.368e-08, Loss_0: 1.781e-12, Loss_r: 7.368e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 7.308e-08, Loss_0: 3.449e-12, Loss_r: 7.307e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 7.250e-08, Loss_0: 6.120e-15, Loss_r: 7.250e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 7.193e-08, Loss_0: 9.672e-13, Loss_r: 7.193e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 7.136e-08, Loss_0: 1.990e-13, Loss_r: 7.136e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 7.082e-08, Loss_0: 2.265e-12, Loss_r: 7.082e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 7.027e-08, Loss_0: 1.232e-11, Loss_r: 7.026e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3310, Loss: 6.975e-08, Loss_0: 1.324e-11, Loss_r: 6.974e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3320, Loss: 6.925e-08, Loss_0: 4.644e-11, Loss_r: 6.920e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3330, Loss: 6.950e-08, Loss_0: 6.810e-10, Loss_r: 6.882e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3340, Loss: 1.121e-07, Loss_0: 3.567e-08, Loss_r: 7.641e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 7.355e-08, Loss_0: 4.773e-09, Loss_r: 6.878e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 6.748e-08, Loss_0: 1.331e-10, Loss_r: 6.735e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 6.886e-08, Loss_0: 1.574e-09, Loss_r: 6.729e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 6.695e-08, Loss_0: 3.884e-10, Loss_r: 6.656e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 6.608e-08, Loss_0: 7.044e-11, Loss_r: 6.601e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 6.568e-08, Loss_0: 7.630e-11, Loss_r: 6.560e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 6.520e-08, Loss_0: 1.423e-11, Loss_r: 6.518e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 6.478e-08, Loss_0: 1.069e-13, Loss_r: 6.478e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 6.438e-08, Loss_0: 5.059e-12, Loss_r: 6.437e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 6.399e-08, Loss_0: 6.994e-13, Loss_r: 6.399e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 6.359e-08, Loss_0: 3.339e-12, Loss_r: 6.359e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 6.320e-08, Loss_0: 9.734e-14, Loss_r: 6.320e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 6.282e-08, Loss_0: 7.795e-13, Loss_r: 6.282e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 6.245e-08, Loss_0: 1.173e-12, Loss_r: 6.244e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 6.207e-08, Loss_0: 7.963e-14, Loss_r: 6.207e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 6.170e-08, Loss_0: 5.464e-14, Loss_r: 6.170e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 6.135e-08, Loss_0: 7.095e-14, Loss_r: 6.135e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 6.099e-08, Loss_0: 1.381e-13, Loss_r: 6.099e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 6.065e-08, Loss_0: 4.209e-12, Loss_r: 6.065e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 6.112e-08, Loss_0: 6.512e-10, Loss_r: 6.047e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 2.361e-07, Loss_0: 1.411e-07, Loss_r: 9.494e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 6.187e-08, Loss_0: 1.743e-09, Loss_r: 6.013e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 7.888e-08, Loss_0: 1.582e-08, Loss_r: 6.306e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 6.638e-08, Loss_0: 5.943e-09, Loss_r: 6.043e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 5.992e-08, Loss_0: 9.000e-10, Loss_r: 5.902e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 5.932e-08, Loss_0: 6.570e-10, Loss_r: 5.867e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 5.864e-08, Loss_0: 3.889e-10, Loss_r: 5.825e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 5.790e-08, Loss_0: 5.088e-12, Loss_r: 5.789e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 5.763e-08, Loss_0: 7.264e-12, Loss_r: 5.762e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 5.734e-08, Loss_0: 1.416e-11, Loss_r: 5.733e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 5.706e-08, Loss_0: 3.610e-12, Loss_r: 5.705e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 5.675e-08, Loss_0: 1.693e-12, Loss_r: 5.675e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 5.649e-08, Loss_0: 1.949e-13, Loss_r: 5.649e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 5.622e-08, Loss_0: 4.879e-13, Loss_r: 5.622e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 5.594e-08, Loss_0: 3.801e-13, Loss_r: 5.594e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 5.568e-08, Loss_0: 3.043e-12, Loss_r: 5.567e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 5.547e-08, Loss_0: 4.465e-11, Loss_r: 5.543e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 5.678e-08, Loss_0: 1.287e-09, Loss_r: 5.549e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 1.553e-07, Loss_0: 8.014e-08, Loss_r: 7.517e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 6.717e-08, Loss_0: 9.929e-09, Loss_r: 5.724e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 5.504e-08, Loss_0: 5.100e-10, Loss_r: 5.453e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 5.897e-08, Loss_0: 3.861e-09, Loss_r: 5.510e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 5.512e-08, Loss_0: 9.448e-10, Loss_r: 5.417e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 5.394e-08, Loss_0: 1.432e-10, Loss_r: 5.380e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 5.373e-08, Loss_0: 1.571e-10, Loss_r: 5.357e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 5.335e-08, Loss_0: 4.172e-11, Loss_r: 5.331e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 5.308e-08, Loss_0: 4.888e-12, Loss_r: 5.308e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 5.286e-08, Loss_0: 4.430e-12, Loss_r: 5.286e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 5.264e-08, Loss_0: 5.262e-12, Loss_r: 5.263e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 5.242e-08, Loss_0: 4.190e-13, Loss_r: 5.242e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 5.221e-08, Loss_0: 1.781e-12, Loss_r: 5.221e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 5.198e-08, Loss_0: 5.975e-15, Loss_r: 5.198e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 5.176e-08, Loss_0: 4.831e-14, Loss_r: 5.176e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 5.155e-08, Loss_0: 5.691e-15, Loss_r: 5.155e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 5.134e-08, Loss_0: 1.761e-12, Loss_r: 5.134e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 5.113e-08, Loss_0: 2.902e-12, Loss_r: 5.113e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 5.095e-08, Loss_0: 3.257e-11, Loss_r: 5.092e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 5.184e-08, Loss_0: 9.339e-10, Loss_r: 5.090e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 1.329e-07, Loss_0: 6.596e-08, Loss_r: 6.691e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 5.924e-08, Loss_0: 7.179e-09, Loss_r: 5.206e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 5.116e-08, Loss_0: 7.974e-10, Loss_r: 5.036e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 5.429e-08, Loss_0: 3.418e-09, Loss_r: 5.087e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 5.044e-08, Loss_0: 5.178e-10, Loss_r: 4.992e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 4.986e-08, Loss_0: 2.283e-10, Loss_r: 4.963e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 4.953e-08, Loss_0: 1.017e-10, Loss_r: 4.942e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 4.929e-08, Loss_0: 4.760e-11, Loss_r: 4.924e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 4.904e-08, Loss_0: 1.149e-13, Loss_r: 4.904e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 4.887e-08, Loss_0: 7.154e-12, Loss_r: 4.886e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 4.869e-08, Loss_0: 3.014e-12, Loss_r: 4.869e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 4.850e-08, Loss_0: 3.536e-12, Loss_r: 4.850e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 4.833e-08, Loss_0: 2.057e-13, Loss_r: 4.833e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 4.815e-08, Loss_0: 2.742e-12, Loss_r: 4.815e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 4.797e-08, Loss_0: 1.430e-13, Loss_r: 4.797e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 4.780e-08, Loss_0: 2.204e-12, Loss_r: 4.780e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 4.762e-08, Loss_0: 8.381e-13, Loss_r: 4.761e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 4.744e-08, Loss_0: 6.164e-13, Loss_r: 4.744e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 4.730e-08, Loss_0: 1.591e-11, Loss_r: 4.728e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 4.777e-08, Loss_0: 5.214e-10, Loss_r: 4.725e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 9.794e-08, Loss_0: 4.049e-08, Loss_r: 5.745e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 5.205e-08, Loss_0: 4.168e-09, Loss_r: 4.789e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 4.733e-08, Loss_0: 5.831e-10, Loss_r: 4.674e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 4.917e-08, Loss_0: 2.195e-09, Loss_r: 4.697e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 4.669e-08, Loss_0: 3.148e-10, Loss_r: 4.637e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 4.633e-08, Loss_0: 1.313e-10, Loss_r: 4.620e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 4.606e-08, Loss_0: 4.763e-11, Loss_r: 4.601e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 4.588e-08, Loss_0: 3.253e-11, Loss_r: 4.584e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 4.569e-08, Loss_0: 1.799e-14, Loss_r: 4.569e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 4.553e-08, Loss_0: 1.562e-12, Loss_r: 4.553e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 4.538e-08, Loss_0: 2.057e-12, Loss_r: 4.538e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 4.523e-08, Loss_0: 1.124e-13, Loss_r: 4.523e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 4.509e-08, Loss_0: 9.581e-13, Loss_r: 4.509e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 4.493e-08, Loss_0: 1.700e-16, Loss_r: 4.493e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 4.478e-08, Loss_0: 3.264e-14, Loss_r: 4.478e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 4.462e-08, Loss_0: 6.580e-13, Loss_r: 4.462e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 4.448e-08, Loss_0: 4.597e-13, Loss_r: 4.447e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 4.431e-08, Loss_0: 7.911e-14, Loss_r: 4.431e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 4.418e-08, Loss_0: 1.167e-12, Loss_r: 4.418e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 4.409e-08, Loss_0: 4.604e-11, Loss_r: 4.405e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 4.801e-08, Loss_0: 3.258e-09, Loss_r: 4.475e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 4.422e-08, Loss_0: 3.655e-10, Loss_r: 4.385e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 4.365e-08, Loss_0: 3.974e-11, Loss_r: 4.361e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 4.369e-08, Loss_0: 1.866e-10, Loss_r: 4.350e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 4.337e-08, Loss_0: 3.464e-11, Loss_r: 4.334e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 4.321e-08, Loss_0: 7.620e-12, Loss_r: 4.321e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 4.307e-08, Loss_0: 4.648e-12, Loss_r: 4.306e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 4.293e-08, Loss_0: 2.596e-12, Loss_r: 4.293e-08, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 4.279e-08, Loss_0: 2.483e-13, Loss_r: 4.279e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 4.266e-08, Loss_0: 2.220e-14, Loss_r: 4.266e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 4.253e-08, Loss_0: 4.750e-15, Loss_r: 4.253e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 4.239e-08, Loss_0: 4.996e-14, Loss_r: 4.239e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 4.227e-08, Loss_0: 2.327e-13, Loss_r: 4.227e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 4.213e-08, Loss_0: 1.175e-13, Loss_r: 4.213e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 4.200e-08, Loss_0: 2.652e-13, Loss_r: 4.200e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 4.186e-08, Loss_0: 1.900e-14, Loss_r: 4.186e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 4.173e-08, Loss_0: 1.025e-12, Loss_r: 4.173e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 4.161e-08, Loss_0: 1.866e-11, Loss_r: 4.159e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 4.166e-08, Loss_0: 1.627e-10, Loss_r: 4.149e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 4.606e-08, Loss_0: 3.796e-09, Loss_r: 4.227e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 4.226e-08, Loss_0: 8.513e-10, Loss_r: 4.141e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 4.113e-08, Loss_0: 3.558e-11, Loss_r: 4.110e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 4.104e-08, Loss_0: 4.704e-11, Loss_r: 4.099e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 4.094e-08, Loss_0: 7.178e-11, Loss_r: 4.087e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 4.074e-08, Loss_0: 5.680e-12, Loss_r: 4.074e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 4.061e-08, Loss_0: 5.215e-12, Loss_r: 4.061e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 4.050e-08, Loss_0: 1.933e-12, Loss_r: 4.049e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 4.038e-08, Loss_0: 1.530e-13, Loss_r: 4.038e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 4.026e-08, Loss_0: 3.789e-14, Loss_r: 4.026e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 4.014e-08, Loss_0: 1.015e-12, Loss_r: 4.014e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 4.002e-08, Loss_0: 3.197e-14, Loss_r: 4.002e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 3.990e-08, Loss_0: 7.651e-14, Loss_r: 3.990e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 3.978e-08, Loss_0: 2.138e-14, Loss_r: 3.978e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 3.966e-08, Loss_0: 2.477e-14, Loss_r: 3.966e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 3.954e-08, Loss_0: 5.999e-14, Loss_r: 3.954e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 3.942e-08, Loss_0: 5.990e-13, Loss_r: 3.942e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 3.930e-08, Loss_0: 2.928e-13, Loss_r: 3.930e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 3.919e-08, Loss_0: 9.042e-13, Loss_r: 3.919e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 3.908e-08, Loss_0: 1.182e-11, Loss_r: 3.906e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 3.915e-08, Loss_0: 1.693e-10, Loss_r: 3.898e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 4.864e-08, Loss_0: 7.836e-09, Loss_r: 4.081e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 4.023e-08, Loss_0: 1.217e-09, Loss_r: 3.901e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 3.861e-08, Loss_0: 2.848e-12, Loss_r: 3.861e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 3.887e-08, Loss_0: 2.802e-10, Loss_r: 3.859e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 3.856e-08, Loss_0: 1.261e-10, Loss_r: 3.844e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 3.829e-08, Loss_0: 2.671e-12, Loss_r: 3.829e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 3.822e-08, Loss_0: 3.069e-11, Loss_r: 3.819e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 3.807e-08, Loss_0: 1.875e-13, Loss_r: 3.807e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 3.797e-08, Loss_0: 2.065e-12, Loss_r: 3.797e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 3.786e-08, Loss_0: 2.087e-12, Loss_r: 3.786e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 3.775e-08, Loss_0: 1.952e-16, Loss_r: 3.775e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 3.765e-08, Loss_0: 5.291e-13, Loss_r: 3.765e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 3.755e-08, Loss_0: 4.996e-16, Loss_r: 3.755e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 3.743e-08, Loss_0: 8.993e-14, Loss_r: 3.743e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 3.733e-08, Loss_0: 1.537e-13, Loss_r: 3.733e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 3.722e-08, Loss_0: 5.036e-13, Loss_r: 3.722e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 3.711e-08, Loss_0: 2.668e-12, Loss_r: 3.711e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 3.700e-08, Loss_0: 1.375e-12, Loss_r: 3.700e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 3.689e-08, Loss_0: 8.882e-12, Loss_r: 3.689e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 3.690e-08, Loss_0: 9.845e-11, Loss_r: 3.681e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 4.081e-08, Loss_0: 3.300e-09, Loss_r: 3.751e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 3.734e-08, Loss_0: 6.111e-10, Loss_r: 3.673e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 3.649e-08, Loss_0: 5.236e-12, Loss_r: 3.648e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 3.649e-08, Loss_0: 7.420e-11, Loss_r: 3.641e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 3.637e-08, Loss_0: 5.925e-11, Loss_r: 3.631e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 3.619e-08, Loss_0: 7.470e-13, Loss_r: 3.619e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 3.610e-08, Loss_0: 6.216e-12, Loss_r: 3.610e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "Training time: 20.8181\n",
            "[1, 128, 128, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.429e-02, Loss_0: 6.358e-04, Loss_r: 3.366e-02, Time: 0.94, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.204e-02, Loss_0: 1.004e-04, Loss_r: 3.194e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.131e-02, Loss_0: 3.146e-06, Loss_r: 3.131e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 2.661e-02, Loss_0: 1.016e-05, Loss_r: 2.660e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.284e-02, Loss_0: 1.345e-05, Loss_r: 1.283e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.895e-03, Loss_0: 3.087e-05, Loss_r: 1.864e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.637e-04, Loss_0: 2.997e-04, Loss_r: 2.639e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 70, Loss: 3.414e-04, Loss_0: 6.412e-05, Loss_r: 2.773e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.049e-04, Loss_0: 3.544e-05, Loss_r: 6.950e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.550e-05, Loss_0: 2.273e-05, Loss_r: 3.277e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 3.383e-05, Loss_0: 3.545e-06, Loss_r: 3.028e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 2.413e-05, Loss_0: 2.445e-06, Loss_r: 2.168e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.916e-05, Loss_0: 1.164e-06, Loss_r: 1.799e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.622e-05, Loss_0: 2.036e-07, Loss_r: 1.602e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.423e-05, Loss_0: 3.946e-08, Loss_r: 1.420e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.276e-05, Loss_0: 2.605e-08, Loss_r: 1.274e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.147e-05, Loss_0: 2.689e-08, Loss_r: 1.145e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.034e-05, Loss_0: 2.228e-08, Loss_r: 1.032e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 9.335e-06, Loss_0: 7.542e-09, Loss_r: 9.327e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 8.449e-06, Loss_0: 5.909e-10, Loss_r: 8.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.662e-06, Loss_0: 4.860e-09, Loss_r: 7.657e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 7.235e-06, Loss_0: 2.962e-07, Loss_r: 6.939e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 6.368e-04, Loss_0: 6.271e-04, Loss_r: 9.705e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 8.413e-06, Loss_0: 1.380e-06, Loss_r: 7.033e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 2.675e-05, Loss_0: 1.970e-05, Loss_r: 7.049e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 3.326e-05, Loss_0: 2.580e-05, Loss_r: 7.461e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.531e-05, Loss_0: 8.769e-06, Loss_r: 6.537e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 8.870e-06, Loss_0: 2.596e-06, Loss_r: 6.274e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 6.975e-06, Loss_0: 1.318e-06, Loss_r: 5.657e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.656e-06, Loss_0: 3.312e-07, Loss_r: 5.324e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.950e-06, Loss_0: 1.693e-08, Loss_r: 4.933e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.636e-06, Loss_0: 4.903e-08, Loss_r: 4.587e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 4.322e-06, Loss_0: 3.364e-08, Loss_r: 4.289e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.035e-06, Loss_0: 1.405e-08, Loss_r: 4.021e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 3.781e-06, Loss_0: 2.584e-09, Loss_r: 3.779e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 3.560e-06, Loss_0: 4.945e-10, Loss_r: 3.560e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.358e-06, Loss_0: 8.241e-10, Loss_r: 3.357e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.175e-06, Loss_0: 6.595e-11, Loss_r: 3.175e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.008e-06, Loss_0: 4.991e-11, Loss_r: 3.008e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.889e-06, Loss_0: 2.990e-08, Loss_r: 2.859e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.319e-05, Loss_0: 3.998e-05, Loss_r: 3.203e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 4.528e-04, Loss_0: 4.466e-04, Loss_r: 6.256e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.437e-04, Loss_0: 1.391e-04, Loss_r: 4.609e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 4.982e-05, Loss_0: 4.589e-05, Loss_r: 3.926e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.411e-05, Loss_0: 9.946e-06, Loss_r: 4.168e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.086e-06, Loss_0: 1.498e-06, Loss_r: 3.588e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 3.748e-06, Loss_0: 2.914e-07, Loss_r: 3.456e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 3.473e-06, Loss_0: 2.412e-07, Loss_r: 3.232e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 3.297e-06, Loss_0: 2.111e-07, Loss_r: 3.086e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.004e-06, Loss_0: 1.009e-07, Loss_r: 2.903e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.765e-06, Loss_0: 1.932e-10, Loss_r: 2.764e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.644e-06, Loss_0: 1.169e-08, Loss_r: 2.632e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.506e-06, Loss_0: 2.829e-10, Loss_r: 2.505e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.389e-06, Loss_0: 7.562e-10, Loss_r: 2.389e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.282e-06, Loss_0: 8.327e-10, Loss_r: 2.281e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.181e-06, Loss_0: 4.172e-10, Loss_r: 2.181e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.088e-06, Loss_0: 1.284e-10, Loss_r: 2.087e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.999e-06, Loss_0: 1.843e-11, Loss_r: 1.999e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.917e-06, Loss_0: 4.079e-11, Loss_r: 1.917e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.838e-06, Loss_0: 8.904e-11, Loss_r: 1.838e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.764e-06, Loss_0: 5.223e-11, Loss_r: 1.764e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.694e-06, Loss_0: 5.258e-11, Loss_r: 1.694e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.628e-06, Loss_0: 3.208e-11, Loss_r: 1.628e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.564e-06, Loss_0: 1.395e-11, Loss_r: 1.564e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.515e-06, Loss_0: 9.364e-09, Loss_r: 1.505e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 6.883e-06, Loss_0: 5.369e-06, Loss_r: 1.514e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 4.951e-04, Loss_0: 4.856e-04, Loss_r: 9.476e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 5.961e-05, Loss_0: 5.457e-05, Loss_r: 5.034e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 8.170e-06, Loss_0: 5.309e-06, Loss_r: 2.861e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 7.081e-06, Loss_0: 4.481e-06, Loss_r: 2.600e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 700, Loss: 5.215e-06, Loss_0: 2.674e-06, Loss_r: 2.542e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.257e-06, Loss_0: 9.850e-07, Loss_r: 2.272e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.304e-06, Loss_0: 1.520e-07, Loss_r: 2.153e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.037e-06, Loss_0: 2.838e-10, Loss_r: 2.037e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.997e-06, Loss_0: 6.958e-08, Loss_r: 1.928e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.887e-06, Loss_0: 4.568e-08, Loss_r: 1.841e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.748e-06, Loss_0: 1.775e-09, Loss_r: 1.746e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.671e-06, Loss_0: 7.407e-09, Loss_r: 1.664e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.589e-06, Loss_0: 2.436e-12, Loss_r: 1.589e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.519e-06, Loss_0: 4.070e-10, Loss_r: 1.519e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.452e-06, Loss_0: 8.228e-11, Loss_r: 1.452e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.390e-06, Loss_0: 3.917e-13, Loss_r: 1.390e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.332e-06, Loss_0: 1.727e-11, Loss_r: 1.332e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.278e-06, Loss_0: 2.022e-11, Loss_r: 1.278e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.226e-06, Loss_0: 1.587e-11, Loss_r: 1.226e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.177e-06, Loss_0: 2.077e-11, Loss_r: 1.177e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.131e-06, Loss_0: 4.625e-11, Loss_r: 1.131e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.087e-06, Loss_0: 3.244e-11, Loss_r: 1.087e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.046e-06, Loss_0: 2.302e-11, Loss_r: 1.046e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.006e-06, Loss_0: 2.034e-11, Loss_r: 1.006e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 900, Loss: 9.687e-07, Loss_0: 3.187e-11, Loss_r: 9.687e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 910, Loss: 9.329e-07, Loss_0: 2.624e-11, Loss_r: 9.329e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 8.988e-07, Loss_0: 2.304e-12, Loss_r: 8.988e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 930, Loss: 8.664e-07, Loss_0: 3.067e-11, Loss_r: 8.664e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 8.388e-07, Loss_0: 3.108e-09, Loss_r: 8.357e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.427e-06, Loss_0: 6.108e-07, Loss_r: 8.165e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.843e-04, Loss_0: 2.810e-04, Loss_r: 3.388e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 7.239e-05, Loss_0: 7.037e-05, Loss_r: 2.017e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 980, Loss: 8.986e-05, Loss_0: 8.699e-05, Loss_r: 2.874e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.070e-05, Loss_0: 9.066e-06, Loss_r: 1.637e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.592e-06, Loss_0: 3.543e-09, Loss_r: 1.588e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.616e-06, Loss_0: 1.099e-06, Loss_r: 1.517e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.736e-06, Loss_0: 1.363e-06, Loss_r: 1.373e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.645e-06, Loss_0: 3.182e-07, Loss_r: 1.327e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.252e-06, Loss_0: 7.243e-09, Loss_r: 1.244e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.245e-06, Loss_0: 7.059e-08, Loss_r: 1.174e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.118e-06, Loss_0: 1.006e-10, Loss_r: 1.118e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.072e-06, Loss_0: 6.936e-09, Loss_r: 1.065e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.015e-06, Loss_0: 1.041e-09, Loss_r: 1.014e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 9.678e-07, Loss_0: 7.041e-11, Loss_r: 9.677e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 9.255e-07, Loss_0: 3.378e-10, Loss_r: 9.251e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 8.861e-07, Loss_0: 2.425e-10, Loss_r: 8.858e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 8.494e-07, Loss_0: 1.285e-10, Loss_r: 8.493e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 8.152e-07, Loss_0: 6.754e-11, Loss_r: 8.151e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 7.832e-07, Loss_0: 2.872e-11, Loss_r: 7.831e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 7.531e-07, Loss_0: 1.098e-11, Loss_r: 7.531e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 7.249e-07, Loss_0: 7.953e-12, Loss_r: 7.249e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 6.983e-07, Loss_0: 1.256e-11, Loss_r: 6.982e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 6.731e-07, Loss_0: 3.225e-11, Loss_r: 6.731e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 6.493e-07, Loss_0: 1.997e-11, Loss_r: 6.493e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 6.268e-07, Loss_0: 1.078e-11, Loss_r: 6.268e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 6.055e-07, Loss_0: 1.769e-11, Loss_r: 6.055e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 5.853e-07, Loss_0: 1.117e-11, Loss_r: 5.853e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 5.661e-07, Loss_0: 2.235e-11, Loss_r: 5.661e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 5.478e-07, Loss_0: 1.065e-11, Loss_r: 5.478e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 5.305e-07, Loss_0: 6.937e-12, Loss_r: 5.305e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 5.140e-07, Loss_0: 5.038e-12, Loss_r: 5.140e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 4.985e-07, Loss_0: 9.797e-11, Loss_r: 4.984e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 5.505e-07, Loss_0: 6.513e-08, Loss_r: 4.854e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 6.038e-05, Loss_0: 5.939e-05, Loss_r: 9.907e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 4.596e-04, Loss_0: 4.548e-04, Loss_r: 4.802e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.089e-05, Loss_0: 7.776e-06, Loss_r: 3.115e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.670e-05, Loss_0: 1.461e-05, Loss_r: 2.092e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.833e-05, Loss_0: 1.622e-05, Loss_r: 2.109e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 7.356e-06, Loss_0: 5.953e-06, Loss_r: 1.403e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.138e-06, Loss_0: 8.226e-07, Loss_r: 1.315e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.208e-06, Loss_0: 2.874e-08, Loss_r: 1.180e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.106e-06, Loss_0: 7.899e-09, Loss_r: 1.098e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.033e-06, Loss_0: 8.801e-09, Loss_r: 1.024e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 9.577e-07, Loss_0: 4.568e-09, Loss_r: 9.531e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 8.955e-07, Loss_0: 1.255e-10, Loss_r: 8.954e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.439e-07, Loss_0: 2.338e-11, Loss_r: 8.439e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 7.987e-07, Loss_0: 9.892e-10, Loss_r: 7.977e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 7.579e-07, Loss_0: 3.243e-10, Loss_r: 7.575e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 7.208e-07, Loss_0: 2.956e-10, Loss_r: 7.205e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 6.873e-07, Loss_0: 2.556e-11, Loss_r: 6.873e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 6.571e-07, Loss_0: 2.390e-14, Loss_r: 6.571e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 6.293e-07, Loss_0: 4.115e-11, Loss_r: 6.292e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 6.037e-07, Loss_0: 3.401e-11, Loss_r: 6.037e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 5.800e-07, Loss_0: 1.571e-11, Loss_r: 5.800e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 5.580e-07, Loss_0: 1.007e-11, Loss_r: 5.580e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 5.375e-07, Loss_0: 1.024e-11, Loss_r: 5.375e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 5.184e-07, Loss_0: 9.722e-12, Loss_r: 5.184e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 5.004e-07, Loss_0: 9.537e-12, Loss_r: 5.004e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 4.836e-07, Loss_0: 9.769e-12, Loss_r: 4.836e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.678e-07, Loss_0: 1.122e-11, Loss_r: 4.677e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.528e-07, Loss_0: 9.909e-12, Loss_r: 4.528e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 4.387e-07, Loss_0: 8.090e-12, Loss_r: 4.387e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 4.254e-07, Loss_0: 7.869e-12, Loss_r: 4.254e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 4.129e-07, Loss_0: 1.005e-11, Loss_r: 4.128e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 4.009e-07, Loss_0: 6.540e-12, Loss_r: 4.009e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.897e-07, Loss_0: 5.491e-12, Loss_r: 3.897e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.790e-07, Loss_0: 5.021e-12, Loss_r: 3.790e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.688e-07, Loss_0: 2.304e-12, Loss_r: 3.688e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 3.591e-07, Loss_0: 1.409e-11, Loss_r: 3.591e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 3.499e-07, Loss_0: 2.724e-12, Loss_r: 3.499e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 3.412e-07, Loss_0: 5.063e-12, Loss_r: 3.412e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 3.328e-07, Loss_0: 4.290e-12, Loss_r: 3.328e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 3.249e-07, Loss_0: 2.089e-12, Loss_r: 3.249e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 3.173e-07, Loss_0: 4.477e-12, Loss_r: 3.173e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 3.100e-07, Loss_0: 5.250e-13, Loss_r: 3.100e-07, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.031e-07, Loss_0: 1.900e-13, Loss_r: 3.031e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.965e-07, Loss_0: 3.555e-11, Loss_r: 2.964e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.936e-07, Loss_0: 3.317e-09, Loss_r: 2.903e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 8.929e-07, Loss_0: 6.012e-07, Loss_r: 2.918e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 2.023e-04, Loss_0: 1.999e-04, Loss_r: 2.371e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.284e-04, Loss_0: 2.270e-04, Loss_r: 1.454e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 3.753e-06, Loss_0: 3.079e-06, Loss_r: 6.734e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.766e-05, Loss_0: 1.670e-05, Loss_r: 9.661e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.090e-05, Loss_0: 1.005e-05, Loss_r: 8.476e-07, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 2.442e-06, Loss_0: 1.746e-06, Loss_r: 6.955e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.521e-07, Loss_0: 1.135e-08, Loss_r: 6.408e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 9.508e-07, Loss_0: 3.657e-07, Loss_r: 5.851e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 6.450e-07, Loss_0: 9.396e-08, Loss_r: 5.511e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 5.240e-07, Loss_0: 9.277e-09, Loss_r: 5.147e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.038e-07, Loss_0: 1.941e-08, Loss_r: 4.844e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 4.623e-07, Loss_0: 1.592e-09, Loss_r: 4.607e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 4.412e-07, Loss_0: 1.253e-09, Loss_r: 4.400e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 4.218e-07, Loss_0: 5.879e-10, Loss_r: 4.212e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 4.045e-07, Loss_0: 4.668e-14, Loss_r: 4.045e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 3.896e-07, Loss_0: 8.463e-11, Loss_r: 3.895e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.761e-07, Loss_0: 8.195e-11, Loss_r: 3.760e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 3.637e-07, Loss_0: 5.435e-11, Loss_r: 3.637e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.523e-07, Loss_0: 2.978e-11, Loss_r: 3.523e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.418e-07, Loss_0: 1.388e-11, Loss_r: 3.418e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 3.320e-07, Loss_0: 1.149e-11, Loss_r: 3.320e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.229e-07, Loss_0: 1.133e-11, Loss_r: 3.229e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 3.143e-07, Loss_0: 9.734e-12, Loss_r: 3.143e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 3.063e-07, Loss_0: 5.818e-12, Loss_r: 3.063e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 2.988e-07, Loss_0: 2.892e-12, Loss_r: 2.988e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.916e-07, Loss_0: 3.532e-12, Loss_r: 2.916e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.849e-07, Loss_0: 2.413e-12, Loss_r: 2.849e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.785e-07, Loss_0: 4.039e-12, Loss_r: 2.784e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.724e-07, Loss_0: 4.938e-12, Loss_r: 2.724e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.666e-07, Loss_0: 5.038e-12, Loss_r: 2.666e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.611e-07, Loss_0: 1.544e-12, Loss_r: 2.611e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.558e-07, Loss_0: 1.932e-13, Loss_r: 2.558e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.508e-07, Loss_0: 3.920e-12, Loss_r: 2.508e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.460e-07, Loss_0: 5.523e-13, Loss_r: 2.460e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 2.414e-07, Loss_0: 6.717e-15, Loss_r: 2.414e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 2.370e-07, Loss_0: 1.312e-12, Loss_r: 2.370e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 2.328e-07, Loss_0: 6.818e-11, Loss_r: 2.327e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 2.312e-07, Loss_0: 2.376e-09, Loss_r: 2.288e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 4.191e-07, Loss_0: 1.914e-07, Loss_r: 2.277e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.382e-07, Loss_0: 1.636e-08, Loss_r: 2.218e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 2.234e-07, Loss_0: 5.489e-09, Loss_r: 2.179e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 2.263e-07, Loss_0: 1.171e-08, Loss_r: 2.146e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 2.120e-07, Loss_0: 3.737e-10, Loss_r: 2.116e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.101e-07, Loss_0: 1.363e-09, Loss_r: 2.087e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 2.057e-07, Loss_0: 2.442e-12, Loss_r: 2.057e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 2.030e-07, Loss_0: 1.902e-10, Loss_r: 2.028e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 2.001e-07, Loss_0: 7.026e-11, Loss_r: 2.000e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.973e-07, Loss_0: 3.550e-11, Loss_r: 1.973e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.946e-07, Loss_0: 2.829e-12, Loss_r: 1.946e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.920e-07, Loss_0: 1.015e-11, Loss_r: 1.920e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.895e-07, Loss_0: 8.674e-17, Loss_r: 1.895e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.870e-07, Loss_0: 5.551e-17, Loss_r: 1.870e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.845e-07, Loss_0: 2.878e-13, Loss_r: 1.845e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.821e-07, Loss_0: 3.079e-12, Loss_r: 1.821e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.799e-07, Loss_0: 3.564e-11, Loss_r: 1.799e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.796e-07, Loss_0: 1.955e-09, Loss_r: 1.777e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 3.941e-07, Loss_0: 2.157e-07, Loss_r: 1.784e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 1.867e-07, Loss_0: 1.296e-08, Loss_r: 1.737e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 1.818e-07, Loss_0: 1.039e-08, Loss_r: 1.714e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 1.825e-07, Loss_0: 1.306e-08, Loss_r: 1.695e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 1.677e-07, Loss_0: 1.562e-11, Loss_r: 1.677e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 1.676e-07, Loss_0: 1.659e-09, Loss_r: 1.660e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 1.641e-07, Loss_0: 7.434e-11, Loss_r: 1.641e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 1.624e-07, Loss_0: 1.156e-10, Loss_r: 1.623e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 1.606e-07, Loss_0: 6.843e-11, Loss_r: 1.606e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.589e-07, Loss_0: 4.151e-11, Loss_r: 1.588e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.571e-07, Loss_0: 6.550e-12, Loss_r: 1.571e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.555e-07, Loss_0: 1.125e-11, Loss_r: 1.554e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.538e-07, Loss_0: 3.040e-13, Loss_r: 1.538e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.522e-07, Loss_0: 2.436e-13, Loss_r: 1.522e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 1.506e-07, Loss_0: 1.112e-13, Loss_r: 1.506e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 1.491e-07, Loss_0: 1.637e-12, Loss_r: 1.491e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 1.475e-07, Loss_0: 4.323e-11, Loss_r: 1.475e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 1.468e-07, Loss_0: 8.622e-10, Loss_r: 1.460e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.128e-07, Loss_0: 6.799e-08, Loss_r: 1.448e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 1.489e-07, Loss_0: 5.742e-09, Loss_r: 1.431e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 1.438e-07, Loss_0: 1.834e-09, Loss_r: 1.420e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.445e-07, Loss_0: 3.768e-09, Loss_r: 1.407e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 1.395e-07, Loss_0: 1.225e-10, Loss_r: 1.394e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.385e-07, Loss_0: 4.949e-10, Loss_r: 1.381e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 1.368e-07, Loss_0: 8.079e-12, Loss_r: 1.368e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 1.357e-07, Loss_0: 5.546e-11, Loss_r: 1.356e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.344e-07, Loss_0: 2.894e-11, Loss_r: 1.344e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.332e-07, Loss_0: 6.415e-13, Loss_r: 1.332e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.321e-07, Loss_0: 4.352e-12, Loss_r: 1.321e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.309e-07, Loss_0: 1.879e-12, Loss_r: 1.309e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.298e-07, Loss_0: 7.692e-12, Loss_r: 1.297e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.286e-07, Loss_0: 1.026e-13, Loss_r: 1.286e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.275e-07, Loss_0: 8.883e-13, Loss_r: 1.275e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.264e-07, Loss_0: 1.069e-12, Loss_r: 1.264e-07, Time: 0.10, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.253e-07, Loss_0: 8.993e-14, Loss_r: 1.253e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.242e-07, Loss_0: 8.743e-13, Loss_r: 1.242e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.232e-07, Loss_0: 3.462e-11, Loss_r: 1.232e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.278e-07, Loss_0: 5.429e-09, Loss_r: 1.223e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.462e-06, Loss_0: 1.324e-06, Loss_r: 1.381e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.292e-07, Loss_0: 8.693e-09, Loss_r: 1.205e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 2.874e-07, Loss_0: 1.665e-07, Loss_r: 1.209e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.659e-07, Loss_0: 4.701e-08, Loss_r: 1.189e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 1.314e-07, Loss_0: 1.332e-08, Loss_r: 1.181e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.212e-07, Loss_0: 4.099e-09, Loss_r: 1.171e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.202e-07, Loss_0: 4.173e-09, Loss_r: 1.160e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.156e-07, Loss_0: 3.761e-10, Loss_r: 1.152e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.144e-07, Loss_0: 2.626e-12, Loss_r: 1.144e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.136e-07, Loss_0: 3.801e-11, Loss_r: 1.135e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.127e-07, Loss_0: 1.019e-12, Loss_r: 1.127e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.119e-07, Loss_0: 2.030e-12, Loss_r: 1.119e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.111e-07, Loss_0: 1.676e-11, Loss_r: 1.111e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.103e-07, Loss_0: 3.325e-12, Loss_r: 1.103e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.095e-07, Loss_0: 1.131e-12, Loss_r: 1.095e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.087e-07, Loss_0: 1.936e-12, Loss_r: 1.087e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.080e-07, Loss_0: 5.113e-12, Loss_r: 1.080e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.072e-07, Loss_0: 3.293e-11, Loss_r: 1.072e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.090e-07, Loss_0: 2.546e-09, Loss_r: 1.064e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 6.237e-07, Loss_0: 5.127e-07, Loss_r: 1.110e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 1.121e-07, Loss_0: 7.068e-09, Loss_r: 1.051e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 1.608e-07, Loss_0: 5.541e-08, Loss_r: 1.054e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 1.258e-07, Loss_0: 2.159e-08, Loss_r: 1.043e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.065e-07, Loss_0: 3.330e-09, Loss_r: 1.031e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.051e-07, Loss_0: 2.580e-09, Loss_r: 1.025e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.033e-07, Loss_0: 1.298e-09, Loss_r: 1.020e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.013e-07, Loss_0: 2.804e-11, Loss_r: 1.013e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.007e-07, Loss_0: 5.167e-11, Loss_r: 1.007e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.001e-07, Loss_0: 4.417e-11, Loss_r: 1.001e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 9.950e-08, Loss_0: 2.582e-11, Loss_r: 9.947e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 9.889e-08, Loss_0: 7.828e-13, Loss_r: 9.889e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 9.829e-08, Loss_0: 3.197e-14, Loss_r: 9.829e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 9.771e-08, Loss_0: 2.083e-13, Loss_r: 9.771e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 9.713e-08, Loss_0: 1.982e-13, Loss_r: 9.713e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 9.657e-08, Loss_0: 8.467e-13, Loss_r: 9.657e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 9.600e-08, Loss_0: 1.539e-12, Loss_r: 9.600e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 9.552e-08, Loss_0: 7.227e-11, Loss_r: 9.545e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.005e-07, Loss_0: 5.441e-09, Loss_r: 9.503e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 9.604e-07, Loss_0: 8.541e-07, Loss_r: 1.063e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.253e-07, Loss_0: 3.083e-08, Loss_r: 9.447e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.535e-07, Loss_0: 5.950e-08, Loss_r: 9.397e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.417e-07, Loss_0: 4.834e-08, Loss_r: 9.337e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 9.275e-08, Loss_0: 2.153e-10, Loss_r: 9.254e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 9.903e-08, Loss_0: 6.798e-09, Loss_r: 9.223e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 9.199e-08, Loss_0: 4.208e-10, Loss_r: 9.157e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 9.151e-08, Loss_0: 3.988e-10, Loss_r: 9.111e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 9.101e-08, Loss_0: 3.259e-10, Loss_r: 9.068e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 9.031e-08, Loss_0: 1.142e-10, Loss_r: 9.020e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 8.978e-08, Loss_0: 2.015e-11, Loss_r: 8.976e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 8.933e-08, Loss_0: 2.402e-11, Loss_r: 8.931e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 8.887e-08, Loss_0: 8.465e-12, Loss_r: 8.887e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 8.842e-08, Loss_0: 1.295e-12, Loss_r: 8.842e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 8.799e-08, Loss_0: 9.377e-12, Loss_r: 8.798e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 8.754e-08, Loss_0: 3.332e-12, Loss_r: 8.754e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 8.714e-08, Loss_0: 2.779e-11, Loss_r: 8.711e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 8.702e-08, Loss_0: 3.506e-10, Loss_r: 8.667e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 9.895e-08, Loss_0: 1.262e-08, Loss_r: 8.633e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 8.811e-08, Loss_0: 2.227e-09, Loss_r: 8.588e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 8.552e-08, Loss_0: 2.265e-12, Loss_r: 8.552e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 8.550e-08, Loss_0: 3.396e-10, Loss_r: 8.516e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 8.499e-08, Loss_0: 1.980e-10, Loss_r: 8.479e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 8.442e-08, Loss_0: 2.117e-13, Loss_r: 8.442e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 8.409e-08, Loss_0: 4.211e-11, Loss_r: 8.405e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 8.369e-08, Loss_0: 7.026e-15, Loss_r: 8.369e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 8.332e-08, Loss_0: 1.720e-12, Loss_r: 8.332e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 8.298e-08, Loss_0: 4.298e-12, Loss_r: 8.297e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 8.263e-08, Loss_0: 5.596e-14, Loss_r: 8.263e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 8.227e-08, Loss_0: 1.373e-12, Loss_r: 8.227e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 8.193e-08, Loss_0: 5.441e-13, Loss_r: 8.193e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 8.158e-08, Loss_0: 1.466e-12, Loss_r: 8.158e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 8.123e-08, Loss_0: 1.544e-12, Loss_r: 8.123e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 8.089e-08, Loss_0: 6.559e-12, Loss_r: 8.088e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 8.056e-08, Loss_0: 2.754e-12, Loss_r: 8.056e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 8.021e-08, Loss_0: 1.015e-12, Loss_r: 8.021e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 7.989e-08, Loss_0: 1.656e-11, Loss_r: 7.987e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 8.005e-08, Loss_0: 5.175e-10, Loss_r: 7.954e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.029e-07, Loss_0: 2.349e-08, Loss_r: 7.941e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 8.251e-08, Loss_0: 3.579e-09, Loss_r: 7.893e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 7.865e-08, Loss_0: 1.831e-11, Loss_r: 7.863e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 7.927e-08, Loss_0: 8.883e-10, Loss_r: 7.838e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 7.841e-08, Loss_0: 3.543e-10, Loss_r: 7.806e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 7.777e-08, Loss_0: 1.064e-11, Loss_r: 7.776e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 7.752e-08, Loss_0: 5.646e-11, Loss_r: 7.747e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 7.718e-08, Loss_0: 9.930e-13, Loss_r: 7.718e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 7.690e-08, Loss_0: 2.083e-13, Loss_r: 7.690e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 7.662e-08, Loss_0: 1.351e-12, Loss_r: 7.662e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 7.634e-08, Loss_0: 5.684e-14, Loss_r: 7.634e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 7.607e-08, Loss_0: 1.590e-12, Loss_r: 7.607e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 7.578e-08, Loss_0: 1.388e-13, Loss_r: 7.578e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 7.551e-08, Loss_0: 1.700e-12, Loss_r: 7.550e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 7.523e-08, Loss_0: 1.502e-12, Loss_r: 7.523e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 7.496e-08, Loss_0: 1.637e-12, Loss_r: 7.495e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 7.468e-08, Loss_0: 2.596e-12, Loss_r: 7.468e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 7.442e-08, Loss_0: 1.424e-11, Loss_r: 7.441e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 7.415e-08, Loss_0: 1.669e-11, Loss_r: 7.414e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 7.412e-08, Loss_0: 2.661e-10, Loss_r: 7.385e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 8.707e-08, Loss_0: 1.336e-08, Loss_r: 7.371e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 7.531e-08, Loss_0: 1.946e-09, Loss_r: 7.337e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 7.315e-08, Loss_0: 1.952e-11, Loss_r: 7.313e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 7.345e-08, Loss_0: 5.399e-10, Loss_r: 7.291e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 7.287e-08, Loss_0: 1.996e-10, Loss_r: 7.267e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 7.243e-08, Loss_0: 1.488e-11, Loss_r: 7.242e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 7.221e-08, Loss_0: 3.230e-11, Loss_r: 7.217e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 7.195e-08, Loss_0: 1.312e-12, Loss_r: 7.195e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 7.172e-08, Loss_0: 2.566e-12, Loss_r: 7.172e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 7.149e-08, Loss_0: 3.138e-12, Loss_r: 7.149e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 7.125e-08, Loss_0: 4.668e-14, Loss_r: 7.125e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 7.102e-08, Loss_0: 5.832e-15, Loss_r: 7.102e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 7.080e-08, Loss_0: 2.858e-13, Loss_r: 7.080e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 7.057e-08, Loss_0: 2.015e-13, Loss_r: 7.057e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 7.033e-08, Loss_0: 2.507e-14, Loss_r: 7.033e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 7.012e-08, Loss_0: 7.341e-15, Loss_r: 7.012e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 6.989e-08, Loss_0: 1.023e-12, Loss_r: 6.989e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 6.966e-08, Loss_0: 2.602e-12, Loss_r: 6.966e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 6.950e-08, Loss_0: 5.266e-11, Loss_r: 6.944e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 7.055e-08, Loss_0: 1.298e-09, Loss_r: 6.925e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.376e-07, Loss_0: 6.746e-08, Loss_r: 7.017e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 7.921e-08, Loss_0: 1.020e-08, Loss_r: 6.901e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 6.864e-08, Loss_0: 5.167e-11, Loss_r: 6.859e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 7.094e-08, Loss_0: 2.551e-09, Loss_r: 6.839e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 6.928e-08, Loss_0: 1.090e-09, Loss_r: 6.819e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 6.802e-08, Loss_0: 1.132e-11, Loss_r: 6.801e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 6.796e-08, Loss_0: 1.460e-10, Loss_r: 6.781e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 6.761e-08, Loss_0: 3.435e-12, Loss_r: 6.761e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 6.742e-08, Loss_0: 1.704e-11, Loss_r: 6.740e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 6.722e-08, Loss_0: 5.999e-12, Loss_r: 6.721e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 6.701e-08, Loss_0: 2.643e-13, Loss_r: 6.701e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 6.681e-08, Loss_0: 1.756e-13, Loss_r: 6.681e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 6.662e-08, Loss_0: 8.195e-13, Loss_r: 6.662e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 6.642e-08, Loss_0: 4.034e-13, Loss_r: 6.642e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 6.623e-08, Loss_0: 2.117e-13, Loss_r: 6.623e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 6.604e-08, Loss_0: 8.535e-13, Loss_r: 6.604e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 6.584e-08, Loss_0: 1.849e-14, Loss_r: 6.584e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 6.565e-08, Loss_0: 4.801e-13, Loss_r: 6.565e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 6.546e-08, Loss_0: 5.038e-12, Loss_r: 6.546e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 6.529e-08, Loss_0: 3.313e-11, Loss_r: 6.526e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 6.640e-08, Loss_0: 1.338e-09, Loss_r: 6.507e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 1.928e-07, Loss_0: 1.262e-07, Loss_r: 6.656e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 7.556e-08, Loss_0: 1.075e-08, Loss_r: 6.481e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 6.762e-08, Loss_0: 3.005e-09, Loss_r: 6.461e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 7.169e-08, Loss_0: 7.154e-09, Loss_r: 6.454e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 6.480e-08, Loss_0: 5.698e-10, Loss_r: 6.423e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 6.466e-08, Loss_0: 6.376e-10, Loss_r: 6.402e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 6.396e-08, Loss_0: 9.942e-11, Loss_r: 6.386e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 6.381e-08, Loss_0: 1.087e-10, Loss_r: 6.370e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 6.352e-08, Loss_0: 8.389e-12, Loss_r: 6.351e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 6.334e-08, Loss_0: 2.073e-12, Loss_r: 6.334e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 6.318e-08, Loss_0: 1.700e-12, Loss_r: 6.318e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 6.301e-08, Loss_0: 4.604e-12, Loss_r: 6.300e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 6.284e-08, Loss_0: 8.708e-13, Loss_r: 6.283e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 6.266e-08, Loss_0: 4.957e-13, Loss_r: 6.266e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 6.250e-08, Loss_0: 2.333e-12, Loss_r: 6.250e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 6.233e-08, Loss_0: 2.203e-13, Loss_r: 6.233e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 6.215e-08, Loss_0: 5.693e-12, Loss_r: 6.214e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 6.200e-08, Loss_0: 3.546e-12, Loss_r: 6.200e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 6.185e-08, Loss_0: 2.473e-11, Loss_r: 6.182e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 6.239e-08, Loss_0: 7.215e-10, Loss_r: 6.167e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.152e-07, Loss_0: 5.273e-08, Loss_r: 6.244e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 6.730e-08, Loss_0: 5.848e-09, Loss_r: 6.146e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 6.169e-08, Loss_0: 5.293e-10, Loss_r: 6.116e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 6.375e-08, Loss_0: 2.714e-09, Loss_r: 6.103e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 6.139e-08, Loss_0: 5.254e-10, Loss_r: 6.086e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 6.086e-08, Loss_0: 1.371e-10, Loss_r: 6.072e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 6.065e-08, Loss_0: 7.521e-11, Loss_r: 6.057e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 6.044e-08, Loss_0: 4.202e-11, Loss_r: 6.040e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 6.026e-08, Loss_0: 2.274e-13, Loss_r: 6.026e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 6.011e-08, Loss_0: 2.111e-12, Loss_r: 6.010e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 5.996e-08, Loss_0: 3.164e-12, Loss_r: 5.996e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 5.981e-08, Loss_0: 1.087e-13, Loss_r: 5.981e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 5.966e-08, Loss_0: 1.628e-12, Loss_r: 5.965e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 5.951e-08, Loss_0: 1.003e-13, Loss_r: 5.951e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 5.935e-08, Loss_0: 3.270e-13, Loss_r: 5.935e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 5.919e-08, Loss_0: 1.999e-12, Loss_r: 5.919e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.904e-08, Loss_0: 3.884e-12, Loss_r: 5.904e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 5.890e-08, Loss_0: 1.584e-11, Loss_r: 5.888e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.886e-08, Loss_0: 1.193e-10, Loss_r: 5.874e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 6.078e-08, Loss_0: 2.188e-09, Loss_r: 5.859e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 5.898e-08, Loss_0: 5.391e-10, Loss_r: 5.844e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 5.834e-08, Loss_0: 3.694e-11, Loss_r: 5.830e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 5.819e-08, Loss_0: 1.161e-11, Loss_r: 5.818e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 5.807e-08, Loss_0: 3.347e-11, Loss_r: 5.803e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 5.790e-08, Loss_0: 5.378e-12, Loss_r: 5.790e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 5.776e-08, Loss_0: 1.666e-12, Loss_r: 5.776e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 5.762e-08, Loss_0: 1.544e-12, Loss_r: 5.762e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 5.749e-08, Loss_0: 8.016e-14, Loss_r: 5.749e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 5.735e-08, Loss_0: 6.415e-15, Loss_r: 5.735e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 5.721e-08, Loss_0: 1.019e-12, Loss_r: 5.721e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 5.707e-08, Loss_0: 5.954e-14, Loss_r: 5.707e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 5.693e-08, Loss_0: 2.858e-13, Loss_r: 5.693e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 5.680e-08, Loss_0: 9.599e-13, Loss_r: 5.680e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 5.666e-08, Loss_0: 3.197e-14, Loss_r: 5.666e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 5.652e-08, Loss_0: 3.920e-12, Loss_r: 5.652e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 5.638e-08, Loss_0: 7.026e-15, Loss_r: 5.638e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 5.624e-08, Loss_0: 2.495e-12, Loss_r: 5.624e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 5.611e-08, Loss_0: 2.969e-12, Loss_r: 5.611e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 5.596e-08, Loss_0: 1.581e-12, Loss_r: 5.596e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 5.583e-08, Loss_0: 1.905e-12, Loss_r: 5.583e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 5.573e-08, Loss_0: 5.023e-11, Loss_r: 5.568e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 5.957e-08, Loss_0: 3.998e-09, Loss_r: 5.557e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 5.582e-08, Loss_0: 3.964e-10, Loss_r: 5.542e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 5.536e-08, Loss_0: 6.249e-11, Loss_r: 5.530e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 5.540e-08, Loss_0: 2.238e-10, Loss_r: 5.518e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 5.507e-08, Loss_0: 2.652e-11, Loss_r: 5.505e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 5.494e-08, Loss_0: 2.514e-11, Loss_r: 5.492e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 5.480e-08, Loss_0: 8.771e-12, Loss_r: 5.479e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 5.466e-08, Loss_0: 1.562e-12, Loss_r: 5.466e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 5.455e-08, Loss_0: 1.649e-13, Loss_r: 5.455e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 5.441e-08, Loss_0: 1.038e-12, Loss_r: 5.441e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 5.428e-08, Loss_0: 2.858e-13, Loss_r: 5.428e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 5.416e-08, Loss_0: 6.595e-13, Loss_r: 5.416e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 5.403e-08, Loss_0: 1.849e-14, Loss_r: 5.403e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 5.390e-08, Loss_0: 1.224e-12, Loss_r: 5.390e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 5.378e-08, Loss_0: 3.356e-13, Loss_r: 5.378e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 5.365e-08, Loss_0: 2.084e-12, Loss_r: 5.365e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 5.353e-08, Loss_0: 2.956e-12, Loss_r: 5.353e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 5.340e-08, Loss_0: 9.676e-14, Loss_r: 5.340e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 5.327e-08, Loss_0: 2.918e-13, Loss_r: 5.327e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 5.318e-08, Loss_0: 3.298e-11, Loss_r: 5.314e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 5.474e-08, Loss_0: 1.729e-09, Loss_r: 5.301e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 5.316e-08, Loss_0: 2.544e-10, Loss_r: 5.290e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 5.278e-08, Loss_0: 1.999e-12, Loss_r: 5.278e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 5.273e-08, Loss_0: 6.167e-11, Loss_r: 5.267e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 5.257e-08, Loss_0: 1.487e-11, Loss_r: 5.255e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 5.244e-08, Loss_0: 5.304e-13, Loss_r: 5.244e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 5.232e-08, Loss_0: 3.001e-12, Loss_r: 5.232e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 5.220e-08, Loss_0: 1.557e-14, Loss_r: 5.220e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 5.208e-08, Loss_0: 2.529e-13, Loss_r: 5.208e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 5.196e-08, Loss_0: 2.327e-13, Loss_r: 5.196e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 5.186e-08, Loss_0: 5.975e-13, Loss_r: 5.186e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 5.173e-08, Loss_0: 5.164e-14, Loss_r: 5.173e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 5.161e-08, Loss_0: 6.800e-16, Loss_r: 5.161e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 21.9065\n",
            "[1, 256, 128, 64, 32, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.326e-02, Loss_0: 6.071e-04, Loss_r: 3.265e-02, Time: 1.04, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.232e-02, Loss_0: 1.130e-04, Loss_r: 3.220e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.055e-02, Loss_0: 4.058e-05, Loss_r: 3.051e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.936e-02, Loss_0: 3.057e-06, Loss_r: 1.936e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.595e-03, Loss_0: 2.358e-04, Loss_r: 2.360e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 50, Loss: 8.472e-04, Loss_0: 2.128e-04, Loss_r: 6.345e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 4.896e-04, Loss_0: 7.332e-05, Loss_r: 4.163e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 70, Loss: 7.857e-05, Loss_0: 9.650e-10, Loss_r: 7.857e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 80, Loss: 6.578e-05, Loss_0: 1.418e-06, Loss_r: 6.436e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 90, Loss: 3.217e-05, Loss_0: 6.251e-07, Loss_r: 3.154e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 100, Loss: 9.394e-06, Loss_0: 9.879e-07, Loss_r: 8.406e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.629e-06, Loss_0: 3.562e-07, Loss_r: 5.273e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 120, Loss: 3.522e-06, Loss_0: 2.683e-07, Loss_r: 3.254e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 2.945e-06, Loss_0: 6.480e-07, Loss_r: 2.297e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.509e-05, Loss_0: 5.238e-05, Loss_r: 2.701e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.031e-04, Loss_0: 9.886e-05, Loss_r: 4.223e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 2.277e-06, Loss_0: 1.734e-09, Loss_r: 2.276e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.616e-05, Loss_0: 1.369e-05, Loss_r: 2.475e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 180, Loss: 9.443e-06, Loss_0: 7.217e-06, Loss_r: 2.226e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 190, Loss: 4.802e-06, Loss_0: 2.669e-06, Loss_r: 2.134e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 200, Loss: 2.017e-06, Loss_0: 9.946e-10, Loss_r: 2.016e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 2.173e-06, Loss_0: 2.297e-07, Loss_r: 1.943e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 2.091e-06, Loss_0: 2.041e-07, Loss_r: 1.887e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 230, Loss: 3.137e-06, Loss_0: 1.306e-06, Loss_r: 1.830e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 240, Loss: 4.672e-04, Loss_0: 4.584e-04, Loss_r: 8.853e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.507e-04, Loss_0: 1.480e-04, Loss_r: 2.650e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.651e-05, Loss_0: 1.425e-05, Loss_r: 2.261e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 8.790e-06, Loss_0: 6.562e-06, Loss_r: 2.227e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 280, Loss: 7.399e-06, Loss_0: 5.370e-06, Loss_r: 2.029e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.347e-06, Loss_0: 1.386e-06, Loss_r: 1.961e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.921e-06, Loss_0: 2.300e-09, Loss_r: 1.918e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.870e-06, Loss_0: 6.779e-09, Loss_r: 1.863e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.820e-06, Loss_0: 1.506e-08, Loss_r: 1.805e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.815e-06, Loss_0: 6.506e-08, Loss_r: 1.750e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.705e-06, Loss_0: 8.190e-10, Loss_r: 1.704e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.656e-06, Loss_0: 3.300e-11, Loss_r: 1.656e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.618e-06, Loss_0: 5.052e-09, Loss_r: 1.613e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.573e-06, Loss_0: 3.717e-09, Loss_r: 1.570e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 2.012e-06, Loss_0: 4.709e-07, Loss_r: 1.542e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.584e-04, Loss_0: 3.548e-04, Loss_r: 3.570e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 8.892e-05, Loss_0: 8.653e-05, Loss_r: 2.389e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.834e-05, Loss_0: 1.494e-05, Loss_r: 3.403e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.355e-05, Loss_0: 2.090e-05, Loss_r: 2.652e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 1.220e-05, Loss_0: 9.831e-06, Loss_r: 2.367e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.240e-06, Loss_0: 1.316e-06, Loss_r: 1.925e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.167e-06, Loss_0: 3.499e-07, Loss_r: 1.817e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.171e-06, Loss_0: 3.862e-07, Loss_r: 1.784e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.798e-06, Loss_0: 7.716e-08, Loss_r: 1.721e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.663e-06, Loss_0: 2.530e-09, Loss_r: 1.660e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.620e-06, Loss_0: 1.126e-08, Loss_r: 1.609e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.568e-06, Loss_0: 5.541e-09, Loss_r: 1.562e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.520e-06, Loss_0: 1.007e-09, Loss_r: 1.519e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.478e-06, Loss_0: 5.063e-11, Loss_r: 1.478e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.438e-06, Loss_0: 2.089e-10, Loss_r: 1.438e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.400e-06, Loss_0: 2.257e-10, Loss_r: 1.400e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.364e-06, Loss_0: 4.296e-11, Loss_r: 1.364e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.330e-06, Loss_0: 6.445e-11, Loss_r: 1.330e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.297e-06, Loss_0: 1.521e-11, Loss_r: 1.297e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.266e-06, Loss_0: 1.431e-11, Loss_r: 1.266e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.236e-06, Loss_0: 1.650e-10, Loss_r: 1.236e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.306e-06, Loss_0: 9.533e-08, Loss_r: 1.211e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.014e-04, Loss_0: 9.959e-05, Loss_r: 1.826e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 3.634e-04, Loss_0: 3.486e-04, Loss_r: 1.480e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.155e-04, Loss_0: 1.127e-04, Loss_r: 2.805e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 4.140e-05, Loss_0: 3.888e-05, Loss_r: 2.524e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.773e-05, Loss_0: 1.473e-05, Loss_r: 3.000e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 7.460e-06, Loss_0: 5.657e-06, Loss_r: 1.804e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.668e-06, Loss_0: 1.861e-06, Loss_r: 1.807e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.212e-06, Loss_0: 4.861e-07, Loss_r: 1.726e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.691e-06, Loss_0: 3.620e-08, Loss_r: 1.655e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.602e-06, Loss_0: 1.372e-08, Loss_r: 1.589e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.565e-06, Loss_0: 3.750e-08, Loss_r: 1.527e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.481e-06, Loss_0: 9.705e-10, Loss_r: 1.480e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.437e-06, Loss_0: 3.074e-09, Loss_r: 1.434e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.389e-06, Loss_0: 1.107e-10, Loss_r: 1.389e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.349e-06, Loss_0: 8.288e-10, Loss_r: 1.348e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.311e-06, Loss_0: 4.716e-10, Loss_r: 1.310e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.275e-06, Loss_0: 2.046e-10, Loss_r: 1.275e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.241e-06, Loss_0: 1.228e-10, Loss_r: 1.241e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.209e-06, Loss_0: 8.259e-11, Loss_r: 1.209e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.178e-06, Loss_0: 5.140e-11, Loss_r: 1.178e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.149e-06, Loss_0: 3.933e-11, Loss_r: 1.149e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.121e-06, Loss_0: 2.995e-11, Loss_r: 1.121e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.095e-06, Loss_0: 3.649e-11, Loss_r: 1.095e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.069e-06, Loss_0: 4.646e-11, Loss_r: 1.069e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.045e-06, Loss_0: 2.137e-11, Loss_r: 1.045e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.022e-06, Loss_0: 2.992e-11, Loss_r: 1.022e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 9.994e-07, Loss_0: 1.777e-11, Loss_r: 9.993e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 9.778e-07, Loss_0: 2.484e-11, Loss_r: 9.778e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 9.571e-07, Loss_0: 5.798e-11, Loss_r: 9.570e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 9.374e-07, Loss_0: 4.953e-10, Loss_r: 9.369e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 9.787e-07, Loss_0: 6.275e-08, Loss_r: 9.160e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.859e-05, Loss_0: 3.752e-05, Loss_r: 1.074e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 2.036e-04, Loss_0: 1.980e-04, Loss_r: 5.609e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.195e-04, Loss_0: 1.112e-04, Loss_r: 8.323e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 5.599e-05, Loss_0: 5.320e-05, Loss_r: 2.790e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.128e-05, Loss_0: 1.818e-05, Loss_r: 3.105e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 8.604e-06, Loss_0: 6.884e-06, Loss_r: 1.720e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 4.045e-06, Loss_0: 2.286e-06, Loss_r: 1.759e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.205e-06, Loss_0: 7.054e-07, Loss_r: 1.500e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.519e-06, Loss_0: 7.066e-08, Loss_r: 1.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.379e-06, Loss_0: 4.179e-09, Loss_r: 1.375e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.352e-06, Loss_0: 3.797e-08, Loss_r: 1.314e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.276e-06, Loss_0: 5.191e-09, Loss_r: 1.270e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.227e-06, Loss_0: 1.285e-09, Loss_r: 1.226e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.186e-06, Loss_0: 1.526e-09, Loss_r: 1.185e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.149e-06, Loss_0: 7.879e-10, Loss_r: 1.148e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.114e-06, Loss_0: 3.343e-11, Loss_r: 1.114e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.082e-06, Loss_0: 7.960e-13, Loss_r: 1.082e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.052e-06, Loss_0: 9.618e-14, Loss_r: 1.052e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.024e-06, Loss_0: 4.066e-12, Loss_r: 1.024e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 9.970e-07, Loss_0: 1.315e-11, Loss_r: 9.970e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 9.716e-07, Loss_0: 1.691e-11, Loss_r: 9.716e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 9.474e-07, Loss_0: 2.809e-11, Loss_r: 9.473e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 9.242e-07, Loss_0: 3.961e-11, Loss_r: 9.242e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 9.022e-07, Loss_0: 2.832e-11, Loss_r: 9.022e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 8.811e-07, Loss_0: 2.056e-11, Loss_r: 8.811e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 8.609e-07, Loss_0: 2.363e-11, Loss_r: 8.609e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 8.415e-07, Loss_0: 1.202e-11, Loss_r: 8.414e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 8.228e-07, Loss_0: 1.922e-11, Loss_r: 8.228e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 8.049e-07, Loss_0: 1.724e-11, Loss_r: 8.049e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 7.876e-07, Loss_0: 1.713e-11, Loss_r: 7.876e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 7.710e-07, Loss_0: 2.023e-11, Loss_r: 7.710e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 7.549e-07, Loss_0: 1.012e-11, Loss_r: 7.549e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 7.394e-07, Loss_0: 1.414e-11, Loss_r: 7.394e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 7.270e-07, Loss_0: 2.282e-09, Loss_r: 7.247e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.112e-06, Loss_0: 3.964e-07, Loss_r: 7.158e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.460e-04, Loss_0: 1.447e-04, Loss_r: 1.259e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 3.089e-04, Loss_0: 3.015e-04, Loss_r: 7.410e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.530e-05, Loss_0: 8.949e-06, Loss_r: 6.355e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 3.687e-05, Loss_0: 3.523e-05, Loss_r: 1.639e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 7.803e-06, Loss_0: 6.493e-06, Loss_r: 1.311e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.483e-06, Loss_0: 1.014e-07, Loss_r: 1.382e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.646e-06, Loss_0: 1.336e-06, Loss_r: 1.311e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.410e-06, Loss_0: 2.501e-07, Loss_r: 1.160e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.201e-06, Loss_0: 8.499e-08, Loss_r: 1.116e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.108e-06, Loss_0: 3.732e-08, Loss_r: 1.071e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.043e-06, Loss_0: 1.617e-08, Loss_r: 1.026e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 9.872e-07, Loss_0: 3.581e-10, Loss_r: 9.868e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 9.550e-07, Loss_0: 3.043e-09, Loss_r: 9.520e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 9.224e-07, Loss_0: 1.641e-09, Loss_r: 9.208e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.925e-07, Loss_0: 5.496e-10, Loss_r: 8.919e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 8.652e-07, Loss_0: 2.013e-10, Loss_r: 8.650e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.398e-07, Loss_0: 1.137e-10, Loss_r: 8.397e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 8.160e-07, Loss_0: 8.354e-11, Loss_r: 8.159e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 7.934e-07, Loss_0: 5.811e-11, Loss_r: 7.933e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 7.720e-07, Loss_0: 2.999e-11, Loss_r: 7.720e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 7.517e-07, Loss_0: 1.628e-11, Loss_r: 7.517e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 7.324e-07, Loss_0: 1.235e-11, Loss_r: 7.324e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 7.139e-07, Loss_0: 2.083e-11, Loss_r: 7.139e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 6.963e-07, Loss_0: 2.102e-11, Loss_r: 6.963e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 6.795e-07, Loss_0: 6.300e-12, Loss_r: 6.795e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.633e-07, Loss_0: 2.151e-11, Loss_r: 6.633e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 6.478e-07, Loss_0: 8.508e-12, Loss_r: 6.478e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 6.328e-07, Loss_0: 5.495e-12, Loss_r: 6.328e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 6.184e-07, Loss_0: 7.199e-12, Loss_r: 6.184e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 6.045e-07, Loss_0: 1.197e-11, Loss_r: 6.045e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 5.912e-07, Loss_0: 3.111e-11, Loss_r: 5.912e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 5.784e-07, Loss_0: 3.022e-10, Loss_r: 5.781e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 5.910e-07, Loss_0: 2.616e-08, Loss_r: 5.648e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.724e-06, Loss_0: 8.158e-06, Loss_r: 5.662e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 5.038e-04, Loss_0: 4.963e-04, Loss_r: 7.477e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.711e-05, Loss_0: 4.220e-05, Loss_r: 4.907e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 6.625e-05, Loss_0: 6.210e-05, Loss_r: 4.150e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.466e-05, Loss_0: 1.327e-05, Loss_r: 1.387e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.632e-06, Loss_0: 1.225e-06, Loss_r: 1.407e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.284e-06, Loss_0: 4.850e-08, Loss_r: 1.236e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.664e-06, Loss_0: 5.337e-07, Loss_r: 1.130e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.382e-06, Loss_0: 3.063e-07, Loss_r: 1.075e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.024e-06, Loss_0: 1.760e-08, Loss_r: 1.006e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 9.766e-07, Loss_0: 2.514e-08, Loss_r: 9.515e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 9.189e-07, Loss_0: 7.606e-09, Loss_r: 9.113e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 8.730e-07, Loss_0: 1.795e-09, Loss_r: 8.712e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 8.361e-07, Loss_0: 9.644e-10, Loss_r: 8.351e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 8.041e-07, Loss_0: 1.014e-09, Loss_r: 8.031e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 7.739e-07, Loss_0: 1.512e-10, Loss_r: 7.737e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 7.465e-07, Loss_0: 7.885e-12, Loss_r: 7.465e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 7.211e-07, Loss_0: 5.733e-13, Loss_r: 7.211e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 6.974e-07, Loss_0: 3.760e-12, Loss_r: 6.973e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 6.751e-07, Loss_0: 6.752e-12, Loss_r: 6.751e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 6.542e-07, Loss_0: 8.546e-12, Loss_r: 6.542e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.345e-07, Loss_0: 9.555e-12, Loss_r: 6.345e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 6.159e-07, Loss_0: 1.009e-11, Loss_r: 6.159e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 5.983e-07, Loss_0: 1.647e-11, Loss_r: 5.983e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 5.816e-07, Loss_0: 1.318e-11, Loss_r: 5.816e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.657e-07, Loss_0: 1.160e-11, Loss_r: 5.657e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 5.506e-07, Loss_0: 1.323e-11, Loss_r: 5.506e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 5.361e-07, Loss_0: 1.215e-11, Loss_r: 5.361e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 5.223e-07, Loss_0: 9.618e-12, Loss_r: 5.223e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 5.091e-07, Loss_0: 7.350e-12, Loss_r: 5.091e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.964e-07, Loss_0: 1.427e-11, Loss_r: 4.964e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.843e-07, Loss_0: 1.073e-11, Loss_r: 4.843e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 4.725e-07, Loss_0: 4.187e-12, Loss_r: 4.725e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 4.612e-07, Loss_0: 7.848e-12, Loss_r: 4.612e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 4.503e-07, Loss_0: 3.008e-11, Loss_r: 4.502e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 4.399e-07, Loss_0: 2.479e-10, Loss_r: 4.396e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 4.403e-07, Loss_0: 1.126e-08, Loss_r: 4.291e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 2.122e-06, Loss_0: 1.704e-06, Loss_r: 4.183e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.026e-04, Loss_0: 4.004e-04, Loss_r: 2.224e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.697e-06, Loss_0: 3.572e-07, Loss_r: 1.339e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 5.401e-05, Loss_0: 5.276e-05, Loss_r: 1.245e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.371e-06, Loss_0: 7.032e-08, Loss_r: 1.301e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 8.330e-06, Loss_0: 6.952e-06, Loss_r: 1.379e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.159e-06, Loss_0: 1.096e-06, Loss_r: 1.062e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.260e-06, Loss_0: 3.560e-07, Loss_r: 9.042e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.090e-06, Loss_0: 2.426e-07, Loss_r: 8.471e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 8.323e-07, Loss_0: 4.097e-08, Loss_r: 7.913e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 7.592e-07, Loss_0: 1.617e-08, Loss_r: 7.430e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 7.224e-07, Loss_0: 1.663e-08, Loss_r: 7.057e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 6.759e-07, Loss_0: 1.909e-09, Loss_r: 6.740e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 6.451e-07, Loss_0: 4.957e-13, Loss_r: 6.451e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 6.186e-07, Loss_0: 1.207e-10, Loss_r: 6.185e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 5.941e-07, Loss_0: 5.893e-11, Loss_r: 5.940e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 5.714e-07, Loss_0: 1.288e-11, Loss_r: 5.714e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 5.505e-07, Loss_0: 9.105e-14, Loss_r: 5.505e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 5.312e-07, Loss_0: 1.026e-11, Loss_r: 5.312e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 5.131e-07, Loss_0: 2.737e-11, Loss_r: 5.131e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 4.963e-07, Loss_0: 1.987e-11, Loss_r: 4.963e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 4.806e-07, Loss_0: 1.704e-11, Loss_r: 4.805e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 4.657e-07, Loss_0: 5.693e-12, Loss_r: 4.657e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 4.517e-07, Loss_0: 1.047e-11, Loss_r: 4.517e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 4.385e-07, Loss_0: 6.248e-12, Loss_r: 4.385e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 4.259e-07, Loss_0: 4.725e-12, Loss_r: 4.259e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 4.141e-07, Loss_0: 1.072e-11, Loss_r: 4.141e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 4.027e-07, Loss_0: 5.000e-12, Loss_r: 4.027e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 3.919e-07, Loss_0: 9.122e-12, Loss_r: 3.919e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 3.816e-07, Loss_0: 1.241e-12, Loss_r: 3.816e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 3.718e-07, Loss_0: 1.430e-12, Loss_r: 3.718e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 3.623e-07, Loss_0: 1.258e-11, Loss_r: 3.623e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 3.536e-07, Loss_0: 2.736e-10, Loss_r: 3.534e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 3.570e-07, Loss_0: 1.191e-08, Loss_r: 3.450e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 1.661e-06, Loss_0: 1.316e-06, Loss_r: 3.448e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 4.004e-07, Loss_0: 6.982e-08, Loss_r: 3.306e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 3.972e-07, Loss_0: 7.561e-08, Loss_r: 3.216e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 3.920e-07, Loss_0: 7.709e-08, Loss_r: 3.150e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 3.099e-07, Loss_0: 4.980e-10, Loss_r: 3.094e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 3.133e-07, Loss_0: 9.943e-09, Loss_r: 3.033e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 2.986e-07, Loss_0: 2.094e-09, Loss_r: 2.965e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 2.906e-07, Loss_0: 2.993e-11, Loss_r: 2.906e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 2.849e-07, Loss_0: 1.277e-10, Loss_r: 2.848e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 2.791e-07, Loss_0: 1.315e-10, Loss_r: 2.790e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 2.735e-07, Loss_0: 3.991e-12, Loss_r: 2.735e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 2.681e-07, Loss_0: 1.488e-14, Loss_r: 2.681e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 2.629e-07, Loss_0: 9.694e-11, Loss_r: 2.629e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 2.578e-07, Loss_0: 6.152e-11, Loss_r: 2.577e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 2.529e-07, Loss_0: 1.010e-10, Loss_r: 2.528e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 2.492e-07, Loss_0: 1.285e-09, Loss_r: 2.479e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 3.091e-07, Loss_0: 6.628e-08, Loss_r: 2.428e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 2.474e-07, Loss_0: 8.384e-09, Loss_r: 2.390e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.359e-07, Loss_0: 5.357e-10, Loss_r: 2.353e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 2.349e-07, Loss_0: 3.339e-09, Loss_r: 2.316e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 2.281e-07, Loss_0: 4.075e-10, Loss_r: 2.277e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 2.242e-07, Loss_0: 3.673e-10, Loss_r: 2.238e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 2.203e-07, Loss_0: 7.018e-11, Loss_r: 2.202e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 2.168e-07, Loss_0: 5.584e-11, Loss_r: 2.168e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 2.133e-07, Loss_0: 3.183e-11, Loss_r: 2.133e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 2.099e-07, Loss_0: 2.409e-13, Loss_r: 2.099e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 2.066e-07, Loss_0: 2.243e-12, Loss_r: 2.066e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 2.034e-07, Loss_0: 5.206e-14, Loss_r: 2.034e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 2.003e-07, Loss_0: 9.526e-13, Loss_r: 2.003e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 1.972e-07, Loss_0: 2.486e-12, Loss_r: 1.972e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 1.942e-07, Loss_0: 4.084e-14, Loss_r: 1.942e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 1.913e-07, Loss_0: 1.171e-11, Loss_r: 1.913e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 1.887e-07, Loss_0: 1.902e-10, Loss_r: 1.885e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 1.957e-07, Loss_0: 9.706e-09, Loss_r: 1.860e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 1.256e-06, Loss_0: 1.066e-06, Loss_r: 1.895e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.460e-07, Loss_0: 6.441e-08, Loss_r: 1.816e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.278e-07, Loss_0: 4.948e-08, Loss_r: 1.783e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.408e-07, Loss_0: 6.474e-08, Loss_r: 1.761e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.743e-07, Loss_0: 1.410e-10, Loss_r: 1.741e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.808e-07, Loss_0: 8.591e-09, Loss_r: 1.722e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.700e-07, Loss_0: 2.591e-10, Loss_r: 1.698e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.683e-07, Loss_0: 6.041e-10, Loss_r: 1.677e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.661e-07, Loss_0: 4.306e-10, Loss_r: 1.657e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.638e-07, Loss_0: 1.876e-10, Loss_r: 1.636e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.617e-07, Loss_0: 5.753e-11, Loss_r: 1.616e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.597e-07, Loss_0: 4.222e-11, Loss_r: 1.597e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 1.577e-07, Loss_0: 2.203e-13, Loss_r: 1.577e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 1.559e-07, Loss_0: 2.108e-12, Loss_r: 1.559e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 1.540e-07, Loss_0: 2.247e-13, Loss_r: 1.540e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 1.522e-07, Loss_0: 9.583e-12, Loss_r: 1.522e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 1.508e-07, Loss_0: 3.499e-10, Loss_r: 1.505e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 1.644e-07, Loss_0: 1.543e-08, Loss_r: 1.490e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 1.868e-06, Loss_0: 1.710e-06, Loss_r: 1.576e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 2.578e-07, Loss_0: 1.109e-07, Loss_r: 1.469e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 2.122e-07, Loss_0: 6.774e-08, Loss_r: 1.444e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 2.456e-07, Loss_0: 1.026e-07, Loss_r: 1.431e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.435e-07, Loss_0: 1.711e-09, Loss_r: 1.418e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.536e-07, Loss_0: 1.288e-08, Loss_r: 1.407e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.390e-07, Loss_0: 3.004e-12, Loss_r: 1.390e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.391e-07, Loss_0: 1.523e-09, Loss_r: 1.376e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.370e-07, Loss_0: 6.045e-10, Loss_r: 1.364e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.351e-07, Loss_0: 1.483e-10, Loss_r: 1.350e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 1.337e-07, Loss_0: 2.429e-11, Loss_r: 1.337e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 1.324e-07, Loss_0: 2.982e-11, Loss_r: 1.324e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 1.312e-07, Loss_0: 7.672e-12, Loss_r: 1.311e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 1.299e-07, Loss_0: 8.271e-12, Loss_r: 1.299e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 1.287e-07, Loss_0: 9.618e-12, Loss_r: 1.287e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 1.275e-07, Loss_0: 1.848e-11, Loss_r: 1.275e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 1.263e-07, Loss_0: 6.592e-11, Loss_r: 1.263e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 1.266e-07, Loss_0: 1.506e-09, Loss_r: 1.251e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 2.176e-07, Loss_0: 9.363e-08, Loss_r: 1.240e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.332e-07, Loss_0: 1.029e-08, Loss_r: 1.229e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.232e-07, Loss_0: 1.189e-09, Loss_r: 1.220e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.260e-07, Loss_0: 4.912e-09, Loss_r: 1.211e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.206e-07, Loss_0: 5.875e-10, Loss_r: 1.200e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.195e-07, Loss_0: 4.710e-10, Loss_r: 1.190e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.182e-07, Loss_0: 8.927e-11, Loss_r: 1.181e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.173e-07, Loss_0: 7.907e-11, Loss_r: 1.172e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.162e-07, Loss_0: 6.598e-12, Loss_r: 1.162e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.153e-07, Loss_0: 5.322e-12, Loss_r: 1.153e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.144e-07, Loss_0: 4.214e-13, Loss_r: 1.144e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 1.135e-07, Loss_0: 6.375e-12, Loss_r: 1.135e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 1.126e-07, Loss_0: 2.878e-13, Loss_r: 1.126e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 1.117e-07, Loss_0: 4.879e-15, Loss_r: 1.117e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 1.109e-07, Loss_0: 9.672e-13, Loss_r: 1.109e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 1.100e-07, Loss_0: 9.508e-13, Loss_r: 1.100e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 1.092e-07, Loss_0: 1.626e-12, Loss_r: 1.092e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 1.084e-07, Loss_0: 5.594e-11, Loss_r: 1.084e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 1.096e-07, Loss_0: 2.022e-09, Loss_r: 1.076e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 2.944e-07, Loss_0: 1.860e-07, Loss_r: 1.085e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.214e-07, Loss_0: 1.513e-08, Loss_r: 1.063e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.104e-07, Loss_0: 5.098e-09, Loss_r: 1.053e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.157e-07, Loss_0: 1.112e-08, Loss_r: 1.046e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.044e-07, Loss_0: 5.332e-10, Loss_r: 1.039e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.045e-07, Loss_0: 1.191e-09, Loss_r: 1.033e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.026e-07, Loss_0: 2.725e-11, Loss_r: 1.026e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.021e-07, Loss_0: 2.085e-10, Loss_r: 1.019e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.012e-07, Loss_0: 2.430e-11, Loss_r: 1.012e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.005e-07, Loss_0: 2.851e-12, Loss_r: 1.005e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 9.985e-08, Loss_0: 8.111e-13, Loss_r: 9.985e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 9.921e-08, Loss_0: 6.535e-13, Loss_r: 9.920e-08, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 9.854e-08, Loss_0: 1.523e-13, Loss_r: 9.854e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 9.789e-08, Loss_0: 5.705e-13, Loss_r: 9.789e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 9.725e-08, Loss_0: 1.270e-12, Loss_r: 9.725e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 9.663e-08, Loss_0: 2.817e-12, Loss_r: 9.662e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 9.600e-08, Loss_0: 1.094e-11, Loss_r: 9.599e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 9.549e-08, Loss_0: 1.163e-10, Loss_r: 9.537e-08, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 9.903e-08, Loss_0: 4.182e-09, Loss_r: 9.485e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 4.659e-07, Loss_0: 3.685e-07, Loss_r: 9.740e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.265e-07, Loss_0: 3.244e-08, Loss_r: 9.405e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.016e-07, Loss_0: 8.527e-09, Loss_r: 9.307e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.140e-07, Loss_0: 2.140e-08, Loss_r: 9.258e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 9.368e-08, Loss_0: 1.671e-09, Loss_r: 9.201e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 9.365e-08, Loss_0: 2.068e-09, Loss_r: 9.158e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 9.118e-08, Loss_0: 1.739e-10, Loss_r: 9.101e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 9.084e-08, Loss_0: 4.122e-10, Loss_r: 9.043e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 8.996e-08, Loss_0: 3.280e-11, Loss_r: 8.993e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 8.943e-08, Loss_0: 1.475e-12, Loss_r: 8.942e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 8.891e-08, Loss_0: 1.110e-11, Loss_r: 8.890e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 8.837e-08, Loss_0: 3.069e-12, Loss_r: 8.837e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 8.788e-08, Loss_0: 6.545e-12, Loss_r: 8.787e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 8.736e-08, Loss_0: 1.366e-12, Loss_r: 8.736e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 8.684e-08, Loss_0: 3.164e-14, Loss_r: 8.684e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 8.635e-08, Loss_0: 1.106e-11, Loss_r: 8.634e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 8.588e-08, Loss_0: 9.903e-12, Loss_r: 8.587e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 8.539e-08, Loss_0: 4.278e-11, Loss_r: 8.535e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 8.593e-08, Loss_0: 1.094e-09, Loss_r: 8.483e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 1.501e-07, Loss_0: 6.545e-08, Loss_r: 8.464e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 9.179e-08, Loss_0: 7.868e-09, Loss_r: 8.392e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 8.407e-08, Loss_0: 5.284e-10, Loss_r: 8.354e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 8.642e-08, Loss_0: 3.261e-09, Loss_r: 8.315e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 8.332e-08, Loss_0: 6.423e-10, Loss_r: 8.268e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 8.239e-08, Loss_0: 1.725e-10, Loss_r: 8.222e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 8.191e-08, Loss_0: 1.393e-10, Loss_r: 8.178e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 8.141e-08, Loss_0: 4.156e-11, Loss_r: 8.137e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 8.095e-08, Loss_0: 6.475e-13, Loss_r: 8.095e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 8.052e-08, Loss_0: 9.105e-12, Loss_r: 8.051e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 8.010e-08, Loss_0: 8.579e-12, Loss_r: 8.010e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 7.967e-08, Loss_0: 9.043e-12, Loss_r: 7.966e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 7.926e-08, Loss_0: 1.581e-12, Loss_r: 7.926e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 7.885e-08, Loss_0: 1.986e-12, Loss_r: 7.885e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 7.843e-08, Loss_0: 3.620e-13, Loss_r: 7.843e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 7.803e-08, Loss_0: 6.351e-12, Loss_r: 7.802e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 7.760e-08, Loss_0: 5.169e-13, Loss_r: 7.760e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 7.720e-08, Loss_0: 4.913e-12, Loss_r: 7.720e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 7.689e-08, Loss_0: 9.665e-11, Loss_r: 7.680e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 7.981e-08, Loss_0: 3.350e-09, Loss_r: 7.645e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 4.063e-07, Loss_0: 3.270e-07, Loss_r: 7.923e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.017e-07, Loss_0: 2.567e-08, Loss_r: 7.600e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 8.459e-08, Loss_0: 9.280e-09, Loss_r: 7.531e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 9.414e-08, Loss_0: 1.914e-08, Loss_r: 7.500e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 7.563e-08, Loss_0: 1.029e-09, Loss_r: 7.460e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 7.619e-08, Loss_0: 1.902e-09, Loss_r: 7.429e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 7.402e-08, Loss_0: 1.039e-10, Loss_r: 7.391e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 7.383e-08, Loss_0: 2.975e-10, Loss_r: 7.354e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 7.322e-08, Loss_0: 2.952e-11, Loss_r: 7.319e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 7.284e-08, Loss_0: 6.686e-13, Loss_r: 7.284e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 7.248e-08, Loss_0: 1.182e-11, Loss_r: 7.246e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 7.212e-08, Loss_0: 1.900e-13, Loss_r: 7.212e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 7.177e-08, Loss_0: 1.846e-12, Loss_r: 7.177e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 7.143e-08, Loss_0: 1.426e-12, Loss_r: 7.143e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 7.107e-08, Loss_0: 2.081e-12, Loss_r: 7.107e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 7.074e-08, Loss_0: 1.868e-11, Loss_r: 7.072e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 7.038e-08, Loss_0: 5.618e-12, Loss_r: 7.038e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 7.006e-08, Loss_0: 2.450e-11, Loss_r: 7.004e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 7.029e-08, Loss_0: 6.050e-10, Loss_r: 6.969e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 9.826e-08, Loss_0: 2.879e-08, Loss_r: 6.947e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 7.331e-08, Loss_0: 4.275e-09, Loss_r: 6.903e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 6.878e-08, Loss_0: 3.566e-11, Loss_r: 6.875e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 6.961e-08, Loss_0: 1.140e-09, Loss_r: 6.847e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 6.856e-08, Loss_0: 4.218e-10, Loss_r: 6.814e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 6.783e-08, Loss_0: 1.486e-11, Loss_r: 6.782e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 6.759e-08, Loss_0: 8.462e-11, Loss_r: 6.751e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 6.722e-08, Loss_0: 5.360e-12, Loss_r: 6.721e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 6.690e-08, Loss_0: 1.749e-12, Loss_r: 6.690e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 6.660e-08, Loss_0: 5.075e-12, Loss_r: 6.659e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 6.631e-08, Loss_0: 1.472e-13, Loss_r: 6.631e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 6.600e-08, Loss_0: 6.323e-14, Loss_r: 6.600e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 6.571e-08, Loss_0: 7.309e-13, Loss_r: 6.571e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 6.540e-08, Loss_0: 3.308e-12, Loss_r: 6.539e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 6.509e-08, Loss_0: 8.830e-13, Loss_r: 6.509e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 6.480e-08, Loss_0: 1.839e-12, Loss_r: 6.480e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 6.450e-08, Loss_0: 3.940e-13, Loss_r: 6.450e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 6.419e-08, Loss_0: 6.565e-13, Loss_r: 6.419e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 6.391e-08, Loss_0: 1.363e-11, Loss_r: 6.389e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 6.388e-08, Loss_0: 2.864e-10, Loss_r: 6.359e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 7.785e-08, Loss_0: 1.447e-08, Loss_r: 6.337e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 6.492e-08, Loss_0: 1.882e-09, Loss_r: 6.303e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 6.284e-08, Loss_0: 5.924e-11, Loss_r: 6.278e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 6.316e-08, Loss_0: 6.314e-10, Loss_r: 6.253e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 6.244e-08, Loss_0: 1.844e-10, Loss_r: 6.226e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 6.201e-08, Loss_0: 3.084e-11, Loss_r: 6.198e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 6.176e-08, Loss_0: 2.724e-11, Loss_r: 6.173e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 6.147e-08, Loss_0: 1.053e-11, Loss_r: 6.146e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 6.120e-08, Loss_0: 1.952e-14, Loss_r: 6.120e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 6.094e-08, Loss_0: 4.213e-12, Loss_r: 6.094e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 6.067e-08, Loss_0: 2.801e-12, Loss_r: 6.067e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 6.040e-08, Loss_0: 2.339e-12, Loss_r: 6.040e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 6.014e-08, Loss_0: 9.949e-13, Loss_r: 6.014e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 5.989e-08, Loss_0: 3.778e-13, Loss_r: 5.989e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 5.963e-08, Loss_0: 3.859e-13, Loss_r: 5.963e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 5.937e-08, Loss_0: 7.026e-13, Loss_r: 5.936e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 5.910e-08, Loss_0: 4.252e-12, Loss_r: 5.910e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 5.884e-08, Loss_0: 1.220e-11, Loss_r: 5.883e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 5.867e-08, Loss_0: 9.335e-11, Loss_r: 5.857e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 6.079e-08, Loss_0: 2.470e-09, Loss_r: 5.832e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 2.107e-07, Loss_0: 1.513e-07, Loss_r: 5.935e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 7.765e-08, Loss_0: 1.969e-08, Loss_r: 5.796e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 5.827e-08, Loss_0: 6.379e-10, Loss_r: 5.764e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 6.422e-08, Loss_0: 6.711e-09, Loss_r: 5.751e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 5.913e-08, Loss_0: 1.936e-09, Loss_r: 5.720e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.713e-08, Loss_0: 2.129e-10, Loss_r: 5.692e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 5.704e-08, Loss_0: 3.409e-10, Loss_r: 5.670e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.652e-08, Loss_0: 4.984e-11, Loss_r: 5.647e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 5.626e-08, Loss_0: 1.415e-11, Loss_r: 5.624e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 5.604e-08, Loss_0: 1.754e-11, Loss_r: 5.602e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 5.578e-08, Loss_0: 5.117e-12, Loss_r: 5.578e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 5.556e-08, Loss_0: 7.060e-12, Loss_r: 5.556e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 5.534e-08, Loss_0: 1.266e-12, Loss_r: 5.534e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 5.509e-08, Loss_0: 2.798e-13, Loss_r: 5.509e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 5.487e-08, Loss_0: 2.536e-14, Loss_r: 5.487e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 5.463e-08, Loss_0: 2.602e-12, Loss_r: 5.463e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 5.441e-08, Loss_0: 1.671e-12, Loss_r: 5.441e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 5.418e-08, Loss_0: 7.615e-12, Loss_r: 5.418e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 5.395e-08, Loss_0: 4.505e-12, Loss_r: 5.395e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 5.387e-08, Loss_0: 1.509e-10, Loss_r: 5.372e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 6.181e-08, Loss_0: 8.284e-09, Loss_r: 5.352e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 5.447e-08, Loss_0: 1.191e-09, Loss_r: 5.328e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 5.311e-08, Loss_0: 1.798e-11, Loss_r: 5.309e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 5.324e-08, Loss_0: 3.451e-10, Loss_r: 5.290e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 5.281e-08, Loss_0: 1.254e-10, Loss_r: 5.269e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 5.249e-08, Loss_0: 8.239e-12, Loss_r: 5.249e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 5.229e-08, Loss_0: 1.444e-11, Loss_r: 5.227e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 5.206e-08, Loss_0: 2.710e-13, Loss_r: 5.206e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 5.187e-08, Loss_0: 2.142e-13, Loss_r: 5.187e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 5.166e-08, Loss_0: 3.851e-12, Loss_r: 5.166e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 5.146e-08, Loss_0: 4.238e-13, Loss_r: 5.146e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 5.126e-08, Loss_0: 2.868e-13, Loss_r: 5.126e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 5.106e-08, Loss_0: 4.737e-13, Loss_r: 5.106e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 5.086e-08, Loss_0: 2.097e-12, Loss_r: 5.085e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 5.065e-08, Loss_0: 6.763e-13, Loss_r: 5.065e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 5.046e-08, Loss_0: 2.057e-13, Loss_r: 5.046e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 5.026e-08, Loss_0: 7.651e-12, Loss_r: 5.025e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 5.005e-08, Loss_0: 5.270e-12, Loss_r: 5.005e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 4.985e-08, Loss_0: 1.502e-12, Loss_r: 4.985e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 4.971e-08, Loss_0: 4.893e-11, Loss_r: 4.966e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 5.233e-08, Loss_0: 2.820e-09, Loss_r: 4.951e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 4.969e-08, Loss_0: 4.061e-10, Loss_r: 4.928e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 4.910e-08, Loss_0: 2.614e-12, Loss_r: 4.910e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 4.900e-08, Loss_0: 1.025e-10, Loss_r: 4.890e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 4.876e-08, Loss_0: 4.324e-11, Loss_r: 4.872e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 4.855e-08, Loss_0: 1.112e-13, Loss_r: 4.855e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 4.839e-08, Loss_0: 5.698e-12, Loss_r: 4.839e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 4.820e-08, Loss_0: 8.077e-13, Loss_r: 4.820e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 4.801e-08, Loss_0: 1.477e-12, Loss_r: 4.801e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 4.783e-08, Loss_0: 8.094e-13, Loss_r: 4.783e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 4.765e-08, Loss_0: 1.477e-12, Loss_r: 4.765e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 4.748e-08, Loss_0: 2.614e-13, Loss_r: 4.748e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 4.730e-08, Loss_0: 1.851e-12, Loss_r: 4.730e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 4.713e-08, Loss_0: 2.092e-12, Loss_r: 4.713e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 4.694e-08, Loss_0: 1.443e-12, Loss_r: 4.694e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 4.676e-08, Loss_0: 1.771e-12, Loss_r: 4.676e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 4.659e-08, Loss_0: 4.484e-13, Loss_r: 4.659e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 4.642e-08, Loss_0: 1.377e-14, Loss_r: 4.642e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 4.623e-08, Loss_0: 2.361e-14, Loss_r: 4.623e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 4.607e-08, Loss_0: 9.676e-12, Loss_r: 4.606e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 4.607e-08, Loss_0: 1.785e-10, Loss_r: 4.589e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 6.174e-08, Loss_0: 1.580e-08, Loss_r: 4.594e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 4.690e-08, Loss_0: 1.317e-09, Loss_r: 4.558e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 20.9380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CSol[idx] = predict_CSol[idx].reshape(exact_C.shape)\n",
        "\n",
        "  error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "  print('Error C Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_C) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618e11ba-873b-4d22-8f71-44c3436ead6f",
        "id": "BtOBqaOt7XUI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Sol [1, 64, 64, 64, 1] : 8.437346e-04\n",
            "Error C Sol [1, 128, 128, 128, 1] : 7.857720e-04\n",
            "Error C Sol [1, 256, 256, 256, 1] : 7.275102e-04\n",
            "Error C Sol [1, 64, 64, 64, 64, 1] : 5.866095e-04\n",
            "Error C Sol [1, 256, 256, 256, 256, 1] : 2.238963e-03\n",
            "Error C Sol [1, 128, 128, 128, 128, 1] : 8.188146e-04\n",
            "Error C Sol [1, 128, 128, 64, 64, 1] : 7.476777e-04\n",
            "Error C Sol [1, 256, 128, 64, 32, 1] : 6.201577e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [2,3,6,7]:\n",
        "  plt.plot(t, predict_CSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_C.flatten(), 'c', label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "84c8edf5-494c-4353-c38f-045e2ed18949",
        "id": "hxXFSmF17XUI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc1fd9aa470>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,1],[1,128,128,128,1],[1,256,256,256,1],[1,64,64,64,64,1],[1,256,256,256,256,1],[1,128,128,128,128,1],[1,128,128,64,64,1],[1,256,128,64,32,1]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = ReactionPINN_DiffSol2(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVaA7rz2oY4G",
        "outputId": "3bc7a602-2843-4961-9b70-795bebb8ae1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.743e-02, Loss_0: 3.219e-04, Loss_r: 3.711e-02, Time: 3.44, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.242e-02, Loss_0: 5.623e-05, Loss_r: 3.237e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.191e-02, Loss_0: 2.327e-05, Loss_r: 3.189e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.187e-02, Loss_0: 5.719e-06, Loss_r: 3.186e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 3.151e-02, Loss_0: 2.739e-06, Loss_r: 3.151e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 3.072e-02, Loss_0: 3.533e-06, Loss_r: 3.072e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.857e-02, Loss_0: 5.762e-06, Loss_r: 2.856e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 2.374e-02, Loss_0: 8.304e-06, Loss_r: 2.373e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.734e-02, Loss_0: 7.228e-06, Loss_r: 1.733e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.095e-02, Loss_0: 2.410e-04, Loss_r: 1.071e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.368e-03, Loss_0: 1.228e-08, Loss_r: 5.368e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 2.533e-03, Loss_0: 5.489e-05, Loss_r: 2.478e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.525e-03, Loss_0: 2.100e-06, Loss_r: 1.523e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 7.777e-04, Loss_0: 1.496e-05, Loss_r: 7.628e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 4.148e-04, Loss_0: 7.215e-06, Loss_r: 4.076e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.395e-04, Loss_0: 5.852e-06, Loss_r: 2.337e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 3.108e-04, Loss_0: 1.663e-04, Loss_r: 1.444e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.634e-04, Loss_0: 6.055e-05, Loss_r: 1.028e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.010e-04, Loss_0: 1.773e-05, Loss_r: 8.327e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 7.870e-05, Loss_0: 8.651e-06, Loss_r: 7.005e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 6.264e-05, Loss_0: 2.183e-06, Loss_r: 6.046e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.283e-05, Loss_0: 4.536e-07, Loss_r: 5.237e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 4.540e-05, Loss_0: 2.051e-09, Loss_r: 4.540e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 3.954e-05, Loss_0: 8.745e-09, Loss_r: 3.954e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 3.468e-05, Loss_0: 8.357e-08, Loss_r: 3.460e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 3.050e-05, Loss_0: 6.432e-08, Loss_r: 3.044e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 2.707e-05, Loss_0: 1.253e-08, Loss_r: 2.705e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 2.650e-05, Loss_0: 2.151e-06, Loss_r: 2.435e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.738e-04, Loss_0: 1.491e-04, Loss_r: 2.467e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 2.294e-05, Loss_0: 2.240e-06, Loss_r: 2.070e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.650e-05, Loss_0: 2.705e-05, Loss_r: 1.945e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.992e-05, Loss_0: 1.571e-06, Loss_r: 1.835e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.142e-05, Loss_0: 4.049e-06, Loss_r: 1.737e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.708e-05, Loss_0: 6.556e-07, Loss_r: 1.643e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.577e-05, Loss_0: 1.314e-07, Loss_r: 1.564e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.512e-05, Loss_0: 1.351e-07, Loss_r: 1.498e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.448e-05, Loss_0: 7.408e-08, Loss_r: 1.441e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.391e-05, Loss_0: 1.812e-08, Loss_r: 1.389e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.344e-05, Loss_0: 5.853e-10, Loss_r: 1.344e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.304e-05, Loss_0: 1.008e-08, Loss_r: 1.303e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.266e-05, Loss_0: 6.025e-10, Loss_r: 1.265e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.230e-05, Loss_0: 1.878e-09, Loss_r: 1.230e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.272e-05, Loss_0: 7.514e-07, Loss_r: 1.196e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.968e-04, Loss_0: 5.816e-04, Loss_r: 1.524e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.026e-05, Loss_0: 8.594e-06, Loss_r: 1.166e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 3.648e-05, Loss_0: 2.467e-05, Loss_r: 1.181e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.656e-05, Loss_0: 1.521e-05, Loss_r: 1.136e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.841e-05, Loss_0: 7.253e-06, Loss_r: 1.116e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.261e-05, Loss_0: 1.805e-06, Loss_r: 1.081e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.057e-05, Loss_0: 7.937e-10, Loss_r: 1.057e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.065e-05, Loss_0: 3.189e-07, Loss_r: 1.033e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.009e-05, Loss_0: 6.784e-09, Loss_r: 1.008e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 9.890e-06, Loss_0: 4.869e-08, Loss_r: 9.841e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 9.623e-06, Loss_0: 1.221e-08, Loss_r: 9.610e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 9.384e-06, Loss_0: 1.678e-09, Loss_r: 9.383e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 9.158e-06, Loss_0: 5.314e-10, Loss_r: 9.158e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 8.937e-06, Loss_0: 6.278e-10, Loss_r: 8.936e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 8.718e-06, Loss_0: 6.235e-10, Loss_r: 8.718e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 8.502e-06, Loss_0: 1.498e-10, Loss_r: 8.502e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 8.290e-06, Loss_0: 2.248e-12, Loss_r: 8.290e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 8.080e-06, Loss_0: 4.094e-11, Loss_r: 8.080e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 7.873e-06, Loss_0: 4.049e-11, Loss_r: 7.873e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 7.670e-06, Loss_0: 4.087e-11, Loss_r: 7.670e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 7.469e-06, Loss_0: 3.238e-12, Loss_r: 7.469e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 7.272e-06, Loss_0: 3.876e-12, Loss_r: 7.272e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 7.080e-06, Loss_0: 1.725e-09, Loss_r: 7.078e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 7.601e-06, Loss_0: 6.995e-07, Loss_r: 6.901e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 4.675e-04, Loss_0: 4.559e-04, Loss_r: 1.162e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.507e-04, Loss_0: 1.421e-04, Loss_r: 8.553e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.963e-05, Loss_0: 1.283e-05, Loss_r: 6.799e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.006e-05, Loss_0: 2.337e-05, Loss_r: 6.683e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 6.770e-06, Loss_0: 3.434e-07, Loss_r: 6.427e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 9.000e-06, Loss_0: 2.691e-06, Loss_r: 6.309e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 6.159e-06, Loss_0: 5.029e-08, Loss_r: 6.109e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 6.345e-06, Loss_0: 3.913e-07, Loss_r: 5.953e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.878e-06, Loss_0: 7.519e-08, Loss_r: 5.803e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 5.658e-06, Loss_0: 1.278e-09, Loss_r: 5.657e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 5.513e-06, Loss_0: 6.979e-10, Loss_r: 5.513e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 5.372e-06, Loss_0: 3.757e-10, Loss_r: 5.371e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 5.233e-06, Loss_0: 1.102e-11, Loss_r: 5.233e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 5.099e-06, Loss_0: 4.588e-10, Loss_r: 5.098e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 4.967e-06, Loss_0: 5.056e-10, Loss_r: 4.967e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 4.839e-06, Loss_0: 6.358e-11, Loss_r: 4.839e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 4.714e-06, Loss_0: 5.386e-12, Loss_r: 4.714e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 4.591e-06, Loss_0: 2.231e-11, Loss_r: 4.591e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 4.472e-06, Loss_0: 2.040e-11, Loss_r: 4.472e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 4.356e-06, Loss_0: 2.072e-11, Loss_r: 4.356e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 4.243e-06, Loss_0: 4.348e-13, Loss_r: 4.243e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 4.134e-06, Loss_0: 2.344e-12, Loss_r: 4.134e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 4.027e-06, Loss_0: 5.881e-12, Loss_r: 4.027e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 3.923e-06, Loss_0: 4.666e-10, Loss_r: 3.923e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 3.899e-06, Loss_0: 7.462e-08, Loss_r: 3.824e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.465e-05, Loss_0: 3.062e-05, Loss_r: 4.034e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 7.809e-05, Loss_0: 7.361e-05, Loss_r: 4.475e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 9.692e-05, Loss_0: 9.280e-05, Loss_r: 4.121e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.958e-05, Loss_0: 1.566e-05, Loss_r: 3.921e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 8.965e-06, Loss_0: 5.221e-06, Loss_r: 3.744e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 7.885e-06, Loss_0: 4.229e-06, Loss_r: 3.656e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 3.549e-06, Loss_0: 3.478e-08, Loss_r: 3.514e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 3.702e-06, Loss_0: 2.755e-07, Loss_r: 3.427e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 3.566e-06, Loss_0: 2.189e-07, Loss_r: 3.347e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 3.351e-06, Loss_0: 8.311e-08, Loss_r: 3.268e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 3.221e-06, Loss_0: 2.931e-08, Loss_r: 3.191e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 3.129e-06, Loss_0: 1.139e-08, Loss_r: 3.118e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 3.051e-06, Loss_0: 4.207e-09, Loss_r: 3.047e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 2.979e-06, Loss_0: 6.208e-10, Loss_r: 2.978e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 2.911e-06, Loss_0: 2.034e-11, Loss_r: 2.911e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.847e-06, Loss_0: 1.196e-10, Loss_r: 2.847e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.784e-06, Loss_0: 9.297e-12, Loss_r: 2.784e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.723e-06, Loss_0: 5.903e-11, Loss_r: 2.723e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 2.664e-06, Loss_0: 1.571e-12, Loss_r: 2.664e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.607e-06, Loss_0: 3.644e-11, Loss_r: 2.607e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.552e-06, Loss_0: 1.949e-13, Loss_r: 2.552e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.499e-06, Loss_0: 7.016e-12, Loss_r: 2.499e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.447e-06, Loss_0: 4.814e-12, Loss_r: 2.447e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.397e-06, Loss_0: 5.021e-12, Loss_r: 2.397e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.348e-06, Loss_0: 1.105e-11, Loss_r: 2.348e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.304e-06, Loss_0: 2.324e-09, Loss_r: 2.302e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.744e-06, Loss_0: 4.810e-07, Loss_r: 2.263e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.057e-04, Loss_0: 2.015e-04, Loss_r: 4.200e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.636e-04, Loss_0: 1.606e-04, Loss_r: 2.934e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 2.971e-06, Loss_0: 5.881e-07, Loss_r: 2.383e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.699e-05, Loss_0: 2.444e-05, Loss_r: 2.552e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 7.285e-06, Loss_0: 4.903e-06, Loss_r: 2.382e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 2.778e-06, Loss_0: 5.179e-07, Loss_r: 2.260e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.501e-06, Loss_0: 1.291e-06, Loss_r: 2.210e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.515e-06, Loss_0: 3.520e-07, Loss_r: 2.163e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.154e-06, Loss_0: 3.143e-08, Loss_r: 2.123e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.084e-06, Loss_0: 3.907e-10, Loss_r: 2.083e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.046e-06, Loss_0: 5.722e-11, Loss_r: 2.046e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.009e-06, Loss_0: 1.009e-11, Loss_r: 2.009e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.975e-06, Loss_0: 3.926e-10, Loss_r: 1.974e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.941e-06, Loss_0: 8.312e-10, Loss_r: 1.941e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.909e-06, Loss_0: 4.526e-10, Loss_r: 1.908e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.877e-06, Loss_0: 4.345e-11, Loss_r: 1.877e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.847e-06, Loss_0: 3.620e-11, Loss_r: 1.847e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.818e-06, Loss_0: 9.909e-14, Loss_r: 1.818e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.790e-06, Loss_0: 4.604e-11, Loss_r: 1.790e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.762e-06, Loss_0: 1.487e-13, Loss_r: 1.762e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.736e-06, Loss_0: 2.466e-12, Loss_r: 1.736e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.710e-06, Loss_0: 2.254e-12, Loss_r: 1.710e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.685e-06, Loss_0: 8.467e-13, Loss_r: 1.685e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.661e-06, Loss_0: 3.414e-12, Loss_r: 1.661e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.638e-06, Loss_0: 2.641e-11, Loss_r: 1.638e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.615e-06, Loss_0: 1.321e-11, Loss_r: 1.615e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.593e-06, Loss_0: 1.641e-11, Loss_r: 1.593e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.572e-06, Loss_0: 3.628e-10, Loss_r: 1.572e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.594e-06, Loss_0: 4.261e-08, Loss_r: 1.551e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.587e-05, Loss_0: 1.423e-05, Loss_r: 1.638e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.414e-04, Loss_0: 1.393e-04, Loss_r: 2.124e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.541e-05, Loss_0: 1.366e-05, Loss_r: 1.748e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.730e-05, Loss_0: 2.552e-05, Loss_r: 1.776e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.554e-05, Loss_0: 1.380e-05, Loss_r: 1.743e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.757e-06, Loss_0: 1.357e-07, Loss_r: 1.621e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 3.617e-06, Loss_0: 2.014e-06, Loss_r: 1.602e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.103e-06, Loss_0: 5.289e-07, Loss_r: 1.574e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.554e-06, Loss_0: 9.346e-09, Loss_r: 1.545e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.539e-06, Loss_0: 1.547e-08, Loss_r: 1.524e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.521e-06, Loss_0: 1.868e-08, Loss_r: 1.502e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.491e-06, Loss_0: 9.290e-09, Loss_r: 1.482e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.466e-06, Loss_0: 3.339e-09, Loss_r: 1.463e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.445e-06, Loss_0: 8.649e-10, Loss_r: 1.444e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.427e-06, Loss_0: 4.245e-11, Loss_r: 1.427e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.410e-06, Loss_0: 4.209e-11, Loss_r: 1.410e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.394e-06, Loss_0: 9.345e-11, Loss_r: 1.393e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.378e-06, Loss_0: 4.622e-11, Loss_r: 1.378e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.362e-06, Loss_0: 2.057e-14, Loss_r: 1.362e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.347e-06, Loss_0: 3.204e-12, Loss_r: 1.347e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.333e-06, Loss_0: 1.608e-11, Loss_r: 1.333e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.319e-06, Loss_0: 8.639e-13, Loss_r: 1.319e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.305e-06, Loss_0: 2.220e-12, Loss_r: 1.305e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.291e-06, Loss_0: 2.835e-12, Loss_r: 1.291e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.278e-06, Loss_0: 2.237e-12, Loss_r: 1.278e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.266e-06, Loss_0: 1.781e-11, Loss_r: 1.266e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.253e-06, Loss_0: 2.542e-12, Loss_r: 1.253e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.241e-06, Loss_0: 1.741e-11, Loss_r: 1.241e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.229e-06, Loss_0: 7.428e-11, Loss_r: 1.229e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.218e-06, Loss_0: 8.980e-10, Loss_r: 1.218e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.244e-06, Loss_0: 3.754e-08, Loss_r: 1.206e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 5.542e-06, Loss_0: 4.314e-06, Loss_r: 1.227e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 4.662e-04, Loss_0: 4.619e-04, Loss_r: 4.233e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.093e-06, Loss_0: 8.026e-07, Loss_r: 1.291e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.442e-06, Loss_0: 1.537e-07, Loss_r: 1.288e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.506e-06, Loss_0: 1.224e-06, Loss_r: 1.282e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 2.436e-06, Loss_0: 1.191e-06, Loss_r: 1.245e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.773e-06, Loss_0: 5.348e-07, Loss_r: 1.238e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.341e-06, Loss_0: 1.232e-07, Loss_r: 1.218e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.205e-06, Loss_0: 3.007e-09, Loss_r: 1.202e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.205e-06, Loss_0: 1.616e-08, Loss_r: 1.189e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.209e-06, Loss_0: 3.169e-08, Loss_r: 1.177e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.176e-06, Loss_0: 1.126e-08, Loss_r: 1.164e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.153e-06, Loss_0: 3.232e-11, Loss_r: 1.153e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.144e-06, Loss_0: 2.025e-09, Loss_r: 1.142e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.131e-06, Loss_0: 1.879e-12, Loss_r: 1.131e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.121e-06, Loss_0: 2.170e-10, Loss_r: 1.121e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.111e-06, Loss_0: 8.747e-11, Loss_r: 1.111e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.101e-06, Loss_0: 1.381e-12, Loss_r: 1.101e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.092e-06, Loss_0: 5.300e-12, Loss_r: 1.092e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.082e-06, Loss_0: 1.346e-13, Loss_r: 1.082e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.073e-06, Loss_0: 1.438e-11, Loss_r: 1.073e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.064e-06, Loss_0: 3.603e-12, Loss_r: 1.064e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.055e-06, Loss_0: 2.384e-12, Loss_r: 1.055e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.047e-06, Loss_0: 1.452e-12, Loss_r: 1.047e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.038e-06, Loss_0: 1.355e-12, Loss_r: 1.038e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.030e-06, Loss_0: 1.850e-11, Loss_r: 1.030e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.024e-06, Loss_0: 1.667e-09, Loss_r: 1.022e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.287e-06, Loss_0: 2.703e-07, Loss_r: 1.017e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 8.707e-05, Loss_0: 8.525e-05, Loss_r: 1.812e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.193e-04, Loss_0: 2.164e-04, Loss_r: 2.923e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.001e-04, Loss_0: 9.742e-05, Loss_r: 2.706e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 2.512e-05, Loss_0: 2.370e-05, Loss_r: 1.416e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.143e-06, Loss_0: 5.223e-08, Loss_r: 1.091e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 4.356e-06, Loss_0: 3.222e-06, Loss_r: 1.134e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 2.552e-06, Loss_0: 1.462e-06, Loss_r: 1.090e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.234e-06, Loss_0: 1.815e-07, Loss_r: 1.052e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.038e-06, Loss_0: 1.150e-09, Loss_r: 1.036e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.031e-06, Loss_0: 5.657e-09, Loss_r: 1.025e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.020e-06, Loss_0: 5.356e-09, Loss_r: 1.014e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.007e-06, Loss_0: 2.040e-09, Loss_r: 1.005e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 9.956e-07, Loss_0: 3.133e-10, Loss_r: 9.952e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 9.864e-07, Loss_0: 5.543e-12, Loss_r: 9.864e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 9.779e-07, Loss_0: 1.302e-10, Loss_r: 9.778e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 9.696e-07, Loss_0: 1.417e-10, Loss_r: 9.694e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 9.614e-07, Loss_0: 2.767e-11, Loss_r: 9.613e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 9.534e-07, Loss_0: 5.430e-12, Loss_r: 9.534e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 9.457e-07, Loss_0: 1.018e-11, Loss_r: 9.457e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 9.381e-07, Loss_0: 2.425e-12, Loss_r: 9.381e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 9.307e-07, Loss_0: 3.164e-13, Loss_r: 9.307e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 9.235e-07, Loss_0: 8.674e-17, Loss_r: 9.235e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 9.163e-07, Loss_0: 6.128e-12, Loss_r: 9.163e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 9.093e-07, Loss_0: 2.127e-12, Loss_r: 9.093e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 9.023e-07, Loss_0: 1.377e-12, Loss_r: 9.023e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 8.954e-07, Loss_0: 2.681e-12, Loss_r: 8.954e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 8.887e-07, Loss_0: 2.413e-12, Loss_r: 8.887e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 8.822e-07, Loss_0: 1.477e-11, Loss_r: 8.821e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 8.757e-07, Loss_0: 6.033e-11, Loss_r: 8.756e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 8.693e-07, Loss_0: 1.090e-10, Loss_r: 8.692e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2370, Loss: 8.643e-07, Loss_0: 1.469e-09, Loss_r: 8.628e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2380, Loss: 9.109e-07, Loss_0: 5.409e-08, Loss_r: 8.568e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2390, Loss: 6.021e-06, Loss_0: 5.129e-06, Loss_r: 8.910e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 1.286e-06, Loss_0: 4.378e-07, Loss_r: 8.486e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 9.589e-07, Loss_0: 1.176e-07, Loss_r: 8.413e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 1.132e-06, Loss_0: 2.944e-07, Loss_r: 8.380e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 8.531e-07, Loss_0: 2.331e-08, Loss_r: 8.298e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 8.526e-07, Loss_0: 2.846e-08, Loss_r: 8.241e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 8.217e-07, Loss_0: 2.823e-09, Loss_r: 8.189e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 8.190e-07, Loss_0: 5.190e-09, Loss_r: 8.138e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 8.091e-07, Loss_0: 5.444e-10, Loss_r: 8.085e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 8.035e-07, Loss_0: 1.150e-11, Loss_r: 8.034e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 7.985e-07, Loss_0: 6.911e-11, Loss_r: 7.984e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 7.934e-07, Loss_0: 5.164e-11, Loss_r: 7.934e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 7.884e-07, Loss_0: 2.138e-12, Loss_r: 7.884e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 7.835e-07, Loss_0: 1.017e-11, Loss_r: 7.835e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 7.786e-07, Loss_0: 5.999e-12, Loss_r: 7.786e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 7.737e-07, Loss_0: 2.658e-11, Loss_r: 7.737e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 7.689e-07, Loss_0: 4.791e-11, Loss_r: 7.688e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2560, Loss: 7.643e-07, Loss_0: 2.571e-10, Loss_r: 7.640e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2570, Loss: 7.669e-07, Loss_0: 7.642e-09, Loss_r: 7.593e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2580, Loss: 1.376e-06, Loss_0: 6.166e-07, Loss_r: 7.591e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 8.081e-07, Loss_0: 5.744e-08, Loss_r: 7.507e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 7.592e-07, Loss_0: 1.278e-08, Loss_r: 7.464e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 7.779e-07, Loss_0: 3.539e-08, Loss_r: 7.425e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 7.407e-07, Loss_0: 2.806e-09, Loss_r: 7.379e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 7.370e-07, Loss_0: 3.320e-09, Loss_r: 7.337e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 7.299e-07, Loss_0: 3.476e-10, Loss_r: 7.295e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 7.261e-07, Loss_0: 5.705e-10, Loss_r: 7.255e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 7.214e-07, Loss_0: 4.915e-11, Loss_r: 7.214e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 7.173e-07, Loss_0: 9.722e-12, Loss_r: 7.173e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 7.133e-07, Loss_0: 1.204e-11, Loss_r: 7.133e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 7.092e-07, Loss_0: 8.904e-12, Loss_r: 7.092e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 7.052e-07, Loss_0: 3.463e-12, Loss_r: 7.052e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 7.013e-07, Loss_0: 4.905e-13, Loss_r: 7.013e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 6.973e-07, Loss_0: 1.038e-13, Loss_r: 6.973e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 6.933e-07, Loss_0: 9.672e-13, Loss_r: 6.933e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 6.894e-07, Loss_0: 2.810e-14, Loss_r: 6.894e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2750, Loss: 6.855e-07, Loss_0: 6.680e-11, Loss_r: 6.855e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2760, Loss: 6.840e-07, Loss_0: 2.340e-09, Loss_r: 6.817e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2770, Loss: 8.377e-07, Loss_0: 1.582e-07, Loss_r: 6.795e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 6.927e-07, Loss_0: 1.818e-08, Loss_r: 6.745e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 6.723e-07, Loss_0: 1.511e-09, Loss_r: 6.707e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 6.754e-07, Loss_0: 8.137e-09, Loss_r: 6.673e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 6.654e-07, Loss_0: 1.570e-09, Loss_r: 6.638e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 6.609e-07, Loss_0: 4.382e-10, Loss_r: 6.604e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 6.572e-07, Loss_0: 1.874e-10, Loss_r: 6.570e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 6.537e-07, Loss_0: 1.232e-10, Loss_r: 6.536e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 6.502e-07, Loss_0: 2.052e-12, Loss_r: 6.502e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 6.469e-07, Loss_0: 2.462e-11, Loss_r: 6.468e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 6.435e-07, Loss_0: 1.268e-11, Loss_r: 6.435e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 6.401e-07, Loss_0: 3.066e-12, Loss_r: 6.401e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 6.368e-07, Loss_0: 2.413e-12, Loss_r: 6.368e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 6.334e-07, Loss_0: 5.010e-15, Loss_r: 6.334e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 6.302e-07, Loss_0: 1.168e-11, Loss_r: 6.301e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 6.268e-07, Loss_0: 1.983e-12, Loss_r: 6.268e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 6.235e-07, Loss_0: 1.131e-12, Loss_r: 6.235e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 6.203e-07, Loss_0: 1.710e-13, Loss_r: 6.203e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2950, Loss: 6.171e-07, Loss_0: 8.733e-11, Loss_r: 6.170e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2960, Loss: 6.165e-07, Loss_0: 2.774e-09, Loss_r: 6.137e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2970, Loss: 8.761e-07, Loss_0: 2.637e-07, Loss_r: 6.125e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 6.274e-07, Loss_0: 1.972e-08, Loss_r: 6.076e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 6.132e-07, Loss_0: 8.410e-09, Loss_r: 6.048e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 6.174e-07, Loss_0: 1.541e-08, Loss_r: 6.020e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 5.995e-07, Loss_0: 5.420e-10, Loss_r: 5.989e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 5.978e-07, Loss_0: 1.827e-09, Loss_r: 5.960e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 5.932e-07, Loss_0: 4.253e-11, Loss_r: 5.932e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 5.906e-07, Loss_0: 2.687e-10, Loss_r: 5.903e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 5.876e-07, Loss_0: 5.546e-11, Loss_r: 5.875e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 5.847e-07, Loss_0: 1.355e-12, Loss_r: 5.847e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 5.819e-07, Loss_0: 2.382e-13, Loss_r: 5.819e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 5.790e-07, Loss_0: 1.088e-12, Loss_r: 5.790e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 5.762e-07, Loss_0: 1.788e-13, Loss_r: 5.762e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 5.735e-07, Loss_0: 3.862e-12, Loss_r: 5.735e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 5.706e-07, Loss_0: 7.278e-13, Loss_r: 5.706e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3120, Loss: 5.679e-07, Loss_0: 6.445e-12, Loss_r: 5.679e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3130, Loss: 5.651e-07, Loss_0: 5.013e-12, Loss_r: 5.651e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3140, Loss: 5.623e-07, Loss_0: 2.489e-12, Loss_r: 5.623e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3150, Loss: 5.596e-07, Loss_0: 2.767e-12, Loss_r: 5.596e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3160, Loss: 5.573e-07, Loss_0: 4.495e-10, Loss_r: 5.569e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3170, Loss: 6.072e-07, Loss_0: 5.273e-08, Loss_r: 5.545e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 5.547e-07, Loss_0: 3.000e-09, Loss_r: 5.517e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 5.518e-07, Loss_0: 2.435e-09, Loss_r: 5.494e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 5.500e-07, Loss_0: 3.033e-09, Loss_r: 5.470e-07, Time: 0.08, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 5.445e-07, Loss_0: 2.757e-11, Loss_r: 5.445e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 5.425e-07, Loss_0: 4.093e-10, Loss_r: 5.421e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 5.397e-07, Loss_0: 4.352e-14, Loss_r: 5.397e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 5.374e-07, Loss_0: 5.044e-11, Loss_r: 5.373e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 5.349e-07, Loss_0: 2.550e-11, Loss_r: 5.349e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 5.325e-07, Loss_0: 3.197e-14, Loss_r: 5.325e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 5.301e-07, Loss_0: 9.930e-13, Loss_r: 5.301e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 5.278e-07, Loss_0: 3.638e-12, Loss_r: 5.278e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 5.255e-07, Loss_0: 5.304e-13, Loss_r: 5.255e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 5.231e-07, Loss_0: 2.924e-12, Loss_r: 5.231e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3310, Loss: 5.208e-07, Loss_0: 5.861e-13, Loss_r: 5.208e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3320, Loss: 5.185e-07, Loss_0: 4.178e-13, Loss_r: 5.185e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3330, Loss: 5.162e-07, Loss_0: 7.806e-16, Loss_r: 5.162e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3340, Loss: 5.139e-07, Loss_0: 8.760e-12, Loss_r: 5.139e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3350, Loss: 5.119e-07, Loss_0: 2.751e-10, Loss_r: 5.116e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3360, Loss: 5.281e-07, Loss_0: 1.850e-08, Loss_r: 5.096e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3370, Loss: 2.866e-06, Loss_0: 2.335e-06, Loss_r: 5.304e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 6.605e-07, Loss_0: 1.539e-07, Loss_r: 5.067e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 5.841e-07, Loss_0: 8.044e-08, Loss_r: 5.037e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 6.406e-07, Loss_0: 1.385e-07, Loss_r: 5.021e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 5.060e-07, Loss_0: 6.965e-09, Loss_r: 4.991e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 5.117e-07, Loss_0: 1.437e-08, Loss_r: 4.973e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 4.960e-07, Loss_0: 8.760e-10, Loss_r: 4.951e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 4.955e-07, Loss_0: 2.421e-09, Loss_r: 4.931e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 4.913e-07, Loss_0: 1.627e-10, Loss_r: 4.911e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 4.891e-07, Loss_0: 2.113e-11, Loss_r: 4.891e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 4.872e-07, Loss_0: 5.773e-11, Loss_r: 4.871e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 4.852e-07, Loss_0: 1.707e-11, Loss_r: 4.852e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 4.832e-07, Loss_0: 8.563e-12, Loss_r: 4.832e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 4.813e-07, Loss_0: 4.855e-12, Loss_r: 4.813e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 4.793e-07, Loss_0: 1.444e-13, Loss_r: 4.793e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 4.774e-07, Loss_0: 2.002e-11, Loss_r: 4.774e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 4.755e-07, Loss_0: 8.409e-11, Loss_r: 4.755e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 4.737e-07, Loss_0: 2.351e-10, Loss_r: 4.735e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 4.747e-07, Loss_0: 3.090e-09, Loss_r: 4.716e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3560, Loss: 5.841e-07, Loss_0: 1.136e-07, Loss_r: 4.706e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 4.887e-07, Loss_0: 2.060e-08, Loss_r: 4.681e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 4.664e-07, Loss_0: 1.231e-10, Loss_r: 4.663e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 4.675e-07, Loss_0: 2.849e-09, Loss_r: 4.647e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 4.651e-07, Loss_0: 2.155e-09, Loss_r: 4.630e-07, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 4.613e-07, Loss_0: 1.698e-11, Loss_r: 4.612e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 4.598e-07, Loss_0: 2.845e-10, Loss_r: 4.595e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 4.579e-07, Loss_0: 7.497e-12, Loss_r: 4.579e-07, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 4.563e-07, Loss_0: 4.108e-11, Loss_r: 4.562e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 4.546e-07, Loss_0: 9.563e-13, Loss_r: 4.546e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 4.529e-07, Loss_0: 7.641e-12, Loss_r: 4.529e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 4.513e-07, Loss_0: 4.913e-12, Loss_r: 4.513e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 4.496e-07, Loss_0: 1.749e-12, Loss_r: 4.496e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 4.480e-07, Loss_0: 2.220e-14, Loss_r: 4.480e-07, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 4.464e-07, Loss_0: 4.250e-15, Loss_r: 4.464e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 4.447e-07, Loss_0: 6.512e-14, Loss_r: 4.447e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 4.432e-07, Loss_0: 8.960e-12, Loss_r: 4.431e-07, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 4.416e-07, Loss_0: 5.649e-11, Loss_r: 4.415e-07, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 4.399e-07, Loss_0: 1.487e-11, Loss_r: 4.399e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3750, Loss: 4.383e-07, Loss_0: 1.243e-11, Loss_r: 4.383e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3760, Loss: 4.371e-07, Loss_0: 3.935e-10, Loss_r: 4.367e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3770, Loss: 4.635e-07, Loss_0: 2.815e-08, Loss_r: 4.353e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 4.369e-07, Loss_0: 3.144e-09, Loss_r: 4.338e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 4.326e-07, Loss_0: 2.716e-10, Loss_r: 4.323e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 4.324e-07, Loss_0: 1.411e-09, Loss_r: 4.309e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 4.298e-07, Loss_0: 2.505e-10, Loss_r: 4.295e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 4.281e-07, Loss_0: 8.181e-11, Loss_r: 4.281e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 4.268e-07, Loss_0: 7.713e-11, Loss_r: 4.267e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 4.254e-07, Loss_0: 2.254e-11, Loss_r: 4.253e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 4.239e-07, Loss_0: 1.351e-12, Loss_r: 4.239e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 4.225e-07, Loss_0: 2.333e-12, Loss_r: 4.225e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 4.212e-07, Loss_0: 1.124e-13, Loss_r: 4.212e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 4.198e-07, Loss_0: 4.996e-16, Loss_r: 4.198e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 4.184e-07, Loss_0: 9.095e-13, Loss_r: 4.184e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 4.170e-07, Loss_0: 2.501e-12, Loss_r: 4.170e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 4.157e-07, Loss_0: 4.011e-13, Loss_r: 4.157e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 4.144e-07, Loss_0: 7.075e-12, Loss_r: 4.144e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 4.130e-07, Loss_0: 2.610e-11, Loss_r: 4.130e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3940, Loss: 4.116e-07, Loss_0: 2.193e-12, Loss_r: 4.116e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3950, Loss: 4.104e-07, Loss_0: 5.849e-11, Loss_r: 4.103e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3960, Loss: 4.105e-07, Loss_0: 1.464e-09, Loss_r: 4.090e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3970, Loss: 4.814e-07, Loss_0: 7.319e-08, Loss_r: 4.082e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 4.167e-07, Loss_0: 1.023e-08, Loss_r: 4.065e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 4.055e-07, Loss_0: 1.448e-10, Loss_r: 4.053e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 4.072e-07, Loss_0: 3.055e-09, Loss_r: 4.042e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 4.040e-07, Loss_0: 1.077e-09, Loss_r: 4.030e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 4.018e-07, Loss_0: 4.709e-11, Loss_r: 4.017e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 4.007e-07, Loss_0: 1.323e-10, Loss_r: 4.006e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 3.994e-07, Loss_0: 9.309e-13, Loss_r: 3.994e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 3.983e-07, Loss_0: 7.900e-12, Loss_r: 3.982e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 3.971e-07, Loss_0: 1.849e-12, Loss_r: 3.971e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 3.959e-07, Loss_0: 1.216e-12, Loss_r: 3.959e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 3.948e-07, Loss_0: 1.308e-12, Loss_r: 3.948e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 3.936e-07, Loss_0: 4.749e-14, Loss_r: 3.936e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 3.925e-07, Loss_0: 3.123e-15, Loss_r: 3.925e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 3.914e-07, Loss_0: 1.088e-14, Loss_r: 3.914e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 3.902e-07, Loss_0: 3.184e-12, Loss_r: 3.902e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 3.891e-07, Loss_0: 1.789e-12, Loss_r: 3.891e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4140, Loss: 3.879e-07, Loss_0: 4.299e-13, Loss_r: 3.879e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4150, Loss: 3.869e-07, Loss_0: 3.129e-11, Loss_r: 3.868e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4160, Loss: 3.864e-07, Loss_0: 6.029e-10, Loss_r: 3.858e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4170, Loss: 4.077e-07, Loss_0: 2.286e-08, Loss_r: 3.849e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 3.878e-07, Loss_0: 4.141e-09, Loss_r: 3.837e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 3.826e-07, Loss_0: 5.378e-12, Loss_r: 3.826e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 3.823e-07, Loss_0: 6.344e-10, Loss_r: 3.816e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 3.810e-07, Loss_0: 4.429e-10, Loss_r: 3.806e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 3.796e-07, Loss_0: 3.486e-13, Loss_r: 3.796e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 3.787e-07, Loss_0: 5.858e-11, Loss_r: 3.787e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 3.777e-07, Loss_0: 9.560e-14, Loss_r: 3.777e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 3.766e-07, Loss_0: 1.340e-11, Loss_r: 3.766e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 3.757e-07, Loss_0: 2.566e-14, Loss_r: 3.757e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 3.747e-07, Loss_0: 7.164e-12, Loss_r: 3.747e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 3.738e-07, Loss_0: 4.790e-12, Loss_r: 3.737e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 3.728e-07, Loss_0: 9.839e-12, Loss_r: 3.728e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 3.718e-07, Loss_0: 2.937e-12, Loss_r: 3.718e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 3.708e-07, Loss_0: 4.009e-12, Loss_r: 3.708e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 3.699e-07, Loss_0: 3.332e-14, Loss_r: 3.699e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 3.689e-07, Loss_0: 1.111e-12, Loss_r: 3.689e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4340, Loss: 3.679e-07, Loss_0: 5.413e-13, Loss_r: 3.679e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4350, Loss: 3.670e-07, Loss_0: 6.996e-12, Loss_r: 3.670e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4360, Loss: 3.664e-07, Loss_0: 3.640e-10, Loss_r: 3.660e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4370, Loss: 3.731e-07, Loss_0: 7.997e-09, Loss_r: 3.651e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4380, Loss: 8.390e-07, Loss_0: 4.705e-07, Loss_r: 3.685e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 4.302e-07, Loss_0: 6.631e-08, Loss_r: 3.639e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 3.632e-07, Loss_0: 7.231e-10, Loss_r: 3.625e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 3.801e-07, Loss_0: 1.821e-08, Loss_r: 3.619e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 3.684e-07, Loss_0: 7.474e-09, Loss_r: 3.609e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 3.601e-07, Loss_0: 1.326e-10, Loss_r: 3.600e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 3.604e-07, Loss_0: 1.224e-09, Loss_r: 3.591e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 3.583e-07, Loss_0: 2.182e-11, Loss_r: 3.583e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 3.575e-07, Loss_0: 1.090e-10, Loss_r: 3.574e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 3.567e-07, Loss_0: 4.353e-11, Loss_r: 3.566e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 3.558e-07, Loss_0: 5.196e-13, Loss_r: 3.558e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 3.550e-07, Loss_0: 1.131e-12, Loss_r: 3.550e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 3.541e-07, Loss_0: 8.069e-12, Loss_r: 3.541e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 3.533e-07, Loss_0: 2.759e-13, Loss_r: 3.533e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 3.525e-07, Loss_0: 6.703e-14, Loss_r: 3.525e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4530, Loss: 3.517e-07, Loss_0: 4.572e-12, Loss_r: 3.517e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4540, Loss: 3.508e-07, Loss_0: 1.377e-12, Loss_r: 3.508e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4550, Loss: 3.500e-07, Loss_0: 1.725e-13, Loss_r: 3.500e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4560, Loss: 3.492e-07, Loss_0: 2.168e-15, Loss_r: 3.492e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4570, Loss: 3.484e-07, Loss_0: 4.549e-12, Loss_r: 3.484e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4580, Loss: 3.479e-07, Loss_0: 3.196e-10, Loss_r: 3.476e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4590, Loss: 3.678e-07, Loss_0: 2.087e-08, Loss_r: 3.469e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 3.484e-07, Loss_0: 2.356e-09, Loss_r: 3.461e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 3.455e-07, Loss_0: 2.002e-10, Loss_r: 3.453e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 3.457e-07, Loss_0: 1.061e-09, Loss_r: 3.446e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 3.441e-07, Loss_0: 2.117e-10, Loss_r: 3.439e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 3.432e-07, Loss_0: 4.404e-11, Loss_r: 3.431e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 3.425e-07, Loss_0: 5.454e-11, Loss_r: 3.425e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 3.417e-07, Loss_0: 1.595e-11, Loss_r: 3.417e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 3.410e-07, Loss_0: 4.749e-14, Loss_r: 3.410e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 3.403e-07, Loss_0: 7.610e-12, Loss_r: 3.403e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 3.396e-07, Loss_0: 8.027e-13, Loss_r: 3.396e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 3.389e-07, Loss_0: 4.823e-12, Loss_r: 3.389e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 3.381e-07, Loss_0: 6.045e-14, Loss_r: 3.381e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 3.375e-07, Loss_0: 1.253e-12, Loss_r: 3.375e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 3.367e-07, Loss_0: 2.511e-13, Loss_r: 3.367e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4740, Loss: 3.360e-07, Loss_0: 2.873e-12, Loss_r: 3.360e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4750, Loss: 3.353e-07, Loss_0: 2.419e-12, Loss_r: 3.353e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4760, Loss: 3.346e-07, Loss_0: 4.175e-12, Loss_r: 3.346e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4770, Loss: 3.339e-07, Loss_0: 8.694e-12, Loss_r: 3.339e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4780, Loss: 3.334e-07, Loss_0: 1.778e-10, Loss_r: 3.332e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4790, Loss: 3.390e-07, Loss_0: 6.449e-09, Loss_r: 3.326e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4800, Loss: 9.475e-07, Loss_0: 6.099e-07, Loss_r: 3.377e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 3.915e-07, Loss_0: 5.974e-08, Loss_r: 3.317e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 3.389e-07, Loss_0: 8.187e-09, Loss_r: 3.307e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 3.622e-07, Loss_0: 3.185e-08, Loss_r: 3.304e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 3.355e-07, Loss_0: 6.015e-09, Loss_r: 3.295e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 3.302e-07, Loss_0: 1.473e-09, Loss_r: 3.287e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 3.292e-07, Loss_0: 1.158e-09, Loss_r: 3.281e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 3.278e-07, Loss_0: 2.825e-10, Loss_r: 3.275e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 3.269e-07, Loss_0: 3.060e-11, Loss_r: 3.269e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 3.263e-07, Loss_0: 8.782e-11, Loss_r: 3.262e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 3.256e-07, Loss_0: 2.440e-11, Loss_r: 3.256e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 3.250e-07, Loss_0: 9.094e-12, Loss_r: 3.250e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 3.244e-07, Loss_0: 8.882e-14, Loss_r: 3.244e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 3.237e-07, Loss_0: 7.246e-13, Loss_r: 3.237e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 3.231e-07, Loss_0: 2.937e-14, Loss_r: 3.231e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4950, Loss: 3.225e-07, Loss_0: 1.708e-11, Loss_r: 3.225e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4960, Loss: 3.219e-07, Loss_0: 4.930e-12, Loss_r: 3.219e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4970, Loss: 3.213e-07, Loss_0: 7.355e-12, Loss_r: 3.213e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4980, Loss: 3.207e-07, Loss_0: 2.644e-12, Loss_r: 3.207e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4990, Loss: 3.201e-07, Loss_0: 5.378e-11, Loss_r: 3.200e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "Training time: 23.2717\n",
            "[1, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.449e-02, Loss_0: 4.957e-04, Loss_r: 3.400e-02, Time: 0.19, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.226e-02, Loss_0: 1.148e-04, Loss_r: 3.214e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.210e-02, Loss_0: 2.399e-05, Loss_r: 3.208e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.200e-02, Loss_0: 3.413e-06, Loss_r: 3.200e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 3.176e-02, Loss_0: 5.649e-07, Loss_r: 3.176e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 3.070e-02, Loss_0: 3.530e-06, Loss_r: 3.070e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.599e-02, Loss_0: 1.787e-05, Loss_r: 2.597e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.697e-02, Loss_0: 1.895e-05, Loss_r: 1.695e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 7.307e-03, Loss_0: 2.542e-04, Loss_r: 7.053e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 2.534e-03, Loss_0: 5.943e-05, Loss_r: 2.474e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.127e-03, Loss_0: 1.824e-05, Loss_r: 1.109e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.894e-04, Loss_0: 2.001e-05, Loss_r: 3.694e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 7.906e-05, Loss_0: 3.268e-06, Loss_r: 7.579e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 6.228e-05, Loss_0: 2.919e-06, Loss_r: 5.936e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 4.341e-05, Loss_0: 6.338e-06, Loss_r: 3.707e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.272e-05, Loss_0: 2.696e-06, Loss_r: 2.003e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 2.860e-04, Loss_0: 2.581e-04, Loss_r: 2.789e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 6.100e-05, Loss_0: 4.963e-05, Loss_r: 1.137e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 3.100e-05, Loss_0: 2.205e-05, Loss_r: 8.945e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.143e-05, Loss_0: 4.618e-06, Loss_r: 6.816e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.393e-06, Loss_0: 1.729e-06, Loss_r: 5.664e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.264e-06, Loss_0: 2.010e-07, Loss_r: 5.063e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.617e-06, Loss_0: 9.075e-07, Loss_r: 4.709e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.787e-06, Loss_0: 1.361e-06, Loss_r: 4.426e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 4.787e-04, Loss_0: 4.546e-04, Loss_r: 2.416e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.710e-04, Loss_0: 1.588e-04, Loss_r: 1.216e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.968e-06, Loss_0: 1.626e-06, Loss_r: 4.342e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 2.618e-05, Loss_0: 2.052e-05, Loss_r: 5.659e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 4.781e-06, Loss_0: 6.600e-07, Loss_r: 4.121e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 4.872e-06, Loss_0: 8.668e-07, Loss_r: 4.005e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.528e-06, Loss_0: 7.114e-07, Loss_r: 3.817e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 3.812e-06, Loss_0: 1.464e-07, Loss_r: 3.666e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 3.574e-06, Loss_0: 2.787e-08, Loss_r: 3.546e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 3.482e-06, Loss_0: 4.741e-08, Loss_r: 3.434e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 3.358e-06, Loss_0: 3.655e-08, Loss_r: 3.321e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 3.226e-06, Loss_0: 5.958e-09, Loss_r: 3.220e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.202e-06, Loss_0: 7.323e-08, Loss_r: 3.129e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.011e-05, Loss_0: 6.705e-06, Loss_r: 3.407e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.874e-04, Loss_0: 3.614e-04, Loss_r: 2.604e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 6.224e-05, Loss_0: 5.501e-05, Loss_r: 7.236e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.955e-05, Loss_0: 1.471e-05, Loss_r: 4.841e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.015e-05, Loss_0: 1.639e-05, Loss_r: 3.755e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.236e-06, Loss_0: 9.382e-08, Loss_r: 3.142e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 4.055e-06, Loss_0: 9.816e-07, Loss_r: 3.073e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.681e-06, Loss_0: 6.510e-07, Loss_r: 3.030e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 3.110e-06, Loss_0: 2.317e-07, Loss_r: 2.879e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.806e-06, Loss_0: 1.460e-08, Loss_r: 2.792e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.740e-06, Loss_0: 2.558e-08, Loss_r: 2.715e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.654e-06, Loss_0: 9.602e-09, Loss_r: 2.644e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.588e-06, Loss_0: 8.198e-09, Loss_r: 2.580e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.520e-06, Loss_0: 4.614e-09, Loss_r: 2.515e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.456e-06, Loss_0: 2.790e-10, Loss_r: 2.456e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.404e-06, Loss_0: 3.934e-09, Loss_r: 2.400e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.492e-06, Loss_0: 1.382e-07, Loss_r: 2.354e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.921e-05, Loss_0: 3.497e-05, Loss_r: 4.242e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.827e-05, Loss_0: 5.279e-05, Loss_r: 5.477e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.202e-04, Loss_0: 1.127e-04, Loss_r: 7.501e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.109e-05, Loss_0: 8.186e-06, Loss_r: 2.906e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.758e-05, Loss_0: 1.431e-05, Loss_r: 3.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.928e-06, Loss_0: 1.185e-06, Loss_r: 2.742e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.588e-06, Loss_0: 9.887e-07, Loss_r: 2.599e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.287e-06, Loss_0: 7.960e-07, Loss_r: 2.491e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.580e-06, Loss_0: 1.768e-07, Loss_r: 2.403e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.382e-06, Loss_0: 3.706e-08, Loss_r: 2.345e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.310e-06, Loss_0: 1.986e-08, Loss_r: 2.290e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.256e-06, Loss_0: 1.365e-08, Loss_r: 2.243e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.200e-06, Loss_0: 4.647e-09, Loss_r: 2.196e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.152e-06, Loss_0: 1.157e-10, Loss_r: 2.152e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.110e-06, Loss_0: 4.898e-10, Loss_r: 2.110e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.069e-06, Loss_0: 4.090e-10, Loss_r: 2.069e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.030e-06, Loss_0: 3.891e-11, Loss_r: 2.030e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.992e-06, Loss_0: 1.251e-10, Loss_r: 1.992e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.956e-06, Loss_0: 1.437e-12, Loss_r: 1.956e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.921e-06, Loss_0: 1.351e-11, Loss_r: 1.921e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.888e-06, Loss_0: 8.766e-10, Loss_r: 1.887e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.021e-06, Loss_0: 1.559e-07, Loss_r: 1.865e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 8.722e-05, Loss_0: 8.071e-05, Loss_r: 6.509e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.474e-04, Loss_0: 4.293e-04, Loss_r: 1.809e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.241e-05, Loss_0: 2.852e-05, Loss_r: 3.887e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 5.449e-05, Loss_0: 4.836e-05, Loss_r: 6.127e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.646e-05, Loss_0: 1.265e-05, Loss_r: 3.810e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.602e-06, Loss_0: 1.624e-07, Loss_r: 2.439e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.618e-06, Loss_0: 1.255e-06, Loss_r: 2.363e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.893e-06, Loss_0: 6.344e-07, Loss_r: 2.259e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.179e-06, Loss_0: 4.413e-08, Loss_r: 2.135e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.179e-06, Loss_0: 8.036e-08, Loss_r: 2.099e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.070e-06, Loss_0: 2.302e-08, Loss_r: 2.047e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.006e-06, Loss_0: 1.318e-10, Loss_r: 2.006e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.971e-06, Loss_0: 3.020e-09, Loss_r: 1.968e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.933e-06, Loss_0: 1.999e-09, Loss_r: 1.931e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.897e-06, Loss_0: 7.943e-10, Loss_r: 1.896e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.863e-06, Loss_0: 2.832e-10, Loss_r: 1.863e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.830e-06, Loss_0: 8.522e-11, Loss_r: 1.830e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.798e-06, Loss_0: 1.013e-11, Loss_r: 1.798e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.768e-06, Loss_0: 1.105e-13, Loss_r: 1.768e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.738e-06, Loss_0: 2.268e-12, Loss_r: 1.738e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.710e-06, Loss_0: 7.497e-12, Loss_r: 1.710e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.682e-06, Loss_0: 6.009e-12, Loss_r: 1.682e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.655e-06, Loss_0: 2.454e-12, Loss_r: 1.655e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.629e-06, Loss_0: 1.872e-12, Loss_r: 1.629e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.603e-06, Loss_0: 1.572e-11, Loss_r: 1.603e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.578e-06, Loss_0: 1.102e-11, Loss_r: 1.578e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.554e-06, Loss_0: 1.124e-11, Loss_r: 1.554e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.530e-06, Loss_0: 2.809e-11, Loss_r: 1.530e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.508e-06, Loss_0: 9.117e-10, Loss_r: 1.507e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.701e-06, Loss_0: 2.064e-07, Loss_r: 1.494e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.459e-04, Loss_0: 1.369e-04, Loss_r: 8.982e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 5.403e-04, Loss_0: 5.012e-04, Loss_r: 3.903e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.784e-05, Loss_0: 2.289e-05, Loss_r: 4.948e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 3.224e-06, Loss_0: 8.056e-07, Loss_r: 2.418e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 6.118e-06, Loss_0: 3.308e-06, Loss_r: 2.810e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 5.973e-06, Loss_0: 3.848e-06, Loss_r: 2.125e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 4.400e-06, Loss_0: 2.232e-06, Loss_r: 2.168e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.712e-06, Loss_0: 7.196e-07, Loss_r: 1.992e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.927e-06, Loss_0: 3.414e-08, Loss_r: 1.893e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.901e-06, Loss_0: 4.977e-08, Loss_r: 1.851e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.849e-06, Loss_0: 3.408e-08, Loss_r: 1.814e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.777e-06, Loss_0: 8.100e-10, Loss_r: 1.777e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.748e-06, Loss_0: 4.367e-09, Loss_r: 1.743e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.711e-06, Loss_0: 3.294e-10, Loss_r: 1.710e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.679e-06, Loss_0: 3.069e-10, Loss_r: 1.679e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.649e-06, Loss_0: 3.304e-10, Loss_r: 1.649e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.620e-06, Loss_0: 1.071e-10, Loss_r: 1.620e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.592e-06, Loss_0: 2.480e-11, Loss_r: 1.592e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.565e-06, Loss_0: 1.142e-11, Loss_r: 1.565e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.539e-06, Loss_0: 8.465e-12, Loss_r: 1.539e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.514e-06, Loss_0: 9.618e-12, Loss_r: 1.514e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.489e-06, Loss_0: 9.909e-12, Loss_r: 1.489e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.465e-06, Loss_0: 7.355e-12, Loss_r: 1.465e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.442e-06, Loss_0: 5.809e-12, Loss_r: 1.442e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.419e-06, Loss_0: 1.088e-12, Loss_r: 1.419e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.397e-06, Loss_0: 6.684e-12, Loss_r: 1.397e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.376e-06, Loss_0: 4.774e-12, Loss_r: 1.376e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.355e-06, Loss_0: 4.164e-12, Loss_r: 1.355e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.335e-06, Loss_0: 4.187e-12, Loss_r: 1.335e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.315e-06, Loss_0: 1.715e-12, Loss_r: 1.315e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.295e-06, Loss_0: 5.995e-12, Loss_r: 1.295e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.276e-06, Loss_0: 2.356e-12, Loss_r: 1.276e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.257e-06, Loss_0: 2.611e-12, Loss_r: 1.257e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.239e-06, Loss_0: 1.647e-12, Loss_r: 1.239e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.221e-06, Loss_0: 2.459e-10, Loss_r: 1.221e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.237e-06, Loss_0: 3.105e-08, Loss_r: 1.206e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.082e-05, Loss_0: 9.018e-06, Loss_r: 1.798e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.412e-04, Loss_0: 7.781e-04, Loss_r: 6.311e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.388e-04, Loss_0: 1.291e-04, Loss_r: 9.679e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 4.950e-05, Loss_0: 4.583e-05, Loss_r: 3.674e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.947e-06, Loss_0: 4.587e-08, Loss_r: 1.901e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 7.970e-06, Loss_0: 5.894e-06, Loss_r: 2.077e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 4.360e-06, Loss_0: 2.572e-06, Loss_r: 1.789e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.608e-06, Loss_0: 5.244e-10, Loss_r: 1.608e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.963e-06, Loss_0: 3.683e-07, Loss_r: 1.595e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.549e-06, Loss_0: 1.115e-08, Loss_r: 1.538e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.559e-06, Loss_0: 4.998e-08, Loss_r: 1.509e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.478e-06, Loss_0: 4.672e-10, Loss_r: 1.478e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.454e-06, Loss_0: 3.813e-09, Loss_r: 1.450e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.425e-06, Loss_0: 1.978e-09, Loss_r: 1.423e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.398e-06, Loss_0: 1.945e-10, Loss_r: 1.397e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.373e-06, Loss_0: 3.929e-13, Loss_r: 1.373e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.349e-06, Loss_0: 1.697e-11, Loss_r: 1.349e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.326e-06, Loss_0: 1.474e-11, Loss_r: 1.326e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.304e-06, Loss_0: 9.537e-12, Loss_r: 1.304e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.283e-06, Loss_0: 2.302e-12, Loss_r: 1.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.262e-06, Loss_0: 3.789e-14, Loss_r: 1.262e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.242e-06, Loss_0: 3.388e-13, Loss_r: 1.242e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.223e-06, Loss_0: 3.394e-12, Loss_r: 1.223e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.204e-06, Loss_0: 7.569e-12, Loss_r: 1.204e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.185e-06, Loss_0: 4.640e-12, Loss_r: 1.185e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.167e-06, Loss_0: 1.050e-12, Loss_r: 1.167e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.150e-06, Loss_0: 6.314e-12, Loss_r: 1.150e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.133e-06, Loss_0: 3.884e-12, Loss_r: 1.133e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.116e-06, Loss_0: 1.685e-12, Loss_r: 1.116e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.100e-06, Loss_0: 5.356e-12, Loss_r: 1.100e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.084e-06, Loss_0: 3.954e-12, Loss_r: 1.084e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.069e-06, Loss_0: 5.986e-12, Loss_r: 1.069e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.054e-06, Loss_0: 6.912e-12, Loss_r: 1.054e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.039e-06, Loss_0: 5.292e-12, Loss_r: 1.039e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.024e-06, Loss_0: 1.729e-11, Loss_r: 1.024e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.011e-06, Loss_0: 8.500e-10, Loss_r: 1.010e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.244e-06, Loss_0: 2.314e-07, Loss_r: 1.012e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.532e-04, Loss_0: 1.420e-04, Loss_r: 1.121e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 5.430e-04, Loss_0: 5.066e-04, Loss_r: 3.633e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.696e-05, Loss_0: 1.404e-05, Loss_r: 2.920e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 3.070e-06, Loss_0: 1.081e-06, Loss_r: 1.990e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.107e-06, Loss_0: 2.234e-06, Loss_r: 1.873e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 3.623e-06, Loss_0: 1.900e-06, Loss_r: 1.723e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 2.974e-06, Loss_0: 1.332e-06, Loss_r: 1.642e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 2.284e-06, Loss_0: 7.428e-07, Loss_r: 1.542e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.723e-06, Loss_0: 2.351e-07, Loss_r: 1.488e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.458e-06, Loss_0: 2.012e-08, Loss_r: 1.438e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.411e-06, Loss_0: 6.401e-09, Loss_r: 1.405e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.387e-06, Loss_0: 1.147e-08, Loss_r: 1.375e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.346e-06, Loss_0: 1.036e-09, Loss_r: 1.345e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.318e-06, Loss_0: 1.100e-09, Loss_r: 1.317e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.292e-06, Loss_0: 1.766e-10, Loss_r: 1.292e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.267e-06, Loss_0: 7.868e-11, Loss_r: 1.267e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.244e-06, Loss_0: 2.920e-11, Loss_r: 1.244e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.221e-06, Loss_0: 5.210e-11, Loss_r: 1.221e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.200e-06, Loss_0: 1.371e-11, Loss_r: 1.200e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.180e-06, Loss_0: 3.483e-12, Loss_r: 1.180e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.160e-06, Loss_0: 2.316e-12, Loss_r: 1.160e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.141e-06, Loss_0: 2.823e-12, Loss_r: 1.141e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.123e-06, Loss_0: 2.810e-12, Loss_r: 1.123e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.105e-06, Loss_0: 2.466e-12, Loss_r: 1.105e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.088e-06, Loss_0: 3.702e-12, Loss_r: 1.088e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.071e-06, Loss_0: 4.668e-12, Loss_r: 1.071e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.055e-06, Loss_0: 4.168e-12, Loss_r: 1.055e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.040e-06, Loss_0: 3.928e-12, Loss_r: 1.040e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.024e-06, Loss_0: 4.002e-12, Loss_r: 1.024e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.010e-06, Loss_0: 1.444e-13, Loss_r: 1.010e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 9.951e-07, Loss_0: 4.236e-12, Loss_r: 9.951e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 9.809e-07, Loss_0: 1.814e-12, Loss_r: 9.809e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 9.672e-07, Loss_0: 1.652e-12, Loss_r: 9.672e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 9.537e-07, Loss_0: 6.917e-12, Loss_r: 9.537e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 9.406e-07, Loss_0: 1.249e-12, Loss_r: 9.406e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 9.278e-07, Loss_0: 5.223e-12, Loss_r: 9.278e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 9.153e-07, Loss_0: 3.987e-13, Loss_r: 9.153e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 9.030e-07, Loss_0: 6.512e-12, Loss_r: 9.030e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 8.910e-07, Loss_0: 9.757e-12, Loss_r: 8.910e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 8.793e-07, Loss_0: 5.892e-11, Loss_r: 8.793e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 8.687e-07, Loss_0: 8.726e-10, Loss_r: 8.678e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 9.109e-07, Loss_0: 4.835e-08, Loss_r: 8.626e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 9.065e-06, Loss_0: 7.230e-06, Loss_r: 1.835e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.135e-06, Loss_0: 2.623e-07, Loss_r: 8.732e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.411e-06, Loss_0: 5.084e-07, Loss_r: 9.023e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.284e-06, Loss_0: 4.056e-07, Loss_r: 8.785e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 8.186e-07, Loss_0: 5.723e-09, Loss_r: 8.129e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 8.673e-07, Loss_0: 5.673e-08, Loss_r: 8.106e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 8.052e-07, Loss_0: 9.168e-09, Loss_r: 7.961e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 7.861e-07, Loss_0: 2.549e-10, Loss_r: 7.858e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 7.789e-07, Loss_0: 1.651e-09, Loss_r: 7.773e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 7.697e-07, Loss_0: 7.644e-10, Loss_r: 7.689e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 7.607e-07, Loss_0: 2.935e-10, Loss_r: 7.604e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 7.522e-07, Loss_0: 1.430e-12, Loss_r: 7.522e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 7.442e-07, Loss_0: 5.534e-11, Loss_r: 7.441e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 7.362e-07, Loss_0: 1.375e-11, Loss_r: 7.362e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 7.285e-07, Loss_0: 6.202e-11, Loss_r: 7.284e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 7.213e-07, Loss_0: 4.987e-10, Loss_r: 7.208e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 7.321e-07, Loss_0: 1.654e-08, Loss_r: 7.156e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 2.666e-06, Loss_0: 1.697e-06, Loss_r: 9.688e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 8.191e-07, Loss_0: 1.040e-07, Loss_r: 7.152e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 7.855e-07, Loss_0: 7.860e-08, Loss_r: 7.069e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 8.041e-07, Loss_0: 9.999e-08, Loss_r: 7.041e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 6.814e-07, Loss_0: 4.760e-11, Loss_r: 6.813e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 6.913e-07, Loss_0: 1.422e-08, Loss_r: 6.771e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 6.698e-07, Loss_0: 5.794e-10, Loss_r: 6.692e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 6.638e-07, Loss_0: 6.035e-10, Loss_r: 6.632e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 6.579e-07, Loss_0: 7.012e-10, Loss_r: 6.572e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 6.517e-07, Loss_0: 2.569e-10, Loss_r: 6.514e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 6.457e-07, Loss_0: 1.266e-10, Loss_r: 6.455e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 6.399e-07, Loss_0: 2.265e-11, Loss_r: 6.399e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 6.342e-07, Loss_0: 8.993e-14, Loss_r: 6.342e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 6.287e-07, Loss_0: 4.145e-12, Loss_r: 6.287e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 6.232e-07, Loss_0: 5.640e-12, Loss_r: 6.232e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 6.178e-07, Loss_0: 3.473e-11, Loss_r: 6.178e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 6.132e-07, Loss_0: 6.458e-10, Loss_r: 6.126e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 6.508e-07, Loss_0: 3.736e-08, Loss_r: 6.134e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 7.221e-06, Loss_0: 5.644e-06, Loss_r: 1.577e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 8.835e-07, Loss_0: 2.444e-07, Loss_r: 6.391e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 9.822e-07, Loss_0: 3.299e-07, Loss_r: 6.523e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 9.841e-07, Loss_0: 3.376e-07, Loss_r: 6.464e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 5.868e-07, Loss_0: 4.065e-11, Loss_r: 5.867e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 6.375e-07, Loss_0: 4.785e-08, Loss_r: 5.896e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 5.796e-07, Loss_0: 1.318e-09, Loss_r: 5.782e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 5.771e-07, Loss_0: 2.862e-09, Loss_r: 5.742e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 5.722e-07, Loss_0: 2.559e-09, Loss_r: 5.696e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 5.661e-07, Loss_0: 8.266e-10, Loss_r: 5.652e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 5.612e-07, Loss_0: 3.542e-10, Loss_r: 5.608e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 5.568e-07, Loss_0: 1.051e-10, Loss_r: 5.567e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 5.525e-07, Loss_0: 4.370e-11, Loss_r: 5.524e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 5.483e-07, Loss_0: 1.487e-11, Loss_r: 5.483e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 5.443e-07, Loss_0: 1.540e-11, Loss_r: 5.443e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 5.404e-07, Loss_0: 1.279e-10, Loss_r: 5.403e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 5.382e-07, Loss_0: 1.572e-09, Loss_r: 5.366e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 6.046e-07, Loss_0: 6.106e-08, Loss_r: 5.436e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 5.406e-07, Loss_0: 9.842e-09, Loss_r: 5.307e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 5.253e-07, Loss_0: 2.660e-11, Loss_r: 5.253e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 5.245e-07, Loss_0: 2.344e-09, Loss_r: 5.222e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 5.195e-07, Loss_0: 9.814e-10, Loss_r: 5.185e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 5.150e-07, Loss_0: 2.418e-11, Loss_r: 5.150e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 5.118e-07, Loss_0: 1.219e-10, Loss_r: 5.116e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 5.082e-07, Loss_0: 1.840e-11, Loss_r: 5.082e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 5.048e-07, Loss_0: 1.218e-11, Loss_r: 5.048e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 5.015e-07, Loss_0: 3.738e-12, Loss_r: 5.015e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 4.982e-07, Loss_0: 4.790e-12, Loss_r: 4.982e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 4.949e-07, Loss_0: 3.778e-15, Loss_r: 4.949e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 4.917e-07, Loss_0: 1.657e-12, Loss_r: 4.917e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 4.885e-07, Loss_0: 1.574e-13, Loss_r: 4.885e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 4.853e-07, Loss_0: 1.559e-13, Loss_r: 4.853e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 4.821e-07, Loss_0: 1.155e-12, Loss_r: 4.821e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 4.789e-07, Loss_0: 3.681e-12, Loss_r: 4.789e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 4.758e-07, Loss_0: 4.615e-11, Loss_r: 4.758e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 4.747e-07, Loss_0: 1.659e-09, Loss_r: 4.731e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 6.291e-07, Loss_0: 1.333e-07, Loss_r: 4.958e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 4.814e-07, Loss_0: 1.218e-08, Loss_r: 4.693e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 4.676e-07, Loss_0: 3.039e-09, Loss_r: 4.645e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 4.705e-07, Loss_0: 7.847e-09, Loss_r: 4.627e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 4.592e-07, Loss_0: 5.219e-10, Loss_r: 4.587e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 4.568e-07, Loss_0: 7.604e-10, Loss_r: 4.561e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 4.532e-07, Loss_0: 2.574e-11, Loss_r: 4.532e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 4.506e-07, Loss_0: 1.474e-10, Loss_r: 4.505e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 4.479e-07, Loss_0: 2.588e-11, Loss_r: 4.478e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 4.451e-07, Loss_0: 9.563e-13, Loss_r: 4.451e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 4.425e-07, Loss_0: 3.305e-12, Loss_r: 4.425e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 4.398e-07, Loss_0: 3.377e-13, Loss_r: 4.398e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 4.372e-07, Loss_0: 4.153e-13, Loss_r: 4.372e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 4.346e-07, Loss_0: 1.147e-12, Loss_r: 4.346e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 4.320e-07, Loss_0: 5.508e-14, Loss_r: 4.320e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 4.293e-07, Loss_0: 2.507e-12, Loss_r: 4.293e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 4.268e-07, Loss_0: 2.265e-11, Loss_r: 4.268e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 4.248e-07, Loss_0: 4.322e-10, Loss_r: 4.243e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 4.474e-07, Loss_0: 2.128e-08, Loss_r: 4.261e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 3.281e-06, Loss_0: 2.382e-06, Loss_r: 8.993e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 6.302e-07, Loss_0: 1.783e-07, Loss_r: 4.520e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 4.983e-07, Loss_0: 7.021e-08, Loss_r: 4.281e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 5.817e-07, Loss_0: 1.423e-07, Loss_r: 4.394e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 4.207e-07, Loss_0: 8.301e-09, Loss_r: 4.124e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 4.260e-07, Loss_0: 1.437e-08, Loss_r: 4.116e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 4.075e-07, Loss_0: 8.606e-10, Loss_r: 4.066e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 4.073e-07, Loss_0: 2.660e-09, Loss_r: 4.046e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 4.024e-07, Loss_0: 3.272e-10, Loss_r: 4.021e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 3.997e-07, Loss_0: 3.972e-14, Loss_r: 3.997e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 3.975e-07, Loss_0: 2.559e-11, Loss_r: 3.975e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 3.953e-07, Loss_0: 6.128e-12, Loss_r: 3.953e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 3.931e-07, Loss_0: 1.619e-13, Loss_r: 3.931e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 3.909e-07, Loss_0: 6.781e-12, Loss_r: 3.909e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 3.887e-07, Loss_0: 3.847e-13, Loss_r: 3.887e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 3.865e-07, Loss_0: 2.662e-13, Loss_r: 3.865e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 3.843e-07, Loss_0: 9.218e-12, Loss_r: 3.843e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 3.823e-07, Loss_0: 1.514e-10, Loss_r: 3.822e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 3.858e-07, Loss_0: 4.983e-09, Loss_r: 3.808e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 8.635e-07, Loss_0: 4.068e-07, Loss_r: 4.567e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 4.202e-07, Loss_0: 3.722e-08, Loss_r: 3.830e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 3.846e-07, Loss_0: 8.685e-09, Loss_r: 3.759e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 4.000e-07, Loss_0: 2.315e-08, Loss_r: 3.769e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 3.724e-07, Loss_0: 1.781e-09, Loss_r: 3.707e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 3.711e-07, Loss_0: 2.310e-09, Loss_r: 3.688e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 3.667e-07, Loss_0: 2.105e-10, Loss_r: 3.665e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 3.651e-07, Loss_0: 4.205e-10, Loss_r: 3.646e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 3.627e-07, Loss_0: 6.270e-11, Loss_r: 3.626e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 3.607e-07, Loss_0: 5.421e-14, Loss_r: 3.607e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 3.588e-07, Loss_0: 1.046e-12, Loss_r: 3.588e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 3.569e-07, Loss_0: 3.972e-12, Loss_r: 3.569e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 3.551e-07, Loss_0: 2.168e-15, Loss_r: 3.551e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 3.532e-07, Loss_0: 1.534e-12, Loss_r: 3.532e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 3.513e-07, Loss_0: 1.220e-12, Loss_r: 3.513e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 3.494e-07, Loss_0: 1.819e-12, Loss_r: 3.494e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 3.476e-07, Loss_0: 4.137e-12, Loss_r: 3.476e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 3.457e-07, Loss_0: 2.826e-11, Loss_r: 3.457e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 3.446e-07, Loss_0: 6.693e-10, Loss_r: 3.439e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 3.965e-07, Loss_0: 4.558e-08, Loss_r: 3.509e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 3.460e-07, Loss_0: 4.825e-09, Loss_r: 3.412e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 3.394e-07, Loss_0: 6.192e-10, Loss_r: 3.388e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 3.399e-07, Loss_0: 2.337e-09, Loss_r: 3.375e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 3.357e-07, Loss_0: 2.974e-10, Loss_r: 3.354e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 3.339e-07, Loss_0: 1.882e-10, Loss_r: 3.337e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 3.321e-07, Loss_0: 5.127e-11, Loss_r: 3.320e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 3.304e-07, Loss_0: 3.259e-11, Loss_r: 3.304e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 3.287e-07, Loss_0: 1.720e-12, Loss_r: 3.287e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 3.271e-07, Loss_0: 2.981e-12, Loss_r: 3.271e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 3.254e-07, Loss_0: 1.187e-13, Loss_r: 3.254e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 3.238e-07, Loss_0: 2.436e-12, Loss_r: 3.238e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 3.221e-07, Loss_0: 3.753e-14, Loss_r: 3.221e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 3.205e-07, Loss_0: 2.662e-13, Loss_r: 3.205e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 3.189e-07, Loss_0: 5.223e-13, Loss_r: 3.189e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 3.173e-07, Loss_0: 1.952e-11, Loss_r: 3.173e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 3.157e-07, Loss_0: 2.860e-11, Loss_r: 3.156e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 3.141e-07, Loss_0: 1.078e-10, Loss_r: 3.140e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 3.151e-07, Loss_0: 2.242e-09, Loss_r: 3.128e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 4.552e-07, Loss_0: 1.198e-07, Loss_r: 3.353e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 3.284e-07, Loss_0: 1.589e-08, Loss_r: 3.125e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 3.085e-07, Loss_0: 4.613e-10, Loss_r: 3.080e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 3.130e-07, Loss_0: 5.369e-09, Loss_r: 3.077e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 3.068e-07, Loss_0: 1.464e-09, Loss_r: 3.054e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 3.038e-07, Loss_0: 2.018e-10, Loss_r: 3.036e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 3.025e-07, Loss_0: 2.806e-10, Loss_r: 3.022e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 3.008e-07, Loss_0: 4.272e-11, Loss_r: 3.007e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 2.992e-07, Loss_0: 8.893e-12, Loss_r: 2.992e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 2.978e-07, Loss_0: 2.338e-11, Loss_r: 2.978e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 2.964e-07, Loss_0: 4.628e-12, Loss_r: 2.964e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 2.950e-07, Loss_0: 6.073e-12, Loss_r: 2.950e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 2.935e-07, Loss_0: 2.209e-12, Loss_r: 2.935e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 2.921e-07, Loss_0: 4.477e-12, Loss_r: 2.921e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 2.906e-07, Loss_0: 6.717e-15, Loss_r: 2.906e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 2.892e-07, Loss_0: 1.835e-15, Loss_r: 2.892e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 2.878e-07, Loss_0: 2.304e-12, Loss_r: 2.878e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 2.864e-07, Loss_0: 4.588e-14, Loss_r: 2.864e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 2.850e-07, Loss_0: 2.804e-12, Loss_r: 2.850e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 2.837e-07, Loss_0: 1.055e-10, Loss_r: 2.836e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 2.933e-07, Loss_0: 9.166e-09, Loss_r: 2.842e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 2.366e-06, Loss_0: 1.718e-06, Loss_r: 6.478e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 3.563e-07, Loss_0: 6.410e-08, Loss_r: 2.922e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 4.063e-07, Loss_0: 1.077e-07, Loss_r: 2.986e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 4.015e-07, Loss_0: 1.038e-07, Loss_r: 2.978e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 2.764e-07, Loss_0: 2.230e-10, Loss_r: 2.761e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 2.920e-07, Loss_0: 1.423e-08, Loss_r: 2.777e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 2.737e-07, Loss_0: 2.810e-11, Loss_r: 2.736e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 2.743e-07, Loss_0: 1.591e-09, Loss_r: 2.727e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 2.718e-07, Loss_0: 5.924e-10, Loss_r: 2.713e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 2.700e-07, Loss_0: 7.788e-11, Loss_r: 2.699e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 2.686e-07, Loss_0: 3.603e-12, Loss_r: 2.686e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 2.674e-07, Loss_0: 3.118e-12, Loss_r: 2.674e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 2.661e-07, Loss_0: 1.439e-12, Loss_r: 2.661e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 2.649e-07, Loss_0: 7.796e-12, Loss_r: 2.648e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 2.636e-07, Loss_0: 4.855e-12, Loss_r: 2.636e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 2.624e-07, Loss_0: 4.114e-12, Loss_r: 2.624e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 2.611e-07, Loss_0: 2.220e-14, Loss_r: 2.611e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 2.599e-07, Loss_0: 1.559e-13, Loss_r: 2.599e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 2.586e-07, Loss_0: 2.442e-11, Loss_r: 2.586e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 2.599e-07, Loss_0: 2.131e-09, Loss_r: 2.578e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 6.262e-07, Loss_0: 3.081e-07, Loss_r: 3.181e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 2.711e-07, Loss_0: 1.330e-08, Loss_r: 2.578e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 2.764e-07, Loss_0: 1.886e-08, Loss_r: 2.575e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 2.742e-07, Loss_0: 1.797e-08, Loss_r: 2.563e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 2.518e-07, Loss_0: 5.508e-14, Loss_r: 2.518e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 2.536e-07, Loss_0: 2.428e-09, Loss_r: 2.512e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 2.496e-07, Loss_0: 5.164e-11, Loss_r: 2.495e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 2.487e-07, Loss_0: 2.076e-10, Loss_r: 2.485e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 2.474e-07, Loss_0: 1.014e-10, Loss_r: 2.473e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 2.462e-07, Loss_0: 2.794e-11, Loss_r: 2.462e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 2.451e-07, Loss_0: 5.464e-12, Loss_r: 2.451e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 2.440e-07, Loss_0: 3.950e-12, Loss_r: 2.440e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 2.429e-07, Loss_0: 2.049e-13, Loss_r: 2.429e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 2.418e-07, Loss_0: 1.652e-14, Loss_r: 2.418e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 2.407e-07, Loss_0: 2.030e-12, Loss_r: 2.407e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 2.396e-07, Loss_0: 7.194e-14, Loss_r: 2.396e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 2.385e-07, Loss_0: 9.218e-14, Loss_r: 2.385e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 2.374e-07, Loss_0: 7.426e-12, Loss_r: 2.374e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 2.366e-07, Loss_0: 2.453e-10, Loss_r: 2.363e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 2.586e-07, Loss_0: 1.940e-08, Loss_r: 2.392e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 2.367e-07, Loss_0: 2.044e-09, Loss_r: 2.347e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 2.336e-07, Loss_0: 2.615e-10, Loss_r: 2.333e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 2.335e-07, Loss_0: 1.028e-09, Loss_r: 2.325e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 2.315e-07, Loss_0: 1.671e-10, Loss_r: 2.313e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 2.304e-07, Loss_0: 6.626e-11, Loss_r: 2.303e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 2.293e-07, Loss_0: 1.348e-11, Loss_r: 2.293e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 2.284e-07, Loss_0: 1.367e-11, Loss_r: 2.284e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 2.274e-07, Loss_0: 5.863e-16, Loss_r: 2.274e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 2.264e-07, Loss_0: 2.978e-13, Loss_r: 2.264e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 2.254e-07, Loss_0: 1.739e-12, Loss_r: 2.254e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 2.244e-07, Loss_0: 8.443e-14, Loss_r: 2.244e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 2.235e-07, Loss_0: 1.139e-12, Loss_r: 2.235e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 2.225e-07, Loss_0: 3.575e-13, Loss_r: 2.225e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 2.215e-07, Loss_0: 9.490e-13, Loss_r: 2.215e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 2.205e-07, Loss_0: 4.190e-12, Loss_r: 2.205e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 2.196e-07, Loss_0: 2.293e-12, Loss_r: 2.196e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 2.186e-07, Loss_0: 5.667e-12, Loss_r: 2.186e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 2.177e-07, Loss_0: 1.755e-11, Loss_r: 2.177e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 2.174e-07, Loss_0: 6.218e-10, Loss_r: 2.168e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 2.772e-07, Loss_0: 5.126e-08, Loss_r: 2.259e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 2.206e-07, Loss_0: 4.809e-09, Loss_r: 2.158e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 2.152e-07, Loss_0: 9.785e-10, Loss_r: 2.142e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 2.165e-07, Loss_0: 2.797e-09, Loss_r: 2.137e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 2.126e-07, Loss_0: 2.905e-10, Loss_r: 2.123e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 2.117e-07, Loss_0: 2.360e-10, Loss_r: 2.114e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 2.106e-07, Loss_0: 4.274e-11, Loss_r: 2.105e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 2.097e-07, Loss_0: 3.730e-11, Loss_r: 2.097e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 2.088e-07, Loss_0: 1.023e-12, Loss_r: 2.088e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 2.079e-07, Loss_0: 5.945e-12, Loss_r: 2.079e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 2.071e-07, Loss_0: 1.204e-12, Loss_r: 2.071e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 2.062e-07, Loss_0: 2.116e-12, Loss_r: 2.062e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 2.053e-07, Loss_0: 3.964e-13, Loss_r: 2.053e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 2.045e-07, Loss_0: 1.819e-13, Loss_r: 2.045e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 2.036e-07, Loss_0: 2.310e-12, Loss_r: 2.036e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 2.028e-07, Loss_0: 3.123e-13, Loss_r: 2.028e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 2.019e-07, Loss_0: 7.664e-15, Loss_r: 2.019e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 2.010e-07, Loss_0: 2.116e-12, Loss_r: 2.010e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 2.003e-07, Loss_0: 7.534e-11, Loss_r: 2.002e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 2.025e-07, Loss_0: 2.609e-09, Loss_r: 1.999e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 4.751e-07, Loss_0: 2.288e-07, Loss_r: 2.463e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 2.241e-07, Loss_0: 2.186e-08, Loss_r: 2.023e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 2.017e-07, Loss_0: 3.944e-09, Loss_r: 1.977e-07, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 2.114e-07, Loss_0: 1.269e-08, Loss_r: 1.987e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 1.974e-07, Loss_0: 1.622e-09, Loss_r: 1.958e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 1.958e-07, Loss_0: 8.629e-10, Loss_r: 1.949e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 1.943e-07, Loss_0: 2.909e-10, Loss_r: 1.940e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 1.934e-07, Loss_0: 2.046e-10, Loss_r: 1.932e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 1.924e-07, Loss_0: 7.026e-15, Loss_r: 1.924e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 1.917e-07, Loss_0: 1.413e-11, Loss_r: 1.917e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 1.909e-07, Loss_0: 1.430e-11, Loss_r: 1.909e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 1.901e-07, Loss_0: 3.105e-12, Loss_r: 1.901e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 1.894e-07, Loss_0: 3.066e-12, Loss_r: 1.894e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 1.886e-07, Loss_0: 1.175e-13, Loss_r: 1.886e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 1.878e-07, Loss_0: 6.717e-13, Loss_r: 1.878e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 1.871e-07, Loss_0: 2.572e-12, Loss_r: 1.871e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 1.863e-07, Loss_0: 2.938e-13, Loss_r: 1.863e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 1.856e-07, Loss_0: 6.907e-12, Loss_r: 1.856e-07, Time: 0.09, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 1.848e-07, Loss_0: 1.230e-11, Loss_r: 1.848e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 1.845e-07, Loss_0: 3.593e-10, Loss_r: 1.841e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 2.149e-07, Loss_0: 2.627e-08, Loss_r: 1.886e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 1.859e-07, Loss_0: 2.770e-09, Loss_r: 1.831e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 1.823e-07, Loss_0: 3.027e-10, Loss_r: 1.820e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 1.829e-07, Loss_0: 1.317e-09, Loss_r: 1.815e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 1.808e-07, Loss_0: 2.143e-10, Loss_r: 1.806e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 1.800e-07, Loss_0: 8.144e-11, Loss_r: 1.799e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 1.793e-07, Loss_0: 5.129e-11, Loss_r: 1.792e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 1.786e-07, Loss_0: 2.367e-11, Loss_r: 1.785e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 1.779e-07, Loss_0: 7.994e-15, Loss_r: 1.779e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 1.772e-07, Loss_0: 1.759e-12, Loss_r: 1.772e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 1.765e-07, Loss_0: 3.687e-13, Loss_r: 1.765e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 1.758e-07, Loss_0: 4.547e-13, Loss_r: 1.758e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 17.7501\n",
            "[1, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.279e-02, Loss_0: 3.765e-04, Loss_r: 3.241e-02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.224e-02, Loss_0: 1.585e-06, Loss_r: 3.224e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.219e-02, Loss_0: 1.536e-05, Loss_r: 3.217e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.185e-02, Loss_0: 9.526e-06, Loss_r: 3.184e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.923e-02, Loss_0: 3.327e-07, Loss_r: 2.923e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.836e-02, Loss_0: 5.009e-05, Loss_r: 1.831e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 6.577e-03, Loss_0: 6.779e-04, Loss_r: 5.900e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.823e-03, Loss_0: 4.374e-04, Loss_r: 1.386e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.128e-04, Loss_0: 3.417e-05, Loss_r: 7.863e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 8.153e-05, Loss_0: 1.300e-07, Loss_r: 8.140e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 6.916e-05, Loss_0: 3.551e-06, Loss_r: 6.561e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 2.725e-05, Loss_0: 1.576e-07, Loss_r: 2.709e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.549e-05, Loss_0: 2.701e-06, Loss_r: 1.279e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.157e-05, Loss_0: 7.652e-07, Loss_r: 1.081e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.116e-05, Loss_0: 6.275e-07, Loss_r: 1.053e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 9.230e-06, Loss_0: 9.513e-08, Loss_r: 9.135e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 8.719e-06, Loss_0: 1.009e-07, Loss_r: 8.618e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 8.675e-06, Loss_0: 3.314e-07, Loss_r: 8.344e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.781e-04, Loss_0: 1.150e-04, Loss_r: 6.310e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 2.610e-05, Loss_0: 1.285e-05, Loss_r: 1.325e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.895e-05, Loss_0: 4.786e-05, Loss_r: 3.109e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.398e-05, Loss_0: 2.464e-05, Loss_r: 1.934e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 2.621e-05, Loss_0: 1.230e-05, Loss_r: 1.391e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.097e-05, Loss_0: 1.859e-06, Loss_r: 9.112e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 9.543e-06, Loss_0: 1.078e-06, Loss_r: 8.465e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 7.745e-06, Loss_0: 1.486e-09, Loss_r: 7.743e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 7.442e-06, Loss_0: 1.615e-08, Loss_r: 7.426e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 7.119e-06, Loss_0: 1.367e-08, Loss_r: 7.105e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 6.873e-06, Loss_0: 5.102e-08, Loss_r: 6.822e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 6.560e-06, Loss_0: 1.907e-08, Loss_r: 6.541e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 6.275e-06, Loss_0: 1.374e-08, Loss_r: 6.261e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 6.347e-06, Loss_0: 2.533e-07, Loss_r: 6.094e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.417e-04, Loss_0: 9.626e-05, Loss_r: 4.546e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.532e-04, Loss_0: 3.131e-04, Loss_r: 1.401e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.540e-04, Loss_0: 1.062e-04, Loss_r: 4.782e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.734e-05, Loss_0: 3.721e-05, Loss_r: 2.013e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 2.254e-05, Loss_0: 1.105e-05, Loss_r: 1.149e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 9.822e-06, Loss_0: 1.971e-06, Loss_r: 7.851e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.807e-06, Loss_0: 1.489e-08, Loss_r: 6.792e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 7.377e-06, Loss_0: 7.247e-07, Loss_r: 6.652e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 6.192e-06, Loss_0: 1.833e-08, Loss_r: 6.174e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 6.024e-06, Loss_0: 9.128e-08, Loss_r: 5.933e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.673e-06, Loss_0: 2.786e-08, Loss_r: 5.645e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.399e-06, Loss_0: 9.254e-09, Loss_r: 5.389e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.158e-06, Loss_0: 4.353e-09, Loss_r: 5.154e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.930e-06, Loss_0: 1.386e-10, Loss_r: 4.930e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.721e-06, Loss_0: 1.543e-09, Loss_r: 4.720e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.521e-06, Loss_0: 3.042e-11, Loss_r: 4.521e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.332e-06, Loss_0: 5.608e-10, Loss_r: 4.331e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.152e-06, Loss_0: 6.833e-11, Loss_r: 4.151e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.981e-06, Loss_0: 4.172e-11, Loss_r: 3.981e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.895e-06, Loss_0: 5.243e-08, Loss_r: 3.842e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.163e-04, Loss_0: 8.106e-05, Loss_r: 3.522e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 7.078e-04, Loss_0: 5.404e-04, Loss_r: 1.675e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.233e-04, Loss_0: 8.311e-05, Loss_r: 4.016e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 4.651e-05, Loss_0: 3.028e-05, Loss_r: 1.624e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.107e-05, Loss_0: 1.076e-05, Loss_r: 1.031e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.222e-05, Loss_0: 4.286e-06, Loss_r: 7.931e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 8.549e-06, Loss_0: 2.017e-06, Loss_r: 6.532e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.766e-06, Loss_0: 5.088e-08, Loss_r: 5.715e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 5.820e-06, Loss_0: 3.579e-07, Loss_r: 5.463e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 5.293e-06, Loss_0: 1.230e-07, Loss_r: 5.170e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 4.906e-06, Loss_0: 3.884e-08, Loss_r: 4.868e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 4.637e-06, Loss_0: 7.024e-09, Loss_r: 4.630e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 4.405e-06, Loss_0: 6.149e-09, Loss_r: 4.398e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 4.190e-06, Loss_0: 1.418e-09, Loss_r: 4.189e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 3.990e-06, Loss_0: 1.278e-09, Loss_r: 3.989e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.802e-06, Loss_0: 5.625e-11, Loss_r: 3.802e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.627e-06, Loss_0: 1.117e-12, Loss_r: 3.627e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.462e-06, Loss_0: 1.164e-10, Loss_r: 3.462e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.306e-06, Loss_0: 1.508e-10, Loss_r: 3.306e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.159e-06, Loss_0: 9.632e-11, Loss_r: 3.159e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 3.021e-06, Loss_0: 6.139e-11, Loss_r: 3.021e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 2.890e-06, Loss_0: 5.040e-11, Loss_r: 2.890e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.767e-06, Loss_0: 4.870e-11, Loss_r: 2.767e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.650e-06, Loss_0: 4.695e-11, Loss_r: 2.650e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.540e-06, Loss_0: 4.584e-11, Loss_r: 2.540e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.436e-06, Loss_0: 4.114e-11, Loss_r: 2.436e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.338e-06, Loss_0: 3.131e-11, Loss_r: 2.338e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.245e-06, Loss_0: 3.727e-11, Loss_r: 2.245e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.157e-06, Loss_0: 2.841e-11, Loss_r: 2.157e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.074e-06, Loss_0: 2.404e-11, Loss_r: 2.074e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.995e-06, Loss_0: 2.439e-11, Loss_r: 1.995e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.920e-06, Loss_0: 3.887e-11, Loss_r: 1.920e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.849e-06, Loss_0: 3.461e-10, Loss_r: 1.849e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.864e-06, Loss_0: 6.268e-08, Loss_r: 1.801e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 8.421e-05, Loss_0: 6.092e-05, Loss_r: 2.329e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 9.430e-04, Loss_0: 6.822e-04, Loss_r: 2.608e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.357e-04, Loss_0: 1.908e-04, Loss_r: 4.488e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 890, Loss: 6.267e-05, Loss_0: 5.225e-05, Loss_r: 1.042e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.499e-05, Loss_0: 9.388e-06, Loss_r: 5.605e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.397e-05, Loss_0: 8.470e-06, Loss_r: 5.497e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 4.043e-06, Loss_0: 2.553e-07, Loss_r: 3.787e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 930, Loss: 5.032e-06, Loss_0: 1.102e-06, Loss_r: 3.930e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 3.559e-06, Loss_0: 1.817e-07, Loss_r: 3.377e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.191e-06, Loss_0: 8.124e-09, Loss_r: 3.183e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 3.050e-06, Loss_0: 2.968e-08, Loss_r: 3.020e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.874e-06, Loss_0: 1.869e-08, Loss_r: 2.855e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.713e-06, Loss_0: 5.081e-09, Loss_r: 2.707e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.570e-06, Loss_0: 3.493e-09, Loss_r: 2.567e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.442e-06, Loss_0: 4.710e-10, Loss_r: 2.441e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.323e-06, Loss_0: 4.860e-10, Loss_r: 2.323e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.213e-06, Loss_0: 7.254e-12, Loss_r: 2.213e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.112e-06, Loss_0: 2.725e-11, Loss_r: 2.112e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.017e-06, Loss_0: 9.832e-11, Loss_r: 2.017e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.928e-06, Loss_0: 2.599e-11, Loss_r: 1.928e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.846e-06, Loss_0: 2.342e-11, Loss_r: 1.846e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.769e-06, Loss_0: 3.951e-11, Loss_r: 1.769e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.697e-06, Loss_0: 3.653e-11, Loss_r: 1.697e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.629e-06, Loss_0: 3.379e-11, Loss_r: 1.629e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.566e-06, Loss_0: 2.528e-11, Loss_r: 1.566e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.507e-06, Loss_0: 2.117e-11, Loss_r: 1.507e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.452e-06, Loss_0: 2.191e-11, Loss_r: 1.452e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.400e-06, Loss_0: 1.982e-11, Loss_r: 1.400e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.351e-06, Loss_0: 1.839e-11, Loss_r: 1.351e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.305e-06, Loss_0: 1.837e-11, Loss_r: 1.305e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.261e-06, Loss_0: 1.829e-11, Loss_r: 1.261e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.220e-06, Loss_0: 1.934e-11, Loss_r: 1.220e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.182e-06, Loss_0: 1.232e-11, Loss_r: 1.182e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.146e-06, Loss_0: 1.593e-11, Loss_r: 1.146e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.111e-06, Loss_0: 1.156e-11, Loss_r: 1.111e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.079e-06, Loss_0: 1.472e-11, Loss_r: 1.079e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.048e-06, Loss_0: 1.371e-11, Loss_r: 1.048e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.019e-06, Loss_0: 1.182e-11, Loss_r: 1.019e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 9.911e-07, Loss_0: 1.487e-11, Loss_r: 9.911e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 9.649e-07, Loss_0: 4.103e-11, Loss_r: 9.648e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 9.405e-07, Loss_0: 5.792e-10, Loss_r: 9.399e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.018e-06, Loss_0: 7.149e-08, Loss_r: 9.466e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 4.531e-05, Loss_0: 3.053e-05, Loss_r: 1.478e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.106e-05, Loss_0: 6.325e-07, Loss_r: 1.043e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.098e-04, Loss_0: 1.904e-04, Loss_r: 1.943e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 7.895e-05, Loss_0: 7.330e-05, Loss_r: 5.649e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.155e-05, Loss_0: 1.827e-05, Loss_r: 3.273e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 5.540e-06, Loss_0: 2.848e-06, Loss_r: 2.692e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.883e-06, Loss_0: 2.447e-07, Loss_r: 2.638e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.245e-06, Loss_0: 2.881e-08, Loss_r: 2.216e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.938e-06, Loss_0: 1.567e-08, Loss_r: 1.923e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.794e-06, Loss_0: 2.741e-08, Loss_r: 1.766e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.691e-06, Loss_0: 2.190e-08, Loss_r: 1.669e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.597e-06, Loss_0: 1.390e-08, Loss_r: 1.583e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.514e-06, Loss_0: 1.829e-09, Loss_r: 1.513e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.448e-06, Loss_0: 6.727e-12, Loss_r: 1.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.389e-06, Loss_0: 6.296e-10, Loss_r: 1.388e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.334e-06, Loss_0: 3.410e-13, Loss_r: 1.334e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.283e-06, Loss_0: 6.717e-13, Loss_r: 1.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.237e-06, Loss_0: 3.155e-11, Loss_r: 1.237e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.194e-06, Loss_0: 6.600e-11, Loss_r: 1.193e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.153e-06, Loss_0: 2.406e-11, Loss_r: 1.153e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.116e-06, Loss_0: 1.383e-11, Loss_r: 1.116e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.081e-06, Loss_0: 1.647e-11, Loss_r: 1.081e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.049e-06, Loss_0: 1.484e-11, Loss_r: 1.049e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.019e-06, Loss_0: 1.342e-11, Loss_r: 1.019e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 9.902e-07, Loss_0: 1.185e-11, Loss_r: 9.902e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.636e-07, Loss_0: 1.169e-11, Loss_r: 9.636e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 9.385e-07, Loss_0: 1.178e-11, Loss_r: 9.385e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.149e-07, Loss_0: 1.222e-11, Loss_r: 9.149e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 8.927e-07, Loss_0: 1.240e-11, Loss_r: 8.927e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 8.716e-07, Loss_0: 9.423e-12, Loss_r: 8.716e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 8.517e-07, Loss_0: 1.092e-11, Loss_r: 8.517e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 8.328e-07, Loss_0: 9.543e-12, Loss_r: 8.328e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 8.148e-07, Loss_0: 7.599e-12, Loss_r: 8.148e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 7.977e-07, Loss_0: 1.000e-11, Loss_r: 7.977e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 7.814e-07, Loss_0: 8.438e-12, Loss_r: 7.814e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 7.658e-07, Loss_0: 6.018e-12, Loss_r: 7.658e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 7.508e-07, Loss_0: 5.512e-12, Loss_r: 7.508e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 7.365e-07, Loss_0: 2.486e-12, Loss_r: 7.365e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.227e-07, Loss_0: 1.364e-12, Loss_r: 7.227e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 7.106e-07, Loss_0: 5.408e-10, Loss_r: 7.100e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 8.693e-07, Loss_0: 9.978e-08, Loss_r: 7.695e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 6.348e-05, Loss_0: 3.685e-05, Loss_r: 2.663e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 6.120e-05, Loss_0: 1.306e-05, Loss_r: 4.814e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.516e-04, Loss_0: 1.442e-04, Loss_r: 7.355e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.621e-05, Loss_0: 1.402e-05, Loss_r: 2.193e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.008e-06, Loss_0: 9.456e-08, Loss_r: 1.914e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 2.483e-06, Loss_0: 4.257e-07, Loss_r: 2.057e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 2.632e-06, Loss_0: 7.217e-07, Loss_r: 1.911e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.974e-06, Loss_0: 4.147e-07, Loss_r: 1.559e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.383e-06, Loss_0: 6.271e-08, Loss_r: 1.320e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.232e-06, Loss_0: 2.780e-09, Loss_r: 1.229e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.206e-06, Loss_0: 1.879e-08, Loss_r: 1.187e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.135e-06, Loss_0: 2.691e-10, Loss_r: 1.134e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.092e-06, Loss_0: 3.086e-09, Loss_r: 1.089e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.050e-06, Loss_0: 7.425e-10, Loss_r: 1.050e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.015e-06, Loss_0: 5.293e-10, Loss_r: 1.015e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 9.822e-07, Loss_0: 1.605e-11, Loss_r: 9.822e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 9.522e-07, Loss_0: 4.872e-12, Loss_r: 9.522e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 9.244e-07, Loss_0: 1.078e-12, Loss_r: 9.244e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 8.986e-07, Loss_0: 1.401e-12, Loss_r: 8.986e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 8.744e-07, Loss_0: 5.729e-12, Loss_r: 8.744e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 8.519e-07, Loss_0: 1.052e-11, Loss_r: 8.519e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 8.307e-07, Loss_0: 1.127e-11, Loss_r: 8.307e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 8.107e-07, Loss_0: 1.077e-11, Loss_r: 8.107e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 7.919e-07, Loss_0: 9.246e-12, Loss_r: 7.919e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 7.741e-07, Loss_0: 1.161e-11, Loss_r: 7.741e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 7.571e-07, Loss_0: 9.705e-12, Loss_r: 7.571e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 7.411e-07, Loss_0: 9.150e-12, Loss_r: 7.410e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 7.257e-07, Loss_0: 8.432e-12, Loss_r: 7.257e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 7.110e-07, Loss_0: 1.037e-11, Loss_r: 7.110e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 6.969e-07, Loss_0: 5.859e-12, Loss_r: 6.969e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 6.833e-07, Loss_0: 9.786e-12, Loss_r: 6.833e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 6.703e-07, Loss_0: 7.209e-12, Loss_r: 6.703e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 6.577e-07, Loss_0: 7.065e-12, Loss_r: 6.577e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 6.455e-07, Loss_0: 1.067e-11, Loss_r: 6.455e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 6.337e-07, Loss_0: 3.681e-12, Loss_r: 6.337e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 6.222e-07, Loss_0: 1.578e-12, Loss_r: 6.222e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 6.110e-07, Loss_0: 3.356e-13, Loss_r: 6.110e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 6.007e-07, Loss_0: 2.219e-10, Loss_r: 6.004e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 6.413e-07, Loss_0: 3.119e-08, Loss_r: 6.101e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.415e-05, Loss_0: 8.326e-06, Loss_r: 5.821e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 7.864e-04, Loss_0: 5.980e-04, Loss_r: 1.884e-04, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 1.713e-04, Loss_0: 1.564e-04, Loss_r: 1.495e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 3.688e-05, Loss_0: 1.790e-05, Loss_r: 1.898e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 7.231e-06, Loss_0: 5.210e-07, Loss_r: 6.710e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 4.327e-06, Loss_0: 7.913e-08, Loss_r: 4.248e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.315e-06, Loss_0: 8.322e-07, Loss_r: 1.483e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 1.979e-06, Loss_0: 5.400e-07, Loss_r: 1.439e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 1.218e-06, Loss_0: 4.048e-08, Loss_r: 1.178e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 1.153e-06, Loss_0: 2.806e-10, Loss_r: 1.152e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 1.066e-06, Loss_0: 2.994e-09, Loss_r: 1.063e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 1.029e-06, Loss_0: 1.084e-09, Loss_r: 1.028e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 9.851e-07, Loss_0: 1.461e-09, Loss_r: 9.837e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 9.495e-07, Loss_0: 3.714e-10, Loss_r: 9.491e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 9.175e-07, Loss_0: 4.151e-10, Loss_r: 9.171e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 8.881e-07, Loss_0: 9.664e-12, Loss_r: 8.881e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 8.611e-07, Loss_0: 5.716e-11, Loss_r: 8.610e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 8.361e-07, Loss_0: 1.609e-11, Loss_r: 8.361e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 8.129e-07, Loss_0: 1.214e-11, Loss_r: 8.129e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 7.911e-07, Loss_0: 2.748e-11, Loss_r: 7.911e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 7.707e-07, Loss_0: 9.734e-12, Loss_r: 7.707e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 7.515e-07, Loss_0: 1.553e-11, Loss_r: 7.515e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 7.333e-07, Loss_0: 1.490e-11, Loss_r: 7.333e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 7.161e-07, Loss_0: 1.241e-11, Loss_r: 7.161e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 6.998e-07, Loss_0: 1.104e-11, Loss_r: 6.998e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 6.843e-07, Loss_0: 1.316e-11, Loss_r: 6.843e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 6.694e-07, Loss_0: 9.915e-12, Loss_r: 6.694e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 6.552e-07, Loss_0: 8.011e-12, Loss_r: 6.552e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 6.416e-07, Loss_0: 1.125e-11, Loss_r: 6.416e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 6.284e-07, Loss_0: 9.874e-12, Loss_r: 6.284e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 6.158e-07, Loss_0: 8.623e-12, Loss_r: 6.158e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 6.036e-07, Loss_0: 7.605e-12, Loss_r: 6.036e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 5.918e-07, Loss_0: 7.310e-12, Loss_r: 5.918e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 5.804e-07, Loss_0: 6.761e-12, Loss_r: 5.804e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 5.692e-07, Loss_0: 7.528e-12, Loss_r: 5.692e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 5.585e-07, Loss_0: 6.479e-12, Loss_r: 5.585e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 5.480e-07, Loss_0: 7.335e-12, Loss_r: 5.480e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 5.377e-07, Loss_0: 6.873e-12, Loss_r: 5.377e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 5.277e-07, Loss_0: 5.335e-12, Loss_r: 5.277e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 5.180e-07, Loss_0: 6.834e-12, Loss_r: 5.180e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 5.085e-07, Loss_0: 5.223e-12, Loss_r: 5.085e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 4.992e-07, Loss_0: 5.747e-12, Loss_r: 4.991e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 4.901e-07, Loss_0: 4.469e-12, Loss_r: 4.900e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 4.811e-07, Loss_0: 4.938e-12, Loss_r: 4.811e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 4.724e-07, Loss_0: 3.387e-12, Loss_r: 4.724e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 4.639e-07, Loss_0: 5.927e-12, Loss_r: 4.639e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 4.555e-07, Loss_0: 3.007e-12, Loss_r: 4.555e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 4.472e-07, Loss_0: 5.296e-12, Loss_r: 4.472e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2560, Loss: 4.391e-07, Loss_0: 5.296e-12, Loss_r: 4.391e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2570, Loss: 4.312e-07, Loss_0: 2.908e-12, Loss_r: 4.312e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2580, Loss: 4.234e-07, Loss_0: 5.543e-12, Loss_r: 4.234e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2590, Loss: 4.158e-07, Loss_0: 1.029e-11, Loss_r: 4.158e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2600, Loss: 4.083e-07, Loss_0: 1.767e-11, Loss_r: 4.083e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2610, Loss: 4.010e-07, Loss_0: 5.720e-11, Loss_r: 4.009e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2620, Loss: 3.948e-07, Loss_0: 1.091e-09, Loss_r: 3.937e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2630, Loss: 4.791e-07, Loss_0: 8.315e-08, Loss_r: 3.960e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 3.879e-07, Loss_0: 7.034e-09, Loss_r: 3.809e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 3.770e-07, Loss_0: 2.318e-09, Loss_r: 3.747e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 3.736e-07, Loss_0: 4.641e-09, Loss_r: 3.690e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 3.622e-07, Loss_0: 7.984e-11, Loss_r: 3.621e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 3.569e-07, Loss_0: 6.865e-10, Loss_r: 3.562e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 3.504e-07, Loss_0: 2.004e-12, Loss_r: 3.504e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 3.447e-07, Loss_0: 4.012e-11, Loss_r: 3.447e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 3.391e-07, Loss_0: 4.575e-11, Loss_r: 3.390e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 3.335e-07, Loss_0: 9.166e-13, Loss_r: 3.335e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 3.281e-07, Loss_0: 1.054e-11, Loss_r: 3.281e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 3.227e-07, Loss_0: 2.193e-14, Loss_r: 3.227e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2750, Loss: 3.175e-07, Loss_0: 6.844e-12, Loss_r: 3.175e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2760, Loss: 3.123e-07, Loss_0: 4.248e-12, Loss_r: 3.123e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2770, Loss: 3.072e-07, Loss_0: 2.097e-12, Loss_r: 3.072e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2780, Loss: 3.022e-07, Loss_0: 6.009e-12, Loss_r: 3.022e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2790, Loss: 2.973e-07, Loss_0: 4.347e-11, Loss_r: 2.972e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2800, Loss: 2.937e-07, Loss_0: 1.231e-09, Loss_r: 2.925e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2810, Loss: 4.260e-07, Loss_0: 1.170e-07, Loss_r: 3.090e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 2.921e-07, Loss_0: 7.530e-09, Loss_r: 2.846e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 2.855e-07, Loss_0: 5.005e-09, Loss_r: 2.805e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 2.835e-07, Loss_0: 6.657e-09, Loss_r: 2.768e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 2.712e-07, Loss_0: 6.545e-12, Loss_r: 2.712e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 2.684e-07, Loss_0: 1.027e-09, Loss_r: 2.674e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 2.634e-07, Loss_0: 1.555e-11, Loss_r: 2.633e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 2.596e-07, Loss_0: 3.912e-11, Loss_r: 2.595e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 2.558e-07, Loss_0: 7.123e-11, Loss_r: 2.557e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 2.520e-07, Loss_0: 1.088e-11, Loss_r: 2.520e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 2.483e-07, Loss_0: 1.409e-11, Loss_r: 2.483e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 2.447e-07, Loss_0: 2.195e-13, Loss_r: 2.447e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 2.412e-07, Loss_0: 3.735e-12, Loss_r: 2.412e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 2.377e-07, Loss_0: 3.088e-12, Loss_r: 2.377e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2950, Loss: 2.342e-07, Loss_0: 7.246e-13, Loss_r: 2.342e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2960, Loss: 2.308e-07, Loss_0: 3.610e-14, Loss_r: 2.308e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2970, Loss: 2.276e-07, Loss_0: 1.508e-11, Loss_r: 2.275e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2980, Loss: 2.255e-07, Loss_0: 9.497e-10, Loss_r: 2.246e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2990, Loss: 3.394e-07, Loss_0: 9.407e-08, Loss_r: 2.453e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 2.267e-07, Loss_0: 6.650e-09, Loss_r: 2.201e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 2.198e-07, Loss_0: 3.578e-09, Loss_r: 2.162e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 2.197e-07, Loss_0: 5.779e-09, Loss_r: 2.140e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 2.101e-07, Loss_0: 1.028e-10, Loss_r: 2.100e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 2.082e-07, Loss_0: 6.574e-10, Loss_r: 2.075e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 2.047e-07, Loss_0: 2.635e-12, Loss_r: 2.047e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 2.022e-07, Loss_0: 9.736e-11, Loss_r: 2.021e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 1.996e-07, Loss_0: 2.627e-11, Loss_r: 1.995e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 1.970e-07, Loss_0: 1.756e-11, Loss_r: 1.970e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 1.945e-07, Loss_0: 5.089e-13, Loss_r: 1.945e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 1.921e-07, Loss_0: 6.517e-12, Loss_r: 1.921e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 1.897e-07, Loss_0: 1.649e-13, Loss_r: 1.897e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3120, Loss: 1.873e-07, Loss_0: 1.574e-12, Loss_r: 1.873e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3130, Loss: 1.850e-07, Loss_0: 5.348e-12, Loss_r: 1.850e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3140, Loss: 1.827e-07, Loss_0: 1.019e-11, Loss_r: 1.827e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3150, Loss: 1.805e-07, Loss_0: 4.345e-11, Loss_r: 1.805e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3160, Loss: 1.793e-07, Loss_0: 8.572e-10, Loss_r: 1.784e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3170, Loss: 2.427e-07, Loss_0: 5.176e-08, Loss_r: 1.909e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 1.814e-07, Loss_0: 5.740e-09, Loss_r: 1.757e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 1.732e-07, Loss_0: 6.274e-10, Loss_r: 1.725e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 1.739e-07, Loss_0: 2.630e-09, Loss_r: 1.713e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 1.690e-07, Loss_0: 3.130e-10, Loss_r: 1.687e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 1.670e-07, Loss_0: 2.393e-10, Loss_r: 1.668e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 1.650e-07, Loss_0: 5.644e-11, Loss_r: 1.650e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 1.633e-07, Loss_0: 3.781e-11, Loss_r: 1.632e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 1.615e-07, Loss_0: 7.594e-12, Loss_r: 1.615e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 1.598e-07, Loss_0: 2.190e-12, Loss_r: 1.598e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 1.581e-07, Loss_0: 3.553e-15, Loss_r: 1.581e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 1.564e-07, Loss_0: 1.375e-12, Loss_r: 1.564e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 1.548e-07, Loss_0: 5.621e-13, Loss_r: 1.548e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 1.532e-07, Loss_0: 8.161e-15, Loss_r: 1.532e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3310, Loss: 1.516e-07, Loss_0: 3.452e-12, Loss_r: 1.516e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3320, Loss: 1.500e-07, Loss_0: 2.832e-12, Loss_r: 1.500e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3330, Loss: 1.485e-07, Loss_0: 2.168e-12, Loss_r: 1.485e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3340, Loss: 1.470e-07, Loss_0: 1.123e-11, Loss_r: 1.470e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3350, Loss: 1.458e-07, Loss_0: 2.405e-10, Loss_r: 1.455e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3360, Loss: 1.694e-07, Loss_0: 1.917e-08, Loss_r: 1.502e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 1.449e-07, Loss_0: 1.643e-09, Loss_r: 1.432e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 1.422e-07, Loss_0: 4.657e-10, Loss_r: 1.417e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 1.417e-07, Loss_0: 1.062e-09, Loss_r: 1.406e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 1.390e-07, Loss_0: 4.228e-11, Loss_r: 1.390e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 1.379e-07, Loss_0: 1.377e-10, Loss_r: 1.378e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 1.365e-07, Loss_0: 7.115e-12, Loss_r: 1.365e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 1.354e-07, Loss_0: 1.178e-11, Loss_r: 1.354e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 1.342e-07, Loss_0: 6.041e-12, Loss_r: 1.342e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 1.330e-07, Loss_0: 5.122e-14, Loss_r: 1.330e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 1.319e-07, Loss_0: 2.142e-13, Loss_r: 1.319e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 1.308e-07, Loss_0: 1.539e-12, Loss_r: 1.308e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 1.296e-07, Loss_0: 6.778e-13, Loss_r: 1.296e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 1.286e-07, Loss_0: 6.505e-13, Loss_r: 1.286e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 1.275e-07, Loss_0: 1.488e-12, Loss_r: 1.275e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 1.264e-07, Loss_0: 8.606e-14, Loss_r: 1.264e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 1.254e-07, Loss_0: 3.227e-13, Loss_r: 1.254e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 1.243e-07, Loss_0: 5.378e-14, Loss_r: 1.243e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 1.233e-07, Loss_0: 1.175e-12, Loss_r: 1.233e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 1.227e-07, Loss_0: 2.212e-10, Loss_r: 1.224e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3560, Loss: 1.847e-07, Loss_0: 4.600e-08, Loss_r: 1.387e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 1.216e-07, Loss_0: 8.076e-10, Loss_r: 1.208e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 1.260e-07, Loss_0: 4.702e-09, Loss_r: 1.213e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 1.216e-07, Loss_0: 2.131e-09, Loss_r: 1.195e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 1.182e-07, Loss_0: 1.964e-10, Loss_r: 1.180e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 1.175e-07, Loss_0: 2.432e-10, Loss_r: 1.172e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 1.164e-07, Loss_0: 1.140e-10, Loss_r: 1.163e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 1.155e-07, Loss_0: 2.448e-14, Loss_r: 1.155e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 1.147e-07, Loss_0: 3.972e-12, Loss_r: 1.147e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 1.139e-07, Loss_0: 7.615e-12, Loss_r: 1.139e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 1.131e-07, Loss_0: 3.905e-13, Loss_r: 1.131e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 1.124e-07, Loss_0: 1.098e-12, Loss_r: 1.124e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 1.116e-07, Loss_0: 1.334e-12, Loss_r: 1.116e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 1.109e-07, Loss_0: 1.147e-14, Loss_r: 1.109e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 1.101e-07, Loss_0: 3.665e-15, Loss_r: 1.101e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 1.094e-07, Loss_0: 1.112e-13, Loss_r: 1.094e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 1.087e-07, Loss_0: 4.309e-12, Loss_r: 1.087e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 1.082e-07, Loss_0: 1.546e-10, Loss_r: 1.081e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 1.229e-07, Loss_0: 1.112e-08, Loss_r: 1.118e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 1.084e-07, Loss_0: 1.215e-09, Loss_r: 1.072e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 1.062e-07, Loss_0: 1.437e-10, Loss_r: 1.061e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 1.063e-07, Loss_0: 6.164e-10, Loss_r: 1.057e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 1.050e-07, Loss_0: 1.014e-10, Loss_r: 1.049e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 1.043e-07, Loss_0: 2.970e-11, Loss_r: 1.043e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 1.037e-07, Loss_0: 1.077e-11, Loss_r: 1.037e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 1.031e-07, Loss_0: 1.089e-11, Loss_r: 1.031e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 1.025e-07, Loss_0: 3.249e-13, Loss_r: 1.025e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 1.020e-07, Loss_0: 7.664e-15, Loss_r: 1.020e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 1.014e-07, Loss_0: 8.726e-13, Loss_r: 1.014e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 1.009e-07, Loss_0: 3.131e-16, Loss_r: 1.009e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 1.003e-07, Loss_0: 2.151e-13, Loss_r: 1.003e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 9.977e-08, Loss_0: 6.898e-14, Loss_r: 9.977e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 9.925e-08, Loss_0: 1.274e-12, Loss_r: 9.925e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 9.871e-08, Loss_0: 1.849e-14, Loss_r: 9.871e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 9.820e-08, Loss_0: 3.356e-12, Loss_r: 9.819e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 9.771e-08, Loss_0: 1.966e-11, Loss_r: 9.769e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 9.754e-08, Loss_0: 2.568e-10, Loss_r: 9.728e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 1.128e-07, Loss_0: 1.144e-08, Loss_r: 1.014e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 9.877e-08, Loss_0: 1.777e-09, Loss_r: 9.699e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 9.579e-08, Loss_0: 5.451e-12, Loss_r: 9.578e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 9.592e-08, Loss_0: 4.292e-10, Loss_r: 9.549e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 9.518e-08, Loss_0: 2.042e-10, Loss_r: 9.498e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 9.450e-08, Loss_0: 1.523e-12, Loss_r: 9.450e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 9.409e-08, Loss_0: 2.519e-11, Loss_r: 9.406e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 9.364e-08, Loss_0: 1.507e-12, Loss_r: 9.364e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 9.322e-08, Loss_0: 2.965e-12, Loss_r: 9.322e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 9.281e-08, Loss_0: 2.032e-13, Loss_r: 9.281e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 9.241e-08, Loss_0: 9.968e-14, Loss_r: 9.241e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 9.200e-08, Loss_0: 2.049e-13, Loss_r: 9.200e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 9.160e-08, Loss_0: 6.512e-14, Loss_r: 9.160e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 9.120e-08, Loss_0: 2.057e-14, Loss_r: 9.120e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 9.081e-08, Loss_0: 4.996e-16, Loss_r: 9.081e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 9.042e-08, Loss_0: 2.040e-13, Loss_r: 9.042e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 9.004e-08, Loss_0: 1.583e-12, Loss_r: 9.003e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 8.966e-08, Loss_0: 4.477e-12, Loss_r: 8.965e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 8.928e-08, Loss_0: 9.167e-12, Loss_r: 8.927e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 8.903e-08, Loss_0: 9.070e-11, Loss_r: 8.894e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 9.508e-08, Loss_0: 4.697e-09, Loss_r: 9.038e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 8.909e-08, Loss_0: 6.419e-10, Loss_r: 8.845e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 8.789e-08, Loss_0: 1.219e-11, Loss_r: 8.788e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 8.784e-08, Loss_0: 1.926e-10, Loss_r: 8.765e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 8.731e-08, Loss_0: 5.558e-11, Loss_r: 8.725e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 8.691e-08, Loss_0: 9.394e-12, Loss_r: 8.691e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 8.660e-08, Loss_0: 1.576e-11, Loss_r: 8.659e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 8.627e-08, Loss_0: 4.584e-13, Loss_r: 8.627e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 8.596e-08, Loss_0: 3.767e-13, Loss_r: 8.595e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 8.565e-08, Loss_0: 1.249e-12, Loss_r: 8.565e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 8.534e-08, Loss_0: 1.804e-13, Loss_r: 8.534e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 8.504e-08, Loss_0: 8.296e-13, Loss_r: 8.504e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 8.472e-08, Loss_0: 2.168e-13, Loss_r: 8.472e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 8.444e-08, Loss_0: 1.056e-13, Loss_r: 8.444e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 8.413e-08, Loss_0: 5.684e-14, Loss_r: 8.413e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 8.384e-08, Loss_0: 1.319e-15, Loss_r: 8.384e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 8.354e-08, Loss_0: 9.709e-13, Loss_r: 8.354e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 8.325e-08, Loss_0: 5.508e-12, Loss_r: 8.325e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 8.302e-08, Loss_0: 4.543e-11, Loss_r: 8.297e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 8.431e-08, Loss_0: 1.180e-09, Loss_r: 8.313e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 1.811e-07, Loss_0: 6.985e-08, Loss_r: 1.113e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 9.483e-08, Loss_0: 9.016e-09, Loss_r: 8.582e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 8.238e-08, Loss_0: 3.277e-10, Loss_r: 8.205e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 8.618e-08, Loss_0: 3.159e-09, Loss_r: 8.302e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 8.261e-08, Loss_0: 8.322e-10, Loss_r: 8.178e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 8.128e-08, Loss_0: 1.079e-10, Loss_r: 8.118e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 8.111e-08, Loss_0: 1.566e-10, Loss_r: 8.096e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 8.068e-08, Loss_0: 2.170e-11, Loss_r: 8.066e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 8.042e-08, Loss_0: 4.411e-12, Loss_r: 8.041e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 8.019e-08, Loss_0: 1.138e-11, Loss_r: 8.018e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 7.994e-08, Loss_0: 6.641e-13, Loss_r: 7.994e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 7.967e-08, Loss_0: 8.953e-13, Loss_r: 7.967e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 7.944e-08, Loss_0: 1.908e-13, Loss_r: 7.944e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 7.921e-08, Loss_0: 3.164e-14, Loss_r: 7.921e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 7.897e-08, Loss_0: 6.076e-13, Loss_r: 7.897e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 7.874e-08, Loss_0: 2.345e-15, Loss_r: 7.874e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 7.851e-08, Loss_0: 1.038e-13, Loss_r: 7.851e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 7.828e-08, Loss_0: 1.050e-12, Loss_r: 7.828e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 7.804e-08, Loss_0: 2.699e-12, Loss_r: 7.804e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 7.785e-08, Loss_0: 3.148e-11, Loss_r: 7.782e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4530, Loss: 7.941e-08, Loss_0: 1.313e-09, Loss_r: 7.809e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4540, Loss: 2.966e-07, Loss_0: 1.547e-07, Loss_r: 1.419e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 9.004e-08, Loss_0: 9.148e-09, Loss_r: 8.089e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 8.658e-08, Loss_0: 6.732e-09, Loss_r: 7.985e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 8.983e-08, Loss_0: 9.126e-09, Loss_r: 8.070e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 7.678e-08, Loss_0: 1.355e-10, Loss_r: 7.665e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 7.801e-08, Loss_0: 1.174e-09, Loss_r: 7.684e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 7.620e-08, Loss_0: 6.617e-12, Loss_r: 7.619e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 7.620e-08, Loss_0: 1.500e-10, Loss_r: 7.605e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 7.584e-08, Loss_0: 4.460e-11, Loss_r: 7.580e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 7.559e-08, Loss_0: 1.002e-12, Loss_r: 7.559e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 7.539e-08, Loss_0: 5.038e-14, Loss_r: 7.539e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 7.519e-08, Loss_0: 1.588e-12, Loss_r: 7.519e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 7.500e-08, Loss_0: 6.559e-14, Loss_r: 7.500e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 7.481e-08, Loss_0: 9.930e-15, Loss_r: 7.481e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 7.461e-08, Loss_0: 2.473e-13, Loss_r: 7.461e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 7.443e-08, Loss_0: 7.806e-16, Loss_r: 7.443e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 7.424e-08, Loss_0: 1.749e-13, Loss_r: 7.424e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 7.404e-08, Loss_0: 3.248e-12, Loss_r: 7.404e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 7.389e-08, Loss_0: 3.541e-11, Loss_r: 7.386e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 7.549e-08, Loss_0: 1.325e-09, Loss_r: 7.416e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4740, Loss: 2.511e-07, Loss_0: 1.255e-07, Loss_r: 1.256e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 8.799e-08, Loss_0: 1.040e-08, Loss_r: 7.759e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 7.753e-08, Loss_0: 3.067e-09, Loss_r: 7.447e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 8.327e-08, Loss_0: 7.204e-09, Loss_r: 7.606e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 7.362e-08, Loss_0: 5.541e-10, Loss_r: 7.307e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 7.354e-08, Loss_0: 6.433e-10, Loss_r: 7.290e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 7.261e-08, Loss_0: 1.051e-10, Loss_r: 7.250e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 7.247e-08, Loss_0: 1.126e-10, Loss_r: 7.236e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 7.215e-08, Loss_0: 5.774e-12, Loss_r: 7.214e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 7.198e-08, Loss_0: 6.234e-12, Loss_r: 7.198e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 7.182e-08, Loss_0: 4.399e-12, Loss_r: 7.181e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 7.165e-08, Loss_0: 3.111e-12, Loss_r: 7.164e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 7.148e-08, Loss_0: 2.292e-13, Loss_r: 7.148e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 7.131e-08, Loss_0: 1.672e-13, Loss_r: 7.131e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 7.114e-08, Loss_0: 1.402e-13, Loss_r: 7.114e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 7.099e-08, Loss_0: 2.539e-13, Loss_r: 7.098e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 7.082e-08, Loss_0: 1.423e-13, Loss_r: 7.082e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 7.066e-08, Loss_0: 3.334e-13, Loss_r: 7.066e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 7.050e-08, Loss_0: 6.479e-12, Loss_r: 7.049e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 7.074e-08, Loss_0: 2.842e-10, Loss_r: 7.045e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 9.936e-08, Loss_0: 2.054e-08, Loss_r: 7.882e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 7.338e-08, Loss_0: 2.357e-09, Loss_r: 7.103e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 7.013e-08, Loss_0: 1.893e-10, Loss_r: 6.994e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 7.118e-08, Loss_0: 1.047e-09, Loss_r: 7.014e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 6.988e-08, Loss_0: 2.118e-10, Loss_r: 6.966e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 6.951e-08, Loss_0: 4.313e-11, Loss_r: 6.946e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "Training time: 18.8770\n",
            "[1, 64, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.491e-02, Loss_0: 3.408e-04, Loss_r: 3.457e-02, Time: 0.26, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.208e-02, Loss_0: 1.205e-04, Loss_r: 3.196e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.150e-02, Loss_0: 4.535e-05, Loss_r: 3.146e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.006e-02, Loss_0: 2.082e-05, Loss_r: 3.004e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 2.515e-02, Loss_0: 3.914e-05, Loss_r: 2.511e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.587e-02, Loss_0: 3.913e-05, Loss_r: 1.583e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.419e-03, Loss_0: 5.197e-04, Loss_r: 4.899e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 9.695e-04, Loss_0: 5.866e-05, Loss_r: 9.108e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 2.644e-04, Loss_0: 1.207e-05, Loss_r: 2.523e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.285e-04, Loss_0: 1.785e-06, Loss_r: 1.267e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.931e-05, Loss_0: 4.248e-06, Loss_r: 5.506e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.814e-05, Loss_0: 5.118e-06, Loss_r: 3.302e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.916e-05, Loss_0: 2.724e-06, Loss_r: 1.643e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.345e-05, Loss_0: 1.865e-07, Loss_r: 1.327e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.305e-05, Loss_0: 2.717e-06, Loss_r: 1.033e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.772e-04, Loss_0: 1.646e-04, Loss_r: 1.264e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.411e-05, Loss_0: 6.619e-06, Loss_r: 7.491e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.077e-05, Loss_0: 4.025e-06, Loss_r: 6.749e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 6.015e-06, Loss_0: 4.044e-08, Loss_r: 5.974e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 9.695e-06, Loss_0: 4.275e-06, Loss_r: 5.420e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.604e-06, Loss_0: 5.892e-07, Loss_r: 5.014e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 7.987e-06, Loss_0: 3.360e-06, Loss_r: 4.628e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 8.448e-05, Loss_0: 7.921e-05, Loss_r: 5.268e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 8.020e-05, Loss_0: 7.440e-05, Loss_r: 5.794e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 6.901e-06, Loss_0: 2.749e-06, Loss_r: 4.152e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.340e-05, Loss_0: 9.447e-06, Loss_r: 3.957e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 3.826e-06, Loss_0: 5.912e-08, Loss_r: 3.767e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 3.762e-06, Loss_0: 1.769e-07, Loss_r: 3.585e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 7.703e-06, Loss_0: 4.171e-06, Loss_r: 3.532e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.188e-04, Loss_0: 1.134e-04, Loss_r: 5.426e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 7.162e-05, Loss_0: 6.774e-05, Loss_r: 3.885e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 3.260e-06, Loss_0: 6.772e-08, Loss_r: 3.192e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.318e-05, Loss_0: 9.860e-06, Loss_r: 3.322e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 6.352e-06, Loss_0: 3.368e-06, Loss_r: 2.985e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.188e-06, Loss_0: 2.291e-06, Loss_r: 2.898e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 2.773e-06, Loss_0: 5.105e-08, Loss_r: 2.722e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 9.510e-06, Loss_0: 6.842e-06, Loss_r: 2.668e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.557e-04, Loss_0: 2.509e-04, Loss_r: 4.819e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 8.808e-05, Loss_0: 8.399e-05, Loss_r: 4.092e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.362e-05, Loss_0: 3.050e-05, Loss_r: 3.117e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.353e-05, Loss_0: 1.093e-05, Loss_r: 2.601e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 6.774e-06, Loss_0: 4.258e-06, Loss_r: 2.516e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.476e-06, Loss_0: 1.086e-06, Loss_r: 2.390e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.318e-06, Loss_0: 3.516e-08, Loss_r: 2.283e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.388e-06, Loss_0: 1.945e-07, Loss_r: 2.193e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.211e-06, Loss_0: 1.189e-07, Loss_r: 2.092e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.065e-06, Loss_0: 5.229e-08, Loss_r: 2.013e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.937e-06, Loss_0: 5.775e-09, Loss_r: 1.931e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.856e-06, Loss_0: 6.125e-10, Loss_r: 1.855e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.876e-06, Loss_0: 8.787e-08, Loss_r: 1.788e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 7.520e-05, Loss_0: 7.225e-05, Loss_r: 2.946e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.348e-04, Loss_0: 3.295e-04, Loss_r: 5.326e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.305e-05, Loss_0: 5.051e-05, Loss_r: 2.539e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.970e-06, Loss_0: 1.810e-06, Loss_r: 2.160e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.903e-06, Loss_0: 7.803e-07, Loss_r: 2.123e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 4.749e-06, Loss_0: 2.659e-06, Loss_r: 2.090e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 3.793e-06, Loss_0: 1.832e-06, Loss_r: 1.961e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.990e-06, Loss_0: 1.095e-07, Loss_r: 1.881e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.952e-06, Loss_0: 1.547e-07, Loss_r: 1.797e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.739e-06, Loss_0: 2.251e-08, Loss_r: 1.716e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.682e-06, Loss_0: 3.375e-08, Loss_r: 1.648e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.592e-06, Loss_0: 5.196e-09, Loss_r: 1.587e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.530e-06, Loss_0: 2.869e-10, Loss_r: 1.530e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.476e-06, Loss_0: 5.121e-11, Loss_r: 1.476e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.425e-06, Loss_0: 1.414e-10, Loss_r: 1.425e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.377e-06, Loss_0: 2.657e-10, Loss_r: 1.377e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.332e-06, Loss_0: 1.633e-10, Loss_r: 1.332e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.289e-06, Loss_0: 1.321e-12, Loss_r: 1.289e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.249e-06, Loss_0: 1.061e-12, Loss_r: 1.249e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.210e-06, Loss_0: 1.808e-11, Loss_r: 1.210e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.174e-06, Loss_0: 6.810e-12, Loss_r: 1.174e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.139e-06, Loss_0: 1.049e-11, Loss_r: 1.139e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.107e-06, Loss_0: 2.804e-12, Loss_r: 1.107e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.076e-06, Loss_0: 2.978e-13, Loss_r: 1.076e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.047e-06, Loss_0: 5.564e-10, Loss_r: 1.046e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.202e-06, Loss_0: 1.785e-07, Loss_r: 1.023e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.425e-04, Loss_0: 1.391e-04, Loss_r: 3.360e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.548e-04, Loss_0: 2.525e-04, Loss_r: 2.345e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 8.107e-05, Loss_0: 7.897e-05, Loss_r: 2.099e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 7.742e-06, Loss_0: 5.954e-06, Loss_r: 1.788e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.460e-06, Loss_0: 1.313e-08, Loss_r: 1.447e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.661e-06, Loss_0: 1.200e-06, Loss_r: 1.462e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.855e-06, Loss_0: 1.535e-06, Loss_r: 1.320e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.761e-06, Loss_0: 5.026e-07, Loss_r: 1.258e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.196e-06, Loss_0: 9.720e-11, Loss_r: 1.196e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.227e-06, Loss_0: 7.916e-08, Loss_r: 1.148e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.112e-06, Loss_0: 2.480e-09, Loss_r: 1.110e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.083e-06, Loss_0: 9.916e-09, Loss_r: 1.073e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.039e-06, Loss_0: 3.012e-10, Loss_r: 1.039e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.007e-06, Loss_0: 4.894e-10, Loss_r: 1.007e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 9.778e-07, Loss_0: 5.449e-10, Loss_r: 9.773e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 9.498e-07, Loss_0: 2.523e-10, Loss_r: 9.495e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 9.234e-07, Loss_0: 9.465e-11, Loss_r: 9.233e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 8.984e-07, Loss_0: 5.367e-11, Loss_r: 8.984e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 8.748e-07, Loss_0: 2.527e-11, Loss_r: 8.748e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 8.522e-07, Loss_0: 4.954e-12, Loss_r: 8.522e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 8.309e-07, Loss_0: 3.581e-12, Loss_r: 8.309e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 8.105e-07, Loss_0: 5.702e-12, Loss_r: 8.105e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 7.910e-07, Loss_0: 9.827e-12, Loss_r: 7.910e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 7.725e-07, Loss_0: 3.421e-13, Loss_r: 7.725e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 7.546e-07, Loss_0: 9.411e-12, Loss_r: 7.546e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 7.376e-07, Loss_0: 1.088e-12, Loss_r: 7.376e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 7.212e-07, Loss_0: 1.122e-11, Loss_r: 7.212e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 7.055e-07, Loss_0: 1.377e-14, Loss_r: 7.055e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 6.904e-07, Loss_0: 1.195e-11, Loss_r: 6.903e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 6.759e-07, Loss_0: 1.697e-10, Loss_r: 6.758e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 6.704e-07, Loss_0: 8.239e-09, Loss_r: 6.621e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.118e-06, Loss_0: 1.444e-06, Loss_r: 6.739e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 4.539e-04, Loss_0: 4.447e-04, Loss_r: 9.192e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 8.080e-07, Loss_0: 2.099e-09, Loss_r: 8.059e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 6.807e-05, Loss_0: 6.570e-05, Loss_r: 2.375e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.437e-06, Loss_0: 1.219e-06, Loss_r: 1.218e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.000e-05, Loss_0: 9.080e-06, Loss_r: 9.243e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 8.978e-07, Loss_0: 2.612e-08, Loss_r: 8.717e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.973e-06, Loss_0: 1.132e-06, Loss_r: 8.412e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 7.936e-07, Loss_0: 9.663e-09, Loss_r: 7.840e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 8.604e-07, Loss_0: 1.038e-07, Loss_r: 7.566e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 7.773e-07, Loss_0: 4.650e-08, Loss_r: 7.308e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 7.124e-07, Loss_0: 4.246e-09, Loss_r: 7.081e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 6.881e-07, Loss_0: 8.060e-13, Loss_r: 6.881e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 6.695e-07, Loss_0: 1.949e-10, Loss_r: 6.693e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 6.518e-07, Loss_0: 8.456e-11, Loss_r: 6.517e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 6.350e-07, Loss_0: 2.176e-12, Loss_r: 6.350e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 6.191e-07, Loss_0: 2.565e-11, Loss_r: 6.191e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 6.041e-07, Loss_0: 7.332e-11, Loss_r: 6.040e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 5.897e-07, Loss_0: 3.787e-11, Loss_r: 5.897e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 5.760e-07, Loss_0: 2.742e-12, Loss_r: 5.760e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 5.628e-07, Loss_0: 2.186e-13, Loss_r: 5.628e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 5.502e-07, Loss_0: 2.742e-12, Loss_r: 5.502e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 5.381e-07, Loss_0: 1.257e-12, Loss_r: 5.381e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 5.265e-07, Loss_0: 7.994e-13, Loss_r: 5.265e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 5.152e-07, Loss_0: 4.407e-12, Loss_r: 5.152e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 5.044e-07, Loss_0: 7.806e-14, Loss_r: 5.044e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 4.940e-07, Loss_0: 2.122e-12, Loss_r: 4.940e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 4.839e-07, Loss_0: 2.590e-12, Loss_r: 4.839e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 4.741e-07, Loss_0: 7.538e-12, Loss_r: 4.741e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 4.646e-07, Loss_0: 2.425e-11, Loss_r: 4.646e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 4.558e-07, Loss_0: 4.823e-10, Loss_r: 4.553e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 5.011e-07, Loss_0: 5.436e-08, Loss_r: 4.467e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.799e-05, Loss_0: 1.731e-05, Loss_r: 6.823e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.360e-04, Loss_0: 1.342e-04, Loss_r: 1.747e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 2.336e-06, Loss_0: 1.359e-06, Loss_r: 9.765e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 4.922e-05, Loss_0: 4.747e-05, Loss_r: 1.753e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.868e-06, Loss_0: 8.133e-06, Loss_r: 7.356e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.299e-06, Loss_0: 6.333e-07, Loss_r: 6.662e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.831e-06, Loss_0: 2.147e-06, Loss_r: 6.834e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 6.993e-07, Loss_0: 1.078e-07, Loss_r: 5.915e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 7.867e-07, Loss_0: 2.195e-07, Loss_r: 5.672e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 5.652e-07, Loss_0: 1.737e-08, Loss_r: 5.478e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 5.637e-07, Loss_0: 3.320e-08, Loss_r: 5.305e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 5.139e-07, Loss_0: 3.413e-10, Loss_r: 5.136e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 5.012e-07, Loss_0: 2.472e-09, Loss_r: 4.987e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 4.866e-07, Loss_0: 1.689e-09, Loss_r: 4.849e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 4.725e-07, Loss_0: 3.760e-10, Loss_r: 4.721e-07, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 4.600e-07, Loss_0: 5.518e-11, Loss_r: 4.600e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.485e-07, Loss_0: 1.437e-11, Loss_r: 4.485e-07, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.376e-07, Loss_0: 1.083e-11, Loss_r: 4.376e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 4.273e-07, Loss_0: 6.781e-12, Loss_r: 4.273e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 4.175e-07, Loss_0: 6.636e-12, Loss_r: 4.175e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 4.081e-07, Loss_0: 6.927e-12, Loss_r: 4.081e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 3.991e-07, Loss_0: 1.390e-12, Loss_r: 3.991e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.905e-07, Loss_0: 1.416e-13, Loss_r: 3.905e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.823e-07, Loss_0: 1.346e-13, Loss_r: 3.823e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.744e-07, Loss_0: 7.405e-13, Loss_r: 3.744e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 3.668e-07, Loss_0: 6.686e-13, Loss_r: 3.668e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 3.594e-07, Loss_0: 4.547e-13, Loss_r: 3.594e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 3.523e-07, Loss_0: 1.784e-12, Loss_r: 3.523e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 3.455e-07, Loss_0: 4.984e-13, Loss_r: 3.455e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 3.388e-07, Loss_0: 1.530e-12, Loss_r: 3.388e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 3.324e-07, Loss_0: 5.663e-13, Loss_r: 3.324e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 3.261e-07, Loss_0: 6.588e-12, Loss_r: 3.261e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.201e-07, Loss_0: 3.992e-11, Loss_r: 3.201e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 3.142e-07, Loss_0: 2.365e-11, Loss_r: 3.142e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 3.085e-07, Loss_0: 2.751e-11, Loss_r: 3.085e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.037e-07, Loss_0: 8.244e-10, Loss_r: 3.029e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 4.045e-07, Loss_0: 1.060e-07, Loss_r: 2.986e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 3.199e-05, Loss_0: 3.124e-05, Loss_r: 7.503e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.887e-06, Loss_0: 1.410e-06, Loss_r: 4.771e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 3.573e-05, Loss_0: 3.507e-05, Loss_r: 6.526e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 3.290e-05, Loss_0: 3.215e-05, Loss_r: 7.449e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.987e-06, Loss_0: 8.266e-06, Loss_r: 7.207e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.245e-06, Loss_0: 7.745e-07, Loss_r: 4.703e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.272e-06, Loss_0: 1.765e-06, Loss_r: 5.078e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.248e-07, Loss_0: 4.731e-10, Loss_r: 4.243e-07, Time: 0.11, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 6.396e-07, Loss_0: 2.334e-07, Loss_r: 4.062e-07, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.983e-07, Loss_0: 5.641e-09, Loss_r: 3.927e-07, Time: 0.11, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 4.012e-07, Loss_0: 1.974e-08, Loss_r: 3.814e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 3.792e-07, Loss_0: 8.429e-09, Loss_r: 3.707e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 3.608e-07, Loss_0: 1.485e-10, Loss_r: 3.606e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 3.517e-07, Loss_0: 3.550e-10, Loss_r: 3.514e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 3.433e-07, Loss_0: 4.209e-10, Loss_r: 3.429e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.351e-07, Loss_0: 2.093e-10, Loss_r: 3.349e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 3.275e-07, Loss_0: 8.768e-11, Loss_r: 3.274e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.204e-07, Loss_0: 3.980e-11, Loss_r: 3.203e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.137e-07, Loss_0: 2.322e-11, Loss_r: 3.136e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 3.072e-07, Loss_0: 1.208e-11, Loss_r: 3.072e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.012e-07, Loss_0: 5.275e-12, Loss_r: 3.012e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 2.954e-07, Loss_0: 2.779e-13, Loss_r: 2.954e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 2.898e-07, Loss_0: 1.794e-12, Loss_r: 2.898e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 2.845e-07, Loss_0: 3.539e-14, Loss_r: 2.845e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.794e-07, Loss_0: 1.720e-12, Loss_r: 2.794e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.744e-07, Loss_0: 1.884e-13, Loss_r: 2.744e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.697e-07, Loss_0: 1.623e-12, Loss_r: 2.697e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.650e-07, Loss_0: 2.068e-12, Loss_r: 2.650e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.606e-07, Loss_0: 2.276e-14, Loss_r: 2.606e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.563e-07, Loss_0: 3.066e-12, Loss_r: 2.563e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.521e-07, Loss_0: 4.352e-14, Loss_r: 2.521e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.481e-07, Loss_0: 9.286e-12, Loss_r: 2.481e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.441e-07, Loss_0: 6.073e-12, Loss_r: 2.441e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 2.403e-07, Loss_0: 3.143e-13, Loss_r: 2.403e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 2.366e-07, Loss_0: 1.252e-11, Loss_r: 2.366e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 2.332e-07, Loss_0: 2.271e-10, Loss_r: 2.330e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 2.405e-07, Loss_0: 1.073e-08, Loss_r: 2.298e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.392e-06, Loss_0: 1.145e-06, Loss_r: 2.471e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.965e-07, Loss_0: 7.180e-08, Loss_r: 2.247e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 2.724e-07, Loss_0: 5.165e-08, Loss_r: 2.207e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 2.873e-07, Loss_0: 6.912e-08, Loss_r: 2.182e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 2.148e-07, Loss_0: 1.993e-10, Loss_r: 2.146e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.214e-07, Loss_0: 9.183e-09, Loss_r: 2.122e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 2.094e-07, Loss_0: 2.368e-10, Loss_r: 2.092e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 2.072e-07, Loss_0: 6.270e-10, Loss_r: 2.066e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 2.045e-07, Loss_0: 4.292e-10, Loss_r: 2.041e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 2.017e-07, Loss_0: 1.825e-10, Loss_r: 2.015e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.991e-07, Loss_0: 4.091e-11, Loss_r: 1.991e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.967e-07, Loss_0: 1.745e-11, Loss_r: 1.967e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.943e-07, Loss_0: 3.359e-12, Loss_r: 1.943e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.921e-07, Loss_0: 8.154e-12, Loss_r: 1.920e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.898e-07, Loss_0: 1.864e-11, Loss_r: 1.898e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.876e-07, Loss_0: 5.855e-11, Loss_r: 1.876e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.861e-07, Loss_0: 6.288e-10, Loss_r: 1.854e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 2.045e-07, Loss_0: 2.099e-08, Loss_r: 1.835e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 2.141e-06, Loss_0: 1.928e-06, Loss_r: 2.129e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 3.366e-07, Loss_0: 1.547e-07, Loss_r: 1.819e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 2.337e-07, Loss_0: 5.454e-08, Loss_r: 1.792e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 2.923e-07, Loss_0: 1.137e-07, Loss_r: 1.787e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 1.795e-07, Loss_0: 4.830e-09, Loss_r: 1.747e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 1.860e-07, Loss_0: 1.309e-08, Loss_r: 1.729e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 1.714e-07, Loss_0: 2.728e-10, Loss_r: 1.711e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 1.715e-07, Loss_0: 1.948e-09, Loss_r: 1.696e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 1.684e-07, Loss_0: 5.361e-10, Loss_r: 1.679e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.664e-07, Loss_0: 3.640e-11, Loss_r: 1.663e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.648e-07, Loss_0: 3.818e-12, Loss_r: 1.648e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.633e-07, Loss_0: 1.595e-12, Loss_r: 1.633e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.618e-07, Loss_0: 2.219e-11, Loss_r: 1.617e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.603e-07, Loss_0: 9.049e-12, Loss_r: 1.603e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 1.588e-07, Loss_0: 6.550e-12, Loss_r: 1.588e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 1.574e-07, Loss_0: 3.272e-11, Loss_r: 1.574e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 1.561e-07, Loss_0: 1.339e-10, Loss_r: 1.560e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 1.565e-07, Loss_0: 1.888e-09, Loss_r: 1.546e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.451e-07, Loss_0: 9.045e-08, Loss_r: 1.546e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 1.653e-07, Loss_0: 1.307e-08, Loss_r: 1.522e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 1.511e-07, Loss_0: 2.030e-10, Loss_r: 1.509e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.537e-07, Loss_0: 3.843e-09, Loss_r: 1.499e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 1.498e-07, Loss_0: 1.155e-09, Loss_r: 1.486e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.476e-07, Loss_0: 1.485e-10, Loss_r: 1.474e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 1.465e-07, Loss_0: 2.003e-10, Loss_r: 1.463e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 1.453e-07, Loss_0: 3.959e-11, Loss_r: 1.452e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.441e-07, Loss_0: 5.304e-13, Loss_r: 1.441e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.431e-07, Loss_0: 1.435e-11, Loss_r: 1.430e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.420e-07, Loss_0: 1.019e-12, Loss_r: 1.420e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.409e-07, Loss_0: 2.041e-12, Loss_r: 1.409e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.399e-07, Loss_0: 5.196e-13, Loss_r: 1.399e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.389e-07, Loss_0: 2.041e-12, Loss_r: 1.389e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.379e-07, Loss_0: 5.975e-13, Loss_r: 1.379e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.369e-07, Loss_0: 6.326e-13, Loss_r: 1.369e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.359e-07, Loss_0: 4.572e-13, Loss_r: 1.359e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.349e-07, Loss_0: 1.667e-11, Loss_r: 1.349e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.344e-07, Loss_0: 4.717e-10, Loss_r: 1.340e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.562e-07, Loss_0: 2.289e-08, Loss_r: 1.333e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.352e-07, Loss_0: 3.036e-09, Loss_r: 1.322e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.314e-07, Loss_0: 8.563e-11, Loss_r: 1.313e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 1.316e-07, Loss_0: 1.034e-09, Loss_r: 1.305e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.300e-07, Loss_0: 2.741e-10, Loss_r: 1.297e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 1.289e-07, Loss_0: 3.321e-11, Loss_r: 1.288e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.281e-07, Loss_0: 4.511e-11, Loss_r: 1.280e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.273e-07, Loss_0: 1.555e-11, Loss_r: 1.272e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.265e-07, Loss_0: 2.220e-16, Loss_r: 1.265e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.257e-07, Loss_0: 6.898e-12, Loss_r: 1.257e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.249e-07, Loss_0: 5.169e-13, Loss_r: 1.249e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.241e-07, Loss_0: 6.840e-13, Loss_r: 1.241e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.233e-07, Loss_0: 1.050e-12, Loss_r: 1.233e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.226e-07, Loss_0: 2.220e-12, Loss_r: 1.226e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.218e-07, Loss_0: 2.596e-12, Loss_r: 1.218e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.211e-07, Loss_0: 3.778e-13, Loss_r: 1.211e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.204e-07, Loss_0: 1.911e-11, Loss_r: 1.203e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.197e-07, Loss_0: 1.312e-10, Loss_r: 1.196e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.214e-07, Loss_0: 2.452e-09, Loss_r: 1.190e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.284e-07, Loss_0: 1.078e-07, Loss_r: 1.205e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 1.350e-07, Loss_0: 1.710e-08, Loss_r: 1.179e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 1.169e-07, Loss_0: 2.844e-11, Loss_r: 1.169e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 1.201e-07, Loss_0: 3.856e-09, Loss_r: 1.163e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 1.175e-07, Loss_0: 1.904e-09, Loss_r: 1.156e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.150e-07, Loss_0: 1.225e-11, Loss_r: 1.150e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.146e-07, Loss_0: 2.792e-10, Loss_r: 1.144e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.137e-07, Loss_0: 1.919e-11, Loss_r: 1.137e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.131e-07, Loss_0: 4.058e-11, Loss_r: 1.131e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.125e-07, Loss_0: 2.008e-11, Loss_r: 1.125e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.119e-07, Loss_0: 6.137e-12, Loss_r: 1.119e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.112e-07, Loss_0: 4.430e-14, Loss_r: 1.112e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.106e-07, Loss_0: 2.620e-12, Loss_r: 1.106e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.100e-07, Loss_0: 3.553e-13, Loss_r: 1.100e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.094e-07, Loss_0: 2.149e-12, Loss_r: 1.094e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.088e-07, Loss_0: 1.470e-12, Loss_r: 1.088e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.082e-07, Loss_0: 2.823e-12, Loss_r: 1.082e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.076e-07, Loss_0: 1.931e-11, Loss_r: 1.076e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.072e-07, Loss_0: 1.183e-10, Loss_r: 1.070e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.088e-07, Loss_0: 2.331e-09, Loss_r: 1.065e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 2.565e-07, Loss_0: 1.479e-07, Loss_r: 1.086e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.229e-07, Loss_0: 1.732e-08, Loss_r: 1.056e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.062e-07, Loss_0: 1.297e-09, Loss_r: 1.049e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.117e-07, Loss_0: 7.195e-09, Loss_r: 1.045e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.053e-07, Loss_0: 1.404e-09, Loss_r: 1.039e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.036e-07, Loss_0: 3.453e-10, Loss_r: 1.033e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.030e-07, Loss_0: 2.764e-10, Loss_r: 1.028e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.023e-07, Loss_0: 7.984e-11, Loss_r: 1.023e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.017e-07, Loss_0: 3.238e-12, Loss_r: 1.017e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.012e-07, Loss_0: 3.110e-11, Loss_r: 1.012e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.007e-07, Loss_0: 8.541e-12, Loss_r: 1.007e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.002e-07, Loss_0: 4.701e-12, Loss_r: 1.002e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 9.970e-08, Loss_0: 1.835e-15, Loss_r: 9.970e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 9.920e-08, Loss_0: 1.151e-12, Loss_r: 9.920e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 9.868e-08, Loss_0: 3.695e-12, Loss_r: 9.867e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 9.819e-08, Loss_0: 3.046e-12, Loss_r: 9.819e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 9.769e-08, Loss_0: 1.241e-11, Loss_r: 9.768e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 9.725e-08, Loss_0: 6.091e-11, Loss_r: 9.719e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 9.730e-08, Loss_0: 5.871e-10, Loss_r: 9.672e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.153e-07, Loss_0: 1.868e-08, Loss_r: 9.667e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 9.931e-08, Loss_0: 3.451e-09, Loss_r: 9.586e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 9.532e-08, Loss_0: 1.427e-11, Loss_r: 9.531e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 9.532e-08, Loss_0: 4.696e-10, Loss_r: 9.485e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 9.477e-08, Loss_0: 3.656e-10, Loss_r: 9.440e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 9.397e-08, Loss_0: 2.100e-12, Loss_r: 9.397e-08, Time: 0.09, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 9.358e-08, Loss_0: 4.645e-11, Loss_r: 9.353e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 9.308e-08, Loss_0: 1.819e-13, Loss_r: 9.308e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 9.265e-08, Loss_0: 8.005e-12, Loss_r: 9.264e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 9.221e-08, Loss_0: 1.965e-13, Loss_r: 9.221e-08, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 9.177e-08, Loss_0: 1.038e-13, Loss_r: 9.177e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 9.134e-08, Loss_0: 1.283e-12, Loss_r: 9.133e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 9.089e-08, Loss_0: 4.198e-14, Loss_r: 9.089e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 9.045e-08, Loss_0: 5.861e-13, Loss_r: 9.045e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 9.003e-08, Loss_0: 9.909e-14, Loss_r: 9.003e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 8.958e-08, Loss_0: 6.703e-14, Loss_r: 8.958e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 8.916e-08, Loss_0: 1.536e-11, Loss_r: 8.914e-08, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 8.872e-08, Loss_0: 1.666e-12, Loss_r: 8.872e-08, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 8.833e-08, Loss_0: 4.360e-11, Loss_r: 8.829e-08, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 8.871e-08, Loss_0: 8.209e-10, Loss_r: 8.789e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.200e-07, Loss_0: 3.181e-08, Loss_r: 8.821e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 9.288e-08, Loss_0: 5.681e-09, Loss_r: 8.720e-08, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 8.666e-08, Loss_0: 2.261e-11, Loss_r: 8.664e-08, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 8.704e-08, Loss_0: 7.834e-10, Loss_r: 8.626e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 8.647e-08, Loss_0: 6.042e-10, Loss_r: 8.587e-08, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 8.548e-08, Loss_0: 7.355e-12, Loss_r: 8.547e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 8.516e-08, Loss_0: 6.270e-11, Loss_r: 8.509e-08, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 8.472e-08, Loss_0: 1.695e-13, Loss_r: 8.472e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 8.435e-08, Loss_0: 1.275e-11, Loss_r: 8.434e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 8.394e-08, Loss_0: 4.931e-13, Loss_r: 8.394e-08, Time: 0.10, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 8.355e-08, Loss_0: 1.124e-13, Loss_r: 8.355e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 8.318e-08, Loss_0: 3.332e-12, Loss_r: 8.318e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 8.280e-08, Loss_0: 1.333e-13, Loss_r: 8.280e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 8.241e-08, Loss_0: 2.567e-13, Loss_r: 8.241e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 8.203e-08, Loss_0: 1.124e-15, Loss_r: 8.203e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 8.163e-08, Loss_0: 1.576e-12, Loss_r: 8.163e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 8.124e-08, Loss_0: 1.026e-13, Loss_r: 8.124e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 8.088e-08, Loss_0: 3.717e-12, Loss_r: 8.087e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 8.054e-08, Loss_0: 5.741e-11, Loss_r: 8.049e-08, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 8.087e-08, Loss_0: 7.619e-10, Loss_r: 8.011e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.056e-07, Loss_0: 2.542e-08, Loss_r: 8.019e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 8.426e-08, Loss_0: 4.809e-09, Loss_r: 7.946e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 7.908e-08, Loss_0: 4.939e-11, Loss_r: 7.903e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 7.931e-08, Loss_0: 5.923e-10, Loss_r: 7.872e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 7.887e-08, Loss_0: 4.799e-10, Loss_r: 7.839e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 7.804e-08, Loss_0: 1.159e-11, Loss_r: 7.803e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 7.774e-08, Loss_0: 7.189e-11, Loss_r: 7.767e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 7.734e-08, Loss_0: 1.633e-12, Loss_r: 7.733e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 7.700e-08, Loss_0: 1.001e-11, Loss_r: 7.699e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 7.665e-08, Loss_0: 1.224e-12, Loss_r: 7.665e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 7.630e-08, Loss_0: 2.448e-14, Loss_r: 7.630e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 7.596e-08, Loss_0: 9.782e-13, Loss_r: 7.596e-08, Time: 0.10, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 7.562e-08, Loss_0: 3.224e-12, Loss_r: 7.562e-08, Time: 0.07, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 7.528e-08, Loss_0: 5.579e-13, Loss_r: 7.528e-08, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 7.493e-08, Loss_0: 8.443e-14, Loss_r: 7.493e-08, Time: 0.09, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 7.460e-08, Loss_0: 9.202e-13, Loss_r: 7.460e-08, Time: 0.07, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 7.426e-08, Loss_0: 6.932e-13, Loss_r: 7.426e-08, Time: 0.07, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 7.391e-08, Loss_0: 1.676e-12, Loss_r: 7.391e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 7.358e-08, Loss_0: 3.171e-12, Loss_r: 7.358e-08, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 7.330e-08, Loss_0: 6.143e-11, Loss_r: 7.324e-08, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 7.497e-08, Loss_0: 2.006e-09, Loss_r: 7.296e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 2.138e-07, Loss_0: 1.380e-07, Loss_r: 7.586e-08, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 8.887e-08, Loss_0: 1.619e-08, Loss_r: 7.268e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 7.302e-08, Loss_0: 1.058e-09, Loss_r: 7.196e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 7.833e-08, Loss_0: 6.569e-09, Loss_r: 7.176e-08, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 7.278e-08, Loss_0: 1.435e-09, Loss_r: 7.135e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 7.133e-08, Loss_0: 2.789e-10, Loss_r: 7.105e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 7.101e-08, Loss_0: 2.606e-10, Loss_r: 7.075e-08, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 7.050e-08, Loss_0: 6.635e-11, Loss_r: 7.043e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 7.013e-08, Loss_0: 1.306e-11, Loss_r: 7.011e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 6.985e-08, Loss_0: 2.544e-11, Loss_r: 6.982e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 6.952e-08, Loss_0: 9.688e-12, Loss_r: 6.951e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 6.921e-08, Loss_0: 4.348e-13, Loss_r: 6.920e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 6.890e-08, Loss_0: 1.734e-12, Loss_r: 6.890e-08, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 6.859e-08, Loss_0: 1.849e-12, Loss_r: 6.859e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 6.829e-08, Loss_0: 6.415e-15, Loss_r: 6.828e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 6.800e-08, Loss_0: 1.364e-11, Loss_r: 6.798e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 6.769e-08, Loss_0: 1.133e-11, Loss_r: 6.768e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 6.739e-08, Loss_0: 1.824e-11, Loss_r: 6.737e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 6.716e-08, Loss_0: 1.007e-10, Loss_r: 6.706e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 6.833e-08, Loss_0: 1.558e-09, Loss_r: 6.677e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 1.439e-07, Loss_0: 7.587e-08, Loss_r: 6.808e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 7.791e-08, Loss_0: 1.151e-08, Loss_r: 6.640e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 6.597e-08, Loss_0: 6.478e-11, Loss_r: 6.591e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 6.852e-08, Loss_0: 2.797e-09, Loss_r: 6.573e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 6.663e-08, Loss_0: 1.213e-09, Loss_r: 6.542e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 6.511e-08, Loss_0: 1.810e-11, Loss_r: 6.509e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 6.504e-08, Loss_0: 2.221e-10, Loss_r: 6.482e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 6.455e-08, Loss_0: 1.104e-12, Loss_r: 6.455e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 6.430e-08, Loss_0: 2.297e-11, Loss_r: 6.427e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 6.400e-08, Loss_0: 1.109e-11, Loss_r: 6.399e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 6.372e-08, Loss_0: 4.831e-14, Loss_r: 6.372e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 6.345e-08, Loss_0: 9.418e-13, Loss_r: 6.344e-08, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 6.317e-08, Loss_0: 6.778e-13, Loss_r: 6.317e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 6.290e-08, Loss_0: 6.607e-14, Loss_r: 6.290e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 6.264e-08, Loss_0: 8.771e-14, Loss_r: 6.264e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 6.235e-08, Loss_0: 3.334e-13, Loss_r: 6.235e-08, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 6.208e-08, Loss_0: 1.844e-12, Loss_r: 6.208e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 6.181e-08, Loss_0: 9.218e-14, Loss_r: 6.181e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 6.153e-08, Loss_0: 1.042e-12, Loss_r: 6.153e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 6.126e-08, Loss_0: 3.400e-14, Loss_r: 6.126e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 6.099e-08, Loss_0: 5.551e-17, Loss_r: 6.099e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 6.071e-08, Loss_0: 2.578e-12, Loss_r: 6.071e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 6.103e-08, Loss_0: 6.029e-10, Loss_r: 6.043e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 2.543e-07, Loss_0: 1.898e-07, Loss_r: 6.446e-08, Time: 0.10, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 5.994e-08, Loss_0: 2.765e-11, Loss_r: 5.992e-08, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 8.953e-08, Loss_0: 2.909e-08, Loss_r: 6.044e-08, Time: 0.10, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 6.356e-08, Loss_0: 4.000e-09, Loss_r: 5.956e-08, Time: 0.09, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 6.253e-08, Loss_0: 3.283e-09, Loss_r: 5.925e-08, Time: 0.09, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 5.912e-08, Loss_0: 1.760e-10, Loss_r: 5.895e-08, Time: 0.11, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 5.932e-08, Loss_0: 5.969e-10, Loss_r: 5.873e-08, Time: 0.08, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 5.858e-08, Loss_0: 1.263e-10, Loss_r: 5.846e-08, Time: 0.11, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 5.821e-08, Loss_0: 5.277e-13, Loss_r: 5.821e-08, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 5.796e-08, Loss_0: 9.454e-13, Loss_r: 5.796e-08, Time: 0.09, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 5.772e-08, Loss_0: 8.296e-13, Loss_r: 5.772e-08, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 5.747e-08, Loss_0: 5.468e-13, Loss_r: 5.747e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.722e-08, Loss_0: 3.870e-13, Loss_r: 5.722e-08, Time: 0.15, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 5.700e-08, Loss_0: 2.643e-11, Loss_r: 5.697e-08, Time: 0.14, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.673e-08, Loss_0: 3.624e-12, Loss_r: 5.673e-08, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 5.649e-08, Loss_0: 1.373e-12, Loss_r: 5.649e-08, Time: 0.13, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 5.627e-08, Loss_0: 2.622e-11, Loss_r: 5.624e-08, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 5.655e-08, Loss_0: 5.548e-10, Loss_r: 5.600e-08, Time: 0.15, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 8.475e-08, Loss_0: 2.839e-08, Loss_r: 5.636e-08, Time: 0.15, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 5.993e-08, Loss_0: 4.318e-09, Loss_r: 5.561e-08, Time: 0.13, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 5.534e-08, Loss_0: 2.641e-11, Loss_r: 5.532e-08, Time: 0.15, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 5.620e-08, Loss_0: 1.069e-09, Loss_r: 5.514e-08, Time: 0.13, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 5.534e-08, Loss_0: 4.398e-10, Loss_r: 5.490e-08, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 5.465e-08, Loss_0: 9.016e-12, Loss_r: 5.465e-08, Time: 0.15, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 5.450e-08, Loss_0: 7.290e-11, Loss_r: 5.443e-08, Time: 0.15, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 5.421e-08, Loss_0: 3.079e-12, Loss_r: 5.421e-08, Time: 0.13, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 5.400e-08, Loss_0: 7.610e-12, Loss_r: 5.399e-08, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 5.378e-08, Loss_0: 1.080e-11, Loss_r: 5.377e-08, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 5.355e-08, Loss_0: 1.377e-12, Loss_r: 5.355e-08, Time: 0.11, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 5.331e-08, Loss_0: 5.975e-13, Loss_r: 5.331e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 5.310e-08, Loss_0: 1.466e-14, Loss_r: 5.310e-08, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 5.287e-08, Loss_0: 1.472e-13, Loss_r: 5.287e-08, Time: 0.09, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 5.265e-08, Loss_0: 2.165e-14, Loss_r: 5.265e-08, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 5.244e-08, Loss_0: 1.137e-13, Loss_r: 5.244e-08, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 5.222e-08, Loss_0: 2.477e-12, Loss_r: 5.222e-08, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 5.199e-08, Loss_0: 1.884e-13, Loss_r: 5.199e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 5.180e-08, Loss_0: 2.590e-11, Loss_r: 5.178e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 5.198e-08, Loss_0: 4.344e-10, Loss_r: 5.155e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 6.511e-08, Loss_0: 1.350e-08, Loss_r: 5.162e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 5.380e-08, Loss_0: 2.642e-09, Loss_r: 5.116e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 5.096e-08, Loss_0: 2.790e-11, Loss_r: 5.093e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 5.100e-08, Loss_0: 2.590e-10, Loss_r: 5.074e-08, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 5.079e-08, Loss_0: 2.475e-10, Loss_r: 5.055e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 5.035e-08, Loss_0: 9.218e-12, Loss_r: 5.034e-08, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 5.017e-08, Loss_0: 3.123e-11, Loss_r: 5.014e-08, Time: 0.09, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 4.994e-08, Loss_0: 2.981e-12, Loss_r: 4.993e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 4.974e-08, Loss_0: 9.734e-12, Loss_r: 4.973e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 4.954e-08, Loss_0: 1.295e-12, Loss_r: 4.954e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 4.934e-08, Loss_0: 6.445e-13, Loss_r: 4.934e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 4.913e-08, Loss_0: 1.864e-12, Loss_r: 4.913e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 4.894e-08, Loss_0: 9.893e-13, Loss_r: 4.894e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 4.874e-08, Loss_0: 5.954e-14, Loss_r: 4.874e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 4.854e-08, Loss_0: 1.421e-14, Loss_r: 4.854e-08, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 4.834e-08, Loss_0: 2.566e-14, Loss_r: 4.834e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 4.814e-08, Loss_0: 5.116e-13, Loss_r: 4.814e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 4.794e-08, Loss_0: 3.197e-14, Loss_r: 4.794e-08, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 4.774e-08, Loss_0: 1.295e-12, Loss_r: 4.774e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 4.760e-08, Loss_0: 4.417e-11, Loss_r: 4.755e-08, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 4.841e-08, Loss_0: 1.030e-09, Loss_r: 4.738e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 1.008e-07, Loss_0: 5.228e-08, Loss_r: 4.853e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 5.558e-08, Loss_0: 8.375e-09, Loss_r: 4.720e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 4.679e-08, Loss_0: 3.709e-12, Loss_r: 4.679e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 4.840e-08, Loss_0: 1.760e-09, Loss_r: 4.664e-08, Time: 0.09, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 4.736e-08, Loss_0: 9.229e-10, Loss_r: 4.644e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 4.626e-08, Loss_0: 2.203e-13, Loss_r: 4.626e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 4.622e-08, Loss_0: 1.411e-10, Loss_r: 4.608e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 4.589e-08, Loss_0: 5.954e-14, Loss_r: 4.589e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 4.573e-08, Loss_0: 1.595e-11, Loss_r: 4.571e-08, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 4.553e-08, Loss_0: 2.271e-12, Loss_r: 4.553e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 4.535e-08, Loss_0: 9.968e-13, Loss_r: 4.535e-08, Time: 0.09, Learning Rate: 0.00021\n",
            "Training time: 26.5210\n",
            "[1, 256, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.386e-02, Loss_0: 1.317e-03, Loss_r: 3.254e-02, Time: 0.67, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.241e-02, Loss_0: 1.547e-04, Loss_r: 3.225e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.185e-02, Loss_0: 4.054e-05, Loss_r: 3.181e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 30, Loss: 2.427e-02, Loss_0: 2.637e-07, Loss_r: 2.427e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.268e-03, Loss_0: 9.697e-04, Loss_r: 6.298e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.504e-03, Loss_0: 8.824e-04, Loss_r: 6.215e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 60, Loss: 3.558e-04, Loss_0: 1.185e-05, Loss_r: 3.440e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 70, Loss: 2.201e-04, Loss_0: 1.116e-04, Loss_r: 1.085e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.095e-05, Loss_0: 2.608e-06, Loss_r: 4.834e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 90, Loss: 3.231e-05, Loss_0: 5.926e-06, Loss_r: 2.638e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.686e-05, Loss_0: 4.702e-06, Loss_r: 1.216e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 9.925e-06, Loss_0: 1.434e-07, Loss_r: 9.781e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.538e-06, Loss_0: 1.811e-07, Loss_r: 5.357e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 130, Loss: 3.872e-06, Loss_0: 1.108e-07, Loss_r: 3.762e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 140, Loss: 3.190e-06, Loss_0: 6.117e-08, Loss_r: 3.129e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.781e-06, Loss_0: 5.247e-09, Loss_r: 2.776e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 160, Loss: 3.333e-06, Loss_0: 3.976e-07, Loss_r: 2.936e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 170, Loss: 7.263e-04, Loss_0: 2.979e-04, Loss_r: 4.284e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 180, Loss: 3.567e-04, Loss_0: 1.559e-04, Loss_r: 2.009e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.079e-04, Loss_0: 3.819e-05, Loss_r: 6.966e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 200, Loss: 3.519e-06, Loss_0: 1.984e-07, Loss_r: 3.321e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 210, Loss: 2.122e-05, Loss_0: 8.031e-06, Loss_r: 1.319e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.613e-06, Loss_0: 4.735e-06, Loss_r: 4.878e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 230, Loss: 3.531e-06, Loss_0: 8.038e-07, Loss_r: 2.727e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 240, Loss: 2.403e-06, Loss_0: 3.945e-07, Loss_r: 2.009e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 250, Loss: 2.008e-06, Loss_0: 2.401e-07, Loss_r: 1.768e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.550e-06, Loss_0: 2.239e-08, Loss_r: 1.527e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.509e-06, Loss_0: 2.788e-08, Loss_r: 1.481e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.417e-06, Loss_0: 6.618e-10, Loss_r: 1.416e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.377e-06, Loss_0: 1.349e-10, Loss_r: 1.377e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.340e-06, Loss_0: 7.020e-14, Loss_r: 1.340e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.305e-06, Loss_0: 3.209e-10, Loss_r: 1.305e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.269e-06, Loss_0: 7.950e-11, Loss_r: 1.269e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.235e-06, Loss_0: 9.195e-11, Loss_r: 1.235e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.203e-06, Loss_0: 3.264e-10, Loss_r: 1.202e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.171e-06, Loss_0: 6.587e-10, Loss_r: 1.171e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.256e-06, Loss_0: 6.445e-08, Loss_r: 1.191e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.760e-04, Loss_0: 9.135e-05, Loss_r: 8.467e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 380, Loss: 7.019e-05, Loss_0: 4.191e-05, Loss_r: 2.828e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.299e-04, Loss_0: 3.463e-05, Loss_r: 9.523e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.271e-05, Loss_0: 3.408e-06, Loss_r: 2.930e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.705e-05, Loss_0: 1.025e-05, Loss_r: 6.797e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 6.433e-06, Loss_0: 5.860e-07, Loss_r: 5.847e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 6.259e-06, Loss_0: 1.122e-06, Loss_r: 5.137e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.420e-06, Loss_0: 3.397e-08, Loss_r: 3.386e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 3.358e-06, Loss_0: 1.961e-07, Loss_r: 3.162e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.898e-06, Loss_0: 5.667e-09, Loss_r: 2.892e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.747e-06, Loss_0: 3.390e-10, Loss_r: 2.746e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.565e-06, Loss_0: 2.572e-13, Loss_r: 2.565e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.434e-06, Loss_0: 4.191e-09, Loss_r: 2.430e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.318e-06, Loss_0: 2.025e-10, Loss_r: 2.318e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.213e-06, Loss_0: 1.051e-09, Loss_r: 2.212e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.117e-06, Loss_0: 3.430e-10, Loss_r: 2.117e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.029e-06, Loss_0: 4.137e-10, Loss_r: 2.029e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.947e-06, Loss_0: 1.379e-10, Loss_r: 1.947e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.871e-06, Loss_0: 2.531e-10, Loss_r: 1.870e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.799e-06, Loss_0: 1.880e-10, Loss_r: 1.799e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.731e-06, Loss_0: 1.972e-10, Loss_r: 1.731e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.667e-06, Loss_0: 1.934e-10, Loss_r: 1.667e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.606e-06, Loss_0: 1.714e-10, Loss_r: 1.606e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.548e-06, Loss_0: 1.658e-10, Loss_r: 1.548e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.493e-06, Loss_0: 1.523e-10, Loss_r: 1.493e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.441e-06, Loss_0: 1.449e-10, Loss_r: 1.440e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.391e-06, Loss_0: 1.285e-10, Loss_r: 1.390e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.343e-06, Loss_0: 9.242e-11, Loss_r: 1.343e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.297e-06, Loss_0: 1.465e-12, Loss_r: 1.297e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.310e-06, Loss_0: 3.628e-08, Loss_r: 1.274e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 670, Loss: 6.711e-05, Loss_0: 4.661e-05, Loss_r: 2.050e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 7.619e-04, Loss_0: 3.837e-04, Loss_r: 3.782e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.487e-04, Loss_0: 2.009e-04, Loss_r: 1.477e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.287e-04, Loss_0: 1.933e-05, Loss_r: 1.094e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.392e-05, Loss_0: 4.521e-06, Loss_r: 9.398e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.365e-05, Loss_0: 4.912e-06, Loss_r: 8.738e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.247e-05, Loss_0: 2.648e-06, Loss_r: 9.823e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.543e-06, Loss_0: 2.080e-07, Loss_r: 5.335e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 4.597e-06, Loss_0: 5.527e-08, Loss_r: 4.542e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 3.952e-06, Loss_0: 5.255e-08, Loss_r: 3.899e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 3.533e-06, Loss_0: 4.379e-08, Loss_r: 3.490e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.182e-06, Loss_0: 3.313e-10, Loss_r: 3.182e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.969e-06, Loss_0: 7.776e-11, Loss_r: 2.968e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.773e-06, Loss_0: 3.546e-09, Loss_r: 2.769e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.603e-06, Loss_0: 8.446e-12, Loss_r: 2.603e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.455e-06, Loss_0: 1.116e-09, Loss_r: 2.453e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.322e-06, Loss_0: 3.901e-10, Loss_r: 2.322e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.203e-06, Loss_0: 3.620e-10, Loss_r: 2.202e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.093e-06, Loss_0: 3.471e-10, Loss_r: 2.093e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.992e-06, Loss_0: 3.556e-10, Loss_r: 1.992e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.899e-06, Loss_0: 3.043e-10, Loss_r: 1.899e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.812e-06, Loss_0: 2.609e-10, Loss_r: 1.812e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.731e-06, Loss_0: 2.335e-10, Loss_r: 1.731e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.655e-06, Loss_0: 2.151e-10, Loss_r: 1.655e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.584e-06, Loss_0: 2.015e-10, Loss_r: 1.584e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.518e-06, Loss_0: 1.868e-10, Loss_r: 1.518e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.455e-06, Loss_0: 1.737e-10, Loss_r: 1.455e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.396e-06, Loss_0: 1.590e-10, Loss_r: 1.396e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.340e-06, Loss_0: 1.463e-10, Loss_r: 1.340e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.288e-06, Loss_0: 1.423e-10, Loss_r: 1.288e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.239e-06, Loss_0: 1.247e-10, Loss_r: 1.239e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.192e-06, Loss_0: 1.239e-10, Loss_r: 1.192e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.148e-06, Loss_0: 1.229e-10, Loss_r: 1.148e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.106e-06, Loss_0: 1.112e-10, Loss_r: 1.106e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.067e-06, Loss_0: 1.094e-10, Loss_r: 1.067e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.030e-06, Loss_0: 1.761e-10, Loss_r: 1.029e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 9.970e-07, Loss_0: 3.452e-09, Loss_r: 9.935e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.575e-06, Loss_0: 1.420e-06, Loss_r: 1.155e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.187e-03, Loss_0: 9.479e-04, Loss_r: 2.392e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 2.578e-04, Loss_0: 8.020e-05, Loss_r: 1.776e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 3.119e-05, Loss_0: 1.927e-05, Loss_r: 1.192e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.138e-05, Loss_0: 1.227e-07, Loss_r: 1.126e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.397e-05, Loss_0: 7.139e-06, Loss_r: 6.826e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 8.057e-06, Loss_0: 4.244e-06, Loss_r: 3.813e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 3.901e-06, Loss_0: 8.704e-07, Loss_r: 3.031e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.787e-06, Loss_0: 4.805e-08, Loss_r: 2.739e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.505e-06, Loss_0: 5.891e-09, Loss_r: 2.500e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.287e-06, Loss_0: 3.140e-11, Loss_r: 2.287e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.131e-06, Loss_0: 5.666e-09, Loss_r: 2.126e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.998e-06, Loss_0: 2.055e-09, Loss_r: 1.996e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.881e-06, Loss_0: 6.072e-09, Loss_r: 1.875e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.776e-06, Loss_0: 1.272e-10, Loss_r: 1.776e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.683e-06, Loss_0: 5.354e-10, Loss_r: 1.683e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.600e-06, Loss_0: 7.466e-10, Loss_r: 1.599e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.523e-06, Loss_0: 1.092e-10, Loss_r: 1.523e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.453e-06, Loss_0: 1.743e-10, Loss_r: 1.453e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.388e-06, Loss_0: 2.700e-10, Loss_r: 1.387e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.327e-06, Loss_0: 2.143e-10, Loss_r: 1.327e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.271e-06, Loss_0: 1.613e-10, Loss_r: 1.271e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.218e-06, Loss_0: 1.428e-10, Loss_r: 1.218e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.169e-06, Loss_0: 1.339e-10, Loss_r: 1.169e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.123e-06, Loss_0: 1.292e-10, Loss_r: 1.123e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.080e-06, Loss_0: 1.187e-10, Loss_r: 1.080e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.040e-06, Loss_0: 1.166e-10, Loss_r: 1.040e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.002e-06, Loss_0: 1.026e-10, Loss_r: 1.002e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 9.663e-07, Loss_0: 8.310e-11, Loss_r: 9.662e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 9.328e-07, Loss_0: 5.270e-11, Loss_r: 9.327e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 9.019e-07, Loss_0: 2.141e-11, Loss_r: 9.019e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.003e-06, Loss_0: 2.795e-08, Loss_r: 9.755e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 5.965e-05, Loss_0: 1.448e-05, Loss_r: 4.517e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 2.778e-04, Loss_0: 1.519e-05, Loss_r: 2.626e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.062e-05, Loss_0: 9.322e-06, Loss_r: 2.130e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.160e-05, Loss_0: 4.586e-07, Loss_r: 2.114e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.608e-05, Loss_0: 6.584e-06, Loss_r: 9.493e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.799e-06, Loss_0: 6.343e-07, Loss_r: 8.165e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 4.389e-06, Loss_0: 1.397e-06, Loss_r: 2.992e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 2.369e-06, Loss_0: 3.488e-08, Loss_r: 2.334e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.822e-06, Loss_0: 6.477e-09, Loss_r: 1.816e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.649e-06, Loss_0: 7.132e-08, Loss_r: 1.578e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.481e-06, Loss_0: 1.894e-09, Loss_r: 1.479e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.389e-06, Loss_0: 1.067e-09, Loss_r: 1.388e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.310e-06, Loss_0: 3.254e-09, Loss_r: 1.307e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.244e-06, Loss_0: 1.580e-10, Loss_r: 1.243e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.185e-06, Loss_0: 9.690e-10, Loss_r: 1.184e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.132e-06, Loss_0: 6.981e-12, Loss_r: 1.132e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.082e-06, Loss_0: 2.327e-10, Loss_r: 1.082e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.036e-06, Loss_0: 3.798e-10, Loss_r: 1.035e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 9.932e-07, Loss_0: 5.924e-10, Loss_r: 9.926e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.579e-07, Loss_0: 6.240e-09, Loss_r: 9.517e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.360e-06, Loss_0: 4.477e-07, Loss_r: 9.119e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.000e-04, Loss_0: 9.702e-05, Loss_r: 3.023e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.791e-04, Loss_0: 1.665e-04, Loss_r: 1.263e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 6.077e-05, Loss_0: 4.992e-05, Loss_r: 1.085e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.131e-05, Loss_0: 3.287e-06, Loss_r: 8.025e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.975e-06, Loss_0: 9.241e-07, Loss_r: 3.051e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.471e-06, Loss_0: 2.398e-07, Loss_r: 1.231e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.502e-06, Loss_0: 1.385e-08, Loss_r: 1.488e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.259e-06, Loss_0: 1.237e-08, Loss_r: 1.246e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.053e-06, Loss_0: 5.365e-10, Loss_r: 1.052e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.006e-06, Loss_0: 1.686e-09, Loss_r: 1.005e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 9.712e-07, Loss_0: 3.207e-11, Loss_r: 9.711e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 9.236e-07, Loss_0: 1.243e-10, Loss_r: 9.234e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 8.859e-07, Loss_0: 1.622e-09, Loss_r: 8.843e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 8.531e-07, Loss_0: 5.810e-10, Loss_r: 8.525e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 8.231e-07, Loss_0: 7.477e-12, Loss_r: 8.231e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 7.949e-07, Loss_0: 2.957e-10, Loss_r: 7.946e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 7.684e-07, Loss_0: 3.943e-11, Loss_r: 7.683e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 7.435e-07, Loss_0: 1.441e-10, Loss_r: 7.434e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 7.200e-07, Loss_0: 9.066e-11, Loss_r: 7.200e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 6.979e-07, Loss_0: 4.169e-11, Loss_r: 6.978e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 6.769e-07, Loss_0: 9.947e-12, Loss_r: 6.769e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 6.588e-07, Loss_0: 5.342e-10, Loss_r: 6.582e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 9.995e-07, Loss_0: 1.691e-07, Loss_r: 8.304e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.718e-04, Loss_0: 8.439e-05, Loss_r: 8.737e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 3.072e-04, Loss_0: 2.025e-04, Loss_r: 1.046e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.383e-04, Loss_0: 1.738e-06, Loss_r: 1.365e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.379e-05, Loss_0: 2.861e-05, Loss_r: 1.519e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.327e-05, Loss_0: 2.024e-09, Loss_r: 1.327e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 4.071e-06, Loss_0: 1.348e-06, Loss_r: 2.723e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 2.386e-06, Loss_0: 2.863e-08, Loss_r: 2.357e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.989e-06, Loss_0: 8.582e-08, Loss_r: 1.903e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.779e-06, Loss_0: 9.102e-09, Loss_r: 1.770e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.635e-06, Loss_0: 3.212e-08, Loss_r: 1.603e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.515e-06, Loss_0: 1.468e-09, Loss_r: 1.514e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.407e-06, Loss_0: 6.427e-09, Loss_r: 1.400e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.316e-06, Loss_0: 7.752e-12, Loss_r: 1.316e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.242e-06, Loss_0: 1.533e-11, Loss_r: 1.242e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.176e-06, Loss_0: 1.904e-10, Loss_r: 1.176e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.116e-06, Loss_0: 2.707e-10, Loss_r: 1.116e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.061e-06, Loss_0: 1.067e-10, Loss_r: 1.061e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.011e-06, Loss_0: 8.849e-11, Loss_r: 1.011e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 9.651e-07, Loss_0: 1.531e-10, Loss_r: 9.649e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 9.225e-07, Loss_0: 9.222e-11, Loss_r: 9.224e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 8.833e-07, Loss_0: 1.018e-10, Loss_r: 8.832e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 8.468e-07, Loss_0: 7.388e-11, Loss_r: 8.467e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 8.129e-07, Loss_0: 6.763e-11, Loss_r: 8.128e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 7.814e-07, Loss_0: 7.054e-11, Loss_r: 7.813e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 7.520e-07, Loss_0: 9.007e-11, Loss_r: 7.520e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 7.245e-07, Loss_0: 2.016e-10, Loss_r: 7.243e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 7.012e-07, Loss_0: 3.131e-09, Loss_r: 6.980e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.044e-06, Loss_0: 3.675e-07, Loss_r: 6.768e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.148e-04, Loss_0: 1.105e-04, Loss_r: 4.339e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.754e-04, Loss_0: 1.683e-04, Loss_r: 7.009e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 5.297e-05, Loss_0: 4.475e-05, Loss_r: 8.222e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 1.105e-05, Loss_0: 4.450e-06, Loss_r: 6.604e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 9.556e-07, Loss_0: 1.677e-08, Loss_r: 9.388e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 2.256e-06, Loss_0: 6.325e-07, Loss_r: 1.623e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 1.505e-06, Loss_0: 6.135e-07, Loss_r: 8.917e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 1.011e-06, Loss_0: 2.344e-07, Loss_r: 7.769e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 7.935e-07, Loss_0: 1.343e-08, Loss_r: 7.801e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 7.259e-07, Loss_0: 3.951e-09, Loss_r: 7.220e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 6.992e-07, Loss_0: 2.133e-11, Loss_r: 6.992e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 6.779e-07, Loss_0: 2.715e-11, Loss_r: 6.778e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 6.576e-07, Loss_0: 9.183e-11, Loss_r: 6.576e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 6.386e-07, Loss_0: 1.216e-11, Loss_r: 6.386e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 6.207e-07, Loss_0: 2.652e-11, Loss_r: 6.207e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 6.037e-07, Loss_0: 1.028e-10, Loss_r: 6.036e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 5.875e-07, Loss_0: 1.784e-11, Loss_r: 5.875e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 5.720e-07, Loss_0: 8.701e-11, Loss_r: 5.719e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 5.572e-07, Loss_0: 2.293e-11, Loss_r: 5.572e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 5.431e-07, Loss_0: 3.754e-11, Loss_r: 5.431e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 5.295e-07, Loss_0: 3.824e-11, Loss_r: 5.295e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 5.165e-07, Loss_0: 3.141e-11, Loss_r: 5.165e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 5.040e-07, Loss_0: 2.815e-11, Loss_r: 5.039e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 4.920e-07, Loss_0: 3.182e-11, Loss_r: 4.919e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 4.804e-07, Loss_0: 2.612e-11, Loss_r: 4.803e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 4.691e-07, Loss_0: 2.892e-11, Loss_r: 4.691e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 4.584e-07, Loss_0: 2.657e-11, Loss_r: 4.584e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 4.480e-07, Loss_0: 2.112e-11, Loss_r: 4.479e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 4.379e-07, Loss_0: 1.451e-11, Loss_r: 4.379e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 4.283e-07, Loss_0: 6.971e-14, Loss_r: 4.283e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 4.308e-07, Loss_0: 2.139e-09, Loss_r: 4.286e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 3.603e-06, Loss_0: 6.898e-07, Loss_r: 2.913e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 4.055e-07, Loss_0: 3.431e-10, Loss_r: 4.052e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 8.873e-07, Loss_0: 1.130e-07, Loss_r: 7.743e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 4.459e-07, Loss_0: 1.377e-08, Loss_r: 4.321e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 4.502e-07, Loss_0: 1.469e-08, Loss_r: 4.355e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 3.749e-07, Loss_0: 5.227e-11, Loss_r: 3.748e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 3.735e-07, Loss_0: 1.623e-09, Loss_r: 3.719e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 3.649e-07, Loss_0: 7.144e-10, Loss_r: 3.642e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 3.559e-07, Loss_0: 5.687e-10, Loss_r: 3.553e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 3.481e-07, Loss_0: 5.119e-11, Loss_r: 3.480e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.412e-07, Loss_0: 2.123e-11, Loss_r: 3.412e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 3.351e-07, Loss_0: 9.850e-11, Loss_r: 3.350e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.289e-07, Loss_0: 4.974e-11, Loss_r: 3.288e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.230e-07, Loss_0: 7.453e-11, Loss_r: 3.229e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 3.197e-07, Loss_0: 8.686e-10, Loss_r: 3.189e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 5.439e-07, Loss_0: 6.021e-08, Loss_r: 4.837e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 3.230e-07, Loss_0: 4.655e-09, Loss_r: 3.184e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 3.110e-07, Loss_0: 2.124e-09, Loss_r: 3.089e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 3.106e-07, Loss_0: 3.197e-09, Loss_r: 3.074e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 2.920e-07, Loss_0: 4.066e-12, Loss_r: 2.920e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 2.893e-07, Loss_0: 6.427e-10, Loss_r: 2.886e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 2.830e-07, Loss_0: 9.941e-12, Loss_r: 2.829e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 2.784e-07, Loss_0: 2.784e-13, Loss_r: 2.784e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 2.740e-07, Loss_0: 4.965e-11, Loss_r: 2.740e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.697e-07, Loss_0: 8.525e-14, Loss_r: 2.697e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.655e-07, Loss_0: 1.837e-11, Loss_r: 2.655e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.614e-07, Loss_0: 6.952e-12, Loss_r: 2.614e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.573e-07, Loss_0: 4.792e-12, Loss_r: 2.573e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.533e-07, Loss_0: 8.008e-12, Loss_r: 2.533e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.494e-07, Loss_0: 1.534e-11, Loss_r: 2.494e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.457e-07, Loss_0: 6.506e-11, Loss_r: 2.457e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 2.475e-07, Loss_0: 1.759e-09, Loss_r: 2.457e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.013e-06, Loss_0: 2.156e-07, Loss_r: 7.971e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 2.617e-07, Loss_0: 7.771e-09, Loss_r: 2.539e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.933e-07, Loss_0: 1.639e-08, Loss_r: 2.769e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 2.693e-07, Loss_0: 1.071e-08, Loss_r: 2.586e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 2.282e-07, Loss_0: 7.232e-10, Loss_r: 2.274e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.279e-07, Loss_0: 1.563e-09, Loss_r: 2.263e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.218e-07, Loss_0: 3.973e-10, Loss_r: 2.214e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 2.171e-07, Loss_0: 2.992e-11, Loss_r: 2.171e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 2.142e-07, Loss_0: 2.606e-11, Loss_r: 2.142e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 2.113e-07, Loss_0: 1.616e-14, Loss_r: 2.113e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 2.085e-07, Loss_0: 8.223e-12, Loss_r: 2.085e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 2.058e-07, Loss_0: 1.685e-11, Loss_r: 2.057e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 2.030e-07, Loss_0: 1.843e-13, Loss_r: 2.030e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 2.004e-07, Loss_0: 3.383e-13, Loss_r: 2.004e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.977e-07, Loss_0: 3.825e-14, Loss_r: 1.977e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.954e-07, Loss_0: 3.466e-11, Loss_r: 1.953e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.028e-07, Loss_0: 2.744e-09, Loss_r: 2.000e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.444e-06, Loss_0: 3.655e-07, Loss_r: 1.079e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 2.483e-07, Loss_0: 1.711e-08, Loss_r: 2.312e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 2.597e-07, Loss_0: 2.227e-08, Loss_r: 2.374e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 2.567e-07, Loss_0: 2.208e-08, Loss_r: 2.346e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.823e-07, Loss_0: 2.629e-11, Loss_r: 1.823e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.902e-07, Loss_0: 2.794e-09, Loss_r: 1.874e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.790e-07, Loss_0: 3.494e-10, Loss_r: 1.786e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.762e-07, Loss_0: 1.093e-10, Loss_r: 1.761e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.744e-07, Loss_0: 8.547e-11, Loss_r: 1.743e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.722e-07, Loss_0: 9.446e-11, Loss_r: 1.721e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.701e-07, Loss_0: 8.489e-12, Loss_r: 1.701e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.681e-07, Loss_0: 1.654e-11, Loss_r: 1.681e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.662e-07, Loss_0: 4.405e-12, Loss_r: 1.662e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.643e-07, Loss_0: 4.416e-13, Loss_r: 1.643e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.625e-07, Loss_0: 1.748e-12, Loss_r: 1.625e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.606e-07, Loss_0: 1.265e-12, Loss_r: 1.606e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.589e-07, Loss_0: 2.679e-12, Loss_r: 1.589e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.594e-07, Loss_0: 6.261e-10, Loss_r: 1.588e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 5.130e-07, Loss_0: 1.088e-07, Loss_r: 4.042e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.638e-07, Loss_0: 2.878e-09, Loss_r: 1.609e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.834e-07, Loss_0: 9.899e-09, Loss_r: 1.735e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.687e-07, Loss_0: 5.767e-09, Loss_r: 1.629e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.506e-07, Loss_0: 3.110e-10, Loss_r: 1.503e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.502e-07, Loss_0: 5.960e-10, Loss_r: 1.496e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.473e-07, Loss_0: 3.011e-10, Loss_r: 1.470e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.451e-07, Loss_0: 8.042e-14, Loss_r: 1.451e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.437e-07, Loss_0: 2.480e-12, Loss_r: 1.437e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.423e-07, Loss_0: 2.061e-11, Loss_r: 1.422e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.409e-07, Loss_0: 1.280e-14, Loss_r: 1.409e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.395e-07, Loss_0: 2.201e-12, Loss_r: 1.395e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.381e-07, Loss_0: 5.782e-12, Loss_r: 1.381e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.368e-07, Loss_0: 3.575e-13, Loss_r: 1.368e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.355e-07, Loss_0: 3.383e-13, Loss_r: 1.355e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.342e-07, Loss_0: 6.618e-13, Loss_r: 1.342e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.332e-07, Loss_0: 8.195e-11, Loss_r: 1.332e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.552e-07, Loss_0: 7.269e-09, Loss_r: 1.480e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.330e-07, Loss_0: 7.313e-10, Loss_r: 1.323e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.297e-07, Loss_0: 1.349e-10, Loss_r: 1.296e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.296e-07, Loss_0: 4.730e-10, Loss_r: 1.291e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.273e-07, Loss_0: 6.673e-11, Loss_r: 1.272e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.262e-07, Loss_0: 2.268e-11, Loss_r: 1.262e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.250e-07, Loss_0: 5.277e-13, Loss_r: 1.250e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.240e-07, Loss_0: 1.773e-11, Loss_r: 1.240e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 1.229e-07, Loss_0: 2.557e-13, Loss_r: 1.229e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.219e-07, Loss_0: 1.685e-12, Loss_r: 1.219e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.208e-07, Loss_0: 3.428e-12, Loss_r: 1.208e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 1.198e-07, Loss_0: 1.310e-12, Loss_r: 1.198e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 1.188e-07, Loss_0: 2.215e-12, Loss_r: 1.188e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 1.178e-07, Loss_0: 1.581e-12, Loss_r: 1.178e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.168e-07, Loss_0: 1.381e-12, Loss_r: 1.168e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.159e-07, Loss_0: 3.231e-12, Loss_r: 1.158e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.149e-07, Loss_0: 6.045e-12, Loss_r: 1.149e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.140e-07, Loss_0: 4.465e-11, Loss_r: 1.140e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.169e-07, Loss_0: 1.344e-09, Loss_r: 1.155e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 5.669e-07, Loss_0: 1.498e-07, Loss_r: 4.171e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.360e-07, Loss_0: 8.330e-09, Loss_r: 1.277e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.337e-07, Loss_0: 7.391e-09, Loss_r: 1.263e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.367e-07, Loss_0: 8.668e-09, Loss_r: 1.281e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.090e-07, Loss_0: 3.191e-12, Loss_r: 1.090e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.119e-07, Loss_0: 1.310e-09, Loss_r: 1.105e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.074e-07, Loss_0: 1.242e-11, Loss_r: 1.074e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.069e-07, Loss_0: 7.295e-11, Loss_r: 1.068e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.060e-07, Loss_0: 8.610e-11, Loss_r: 1.059e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.051e-07, Loss_0: 1.175e-11, Loss_r: 1.051e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.043e-07, Loss_0: 1.493e-11, Loss_r: 1.042e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.035e-07, Loss_0: 4.597e-13, Loss_r: 1.035e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.027e-07, Loss_0: 6.583e-12, Loss_r: 1.027e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.020e-07, Loss_0: 2.710e-13, Loss_r: 1.020e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.012e-07, Loss_0: 3.469e-14, Loss_r: 1.012e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.005e-07, Loss_0: 6.323e-16, Loss_r: 1.005e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 9.977e-08, Loss_0: 4.323e-13, Loss_r: 9.977e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 9.921e-08, Loss_0: 4.173e-11, Loss_r: 9.917e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.093e-07, Loss_0: 3.562e-09, Loss_r: 1.057e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 9.901e-08, Loss_0: 3.962e-10, Loss_r: 9.861e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 9.715e-08, Loss_0: 4.833e-11, Loss_r: 9.710e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 9.696e-08, Loss_0: 2.171e-10, Loss_r: 9.675e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 9.590e-08, Loss_0: 4.730e-11, Loss_r: 9.585e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 9.520e-08, Loss_0: 5.881e-12, Loss_r: 9.520e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 9.456e-08, Loss_0: 1.518e-12, Loss_r: 9.456e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 9.395e-08, Loss_0: 7.801e-12, Loss_r: 9.394e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 9.333e-08, Loss_0: 3.421e-13, Loss_r: 9.333e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 9.273e-08, Loss_0: 5.932e-13, Loss_r: 9.273e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 9.213e-08, Loss_0: 1.661e-12, Loss_r: 9.213e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 9.154e-08, Loss_0: 1.074e-12, Loss_r: 9.153e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 9.094e-08, Loss_0: 1.210e-12, Loss_r: 9.093e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 9.035e-08, Loss_0: 1.253e-12, Loss_r: 9.035e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 8.977e-08, Loss_0: 8.708e-13, Loss_r: 8.977e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 8.918e-08, Loss_0: 1.319e-12, Loss_r: 8.918e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 8.861e-08, Loss_0: 6.062e-13, Loss_r: 8.861e-08, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 8.804e-08, Loss_0: 5.684e-14, Loss_r: 8.804e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 8.758e-08, Loss_0: 2.493e-11, Loss_r: 8.755e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 9.396e-08, Loss_0: 2.326e-09, Loss_r: 9.163e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 8.722e-08, Loss_0: 2.391e-10, Loss_r: 8.698e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 8.600e-08, Loss_0: 3.585e-11, Loss_r: 8.596e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 8.578e-08, Loss_0: 1.467e-10, Loss_r: 8.563e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 8.499e-08, Loss_0: 2.908e-11, Loss_r: 8.496e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 8.446e-08, Loss_0: 3.785e-12, Loss_r: 8.445e-08, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 8.396e-08, Loss_0: 7.057e-13, Loss_r: 8.396e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 8.347e-08, Loss_0: 5.543e-12, Loss_r: 8.347e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 8.299e-08, Loss_0: 3.291e-13, Loss_r: 8.299e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 8.250e-08, Loss_0: 3.699e-13, Loss_r: 8.250e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 8.204e-08, Loss_0: 1.357e-12, Loss_r: 8.204e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 8.156e-08, Loss_0: 5.523e-13, Loss_r: 8.156e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 8.108e-08, Loss_0: 5.733e-13, Loss_r: 8.108e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 8.062e-08, Loss_0: 8.415e-13, Loss_r: 8.062e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 8.016e-08, Loss_0: 4.879e-13, Loss_r: 8.016e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 7.968e-08, Loss_0: 1.108e-12, Loss_r: 7.968e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 7.923e-08, Loss_0: 1.607e-12, Loss_r: 7.922e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 7.878e-08, Loss_0: 7.779e-13, Loss_r: 7.878e-08, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 7.832e-08, Loss_0: 1.649e-13, Loss_r: 7.832e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 7.792e-08, Loss_0: 1.094e-11, Loss_r: 7.790e-08, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 8.371e-08, Loss_0: 2.129e-09, Loss_r: 8.158e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 7.734e-08, Loss_0: 9.295e-11, Loss_r: 7.725e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 7.698e-08, Loss_0: 1.469e-10, Loss_r: 7.683e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 7.659e-08, Loss_0: 1.508e-10, Loss_r: 7.644e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 7.582e-08, Loss_0: 5.156e-13, Loss_r: 7.582e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 7.548e-08, Loss_0: 1.147e-11, Loss_r: 7.547e-08, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 7.505e-08, Loss_0: 2.804e-12, Loss_r: 7.504e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 7.464e-08, Loss_0: 3.599e-12, Loss_r: 7.464e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 7.426e-08, Loss_0: 1.050e-16, Loss_r: 7.426e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 7.388e-08, Loss_0: 2.028e-12, Loss_r: 7.388e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 7.349e-08, Loss_0: 1.530e-13, Loss_r: 7.349e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 7.311e-08, Loss_0: 1.019e-12, Loss_r: 7.311e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 7.273e-08, Loss_0: 7.664e-13, Loss_r: 7.273e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 7.234e-08, Loss_0: 5.846e-13, Loss_r: 7.234e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 7.196e-08, Loss_0: 9.238e-13, Loss_r: 7.196e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 7.159e-08, Loss_0: 7.911e-13, Loss_r: 7.159e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 7.121e-08, Loss_0: 8.389e-14, Loss_r: 7.121e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 7.084e-08, Loss_0: 1.410e-12, Loss_r: 7.084e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 7.088e-08, Loss_0: 1.268e-10, Loss_r: 7.076e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 1.095e-07, Loss_0: 1.391e-08, Loss_r: 9.557e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 7.279e-08, Loss_0: 1.025e-09, Loss_r: 7.176e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 7.063e-08, Loss_0: 4.651e-10, Loss_r: 7.016e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 7.145e-08, Loss_0: 8.743e-10, Loss_r: 7.057e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 6.889e-08, Loss_0: 4.524e-11, Loss_r: 6.884e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 6.871e-08, Loss_0: 7.562e-11, Loss_r: 6.864e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 6.814e-08, Loss_0: 1.034e-12, Loss_r: 6.814e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 6.785e-08, Loss_0: 1.928e-11, Loss_r: 6.783e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 6.750e-08, Loss_0: 8.415e-13, Loss_r: 6.750e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 6.716e-08, Loss_0: 1.560e-12, Loss_r: 6.716e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 6.685e-08, Loss_0: 4.572e-13, Loss_r: 6.685e-08, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 6.653e-08, Loss_0: 3.642e-13, Loss_r: 6.653e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 6.620e-08, Loss_0: 7.325e-13, Loss_r: 6.620e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 6.589e-08, Loss_0: 3.519e-13, Loss_r: 6.589e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 6.557e-08, Loss_0: 2.032e-13, Loss_r: 6.557e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 6.526e-08, Loss_0: 1.791e-12, Loss_r: 6.526e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 6.494e-08, Loss_0: 2.370e-12, Loss_r: 6.494e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 6.465e-08, Loss_0: 9.406e-12, Loss_r: 6.464e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 6.462e-08, Loss_0: 1.291e-10, Loss_r: 6.449e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 8.250e-08, Loss_0: 6.798e-09, Loss_r: 7.570e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 6.609e-08, Loss_0: 9.031e-10, Loss_r: 6.518e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 6.355e-08, Loss_0: 2.848e-11, Loss_r: 6.352e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 6.405e-08, Loss_0: 3.023e-10, Loss_r: 6.375e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 6.311e-08, Loss_0: 6.665e-11, Loss_r: 6.304e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 6.266e-08, Loss_0: 1.860e-11, Loss_r: 6.265e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 6.238e-08, Loss_0: 1.914e-11, Loss_r: 6.236e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 6.207e-08, Loss_0: 1.137e-12, Loss_r: 6.207e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 6.179e-08, Loss_0: 5.551e-15, Loss_r: 6.179e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 6.152e-08, Loss_0: 2.302e-12, Loss_r: 6.152e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 6.125e-08, Loss_0: 2.687e-14, Loss_r: 6.125e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 6.098e-08, Loss_0: 1.434e-12, Loss_r: 6.097e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 6.071e-08, Loss_0: 2.748e-14, Loss_r: 6.071e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 6.043e-08, Loss_0: 1.537e-12, Loss_r: 6.043e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 6.016e-08, Loss_0: 4.954e-14, Loss_r: 6.016e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 5.989e-08, Loss_0: 8.335e-14, Loss_r: 5.989e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 5.962e-08, Loss_0: 5.076e-13, Loss_r: 5.962e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 5.935e-08, Loss_0: 6.717e-13, Loss_r: 5.935e-08, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 5.910e-08, Loss_0: 5.609e-12, Loss_r: 5.909e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 5.901e-08, Loss_0: 7.643e-11, Loss_r: 5.894e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 7.186e-08, Loss_0: 4.988e-09, Loss_r: 6.687e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 5.968e-08, Loss_0: 5.270e-10, Loss_r: 5.915e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 5.827e-08, Loss_0: 6.391e-11, Loss_r: 5.821e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 5.856e-08, Loss_0: 2.445e-10, Loss_r: 5.832e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 5.770e-08, Loss_0: 2.890e-11, Loss_r: 5.767e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 5.743e-08, Loss_0: 2.438e-11, Loss_r: 5.740e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 5.715e-08, Loss_0: 8.793e-12, Loss_r: 5.714e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 5.691e-08, Loss_0: 1.821e-12, Loss_r: 5.691e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 5.667e-08, Loss_0: 4.360e-13, Loss_r: 5.666e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 5.643e-08, Loss_0: 1.214e-12, Loss_r: 5.643e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 5.619e-08, Loss_0: 5.378e-14, Loss_r: 5.619e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 5.596e-08, Loss_0: 1.153e-12, Loss_r: 5.596e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 5.573e-08, Loss_0: 3.298e-14, Loss_r: 5.573e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 5.549e-08, Loss_0: 8.178e-13, Loss_r: 5.549e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 5.526e-08, Loss_0: 4.698e-13, Loss_r: 5.526e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 5.502e-08, Loss_0: 2.265e-13, Loss_r: 5.502e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 5.479e-08, Loss_0: 3.302e-13, Loss_r: 5.479e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 5.457e-08, Loss_0: 1.252e-15, Loss_r: 5.457e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 5.435e-08, Loss_0: 3.560e-12, Loss_r: 5.434e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 5.463e-08, Loss_0: 1.791e-10, Loss_r: 5.445e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 9.695e-08, Loss_0: 1.597e-08, Loss_r: 8.098e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 5.760e-08, Loss_0: 1.432e-09, Loss_r: 5.617e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 5.434e-08, Loss_0: 3.522e-10, Loss_r: 5.399e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 5.568e-08, Loss_0: 9.382e-10, Loss_r: 5.474e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 5.327e-08, Loss_0: 9.188e-11, Loss_r: 5.318e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 5.305e-08, Loss_0: 6.641e-11, Loss_r: 5.298e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 5.267e-08, Loss_0: 8.943e-12, Loss_r: 5.267e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 5.247e-08, Loss_0: 1.894e-11, Loss_r: 5.246e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 5.223e-08, Loss_0: 2.596e-14, Loss_r: 5.223e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 5.202e-08, Loss_0: 2.507e-14, Loss_r: 5.202e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 5.181e-08, Loss_0: 1.255e-12, Loss_r: 5.181e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 5.161e-08, Loss_0: 2.818e-15, Loss_r: 5.161e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 5.140e-08, Loss_0: 8.347e-13, Loss_r: 5.140e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 5.121e-08, Loss_0: 2.691e-13, Loss_r: 5.121e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 5.100e-08, Loss_0: 8.175e-14, Loss_r: 5.100e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 5.079e-08, Loss_0: 1.355e-14, Loss_r: 5.079e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 5.059e-08, Loss_0: 1.014e-12, Loss_r: 5.059e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 5.039e-08, Loss_0: 3.946e-12, Loss_r: 5.039e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 5.030e-08, Loss_0: 5.188e-11, Loss_r: 5.025e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 5.404e-08, Loss_0: 1.562e-09, Loss_r: 5.248e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 5.053e-08, Loss_0: 2.890e-10, Loss_r: 5.024e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 4.962e-08, Loss_0: 1.829e-12, Loss_r: 4.962e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 4.954e-08, Loss_0: 3.335e-11, Loss_r: 4.951e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "Training time: 22.8113\n",
            "[1, 128, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.327e-02, Loss_0: 4.277e-04, Loss_r: 3.284e-02, Time: 0.26, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.239e-02, Loss_0: 4.762e-05, Loss_r: 3.234e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 3.181e-02, Loss_0: 1.295e-07, Loss_r: 3.181e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 3.038e-02, Loss_0: 2.195e-07, Loss_r: 3.038e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.986e-02, Loss_0: 2.589e-07, Loss_r: 1.986e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 3.677e-03, Loss_0: 3.911e-06, Loss_r: 3.673e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.493e-03, Loss_0: 1.583e-03, Loss_r: 9.101e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 3.228e-04, Loss_0: 1.676e-05, Loss_r: 3.061e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 2.548e-04, Loss_0: 6.171e-05, Loss_r: 1.930e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.199e-04, Loss_0: 2.912e-05, Loss_r: 9.075e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.923e-05, Loss_0: 1.732e-05, Loss_r: 4.191e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.364e-05, Loss_0: 1.077e-06, Loss_r: 3.256e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.346e-05, Loss_0: 6.224e-07, Loss_r: 2.284e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.987e-05, Loss_0: 7.179e-07, Loss_r: 1.915e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.699e-05, Loss_0: 3.802e-07, Loss_r: 1.661e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.494e-05, Loss_0: 1.074e-07, Loss_r: 1.483e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.351e-05, Loss_0: 4.488e-09, Loss_r: 1.350e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.240e-05, Loss_0: 2.814e-09, Loss_r: 1.239e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.141e-05, Loss_0: 6.084e-09, Loss_r: 1.140e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.052e-05, Loss_0: 6.132e-09, Loss_r: 1.051e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 9.698e-06, Loss_0: 9.585e-10, Loss_r: 9.697e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 8.958e-06, Loss_0: 1.799e-10, Loss_r: 8.958e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 8.285e-06, Loss_0: 1.559e-09, Loss_r: 8.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 7.676e-06, Loss_0: 3.645e-10, Loss_r: 7.675e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 7.126e-06, Loss_0: 5.839e-10, Loss_r: 7.125e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 6.631e-06, Loss_0: 6.067e-10, Loss_r: 6.630e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 6.186e-06, Loss_0: 4.407e-10, Loss_r: 6.186e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.787e-06, Loss_0: 3.816e-10, Loss_r: 5.787e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.430e-06, Loss_0: 3.371e-10, Loss_r: 5.430e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.109e-06, Loss_0: 2.778e-10, Loss_r: 5.109e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.822e-06, Loss_0: 2.748e-10, Loss_r: 4.822e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.564e-06, Loss_0: 2.145e-10, Loss_r: 4.564e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 4.333e-06, Loss_0: 1.959e-11, Loss_r: 4.333e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.363e-06, Loss_0: 2.022e-07, Loss_r: 4.161e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 2.149e-03, Loss_0: 1.918e-03, Loss_r: 2.314e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 7.044e-04, Loss_0: 6.371e-04, Loss_r: 6.729e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 2.580e-04, Loss_0: 2.253e-04, Loss_r: 3.272e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 8.116e-05, Loss_0: 6.294e-05, Loss_r: 1.822e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.425e-05, Loss_0: 4.351e-07, Loss_r: 1.381e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.052e-05, Loss_0: 8.117e-06, Loss_r: 1.240e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.391e-05, Loss_0: 3.775e-06, Loss_r: 1.013e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 9.754e-06, Loss_0: 1.113e-06, Loss_r: 8.641e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 8.138e-06, Loss_0: 3.891e-07, Loss_r: 7.749e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 7.269e-06, Loss_0: 1.630e-07, Loss_r: 7.106e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 6.622e-06, Loss_0: 3.960e-08, Loss_r: 6.583e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.136e-06, Loss_0: 5.733e-11, Loss_r: 6.136e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.763e-06, Loss_0: 1.113e-08, Loss_r: 5.752e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.419e-06, Loss_0: 1.976e-09, Loss_r: 5.417e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.120e-06, Loss_0: 2.313e-10, Loss_r: 5.120e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.852e-06, Loss_0: 5.666e-10, Loss_r: 4.851e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.612e-06, Loss_0: 3.762e-10, Loss_r: 4.612e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.397e-06, Loss_0: 1.001e-10, Loss_r: 4.397e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.202e-06, Loss_0: 2.910e-10, Loss_r: 4.202e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.025e-06, Loss_0: 1.402e-10, Loss_r: 4.025e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.863e-06, Loss_0: 1.301e-10, Loss_r: 3.863e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 3.714e-06, Loss_0: 1.570e-10, Loss_r: 3.714e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 3.576e-06, Loss_0: 1.075e-10, Loss_r: 3.576e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.448e-06, Loss_0: 1.046e-10, Loss_r: 3.448e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.328e-06, Loss_0: 9.880e-11, Loss_r: 3.328e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.216e-06, Loss_0: 8.934e-11, Loss_r: 3.216e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.110e-06, Loss_0: 7.511e-11, Loss_r: 3.109e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.009e-06, Loss_0: 7.851e-11, Loss_r: 3.009e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.914e-06, Loss_0: 6.729e-11, Loss_r: 2.914e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.823e-06, Loss_0: 6.610e-11, Loss_r: 2.822e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.735e-06, Loss_0: 6.132e-11, Loss_r: 2.735e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.652e-06, Loss_0: 4.460e-11, Loss_r: 2.652e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.572e-06, Loss_0: 4.960e-11, Loss_r: 2.572e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.495e-06, Loss_0: 5.177e-11, Loss_r: 2.495e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.420e-06, Loss_0: 4.056e-11, Loss_r: 2.420e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.348e-06, Loss_0: 2.147e-11, Loss_r: 2.348e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.279e-06, Loss_0: 1.656e-11, Loss_r: 2.279e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.239e-06, Loss_0: 2.203e-08, Loss_r: 2.217e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.557e-05, Loss_0: 2.120e-05, Loss_r: 4.365e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.811e-05, Loss_0: 1.300e-05, Loss_r: 5.112e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.825e-06, Loss_0: 2.471e-07, Loss_r: 7.578e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.394e-04, Loss_0: 1.219e-04, Loss_r: 1.749e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.764e-05, Loss_0: 9.430e-06, Loss_r: 8.209e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 9.954e-06, Loss_0: 3.102e-06, Loss_r: 6.852e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.063e-05, Loss_0: 4.649e-06, Loss_r: 5.980e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 4.788e-06, Loss_0: 1.272e-13, Loss_r: 4.788e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 5.105e-06, Loss_0: 6.752e-07, Loss_r: 4.430e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 4.152e-06, Loss_0: 3.929e-08, Loss_r: 4.112e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.862e-06, Loss_0: 1.766e-08, Loss_r: 3.844e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.637e-06, Loss_0: 3.232e-08, Loss_r: 3.605e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 3.416e-06, Loss_0: 7.501e-09, Loss_r: 3.409e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 3.232e-06, Loss_0: 4.453e-09, Loss_r: 3.227e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 3.074e-06, Loss_0: 3.998e-10, Loss_r: 3.073e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.934e-06, Loss_0: 1.019e-09, Loss_r: 2.933e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 2.808e-06, Loss_0: 8.303e-12, Loss_r: 2.808e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 2.694e-06, Loss_0: 2.553e-10, Loss_r: 2.694e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.589e-06, Loss_0: 9.517e-11, Loss_r: 2.589e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.493e-06, Loss_0: 3.745e-11, Loss_r: 2.493e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.404e-06, Loss_0: 8.359e-11, Loss_r: 2.404e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 2.320e-06, Loss_0: 8.136e-11, Loss_r: 2.320e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.242e-06, Loss_0: 5.814e-11, Loss_r: 2.242e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 2.168e-06, Loss_0: 5.047e-11, Loss_r: 2.168e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.098e-06, Loss_0: 4.287e-11, Loss_r: 2.098e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.031e-06, Loss_0: 4.396e-11, Loss_r: 2.031e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.968e-06, Loss_0: 3.919e-11, Loss_r: 1.968e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.908e-06, Loss_0: 2.936e-11, Loss_r: 1.908e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.850e-06, Loss_0: 3.629e-11, Loss_r: 1.850e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.795e-06, Loss_0: 3.840e-11, Loss_r: 1.795e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.742e-06, Loss_0: 3.327e-11, Loss_r: 1.742e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.692e-06, Loss_0: 2.280e-11, Loss_r: 1.692e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.644e-06, Loss_0: 2.452e-11, Loss_r: 1.644e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.597e-06, Loss_0: 2.951e-11, Loss_r: 1.597e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.553e-06, Loss_0: 1.661e-11, Loss_r: 1.553e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.510e-06, Loss_0: 2.073e-11, Loss_r: 1.510e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.469e-06, Loss_0: 3.709e-11, Loss_r: 1.469e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.430e-06, Loss_0: 2.866e-11, Loss_r: 1.430e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.392e-06, Loss_0: 2.337e-11, Loss_r: 1.392e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.356e-06, Loss_0: 2.829e-11, Loss_r: 1.356e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.321e-06, Loss_0: 6.377e-10, Loss_r: 1.321e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.901e-06, Loss_0: 5.631e-07, Loss_r: 1.338e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.289e-03, Loss_0: 1.180e-03, Loss_r: 1.085e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.442e-04, Loss_0: 3.108e-04, Loss_r: 3.339e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.878e-04, Loss_0: 1.730e-04, Loss_r: 1.478e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 5.874e-05, Loss_0: 4.810e-05, Loss_r: 1.064e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.337e-05, Loss_0: 1.556e-05, Loss_r: 7.805e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.039e-05, Loss_0: 4.782e-06, Loss_r: 5.607e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 5.747e-06, Loss_0: 1.024e-06, Loss_r: 4.723e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 4.332e-06, Loss_0: 2.867e-08, Loss_r: 4.304e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.985e-06, Loss_0: 6.418e-08, Loss_r: 3.921e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.689e-06, Loss_0: 9.173e-08, Loss_r: 3.597e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.330e-06, Loss_0: 1.828e-08, Loss_r: 3.311e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.087e-06, Loss_0: 2.643e-09, Loss_r: 3.084e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.907e-06, Loss_0: 7.541e-09, Loss_r: 2.899e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.745e-06, Loss_0: 1.580e-10, Loss_r: 2.745e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.610e-06, Loss_0: 1.632e-10, Loss_r: 2.609e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.490e-06, Loss_0: 2.870e-10, Loss_r: 2.489e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.384e-06, Loss_0: 2.920e-10, Loss_r: 2.383e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.288e-06, Loss_0: 2.260e-11, Loss_r: 2.288e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.201e-06, Loss_0: 1.309e-10, Loss_r: 2.201e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.121e-06, Loss_0: 8.054e-11, Loss_r: 2.121e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.047e-06, Loss_0: 6.784e-11, Loss_r: 2.047e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.978e-06, Loss_0: 7.219e-11, Loss_r: 1.978e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.913e-06, Loss_0: 6.010e-11, Loss_r: 1.913e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.852e-06, Loss_0: 5.430e-11, Loss_r: 1.852e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.795e-06, Loss_0: 5.324e-11, Loss_r: 1.795e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.740e-06, Loss_0: 4.361e-11, Loss_r: 1.740e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.689e-06, Loss_0: 5.204e-11, Loss_r: 1.689e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.640e-06, Loss_0: 3.467e-11, Loss_r: 1.640e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.594e-06, Loss_0: 4.153e-11, Loss_r: 1.594e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.550e-06, Loss_0: 3.781e-11, Loss_r: 1.550e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.508e-06, Loss_0: 2.736e-11, Loss_r: 1.508e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.467e-06, Loss_0: 2.957e-11, Loss_r: 1.467e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.429e-06, Loss_0: 2.826e-11, Loss_r: 1.429e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.393e-06, Loss_0: 2.968e-11, Loss_r: 1.393e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.358e-06, Loss_0: 2.194e-11, Loss_r: 1.358e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.325e-06, Loss_0: 2.323e-11, Loss_r: 1.325e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.293e-06, Loss_0: 2.303e-11, Loss_r: 1.293e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.262e-06, Loss_0: 2.413e-11, Loss_r: 1.262e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.233e-06, Loss_0: 2.137e-11, Loss_r: 1.233e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.205e-06, Loss_0: 1.811e-11, Loss_r: 1.205e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.178e-06, Loss_0: 1.708e-11, Loss_r: 1.178e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.153e-06, Loss_0: 1.545e-11, Loss_r: 1.153e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.128e-06, Loss_0: 1.702e-11, Loss_r: 1.128e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.104e-06, Loss_0: 1.351e-11, Loss_r: 1.104e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.081e-06, Loss_0: 1.967e-11, Loss_r: 1.081e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.059e-06, Loss_0: 1.135e-11, Loss_r: 1.059e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.038e-06, Loss_0: 1.115e-11, Loss_r: 1.038e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.018e-06, Loss_0: 1.254e-11, Loss_r: 1.018e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 9.980e-07, Loss_0: 1.082e-11, Loss_r: 9.980e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 9.789e-07, Loss_0: 3.724e-12, Loss_r: 9.789e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 9.605e-07, Loss_0: 3.339e-12, Loss_r: 9.605e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 9.427e-07, Loss_0: 4.509e-12, Loss_r: 9.427e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 9.255e-07, Loss_0: 4.652e-12, Loss_r: 9.255e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 9.111e-07, Loss_0: 1.814e-09, Loss_r: 9.092e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.417e-06, Loss_0: 4.616e-07, Loss_r: 9.557e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 2.722e-04, Loss_0: 2.397e-04, Loss_r: 3.251e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.530e-04, Loss_0: 2.343e-04, Loss_r: 1.871e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.255e-04, Loss_0: 1.135e-04, Loss_r: 1.197e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.122e-05, Loss_0: 6.843e-06, Loss_r: 4.379e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 9.710e-06, Loss_0: 6.439e-06, Loss_r: 3.271e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 9.668e-06, Loss_0: 6.407e-06, Loss_r: 3.261e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 3.731e-06, Loss_0: 1.051e-06, Loss_r: 2.680e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.428e-06, Loss_0: 3.871e-09, Loss_r: 2.424e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 2.268e-06, Loss_0: 3.580e-08, Loss_r: 2.232e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 2.118e-06, Loss_0: 4.553e-08, Loss_r: 2.072e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.972e-06, Loss_0: 1.501e-08, Loss_r: 1.957e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.856e-06, Loss_0: 7.031e-09, Loss_r: 1.849e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.764e-06, Loss_0: 4.883e-10, Loss_r: 1.764e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.687e-06, Loss_0: 1.411e-10, Loss_r: 1.687e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.620e-06, Loss_0: 3.993e-10, Loss_r: 1.620e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.559e-06, Loss_0: 4.222e-11, Loss_r: 1.559e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.504e-06, Loss_0: 1.833e-10, Loss_r: 1.504e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.453e-06, Loss_0: 7.813e-11, Loss_r: 1.453e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.405e-06, Loss_0: 1.082e-11, Loss_r: 1.405e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.361e-06, Loss_0: 3.486e-11, Loss_r: 1.361e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.319e-06, Loss_0: 4.867e-11, Loss_r: 1.319e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.280e-06, Loss_0: 3.878e-11, Loss_r: 1.280e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.243e-06, Loss_0: 2.857e-11, Loss_r: 1.243e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.208e-06, Loss_0: 2.404e-11, Loss_r: 1.208e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.175e-06, Loss_0: 2.153e-11, Loss_r: 1.175e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.144e-06, Loss_0: 2.646e-11, Loss_r: 1.144e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.114e-06, Loss_0: 2.555e-11, Loss_r: 1.114e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.085e-06, Loss_0: 2.634e-11, Loss_r: 1.085e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.058e-06, Loss_0: 1.973e-11, Loss_r: 1.058e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.032e-06, Loss_0: 1.555e-11, Loss_r: 1.032e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.007e-06, Loss_0: 1.457e-11, Loss_r: 1.007e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 9.828e-07, Loss_0: 1.969e-11, Loss_r: 9.828e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 9.599e-07, Loss_0: 1.833e-11, Loss_r: 9.599e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 9.378e-07, Loss_0: 1.094e-11, Loss_r: 9.378e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 9.166e-07, Loss_0: 2.307e-11, Loss_r: 9.165e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 8.961e-07, Loss_0: 5.636e-12, Loss_r: 8.961e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 8.764e-07, Loss_0: 1.461e-11, Loss_r: 8.763e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 8.573e-07, Loss_0: 5.818e-12, Loss_r: 8.573e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 8.388e-07, Loss_0: 7.900e-12, Loss_r: 8.388e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 8.209e-07, Loss_0: 5.283e-12, Loss_r: 8.209e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 8.036e-07, Loss_0: 4.721e-12, Loss_r: 8.036e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 7.868e-07, Loss_0: 1.450e-11, Loss_r: 7.867e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 7.705e-07, Loss_0: 8.558e-11, Loss_r: 7.704e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 7.590e-07, Loss_0: 4.250e-09, Loss_r: 7.547e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.878e-06, Loss_0: 1.008e-06, Loss_r: 8.697e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 7.261e-07, Loss_0: 4.796e-11, Loss_r: 7.260e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 9.060e-07, Loss_0: 1.673e-07, Loss_r: 7.387e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 7.089e-07, Loss_0: 7.047e-09, Loss_r: 7.018e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 7.171e-07, Loss_0: 2.657e-08, Loss_r: 6.905e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 6.779e-07, Loss_0: 2.017e-09, Loss_r: 6.759e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 6.633e-07, Loss_0: 4.260e-11, Loss_r: 6.632e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 6.515e-07, Loss_0: 3.358e-10, Loss_r: 6.512e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 6.395e-07, Loss_0: 1.551e-11, Loss_r: 6.395e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 6.280e-07, Loss_0: 4.009e-12, Loss_r: 6.280e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 6.168e-07, Loss_0: 1.824e-10, Loss_r: 6.167e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 6.056e-07, Loss_0: 1.434e-11, Loss_r: 6.056e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 5.948e-07, Loss_0: 5.408e-12, Loss_r: 5.948e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 5.846e-07, Loss_0: 3.379e-10, Loss_r: 5.842e-07, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 6.019e-07, Loss_0: 2.397e-08, Loss_r: 5.779e-07, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 5.166e-06, Loss_0: 3.995e-06, Loss_r: 1.171e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 6.596e-07, Loss_0: 8.911e-08, Loss_r: 5.704e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 9.819e-07, Loss_0: 3.801e-07, Loss_r: 6.018e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 7.536e-07, Loss_0: 1.878e-07, Loss_r: 5.658e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 5.571e-07, Loss_0: 2.188e-08, Loss_r: 5.352e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 5.440e-07, Loss_0: 1.794e-08, Loss_r: 5.261e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 5.282e-07, Loss_0: 1.250e-08, Loss_r: 5.157e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 5.077e-07, Loss_0: 1.438e-09, Loss_r: 5.063e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 4.977e-07, Loss_0: 1.419e-10, Loss_r: 4.975e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 4.894e-07, Loss_0: 3.479e-11, Loss_r: 4.894e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 4.815e-07, Loss_0: 1.976e-10, Loss_r: 4.813e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 4.736e-07, Loss_0: 1.197e-10, Loss_r: 4.735e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 4.656e-07, Loss_0: 1.075e-13, Loss_r: 4.656e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 4.580e-07, Loss_0: 9.678e-11, Loss_r: 4.579e-07, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 4.511e-07, Loss_0: 7.941e-10, Loss_r: 4.503e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 4.709e-07, Loss_0: 2.471e-08, Loss_r: 4.462e-07, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 3.173e-06, Loss_0: 2.367e-06, Loss_r: 8.060e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2450, Loss: 6.116e-07, Loss_0: 1.581e-07, Loss_r: 4.536e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 5.387e-07, Loss_0: 9.782e-08, Loss_r: 4.409e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 5.830e-07, Loss_0: 1.410e-07, Loss_r: 4.420e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 4.122e-07, Loss_0: 3.272e-10, Loss_r: 4.119e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 4.282e-07, Loss_0: 1.984e-08, Loss_r: 4.083e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 4.004e-07, Loss_0: 7.005e-10, Loss_r: 3.997e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 3.945e-07, Loss_0: 7.839e-10, Loss_r: 3.937e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 3.886e-07, Loss_0: 1.046e-09, Loss_r: 3.876e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 3.821e-07, Loss_0: 3.793e-10, Loss_r: 3.817e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 3.760e-07, Loss_0: 2.063e-10, Loss_r: 3.758e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 3.701e-07, Loss_0: 2.342e-11, Loss_r: 3.701e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 3.644e-07, Loss_0: 5.523e-13, Loss_r: 3.644e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 3.588e-07, Loss_0: 2.501e-11, Loss_r: 3.588e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 3.533e-07, Loss_0: 2.363e-11, Loss_r: 3.533e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 3.480e-07, Loss_0: 1.347e-10, Loss_r: 3.479e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 3.469e-07, Loss_0: 3.885e-09, Loss_r: 3.430e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 7.618e-07, Loss_0: 3.647e-07, Loss_r: 3.971e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2620, Loss: 3.603e-07, Loss_0: 2.399e-08, Loss_r: 3.363e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2630, Loss: 3.470e-07, Loss_0: 1.578e-08, Loss_r: 3.312e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2640, Loss: 3.490e-07, Loss_0: 2.128e-08, Loss_r: 3.277e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 3.191e-07, Loss_0: 2.739e-12, Loss_r: 3.191e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 3.182e-07, Loss_0: 3.154e-09, Loss_r: 3.151e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 3.105e-07, Loss_0: 1.369e-10, Loss_r: 3.104e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 3.061e-07, Loss_0: 8.340e-11, Loss_r: 3.060e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 3.018e-07, Loss_0: 1.500e-10, Loss_r: 3.017e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 2.975e-07, Loss_0: 3.184e-11, Loss_r: 2.975e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 2.933e-07, Loss_0: 3.852e-11, Loss_r: 2.933e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 2.892e-07, Loss_0: 3.557e-12, Loss_r: 2.892e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.851e-07, Loss_0: 2.419e-12, Loss_r: 2.851e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 2.811e-07, Loss_0: 3.030e-11, Loss_r: 2.811e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 2.772e-07, Loss_0: 3.974e-11, Loss_r: 2.771e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.735e-07, Loss_0: 2.174e-10, Loss_r: 2.732e-07, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.747e-07, Loss_0: 4.635e-09, Loss_r: 2.700e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 6.416e-07, Loss_0: 3.204e-07, Loss_r: 3.212e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2790, Loss: 2.992e-07, Loss_0: 3.166e-08, Loss_r: 2.675e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2800, Loss: 2.666e-07, Loss_0: 6.193e-09, Loss_r: 2.604e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2810, Loss: 2.775e-07, Loss_0: 1.814e-08, Loss_r: 2.594e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 2.539e-07, Loss_0: 1.097e-09, Loss_r: 2.528e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 2.518e-07, Loss_0: 2.152e-09, Loss_r: 2.496e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 2.462e-07, Loss_0: 6.679e-11, Loss_r: 2.462e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 2.434e-07, Loss_0: 2.957e-10, Loss_r: 2.432e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 2.401e-07, Loss_0: 1.269e-10, Loss_r: 2.399e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 2.369e-07, Loss_0: 8.563e-12, Loss_r: 2.369e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 2.339e-07, Loss_0: 6.096e-12, Loss_r: 2.339e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 2.309e-07, Loss_0: 2.501e-13, Loss_r: 2.309e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 2.279e-07, Loss_0: 4.329e-12, Loss_r: 2.279e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 2.250e-07, Loss_0: 7.073e-13, Loss_r: 2.250e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 2.222e-07, Loss_0: 7.183e-15, Loss_r: 2.222e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 2.193e-07, Loss_0: 3.574e-14, Loss_r: 2.193e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 2.166e-07, Loss_0: 2.969e-12, Loss_r: 2.166e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 2.138e-07, Loss_0: 7.615e-12, Loss_r: 2.138e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 2.113e-07, Loss_0: 2.690e-10, Loss_r: 2.111e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 2.645e-07, Loss_0: 4.772e-08, Loss_r: 2.168e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2980, Loss: 2.066e-07, Loss_0: 5.398e-10, Loss_r: 2.061e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 2990, Loss: 2.104e-07, Loss_0: 5.539e-09, Loss_r: 2.048e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3000, Loss: 2.034e-07, Loss_0: 1.651e-09, Loss_r: 2.017e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3010, Loss: 1.996e-07, Loss_0: 5.402e-10, Loss_r: 1.991e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 1.969e-07, Loss_0: 1.493e-10, Loss_r: 1.968e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 1.947e-07, Loss_0: 1.351e-10, Loss_r: 1.946e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 1.923e-07, Loss_0: 3.770e-11, Loss_r: 1.923e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 1.901e-07, Loss_0: 5.640e-14, Loss_r: 1.901e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 1.879e-07, Loss_0: 1.766e-12, Loss_r: 1.879e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.858e-07, Loss_0: 7.135e-13, Loss_r: 1.858e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.837e-07, Loss_0: 4.462e-12, Loss_r: 1.837e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.816e-07, Loss_0: 6.490e-13, Loss_r: 1.816e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.795e-07, Loss_0: 4.775e-13, Loss_r: 1.795e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.775e-07, Loss_0: 4.827e-13, Loss_r: 1.775e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.755e-07, Loss_0: 1.259e-13, Loss_r: 1.755e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.735e-07, Loss_0: 4.393e-11, Loss_r: 1.735e-07, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.771e-07, Loss_0: 4.600e-09, Loss_r: 1.725e-07, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.173e-06, Loss_0: 8.394e-07, Loss_r: 3.339e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3160, Loss: 1.895e-07, Loss_0: 1.781e-08, Loss_r: 1.717e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3170, Loss: 2.604e-07, Loss_0: 7.919e-08, Loss_r: 1.813e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3180, Loss: 2.128e-07, Loss_0: 4.049e-08, Loss_r: 1.723e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3190, Loss: 1.670e-07, Loss_0: 3.126e-09, Loss_r: 1.639e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3200, Loss: 1.676e-07, Loss_0: 4.948e-09, Loss_r: 1.626e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 1.621e-07, Loss_0: 1.898e-09, Loss_r: 1.602e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 1.583e-07, Loss_0: 4.335e-13, Loss_r: 1.583e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 1.568e-07, Loss_0: 9.113e-11, Loss_r: 1.567e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.552e-07, Loss_0: 1.021e-10, Loss_r: 1.551e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.536e-07, Loss_0: 2.917e-11, Loss_r: 1.536e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.520e-07, Loss_0: 9.968e-12, Loss_r: 1.520e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.505e-07, Loss_0: 6.660e-12, Loss_r: 1.505e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.490e-07, Loss_0: 8.027e-13, Loss_r: 1.490e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.475e-07, Loss_0: 2.364e-12, Loss_r: 1.475e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.460e-07, Loss_0: 1.530e-11, Loss_r: 1.460e-07, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 1.448e-07, Loss_0: 2.205e-10, Loss_r: 1.446e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.553e-07, Loss_0: 1.003e-08, Loss_r: 1.453e-07, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.412e-06, Loss_0: 1.058e-06, Loss_r: 3.543e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3340, Loss: 2.310e-07, Loss_0: 7.501e-08, Loss_r: 1.560e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3350, Loss: 1.820e-07, Loss_0: 3.572e-08, Loss_r: 1.463e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3360, Loss: 2.136e-07, Loss_0: 6.313e-08, Loss_r: 1.505e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3370, Loss: 1.396e-07, Loss_0: 2.120e-09, Loss_r: 1.375e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3380, Loss: 1.447e-07, Loss_0: 7.208e-09, Loss_r: 1.375e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3390, Loss: 1.348e-07, Loss_0: 9.198e-11, Loss_r: 1.347e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3400, Loss: 1.348e-07, Loss_0: 1.115e-09, Loss_r: 1.337e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3410, Loss: 1.327e-07, Loss_0: 2.503e-10, Loss_r: 1.324e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 1.312e-07, Loss_0: 2.420e-11, Loss_r: 1.312e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.300e-07, Loss_0: 1.952e-12, Loss_r: 1.300e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.289e-07, Loss_0: 1.804e-13, Loss_r: 1.289e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.277e-07, Loss_0: 2.989e-13, Loss_r: 1.277e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.266e-07, Loss_0: 8.175e-12, Loss_r: 1.266e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.255e-07, Loss_0: 2.448e-14, Loss_r: 1.255e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.244e-07, Loss_0: 3.302e-13, Loss_r: 1.244e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.233e-07, Loss_0: 5.496e-13, Loss_r: 1.233e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.222e-07, Loss_0: 1.611e-13, Loss_r: 1.222e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.212e-07, Loss_0: 2.473e-13, Loss_r: 1.212e-07, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.202e-07, Loss_0: 3.415e-11, Loss_r: 1.201e-07, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.301e-07, Loss_0: 9.053e-09, Loss_r: 1.211e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3540, Loss: 1.183e-07, Loss_0: 8.352e-11, Loss_r: 1.182e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3550, Loss: 1.186e-07, Loss_0: 1.125e-09, Loss_r: 1.174e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3560, Loss: 1.168e-07, Loss_0: 3.644e-10, Loss_r: 1.164e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3570, Loss: 1.155e-07, Loss_0: 7.088e-11, Loss_r: 1.155e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3580, Loss: 1.146e-07, Loss_0: 2.758e-11, Loss_r: 1.146e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3590, Loss: 1.137e-07, Loss_0: 3.290e-11, Loss_r: 1.137e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3600, Loss: 1.128e-07, Loss_0: 2.074e-13, Loss_r: 1.128e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 1.119e-07, Loss_0: 2.520e-13, Loss_r: 1.119e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 1.111e-07, Loss_0: 6.886e-13, Loss_r: 1.111e-07, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 1.102e-07, Loss_0: 8.552e-14, Loss_r: 1.102e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 1.094e-07, Loss_0: 3.824e-13, Loss_r: 1.094e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 1.086e-07, Loss_0: 6.464e-14, Loss_r: 1.086e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 1.077e-07, Loss_0: 8.778e-13, Loss_r: 1.077e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 1.070e-07, Loss_0: 4.724e-13, Loss_r: 1.070e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 1.061e-07, Loss_0: 2.433e-12, Loss_r: 1.061e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 1.054e-07, Loss_0: 6.355e-11, Loss_r: 1.053e-07, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 1.074e-07, Loss_0: 2.329e-09, Loss_r: 1.051e-07, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 3.617e-07, Loss_0: 2.130e-07, Loss_r: 1.486e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3720, Loss: 1.243e-07, Loss_0: 1.744e-08, Loss_r: 1.069e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3730, Loss: 1.093e-07, Loss_0: 5.773e-09, Loss_r: 1.035e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3740, Loss: 1.170e-07, Loss_0: 1.271e-08, Loss_r: 1.043e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3750, Loss: 1.019e-07, Loss_0: 7.566e-10, Loss_r: 1.012e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3760, Loss: 1.020e-07, Loss_0: 1.322e-09, Loss_r: 1.007e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3770, Loss: 9.983e-08, Loss_0: 7.473e-11, Loss_r: 9.975e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3780, Loss: 9.935e-08, Loss_0: 2.517e-10, Loss_r: 9.910e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3790, Loss: 9.843e-08, Loss_0: 2.156e-11, Loss_r: 9.841e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 9.775e-08, Loss_0: 1.978e-14, Loss_r: 9.775e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 9.711e-08, Loss_0: 8.292e-12, Loss_r: 9.710e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 9.646e-08, Loss_0: 5.391e-12, Loss_r: 9.646e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 9.583e-08, Loss_0: 4.766e-12, Loss_r: 9.582e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 9.521e-08, Loss_0: 4.513e-12, Loss_r: 9.520e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 9.458e-08, Loss_0: 4.096e-12, Loss_r: 9.457e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 9.398e-08, Loss_0: 1.086e-11, Loss_r: 9.397e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 9.339e-08, Loss_0: 4.020e-11, Loss_r: 9.335e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 9.298e-08, Loss_0: 2.002e-10, Loss_r: 9.278e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 9.720e-08, Loss_0: 4.235e-09, Loss_r: 9.297e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 4.150e-07, Loss_0: 2.674e-07, Loss_r: 1.475e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3910, Loss: 1.302e-07, Loss_0: 3.249e-08, Loss_r: 9.775e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3920, Loss: 9.284e-08, Loss_0: 1.864e-09, Loss_r: 9.097e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3930, Loss: 1.059e-07, Loss_0: 1.299e-08, Loss_r: 9.290e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3940, Loss: 9.298e-08, Loss_0: 2.811e-09, Loss_r: 9.017e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3950, Loss: 8.972e-08, Loss_0: 6.143e-10, Loss_r: 8.911e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3960, Loss: 8.911e-08, Loss_0: 5.360e-10, Loss_r: 8.858e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3970, Loss: 8.817e-08, Loss_0: 1.394e-10, Loss_r: 8.803e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3980, Loss: 8.748e-08, Loss_0: 2.696e-12, Loss_r: 8.748e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 3990, Loss: 8.701e-08, Loss_0: 2.910e-11, Loss_r: 8.698e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 8.651e-08, Loss_0: 1.357e-11, Loss_r: 8.649e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 8.600e-08, Loss_0: 6.179e-12, Loss_r: 8.599e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 8.550e-08, Loss_0: 1.087e-13, Loss_r: 8.550e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 8.501e-08, Loss_0: 1.187e-13, Loss_r: 8.501e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 8.453e-08, Loss_0: 3.717e-14, Loss_r: 8.453e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 8.406e-08, Loss_0: 3.377e-13, Loss_r: 8.406e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 8.359e-08, Loss_0: 1.839e-12, Loss_r: 8.359e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 8.312e-08, Loss_0: 1.908e-12, Loss_r: 8.312e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 8.265e-08, Loss_0: 4.778e-12, Loss_r: 8.265e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 8.229e-08, Loss_0: 7.641e-11, Loss_r: 8.221e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 8.994e-08, Loss_0: 6.704e-09, Loss_r: 8.324e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4110, Loss: 8.203e-08, Loss_0: 5.766e-10, Loss_r: 8.145e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4120, Loss: 8.109e-08, Loss_0: 1.650e-10, Loss_r: 8.092e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4130, Loss: 8.096e-08, Loss_0: 3.989e-10, Loss_r: 8.056e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4140, Loss: 8.012e-08, Loss_0: 2.464e-11, Loss_r: 8.010e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4150, Loss: 7.974e-08, Loss_0: 3.674e-11, Loss_r: 7.970e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4160, Loss: 7.930e-08, Loss_0: 1.614e-12, Loss_r: 7.930e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4170, Loss: 7.891e-08, Loss_0: 9.711e-12, Loss_r: 7.890e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4180, Loss: 7.852e-08, Loss_0: 2.810e-16, Loss_r: 7.852e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 7.812e-08, Loss_0: 1.105e-13, Loss_r: 7.812e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 7.774e-08, Loss_0: 7.502e-13, Loss_r: 7.774e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 7.734e-08, Loss_0: 4.931e-13, Loss_r: 7.734e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 7.696e-08, Loss_0: 4.954e-14, Loss_r: 7.696e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 7.659e-08, Loss_0: 2.409e-13, Loss_r: 7.659e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 7.622e-08, Loss_0: 2.138e-14, Loss_r: 7.622e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 7.584e-08, Loss_0: 2.097e-12, Loss_r: 7.584e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 7.548e-08, Loss_0: 6.464e-12, Loss_r: 7.547e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 7.510e-08, Loss_0: 8.865e-12, Loss_r: 7.509e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 7.481e-08, Loss_0: 7.361e-11, Loss_r: 7.474e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 7.795e-08, Loss_0: 2.986e-09, Loss_r: 7.497e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 4.539e-07, Loss_0: 3.126e-07, Loss_r: 1.413e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4310, Loss: 1.022e-07, Loss_0: 2.350e-08, Loss_r: 7.866e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4320, Loss: 8.484e-08, Loss_0: 9.331e-09, Loss_r: 7.551e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4330, Loss: 9.557e-08, Loss_0: 1.835e-08, Loss_r: 7.722e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4340, Loss: 7.395e-08, Loss_0: 9.273e-10, Loss_r: 7.302e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4350, Loss: 7.477e-08, Loss_0: 1.913e-09, Loss_r: 7.286e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4360, Loss: 7.233e-08, Loss_0: 1.404e-10, Loss_r: 7.219e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 7.225e-08, Loss_0: 3.170e-10, Loss_r: 7.193e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 7.158e-08, Loss_0: 4.051e-11, Loss_r: 7.154e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 7.122e-08, Loss_0: 1.224e-12, Loss_r: 7.122e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 7.093e-08, Loss_0: 6.898e-12, Loss_r: 7.093e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 7.062e-08, Loss_0: 5.139e-12, Loss_r: 7.061e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 7.030e-08, Loss_0: 1.153e-12, Loss_r: 7.030e-08, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 7.000e-08, Loss_0: 3.627e-12, Loss_r: 7.000e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 6.970e-08, Loss_0: 1.165e-12, Loss_r: 6.970e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 6.941e-08, Loss_0: 6.873e-12, Loss_r: 6.940e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 6.910e-08, Loss_0: 5.404e-12, Loss_r: 6.910e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 6.881e-08, Loss_0: 2.442e-12, Loss_r: 6.881e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 6.858e-08, Loss_0: 5.548e-11, Loss_r: 6.852e-08, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 7.130e-08, Loss_0: 2.501e-09, Loss_r: 6.880e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4500, Loss: 6.846e-08, Loss_0: 4.003e-10, Loss_r: 6.806e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4510, Loss: 6.770e-08, Loss_0: 1.137e-12, Loss_r: 6.770e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4520, Loss: 6.754e-08, Loss_0: 9.958e-11, Loss_r: 6.744e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4530, Loss: 6.723e-08, Loss_0: 5.376e-11, Loss_r: 6.718e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4540, Loss: 6.691e-08, Loss_0: 6.370e-14, Loss_r: 6.691e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4550, Loss: 6.667e-08, Loss_0: 4.847e-12, Loss_r: 6.666e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4560, Loss: 6.640e-08, Loss_0: 4.547e-13, Loss_r: 6.640e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4570, Loss: 6.615e-08, Loss_0: 1.266e-12, Loss_r: 6.615e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 6.590e-08, Loss_0: 3.645e-14, Loss_r: 6.590e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 6.564e-08, Loss_0: 3.464e-13, Loss_r: 6.564e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 6.538e-08, Loss_0: 9.273e-13, Loss_r: 6.538e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 6.513e-08, Loss_0: 4.236e-14, Loss_r: 6.513e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 6.489e-08, Loss_0: 1.952e-14, Loss_r: 6.489e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 6.464e-08, Loss_0: 1.444e-13, Loss_r: 6.464e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 6.439e-08, Loss_0: 1.032e-13, Loss_r: 6.439e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 6.415e-08, Loss_0: 4.709e-14, Loss_r: 6.415e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 6.391e-08, Loss_0: 6.612e-12, Loss_r: 6.390e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 6.372e-08, Loss_0: 5.032e-11, Loss_r: 6.367e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 6.448e-08, Loss_0: 8.561e-10, Loss_r: 6.363e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 1.046e-07, Loss_0: 3.381e-08, Loss_r: 7.078e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4700, Loss: 6.989e-08, Loss_0: 5.627e-09, Loss_r: 6.426e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4710, Loss: 6.275e-08, Loss_0: 1.867e-13, Loss_r: 6.275e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4720, Loss: 6.378e-08, Loss_0: 1.047e-09, Loss_r: 6.274e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4730, Loss: 6.301e-08, Loss_0: 5.821e-10, Loss_r: 6.243e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4740, Loss: 6.209e-08, Loss_0: 8.044e-13, Loss_r: 6.209e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4750, Loss: 6.198e-08, Loss_0: 7.120e-11, Loss_r: 6.191e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4760, Loss: 6.167e-08, Loss_0: 5.413e-13, Loss_r: 6.167e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4770, Loss: 6.146e-08, Loss_0: 7.744e-12, Loss_r: 6.145e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 6.125e-08, Loss_0: 8.245e-13, Loss_r: 6.125e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 6.104e-08, Loss_0: 6.747e-13, Loss_r: 6.104e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 6.083e-08, Loss_0: 1.604e-14, Loss_r: 6.083e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 6.062e-08, Loss_0: 7.518e-13, Loss_r: 6.062e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 6.042e-08, Loss_0: 4.372e-15, Loss_r: 6.042e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 6.020e-08, Loss_0: 1.581e-12, Loss_r: 6.019e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 5.999e-08, Loss_0: 8.467e-13, Loss_r: 5.999e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 5.979e-08, Loss_0: 3.929e-13, Loss_r: 5.979e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 5.958e-08, Loss_0: 9.875e-13, Loss_r: 5.958e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 5.941e-08, Loss_0: 2.423e-11, Loss_r: 5.939e-08, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 5.943e-08, Loss_0: 2.180e-10, Loss_r: 5.921e-08, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 6.743e-08, Loss_0: 6.956e-09, Loss_r: 6.047e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4900, Loss: 6.033e-08, Loss_0: 1.287e-09, Loss_r: 5.905e-08, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4910, Loss: 5.862e-08, Loss_0: 9.088e-12, Loss_r: 5.861e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4920, Loss: 5.865e-08, Loss_0: 1.800e-10, Loss_r: 5.847e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4930, Loss: 5.841e-08, Loss_0: 1.350e-10, Loss_r: 5.827e-08, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4940, Loss: 5.806e-08, Loss_0: 1.628e-12, Loss_r: 5.806e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4950, Loss: 5.791e-08, Loss_0: 2.510e-11, Loss_r: 5.788e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4960, Loss: 5.770e-08, Loss_0: 7.325e-13, Loss_r: 5.770e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 5.752e-08, Loss_0: 4.925e-12, Loss_r: 5.752e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 5.734e-08, Loss_0: 2.681e-12, Loss_r: 5.734e-08, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 5.715e-08, Loss_0: 1.175e-13, Loss_r: 5.715e-08, Time: 0.03, Learning Rate: 0.00019\n",
            "Training time: 20.5958\n",
            "[1, 128, 128, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.448e-02, Loss_0: 4.999e-04, Loss_r: 3.398e-02, Time: 0.27, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.157e-02, Loss_0: 1.231e-04, Loss_r: 3.145e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.848e-02, Loss_0: 9.461e-05, Loss_r: 2.838e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.706e-02, Loss_0: 1.246e-05, Loss_r: 1.705e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 3.176e-03, Loss_0: 6.712e-06, Loss_r: 3.169e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 7.692e-04, Loss_0: 3.036e-04, Loss_r: 4.656e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 2.863e-04, Loss_0: 3.353e-07, Loss_r: 2.860e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.363e-04, Loss_0: 4.015e-05, Loss_r: 9.616e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.582e-05, Loss_0: 1.516e-05, Loss_r: 4.066e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 3.322e-05, Loss_0: 7.029e-06, Loss_r: 2.619e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.678e-05, Loss_0: 1.216e-08, Loss_r: 1.677e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.170e-05, Loss_0: 3.095e-07, Loss_r: 1.139e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 9.555e-06, Loss_0: 1.888e-07, Loss_r: 9.367e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 8.686e-06, Loss_0: 3.365e-07, Loss_r: 8.349e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.124e-05, Loss_0: 3.753e-06, Loss_r: 7.490e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.564e-05, Loss_0: 8.299e-06, Loss_r: 7.338e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 6.066e-05, Loss_0: 5.176e-05, Loss_r: 8.897e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 170, Loss: 3.856e-05, Loss_0: 3.152e-05, Loss_r: 7.037e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.752e-05, Loss_0: 1.061e-05, Loss_r: 6.911e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 6.318e-06, Loss_0: 7.132e-08, Loss_r: 6.247e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.186e-06, Loss_0: 1.410e-06, Loss_r: 5.777e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 6.165e-06, Loss_0: 7.510e-07, Loss_r: 5.414e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.203e-06, Loss_0: 1.150e-07, Loss_r: 5.088e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 4.931e-06, Loss_0: 1.271e-07, Loss_r: 4.803e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 4.609e-06, Loss_0: 1.121e-07, Loss_r: 4.497e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 4.329e-06, Loss_0: 8.204e-08, Loss_r: 4.247e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.978e-05, Loss_0: 1.571e-05, Loss_r: 4.077e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.670e-04, Loss_0: 1.613e-04, Loss_r: 5.702e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.799e-05, Loss_0: 3.305e-05, Loss_r: 4.945e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.938e-05, Loss_0: 3.479e-05, Loss_r: 4.584e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.528e-05, Loss_0: 1.059e-05, Loss_r: 4.690e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.256e-06, Loss_0: 4.589e-11, Loss_r: 4.256e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.970e-06, Loss_0: 1.929e-06, Loss_r: 4.041e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.023e-06, Loss_0: 1.503e-07, Loss_r: 3.873e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 3.721e-06, Loss_0: 5.175e-10, Loss_r: 3.721e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 3.568e-06, Loss_0: 2.249e-10, Loss_r: 3.568e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.447e-06, Loss_0: 2.297e-08, Loss_r: 3.424e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.309e-06, Loss_0: 1.443e-08, Loss_r: 3.295e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.185e-06, Loss_0: 5.691e-09, Loss_r: 3.179e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.070e-06, Loss_0: 6.045e-09, Loss_r: 3.064e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.962e-06, Loss_0: 5.235e-10, Loss_r: 2.962e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.865e-06, Loss_0: 4.009e-14, Loss_r: 2.865e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.811e-06, Loss_0: 3.269e-08, Loss_r: 2.779e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 8.222e-05, Loss_0: 7.844e-05, Loss_r: 3.774e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.615e-04, Loss_0: 1.554e-04, Loss_r: 6.107e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.428e-05, Loss_0: 3.966e-05, Loss_r: 4.620e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 7.349e-06, Loss_0: 3.000e-06, Loss_r: 4.348e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.921e-05, Loss_0: 1.521e-05, Loss_r: 3.997e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 9.924e-06, Loss_0: 5.907e-06, Loss_r: 4.017e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.735e-06, Loss_0: 1.112e-06, Loss_r: 3.623e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.703e-06, Loss_0: 1.848e-07, Loss_r: 3.518e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.456e-06, Loss_0: 9.344e-08, Loss_r: 3.363e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.320e-06, Loss_0: 6.400e-08, Loss_r: 3.256e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.176e-06, Loss_0: 4.672e-08, Loss_r: 3.129e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 3.032e-06, Loss_0: 2.849e-09, Loss_r: 3.029e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.930e-06, Loss_0: 1.608e-09, Loss_r: 2.929e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.835e-06, Loss_0: 1.634e-09, Loss_r: 2.834e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.748e-06, Loss_0: 1.356e-09, Loss_r: 2.746e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.665e-06, Loss_0: 3.310e-10, Loss_r: 2.664e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 2.587e-06, Loss_0: 1.244e-10, Loss_r: 2.587e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 2.513e-06, Loss_0: 1.035e-10, Loss_r: 2.513e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 2.443e-06, Loss_0: 1.289e-10, Loss_r: 2.443e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.376e-06, Loss_0: 1.169e-10, Loss_r: 2.376e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.313e-06, Loss_0: 7.004e-11, Loss_r: 2.312e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.252e-06, Loss_0: 3.999e-11, Loss_r: 2.252e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.193e-06, Loss_0: 7.329e-11, Loss_r: 2.193e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.138e-06, Loss_0: 5.264e-11, Loss_r: 2.138e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.084e-06, Loss_0: 4.534e-11, Loss_r: 2.084e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.032e-06, Loss_0: 1.764e-11, Loss_r: 2.032e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.983e-06, Loss_0: 7.063e-11, Loss_r: 1.983e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.978e-06, Loss_0: 3.934e-08, Loss_r: 1.938e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.310e-05, Loss_0: 3.080e-05, Loss_r: 2.303e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 2.422e-04, Loss_0: 2.339e-04, Loss_r: 8.247e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 4.314e-05, Loss_0: 3.835e-05, Loss_r: 4.794e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 2.350e-05, Loss_0: 2.002e-05, Loss_r: 3.476e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 8.812e-06, Loss_0: 5.757e-06, Loss_r: 3.055e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 4.865e-06, Loss_0: 1.785e-06, Loss_r: 3.079e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 3.876e-06, Loss_0: 1.046e-06, Loss_r: 2.830e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.424e-06, Loss_0: 6.592e-07, Loss_r: 2.765e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.961e-06, Loss_0: 3.353e-07, Loss_r: 2.625e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.585e-06, Loss_0: 3.687e-08, Loss_r: 2.548e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.466e-06, Loss_0: 6.808e-09, Loss_r: 2.459e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.392e-06, Loss_0: 1.658e-08, Loss_r: 2.375e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.304e-06, Loss_0: 1.022e-09, Loss_r: 2.303e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.235e-06, Loss_0: 7.142e-10, Loss_r: 2.235e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.170e-06, Loss_0: 2.930e-10, Loss_r: 2.170e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.109e-06, Loss_0: 1.666e-12, Loss_r: 2.109e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.051e-06, Loss_0: 3.285e-11, Loss_r: 2.051e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.997e-06, Loss_0: 4.221e-11, Loss_r: 1.997e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.946e-06, Loss_0: 3.177e-11, Loss_r: 1.946e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.897e-06, Loss_0: 3.287e-11, Loss_r: 1.897e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.851e-06, Loss_0: 3.705e-11, Loss_r: 1.851e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.806e-06, Loss_0: 5.231e-11, Loss_r: 1.806e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.764e-06, Loss_0: 4.484e-11, Loss_r: 1.764e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.724e-06, Loss_0: 2.765e-11, Loss_r: 1.724e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.685e-06, Loss_0: 4.775e-11, Loss_r: 1.685e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.648e-06, Loss_0: 2.054e-11, Loss_r: 1.648e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.613e-06, Loss_0: 2.110e-11, Loss_r: 1.613e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.578e-06, Loss_0: 1.337e-11, Loss_r: 1.578e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.546e-06, Loss_0: 8.915e-12, Loss_r: 1.546e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.515e-06, Loss_0: 1.409e-10, Loss_r: 1.514e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.557e-06, Loss_0: 6.945e-08, Loss_r: 1.487e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 5.599e-05, Loss_0: 5.386e-05, Loss_r: 2.133e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 4.103e-04, Loss_0: 4.022e-04, Loss_r: 8.103e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 5.638e-06, Loss_0: 2.880e-06, Loss_r: 2.758e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 5.152e-06, Loss_0: 2.144e-06, Loss_r: 3.008e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 2.831e-06, Loss_0: 1.239e-08, Loss_r: 2.818e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.786e-06, Loss_0: 1.393e-07, Loss_r: 2.647e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.598e-06, Loss_0: 6.019e-08, Loss_r: 2.538e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.428e-06, Loss_0: 4.587e-10, Loss_r: 2.427e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 2.361e-06, Loss_0: 3.603e-08, Loss_r: 2.325e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.303e-06, Loss_0: 6.141e-08, Loss_r: 2.242e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.195e-06, Loss_0: 4.318e-08, Loss_r: 2.152e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.085e-06, Loss_0: 2.744e-09, Loss_r: 2.083e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.016e-06, Loss_0: 1.219e-09, Loss_r: 2.015e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.954e-06, Loss_0: 2.092e-09, Loss_r: 1.952e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.896e-06, Loss_0: 4.840e-10, Loss_r: 1.895e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.842e-06, Loss_0: 5.038e-12, Loss_r: 1.842e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.793e-06, Loss_0: 2.554e-12, Loss_r: 1.793e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.747e-06, Loss_0: 1.698e-11, Loss_r: 1.747e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.703e-06, Loss_0: 4.604e-11, Loss_r: 1.703e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.662e-06, Loss_0: 4.037e-11, Loss_r: 1.662e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.624e-06, Loss_0: 3.604e-11, Loss_r: 1.624e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.587e-06, Loss_0: 3.162e-11, Loss_r: 1.587e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.552e-06, Loss_0: 3.011e-11, Loss_r: 1.552e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.519e-06, Loss_0: 2.785e-11, Loss_r: 1.519e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.487e-06, Loss_0: 2.818e-11, Loss_r: 1.487e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.457e-06, Loss_0: 2.371e-11, Loss_r: 1.457e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.428e-06, Loss_0: 3.660e-11, Loss_r: 1.428e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.400e-06, Loss_0: 1.705e-11, Loss_r: 1.400e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.373e-06, Loss_0: 2.317e-11, Loss_r: 1.373e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.348e-06, Loss_0: 1.753e-11, Loss_r: 1.348e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.323e-06, Loss_0: 2.767e-11, Loss_r: 1.323e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.300e-06, Loss_0: 2.003e-11, Loss_r: 1.300e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.277e-06, Loss_0: 2.372e-11, Loss_r: 1.277e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.255e-06, Loss_0: 2.433e-11, Loss_r: 1.255e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.234e-06, Loss_0: 8.964e-11, Loss_r: 1.234e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.218e-06, Loss_0: 4.870e-09, Loss_r: 1.213e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.567e-06, Loss_0: 2.361e-06, Loss_r: 1.206e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.079e-03, Loss_0: 1.069e-03, Loss_r: 9.459e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 2.177e-04, Loss_0: 2.137e-04, Loss_r: 3.967e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 9.147e-05, Loss_0: 8.589e-05, Loss_r: 5.577e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 2.454e-05, Loss_0: 2.149e-05, Loss_r: 3.046e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 5.486e-06, Loss_0: 2.719e-06, Loss_r: 2.768e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.525e-06, Loss_0: 1.248e-07, Loss_r: 2.400e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.290e-06, Loss_0: 6.574e-09, Loss_r: 2.283e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.188e-06, Loss_0: 6.439e-09, Loss_r: 2.182e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.083e-06, Loss_0: 3.050e-10, Loss_r: 2.083e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 1.999e-06, Loss_0: 5.249e-09, Loss_r: 1.994e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.927e-06, Loss_0: 8.184e-09, Loss_r: 1.919e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.856e-06, Loss_0: 9.298e-09, Loss_r: 1.847e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.789e-06, Loss_0: 1.225e-09, Loss_r: 1.787e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.731e-06, Loss_0: 2.374e-11, Loss_r: 1.731e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.680e-06, Loss_0: 6.292e-10, Loss_r: 1.679e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.632e-06, Loss_0: 5.526e-12, Loss_r: 1.632e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.589e-06, Loss_0: 3.066e-14, Loss_r: 1.589e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 1.548e-06, Loss_0: 4.387e-11, Loss_r: 1.548e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.510e-06, Loss_0: 7.531e-11, Loss_r: 1.509e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.474e-06, Loss_0: 5.691e-11, Loss_r: 1.474e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.440e-06, Loss_0: 3.877e-11, Loss_r: 1.440e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 1.408e-06, Loss_0: 2.633e-11, Loss_r: 1.408e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.378e-06, Loss_0: 3.034e-11, Loss_r: 1.378e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.349e-06, Loss_0: 2.940e-11, Loss_r: 1.349e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.321e-06, Loss_0: 2.718e-11, Loss_r: 1.321e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.295e-06, Loss_0: 2.034e-11, Loss_r: 1.295e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.270e-06, Loss_0: 2.427e-11, Loss_r: 1.270e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.246e-06, Loss_0: 1.599e-11, Loss_r: 1.246e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.223e-06, Loss_0: 1.791e-11, Loss_r: 1.223e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.201e-06, Loss_0: 1.531e-11, Loss_r: 1.201e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.180e-06, Loss_0: 2.761e-11, Loss_r: 1.180e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.159e-06, Loss_0: 1.153e-11, Loss_r: 1.159e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.140e-06, Loss_0: 2.398e-11, Loss_r: 1.140e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.121e-06, Loss_0: 1.038e-11, Loss_r: 1.121e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.103e-06, Loss_0: 1.589e-11, Loss_r: 1.103e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.085e-06, Loss_0: 1.882e-11, Loss_r: 1.085e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.068e-06, Loss_0: 1.145e-11, Loss_r: 1.068e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.051e-06, Loss_0: 7.385e-12, Loss_r: 1.051e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.035e-06, Loss_0: 3.522e-11, Loss_r: 1.035e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.029e-06, Loss_0: 8.290e-09, Loss_r: 1.020e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 3.304e-06, Loss_0: 2.265e-06, Loss_r: 1.040e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 7.272e-04, Loss_0: 7.149e-04, Loss_r: 1.236e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.568e-04, Loss_0: 1.537e-04, Loss_r: 3.097e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 8.029e-06, Loss_0: 4.540e-06, Loss_r: 3.490e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.609e-05, Loss_0: 1.401e-05, Loss_r: 2.079e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.062e-05, Loss_0: 8.485e-06, Loss_r: 2.136e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.063e-06, Loss_0: 3.158e-06, Loss_r: 1.906e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 2.334e-06, Loss_0: 4.900e-07, Loss_r: 1.844e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.721e-06, Loss_0: 1.515e-09, Loss_r: 1.720e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.760e-06, Loss_0: 1.162e-07, Loss_r: 1.644e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.615e-06, Loss_0: 3.113e-08, Loss_r: 1.584e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.526e-06, Loss_0: 2.329e-09, Loss_r: 1.524e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.477e-06, Loss_0: 6.822e-09, Loss_r: 1.471e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.426e-06, Loss_0: 1.034e-09, Loss_r: 1.425e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.384e-06, Loss_0: 1.407e-10, Loss_r: 1.384e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.346e-06, Loss_0: 1.489e-10, Loss_r: 1.345e-06, Time: 0.13, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.310e-06, Loss_0: 1.183e-12, Loss_r: 1.310e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.277e-06, Loss_0: 1.932e-11, Loss_r: 1.277e-06, Time: 0.12, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.246e-06, Loss_0: 2.987e-11, Loss_r: 1.246e-06, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.217e-06, Loss_0: 2.567e-11, Loss_r: 1.217e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.189e-06, Loss_0: 2.433e-11, Loss_r: 1.189e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.163e-06, Loss_0: 1.885e-11, Loss_r: 1.163e-06, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.139e-06, Loss_0: 1.856e-11, Loss_r: 1.139e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.115e-06, Loss_0: 1.445e-11, Loss_r: 1.115e-06, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.093e-06, Loss_0: 1.523e-11, Loss_r: 1.093e-06, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.072e-06, Loss_0: 1.438e-11, Loss_r: 1.071e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.051e-06, Loss_0: 2.509e-11, Loss_r: 1.051e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.031e-06, Loss_0: 1.185e-11, Loss_r: 1.031e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.012e-06, Loss_0: 1.745e-11, Loss_r: 1.012e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 9.943e-07, Loss_0: 6.036e-12, Loss_r: 9.943e-07, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 9.769e-07, Loss_0: 2.148e-11, Loss_r: 9.769e-07, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 9.600e-07, Loss_0: 1.993e-12, Loss_r: 9.600e-07, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 9.438e-07, Loss_0: 3.674e-11, Loss_r: 9.437e-07, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 9.280e-07, Loss_0: 2.633e-11, Loss_r: 9.280e-07, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 9.128e-07, Loss_0: 1.059e-11, Loss_r: 9.128e-07, Time: 0.11, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 8.981e-07, Loss_0: 6.483e-12, Loss_r: 8.981e-07, Time: 0.11, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 8.838e-07, Loss_0: 4.801e-11, Loss_r: 8.837e-07, Time: 0.14, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 8.722e-07, Loss_0: 2.607e-09, Loss_r: 8.696e-07, Time: 0.14, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.628e-06, Loss_0: 7.674e-07, Loss_r: 8.601e-07, Time: 0.11, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 8.473e-07, Loss_0: 2.261e-09, Loss_r: 8.451e-07, Time: 0.09, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 9.770e-07, Loss_0: 1.400e-07, Loss_r: 8.370e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 8.230e-07, Loss_0: 6.936e-10, Loss_r: 8.223e-07, Time: 0.09, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 8.308e-07, Loss_0: 2.019e-08, Loss_r: 8.106e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 8.044e-07, Loss_0: 3.800e-09, Loss_r: 8.006e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 7.897e-07, Loss_0: 1.757e-10, Loss_r: 7.895e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 7.791e-07, Loss_0: 1.682e-11, Loss_r: 7.791e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 7.688e-07, Loss_0: 2.622e-11, Loss_r: 7.688e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 7.588e-07, Loss_0: 5.649e-11, Loss_r: 7.587e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 7.489e-07, Loss_0: 1.252e-10, Loss_r: 7.487e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 7.390e-07, Loss_0: 5.191e-11, Loss_r: 7.390e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 7.294e-07, Loss_0: 2.539e-11, Loss_r: 7.294e-07, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 7.201e-07, Loss_0: 1.558e-10, Loss_r: 7.199e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 7.163e-07, Loss_0: 5.852e-09, Loss_r: 7.104e-07, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.504e-06, Loss_0: 7.965e-07, Loss_r: 7.078e-07, Time: 0.11, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 7.164e-07, Loss_0: 2.315e-08, Loss_r: 6.933e-07, Time: 0.12, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 7.571e-07, Loss_0: 6.935e-08, Loss_r: 6.878e-07, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 7.160e-07, Loss_0: 3.676e-08, Loss_r: 6.793e-07, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 6.746e-07, Loss_0: 4.596e-09, Loss_r: 6.700e-07, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 6.663e-07, Loss_0: 3.977e-09, Loss_r: 6.623e-07, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 6.573e-07, Loss_0: 2.241e-09, Loss_r: 6.551e-07, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 6.476e-07, Loss_0: 3.766e-10, Loss_r: 6.472e-07, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 6.398e-07, Loss_0: 7.234e-12, Loss_r: 6.398e-07, Time: 0.09, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 6.324e-07, Loss_0: 1.471e-11, Loss_r: 6.324e-07, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 6.251e-07, Loss_0: 3.066e-12, Loss_r: 6.251e-07, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 6.179e-07, Loss_0: 5.626e-11, Loss_r: 6.178e-07, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 6.107e-07, Loss_0: 1.448e-12, Loss_r: 6.107e-07, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 6.036e-07, Loss_0: 8.122e-12, Loss_r: 6.036e-07, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 5.967e-07, Loss_0: 2.736e-11, Loss_r: 5.966e-07, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 5.900e-07, Loss_0: 3.160e-10, Loss_r: 5.897e-07, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 6.019e-07, Loss_0: 1.925e-08, Loss_r: 5.826e-07, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.994e-06, Loss_0: 3.378e-06, Loss_r: 6.160e-07, Time: 0.08, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 6.407e-07, Loss_0: 6.974e-08, Loss_r: 5.710e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 8.952e-07, Loss_0: 3.233e-07, Loss_r: 5.720e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 7.211e-07, Loss_0: 1.577e-07, Loss_r: 5.634e-07, Time: 0.08, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 5.706e-07, Loss_0: 1.666e-08, Loss_r: 5.539e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 5.667e-07, Loss_0: 1.856e-08, Loss_r: 5.481e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 5.516e-07, Loss_0: 8.682e-09, Loss_r: 5.429e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 5.372e-07, Loss_0: 5.026e-10, Loss_r: 5.367e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 5.311e-07, Loss_0: 5.775e-11, Loss_r: 5.310e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 5.254e-07, Loss_0: 3.215e-11, Loss_r: 5.254e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 5.197e-07, Loss_0: 6.588e-12, Loss_r: 5.197e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 5.142e-07, Loss_0: 6.793e-11, Loss_r: 5.141e-07, Time: 0.10, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 5.087e-07, Loss_0: 2.708e-11, Loss_r: 5.087e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 5.032e-07, Loss_0: 9.400e-12, Loss_r: 5.031e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 4.978e-07, Loss_0: 8.400e-12, Loss_r: 4.977e-07, Time: 0.08, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 4.925e-07, Loss_0: 1.064e-10, Loss_r: 4.924e-07, Time: 0.10, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 4.934e-07, Loss_0: 6.047e-09, Loss_r: 4.873e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.186e-06, Loss_0: 6.909e-07, Loss_r: 4.946e-07, Time: 0.09, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 5.154e-07, Loss_0: 3.721e-08, Loss_r: 4.782e-07, Time: 0.10, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 5.104e-07, Loss_0: 3.776e-08, Loss_r: 4.726e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 5.093e-07, Loss_0: 4.125e-08, Loss_r: 4.680e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 4.634e-07, Loss_0: 1.972e-11, Loss_r: 4.634e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 4.646e-07, Loss_0: 5.499e-09, Loss_r: 4.591e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 4.546e-07, Loss_0: 4.863e-10, Loss_r: 4.542e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 4.498e-07, Loss_0: 2.175e-10, Loss_r: 4.496e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 4.454e-07, Loss_0: 2.015e-10, Loss_r: 4.452e-07, Time: 0.08, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 4.409e-07, Loss_0: 1.599e-10, Loss_r: 4.407e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 4.364e-07, Loss_0: 3.787e-11, Loss_r: 4.363e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 4.319e-07, Loss_0: 2.295e-11, Loss_r: 4.319e-07, Time: 0.08, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 4.276e-07, Loss_0: 2.379e-12, Loss_r: 4.276e-07, Time: 0.07, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 4.232e-07, Loss_0: 8.060e-13, Loss_r: 4.232e-07, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 4.190e-07, Loss_0: 1.647e-12, Loss_r: 4.190e-07, Time: 0.08, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 4.147e-07, Loss_0: 1.284e-11, Loss_r: 4.146e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 4.106e-07, Loss_0: 1.582e-10, Loss_r: 4.104e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 4.138e-07, Loss_0: 7.661e-09, Loss_r: 4.061e-07, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.484e-06, Loss_0: 1.068e-06, Loss_r: 4.168e-07, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 4.391e-07, Loss_0: 4.034e-08, Loss_r: 3.988e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 4.721e-07, Loss_0: 7.514e-08, Loss_r: 3.970e-07, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 4.508e-07, Loss_0: 5.785e-08, Loss_r: 3.930e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 3.889e-07, Loss_0: 1.058e-09, Loss_r: 3.878e-07, Time: 0.09, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 3.925e-07, Loss_0: 8.354e-09, Loss_r: 3.842e-07, Time: 0.08, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 3.821e-07, Loss_0: 1.292e-09, Loss_r: 3.808e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 3.772e-07, Loss_0: 5.124e-11, Loss_r: 3.771e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 3.739e-07, Loss_0: 3.372e-10, Loss_r: 3.735e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 3.702e-07, Loss_0: 1.502e-10, Loss_r: 3.701e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 3.666e-07, Loss_0: 6.763e-11, Loss_r: 3.665e-07, Time: 0.09, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 3.631e-07, Loss_0: 1.842e-11, Loss_r: 3.630e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 3.596e-07, Loss_0: 9.968e-13, Loss_r: 3.596e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 3.562e-07, Loss_0: 1.224e-11, Loss_r: 3.562e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 3.528e-07, Loss_0: 4.801e-13, Loss_r: 3.528e-07, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 3.494e-07, Loss_0: 2.866e-11, Loss_r: 3.494e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 3.467e-07, Loss_0: 5.765e-10, Loss_r: 3.461e-07, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 3.733e-07, Loss_0: 2.974e-08, Loss_r: 3.436e-07, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 4.156e-06, Loss_0: 3.746e-06, Loss_r: 4.101e-07, Time: 0.11, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 5.864e-07, Loss_0: 2.440e-07, Loss_r: 3.424e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 4.670e-07, Loss_0: 1.309e-07, Loss_r: 3.361e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 5.594e-07, Loss_0: 2.244e-07, Loss_r: 3.350e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 3.387e-07, Loss_0: 9.517e-09, Loss_r: 3.292e-07, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 3.519e-07, Loss_0: 2.473e-08, Loss_r: 3.272e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 3.243e-07, Loss_0: 6.842e-10, Loss_r: 3.236e-07, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 3.248e-07, Loss_0: 4.156e-09, Loss_r: 3.207e-07, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 3.188e-07, Loss_0: 8.069e-10, Loss_r: 3.180e-07, Time: 0.09, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 3.151e-07, Loss_0: 5.995e-11, Loss_r: 3.151e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 3.123e-07, Loss_0: 1.498e-12, Loss_r: 3.123e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 3.095e-07, Loss_0: 3.251e-12, Loss_r: 3.095e-07, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 3.067e-07, Loss_0: 3.407e-12, Loss_r: 3.067e-07, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 3.040e-07, Loss_0: 1.797e-11, Loss_r: 3.040e-07, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 3.013e-07, Loss_0: 9.856e-13, Loss_r: 3.013e-07, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 2.986e-07, Loss_0: 1.454e-11, Loss_r: 2.985e-07, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 2.960e-07, Loss_0: 8.768e-11, Loss_r: 2.959e-07, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 2.947e-07, Loss_0: 1.391e-09, Loss_r: 2.933e-07, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 3.619e-07, Loss_0: 6.957e-08, Loss_r: 2.923e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 2.978e-07, Loss_0: 9.290e-09, Loss_r: 2.885e-07, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 2.861e-07, Loss_0: 3.414e-10, Loss_r: 2.858e-07, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 2.867e-07, Loss_0: 3.308e-09, Loss_r: 2.834e-07, Time: 0.09, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 2.819e-07, Loss_0: 8.088e-10, Loss_r: 2.811e-07, Time: 0.12, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 2.789e-07, Loss_0: 1.298e-10, Loss_r: 2.788e-07, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 2.766e-07, Loss_0: 7.243e-11, Loss_r: 2.765e-07, Time: 0.14, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 2.742e-07, Loss_0: 4.192e-11, Loss_r: 2.742e-07, Time: 0.11, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 2.719e-07, Loss_0: 3.525e-12, Loss_r: 2.719e-07, Time: 0.12, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 2.697e-07, Loss_0: 6.540e-12, Loss_r: 2.697e-07, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 2.674e-07, Loss_0: 1.856e-11, Loss_r: 2.674e-07, Time: 0.11, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 2.652e-07, Loss_0: 3.400e-12, Loss_r: 2.652e-07, Time: 0.11, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 2.630e-07, Loss_0: 5.534e-12, Loss_r: 2.630e-07, Time: 0.11, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 2.607e-07, Loss_0: 6.091e-13, Loss_r: 2.607e-07, Time: 0.11, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 2.585e-07, Loss_0: 3.373e-12, Loss_r: 2.585e-07, Time: 0.12, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 2.564e-07, Loss_0: 5.223e-13, Loss_r: 2.564e-07, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 2.542e-07, Loss_0: 3.283e-11, Loss_r: 2.542e-07, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 2.525e-07, Loss_0: 4.339e-10, Loss_r: 2.521e-07, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 2.643e-07, Loss_0: 1.395e-08, Loss_r: 2.504e-07, Time: 0.10, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.347e-06, Loss_0: 1.076e-06, Loss_r: 2.704e-07, Time: 0.09, Learning Rate: 0.00043\n",
            "It: 3400, Loss: 3.628e-07, Loss_0: 1.139e-07, Loss_r: 2.488e-07, Time: 0.09, Learning Rate: 0.00043\n",
            "It: 3410, Loss: 2.579e-07, Loss_0: 1.358e-08, Loss_r: 2.443e-07, Time: 0.09, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 3.005e-07, Loss_0: 5.732e-08, Loss_r: 2.432e-07, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 2.495e-07, Loss_0: 8.898e-09, Loss_r: 2.406e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 2.427e-07, Loss_0: 3.730e-09, Loss_r: 2.390e-07, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 2.383e-07, Loss_0: 1.315e-09, Loss_r: 2.370e-07, Time: 0.09, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 2.360e-07, Loss_0: 9.812e-10, Loss_r: 2.351e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 2.333e-07, Loss_0: 2.187e-12, Loss_r: 2.333e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 2.315e-07, Loss_0: 3.931e-11, Loss_r: 2.315e-07, Time: 0.09, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 2.297e-07, Loss_0: 6.214e-11, Loss_r: 2.296e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 2.279e-07, Loss_0: 1.235e-11, Loss_r: 2.279e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 2.261e-07, Loss_0: 8.346e-12, Loss_r: 2.261e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 2.243e-07, Loss_0: 1.589e-13, Loss_r: 2.243e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 2.225e-07, Loss_0: 6.840e-13, Loss_r: 2.225e-07, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 2.208e-07, Loss_0: 1.864e-12, Loss_r: 2.208e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 2.190e-07, Loss_0: 2.165e-12, Loss_r: 2.190e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 2.174e-07, Loss_0: 8.901e-11, Loss_r: 2.173e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 2.188e-07, Loss_0: 3.125e-09, Loss_r: 2.157e-07, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 4.424e-07, Loss_0: 2.234e-07, Loss_r: 2.190e-07, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3590, Loss: 2.371e-07, Loss_0: 2.398e-08, Loss_r: 2.131e-07, Time: 0.07, Learning Rate: 0.00039\n",
            "It: 3600, Loss: 2.138e-07, Loss_0: 2.935e-09, Loss_r: 2.108e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 2.215e-07, Loss_0: 1.207e-08, Loss_r: 2.094e-07, Time: 0.11, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 2.096e-07, Loss_0: 1.825e-09, Loss_r: 2.078e-07, Time: 0.11, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 2.072e-07, Loss_0: 8.295e-10, Loss_r: 2.064e-07, Time: 0.09, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 2.051e-07, Loss_0: 2.464e-10, Loss_r: 2.049e-07, Time: 0.10, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 2.036e-07, Loss_0: 2.249e-10, Loss_r: 2.034e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 2.019e-07, Loss_0: 0.000e+00, Loss_r: 2.019e-07, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 2.004e-07, Loss_0: 4.988e-12, Loss_r: 2.004e-07, Time: 0.10, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 1.990e-07, Loss_0: 9.049e-12, Loss_r: 1.990e-07, Time: 0.11, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 1.975e-07, Loss_0: 5.164e-14, Loss_r: 1.975e-07, Time: 0.09, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 1.961e-07, Loss_0: 4.226e-13, Loss_r: 1.961e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 1.946e-07, Loss_0: 2.111e-12, Loss_r: 1.946e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 1.932e-07, Loss_0: 2.969e-12, Loss_r: 1.932e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 1.918e-07, Loss_0: 4.250e-13, Loss_r: 1.918e-07, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 1.904e-07, Loss_0: 1.010e-11, Loss_r: 1.904e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.891e-07, Loss_0: 1.029e-10, Loss_r: 1.890e-07, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.901e-07, Loss_0: 2.343e-09, Loss_r: 1.877e-07, Time: 0.07, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 3.452e-07, Loss_0: 1.552e-07, Loss_r: 1.900e-07, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3780, Loss: 2.040e-07, Loss_0: 1.844e-08, Loss_r: 1.856e-07, Time: 0.09, Learning Rate: 0.00035\n",
            "It: 3790, Loss: 1.851e-07, Loss_0: 1.383e-09, Loss_r: 1.837e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 1.906e-07, Loss_0: 8.025e-09, Loss_r: 1.826e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 1.828e-07, Loss_0: 1.524e-09, Loss_r: 1.813e-07, Time: 0.09, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 1.806e-07, Loss_0: 4.054e-10, Loss_r: 1.802e-07, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 1.792e-07, Loss_0: 2.319e-10, Loss_r: 1.789e-07, Time: 0.10, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 1.778e-07, Loss_0: 1.131e-10, Loss_r: 1.777e-07, Time: 0.09, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 1.765e-07, Loss_0: 2.854e-12, Loss_r: 1.765e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 1.753e-07, Loss_0: 5.954e-12, Loss_r: 1.753e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 1.741e-07, Loss_0: 1.047e-11, Loss_r: 1.741e-07, Time: 0.14, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 1.729e-07, Loss_0: 1.557e-12, Loss_r: 1.729e-07, Time: 0.11, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 1.718e-07, Loss_0: 6.646e-12, Loss_r: 1.718e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 1.706e-07, Loss_0: 3.019e-13, Loss_r: 1.706e-07, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 1.694e-07, Loss_0: 2.798e-13, Loss_r: 1.694e-07, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 1.683e-07, Loss_0: 4.790e-12, Loss_r: 1.683e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 1.671e-07, Loss_0: 5.054e-12, Loss_r: 1.671e-07, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 1.660e-07, Loss_0: 1.245e-11, Loss_r: 1.660e-07, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 1.651e-07, Loss_0: 2.564e-10, Loss_r: 1.648e-07, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 1.751e-07, Loss_0: 1.123e-08, Loss_r: 1.638e-07, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 1.453e-06, Loss_0: 1.264e-06, Loss_r: 1.882e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 3980, Loss: 2.603e-07, Loss_0: 9.687e-08, Loss_r: 1.635e-07, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 3990, Loss: 1.948e-07, Loss_0: 3.300e-08, Loss_r: 1.618e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 2.351e-07, Loss_0: 7.330e-08, Loss_r: 1.618e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 1.652e-07, Loss_0: 6.024e-09, Loss_r: 1.592e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 1.647e-07, Loss_0: 6.669e-09, Loss_r: 1.580e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 1.580e-07, Loss_0: 9.937e-10, Loss_r: 1.570e-07, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 1.573e-07, Loss_0: 1.210e-09, Loss_r: 1.561e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 1.551e-07, Loss_0: 4.853e-11, Loss_r: 1.551e-07, Time: 0.08, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 1.541e-07, Loss_0: 5.866e-11, Loss_r: 1.541e-07, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 1.532e-07, Loss_0: 3.473e-11, Loss_r: 1.531e-07, Time: 0.08, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 1.522e-07, Loss_0: 2.822e-11, Loss_r: 1.522e-07, Time: 0.08, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 1.512e-07, Loss_0: 8.628e-12, Loss_r: 1.512e-07, Time: 0.09, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 1.503e-07, Loss_0: 1.864e-12, Loss_r: 1.503e-07, Time: 0.11, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 1.493e-07, Loss_0: 4.313e-12, Loss_r: 1.493e-07, Time: 0.08, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 1.484e-07, Loss_0: 3.824e-13, Loss_r: 1.484e-07, Time: 0.11, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 1.475e-07, Loss_0: 1.351e-12, Loss_r: 1.475e-07, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 1.465e-07, Loss_0: 1.565e-11, Loss_r: 1.465e-07, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 1.461e-07, Loss_0: 4.531e-10, Loss_r: 1.456e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 1.693e-07, Loss_0: 2.393e-08, Loss_r: 1.454e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4170, Loss: 1.473e-07, Loss_0: 3.316e-09, Loss_r: 1.440e-07, Time: 0.09, Learning Rate: 0.00028\n",
            "It: 4180, Loss: 1.431e-07, Loss_0: 5.878e-11, Loss_r: 1.430e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 1.432e-07, Loss_0: 9.691e-10, Loss_r: 1.422e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 1.417e-07, Loss_0: 3.329e-10, Loss_r: 1.414e-07, Time: 0.10, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 1.406e-07, Loss_0: 1.593e-11, Loss_r: 1.406e-07, Time: 0.08, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 1.398e-07, Loss_0: 4.459e-11, Loss_r: 1.398e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 1.390e-07, Loss_0: 1.576e-11, Loss_r: 1.390e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 1.382e-07, Loss_0: 3.060e-13, Loss_r: 1.382e-07, Time: 0.09, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 1.374e-07, Loss_0: 1.993e-12, Loss_r: 1.374e-07, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 1.366e-07, Loss_0: 3.204e-12, Loss_r: 1.366e-07, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 1.358e-07, Loss_0: 4.198e-14, Loss_r: 1.358e-07, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 1.350e-07, Loss_0: 9.381e-15, Loss_r: 1.350e-07, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 1.342e-07, Loss_0: 3.847e-12, Loss_r: 1.342e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 1.334e-07, Loss_0: 1.388e-17, Loss_r: 1.334e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 1.327e-07, Loss_0: 6.120e-15, Loss_r: 1.327e-07, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 1.319e-07, Loss_0: 7.713e-12, Loss_r: 1.319e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 1.312e-07, Loss_0: 3.268e-11, Loss_r: 1.311e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 1.306e-07, Loss_0: 2.146e-10, Loss_r: 1.304e-07, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 1.358e-07, Loss_0: 5.982e-09, Loss_r: 1.298e-07, Time: 0.08, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 5.870e-07, Loss_0: 4.472e-07, Loss_r: 1.398e-07, Time: 0.09, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 1.806e-07, Loss_0: 5.095e-08, Loss_r: 1.296e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 1.314e-07, Loss_0: 3.831e-09, Loss_r: 1.276e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 1.493e-07, Loss_0: 2.205e-08, Loss_r: 1.273e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 1.312e-07, Loss_0: 4.823e-09, Loss_r: 1.263e-07, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 1.266e-07, Loss_0: 9.178e-10, Loss_r: 1.257e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 1.258e-07, Loss_0: 8.484e-10, Loss_r: 1.250e-07, Time: 0.09, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 1.245e-07, Loss_0: 2.527e-10, Loss_r: 1.243e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 1.237e-07, Loss_0: 2.660e-11, Loss_r: 1.236e-07, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 1.230e-07, Loss_0: 3.694e-11, Loss_r: 1.230e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 1.223e-07, Loss_0: 2.153e-11, Loss_r: 1.223e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 1.217e-07, Loss_0: 5.412e-12, Loss_r: 1.217e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 1.210e-07, Loss_0: 5.693e-12, Loss_r: 1.210e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 1.204e-07, Loss_0: 3.421e-13, Loss_r: 1.204e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 1.198e-07, Loss_0: 6.626e-12, Loss_r: 1.198e-07, Time: 0.09, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 1.191e-07, Loss_0: 1.739e-12, Loss_r: 1.191e-07, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 1.185e-07, Loss_0: 5.010e-15, Loss_r: 1.185e-07, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 1.178e-07, Loss_0: 5.080e-12, Loss_r: 1.178e-07, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 1.173e-07, Loss_0: 9.727e-11, Loss_r: 1.172e-07, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 1.192e-07, Loss_0: 2.647e-09, Loss_r: 1.166e-07, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 3.143e-07, Loss_0: 1.942e-07, Loss_r: 1.201e-07, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4570, Loss: 1.368e-07, Loss_0: 2.102e-08, Loss_r: 1.158e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 1.172e-07, Loss_0: 2.208e-09, Loss_r: 1.150e-07, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 1.246e-07, Loss_0: 9.968e-09, Loss_r: 1.147e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 1.156e-07, Loss_0: 1.688e-09, Loss_r: 1.139e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 1.138e-07, Loss_0: 5.663e-10, Loss_r: 1.132e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 1.130e-07, Loss_0: 3.182e-10, Loss_r: 1.127e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 1.123e-07, Loss_0: 1.242e-10, Loss_r: 1.121e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 1.116e-07, Loss_0: 1.498e-12, Loss_r: 1.116e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 1.110e-07, Loss_0: 2.172e-11, Loss_r: 1.110e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 1.105e-07, Loss_0: 1.044e-11, Loss_r: 1.105e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 1.100e-07, Loss_0: 9.354e-12, Loss_r: 1.100e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 1.094e-07, Loss_0: 2.187e-12, Loss_r: 1.094e-07, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 1.089e-07, Loss_0: 1.700e-12, Loss_r: 1.089e-07, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 1.084e-07, Loss_0: 1.511e-12, Loss_r: 1.083e-07, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 1.078e-07, Loss_0: 1.549e-11, Loss_r: 1.078e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 1.073e-07, Loss_0: 1.705e-11, Loss_r: 1.073e-07, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 1.068e-07, Loss_0: 1.550e-11, Loss_r: 1.068e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 1.064e-07, Loss_0: 1.126e-10, Loss_r: 1.062e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 1.092e-07, Loss_0: 3.443e-09, Loss_r: 1.058e-07, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 3.725e-07, Loss_0: 2.616e-07, Loss_r: 1.110e-07, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4770, Loss: 1.339e-07, Loss_0: 2.856e-08, Loss_r: 1.053e-07, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 1.071e-07, Loss_0: 2.660e-09, Loss_r: 1.044e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 1.174e-07, Loss_0: 1.315e-08, Loss_r: 1.043e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 1.060e-07, Loss_0: 2.526e-09, Loss_r: 1.035e-07, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 1.036e-07, Loss_0: 6.533e-10, Loss_r: 1.029e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 1.030e-07, Loss_0: 4.945e-10, Loss_r: 1.025e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 1.022e-07, Loss_0: 1.301e-10, Loss_r: 1.020e-07, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 1.016e-07, Loss_0: 7.426e-12, Loss_r: 1.016e-07, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 1.012e-07, Loss_0: 4.494e-11, Loss_r: 1.011e-07, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 1.007e-07, Loss_0: 1.514e-11, Loss_r: 1.007e-07, Time: 0.09, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 1.002e-07, Loss_0: 1.052e-11, Loss_r: 1.002e-07, Time: 0.09, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 9.980e-08, Loss_0: 7.124e-12, Loss_r: 9.980e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 9.933e-08, Loss_0: 2.310e-12, Loss_r: 9.932e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 9.888e-08, Loss_0: 6.963e-13, Loss_r: 9.888e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 9.845e-08, Loss_0: 1.094e-11, Loss_r: 9.844e-08, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 9.801e-08, Loss_0: 2.567e-13, Loss_r: 9.800e-08, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 9.758e-08, Loss_0: 7.224e-12, Loss_r: 9.757e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 9.721e-08, Loss_0: 7.930e-11, Loss_r: 9.713e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 9.952e-08, Loss_0: 2.717e-09, Loss_r: 9.680e-08, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 2.860e-07, Loss_0: 1.849e-07, Loss_r: 1.011e-07, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 1.194e-07, Loss_0: 2.285e-08, Loss_r: 9.655e-08, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 9.654e-08, Loss_0: 1.049e-09, Loss_r: 9.549e-08, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 1.039e-07, Loss_0: 8.651e-09, Loss_r: 9.526e-08, Time: 0.06, Learning Rate: 0.00019\n",
            "Training time: 30.8717\n",
            "[1, 256, 128, 64, 32, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.332e-02, Loss_0: 9.524e-04, Loss_r: 3.237e-02, Time: 1.16, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.935e-02, Loss_0: 1.069e-05, Loss_r: 2.934e-02, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.605e-02, Loss_0: 1.264e-06, Loss_r: 1.605e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.970e-03, Loss_0: 2.430e-05, Loss_r: 1.946e-03, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.198e-04, Loss_0: 3.259e-05, Loss_r: 6.872e-04, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 50, Loss: 2.909e-04, Loss_0: 4.702e-05, Loss_r: 2.439e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 60, Loss: 4.990e-05, Loss_0: 1.327e-05, Loss_r: 3.663e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.146e-05, Loss_0: 9.641e-06, Loss_r: 4.182e-05, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 80, Loss: 3.283e-05, Loss_0: 8.538e-06, Loss_r: 2.429e-05, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 90, Loss: 3.024e-05, Loss_0: 1.732e-05, Loss_r: 1.292e-05, Time: 0.10, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.163e-04, Loss_0: 1.006e-04, Loss_r: 1.570e-05, Time: 0.11, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.741e-05, Loss_0: 4.651e-05, Loss_r: 1.090e-05, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.355e-05, Loss_0: 1.366e-05, Loss_r: 9.884e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.719e-05, Loss_0: 8.500e-06, Loss_r: 8.692e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 8.200e-06, Loss_0: 5.864e-07, Loss_r: 7.613e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.309e-05, Loss_0: 5.945e-06, Loss_r: 7.141e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.566e-04, Loss_0: 1.479e-04, Loss_r: 8.696e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 170, Loss: 3.790e-05, Loss_0: 3.046e-05, Loss_r: 7.437e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 180, Loss: 3.841e-05, Loss_0: 3.277e-05, Loss_r: 5.647e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.365e-06, Loss_0: 2.548e-08, Loss_r: 5.340e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 200, Loss: 8.747e-06, Loss_0: 3.669e-06, Loss_r: 5.078e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.512e-06, Loss_0: 6.399e-10, Loss_r: 4.511e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 220, Loss: 2.658e-05, Loss_0: 2.211e-05, Loss_r: 4.473e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.838e-06, Loss_0: 1.586e-06, Loss_r: 4.252e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 240, Loss: 8.729e-06, Loss_0: 3.845e-06, Loss_r: 4.884e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.161e-06, Loss_0: 1.347e-06, Loss_r: 3.814e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 6.132e-06, Loss_0: 2.354e-06, Loss_r: 3.778e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 270, Loss: 7.412e-06, Loss_0: 4.038e-06, Loss_r: 3.373e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.210e-06, Loss_0: 1.592e-08, Loss_r: 3.194e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.034e-06, Loss_0: 3.388e-08, Loss_r: 3.000e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.387e-06, Loss_0: 5.371e-07, Loss_r: 2.849e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.919e-06, Loss_0: 3.184e-06, Loss_r: 2.735e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 320, Loss: 3.969e-04, Loss_0: 3.911e-04, Loss_r: 5.736e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.572e-05, Loss_0: 5.135e-05, Loss_r: 4.361e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.397e-05, Loss_0: 4.092e-05, Loss_r: 3.042e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.776e-05, Loss_0: 1.431e-05, Loss_r: 3.453e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.407e-06, Loss_0: 2.813e-06, Loss_r: 2.594e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.472e-06, Loss_0: 9.917e-07, Loss_r: 2.480e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.134e-06, Loss_0: 7.692e-07, Loss_r: 2.364e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.415e-06, Loss_0: 1.942e-07, Loss_r: 2.221e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.211e-06, Loss_0: 1.040e-07, Loss_r: 2.107e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.029e-06, Loss_0: 8.254e-09, Loss_r: 2.021e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.954e-06, Loss_0: 2.254e-08, Loss_r: 1.931e-06, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 430, Loss: 1.863e-06, Loss_0: 8.110e-09, Loss_r: 1.855e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.091e-06, Loss_0: 2.990e-07, Loss_r: 1.792e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 7.520e-05, Loss_0: 7.250e-05, Loss_r: 2.703e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.783e-04, Loss_0: 2.733e-04, Loss_r: 4.986e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 470, Loss: 7.665e-05, Loss_0: 7.398e-05, Loss_r: 2.677e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 480, Loss: 8.976e-06, Loss_0: 6.746e-06, Loss_r: 2.230e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.190e-05, Loss_0: 9.386e-06, Loss_r: 2.512e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.233e-06, Loss_0: 3.177e-07, Loss_r: 1.915e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.385e-06, Loss_0: 1.478e-06, Loss_r: 1.907e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.215e-06, Loss_0: 4.453e-07, Loss_r: 1.770e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.787e-06, Loss_0: 8.356e-08, Loss_r: 1.703e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.672e-06, Loss_0: 3.228e-08, Loss_r: 1.640e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.602e-06, Loss_0: 2.687e-08, Loss_r: 1.575e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.528e-06, Loss_0: 1.110e-08, Loss_r: 1.517e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.463e-06, Loss_0: 1.364e-10, Loss_r: 1.463e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.412e-06, Loss_0: 8.849e-10, Loss_r: 1.411e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.362e-06, Loss_0: 1.189e-09, Loss_r: 1.361e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.315e-06, Loss_0: 2.022e-10, Loss_r: 1.315e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.270e-06, Loss_0: 1.256e-10, Loss_r: 1.270e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.228e-06, Loss_0: 7.408e-10, Loss_r: 1.227e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.198e-06, Loss_0: 1.174e-08, Loss_r: 1.186e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.850e-06, Loss_0: 1.689e-06, Loss_r: 1.160e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 650, Loss: 5.988e-04, Loss_0: 5.822e-04, Loss_r: 1.657e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.223e-04, Loss_0: 1.207e-04, Loss_r: 1.639e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 670, Loss: 4.272e-05, Loss_0: 3.800e-05, Loss_r: 4.728e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.759e-05, Loss_0: 2.581e-05, Loss_r: 1.775e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.346e-06, Loss_0: 1.779e-06, Loss_r: 1.567e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.621e-06, Loss_0: 1.146e-06, Loss_r: 1.475e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.567e-06, Loss_0: 1.144e-06, Loss_r: 1.424e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.371e-06, Loss_0: 1.736e-08, Loss_r: 1.354e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.445e-06, Loss_0: 1.491e-07, Loss_r: 1.296e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.284e-06, Loss_0: 3.359e-08, Loss_r: 1.250e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.208e-06, Loss_0: 1.098e-12, Loss_r: 1.208e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.168e-06, Loss_0: 1.982e-09, Loss_r: 1.166e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.128e-06, Loss_0: 1.161e-09, Loss_r: 1.126e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.089e-06, Loss_0: 2.921e-10, Loss_r: 1.089e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.054e-06, Loss_0: 8.449e-12, Loss_r: 1.054e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.021e-06, Loss_0: 7.267e-11, Loss_r: 1.021e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 9.900e-07, Loss_0: 1.378e-10, Loss_r: 9.898e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 9.603e-07, Loss_0: 2.668e-11, Loss_r: 9.603e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 9.322e-07, Loss_0: 2.324e-12, Loss_r: 9.322e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 9.056e-07, Loss_0: 3.901e-11, Loss_r: 9.055e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 8.801e-07, Loss_0: 2.605e-12, Loss_r: 8.801e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 8.559e-07, Loss_0: 4.027e-11, Loss_r: 8.559e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 8.329e-07, Loss_0: 8.195e-13, Loss_r: 8.329e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 8.109e-07, Loss_0: 1.523e-13, Loss_r: 8.109e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 7.900e-07, Loss_0: 3.254e-12, Loss_r: 7.900e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 900, Loss: 7.714e-07, Loss_0: 1.232e-09, Loss_r: 7.702e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.123e-06, Loss_0: 3.632e-07, Loss_r: 7.595e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 2.306e-04, Loss_0: 2.280e-04, Loss_r: 2.671e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.644e-04, Loss_0: 1.454e-04, Loss_r: 1.900e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 9.834e-05, Loss_0: 9.642e-05, Loss_r: 1.916e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.749e-05, Loss_0: 1.624e-05, Loss_r: 1.253e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 4.190e-06, Loss_0: 2.588e-06, Loss_r: 1.602e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.434e-06, Loss_0: 2.350e-07, Loss_r: 1.199e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.205e-06, Loss_0: 3.066e-08, Loss_r: 1.175e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.355e-06, Loss_0: 2.419e-07, Loss_r: 1.113e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.284e-06, Loss_0: 2.187e-07, Loss_r: 1.065e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.076e-06, Loss_0: 4.443e-08, Loss_r: 1.032e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 9.942e-07, Loss_0: 7.937e-10, Loss_r: 9.934e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 9.712e-07, Loss_0: 1.145e-08, Loss_r: 9.597e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 9.315e-07, Loss_0: 1.069e-11, Loss_r: 9.315e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 9.056e-07, Loss_0: 1.015e-09, Loss_r: 9.046e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 8.789e-07, Loss_0: 1.294e-11, Loss_r: 8.789e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 8.550e-07, Loss_0: 1.309e-10, Loss_r: 8.549e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 8.326e-07, Loss_0: 1.316e-10, Loss_r: 8.325e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 8.113e-07, Loss_0: 6.798e-11, Loss_r: 8.113e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 7.912e-07, Loss_0: 4.130e-11, Loss_r: 7.912e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 7.721e-07, Loss_0: 2.481e-11, Loss_r: 7.721e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 7.540e-07, Loss_0: 1.603e-11, Loss_r: 7.540e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 7.366e-07, Loss_0: 1.173e-11, Loss_r: 7.366e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 7.200e-07, Loss_0: 1.621e-11, Loss_r: 7.200e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 7.042e-07, Loss_0: 1.168e-11, Loss_r: 7.042e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 6.891e-07, Loss_0: 8.639e-12, Loss_r: 6.891e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 6.746e-07, Loss_0: 4.179e-12, Loss_r: 6.746e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 6.607e-07, Loss_0: 1.281e-11, Loss_r: 6.606e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 6.473e-07, Loss_0: 7.564e-12, Loss_r: 6.473e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 6.344e-07, Loss_0: 7.518e-12, Loss_r: 6.344e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 6.220e-07, Loss_0: 1.625e-11, Loss_r: 6.220e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 6.101e-07, Loss_0: 5.468e-13, Loss_r: 6.101e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 5.986e-07, Loss_0: 4.391e-14, Loss_r: 5.986e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 5.875e-07, Loss_0: 1.867e-13, Loss_r: 5.875e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 5.769e-07, Loss_0: 8.195e-11, Loss_r: 5.768e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 5.739e-07, Loss_0: 7.041e-09, Loss_r: 5.668e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.018e-06, Loss_0: 1.440e-06, Loss_r: 5.787e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 4.711e-04, Loss_0: 4.684e-04, Loss_r: 2.715e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.504e-06, Loss_0: 5.996e-07, Loss_r: 9.044e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 8.629e-05, Loss_0: 8.298e-05, Loss_r: 3.303e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 6.088e-06, Loss_0: 5.023e-06, Loss_r: 1.065e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.419e-06, Loss_0: 1.397e-06, Loss_r: 1.022e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 4.059e-06, Loss_0: 3.102e-06, Loss_r: 9.570e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.717e-06, Loss_0: 7.931e-07, Loss_r: 9.242e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 8.705e-07, Loss_0: 1.083e-08, Loss_r: 8.597e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 9.828e-07, Loss_0: 1.539e-07, Loss_r: 8.290e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 8.031e-07, Loss_0: 3.489e-09, Loss_r: 7.996e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 7.956e-07, Loss_0: 2.017e-08, Loss_r: 7.754e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 7.544e-07, Loss_0: 1.103e-10, Loss_r: 7.543e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 7.362e-07, Loss_0: 1.606e-09, Loss_r: 7.346e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 7.164e-07, Loss_0: 5.908e-10, Loss_r: 7.158e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 6.980e-07, Loss_0: 1.927e-11, Loss_r: 6.980e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 6.813e-07, Loss_0: 2.075e-11, Loss_r: 6.813e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 6.653e-07, Loss_0: 3.477e-11, Loss_r: 6.653e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 6.501e-07, Loss_0: 2.529e-11, Loss_r: 6.501e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 6.356e-07, Loss_0: 1.409e-11, Loss_r: 6.356e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 6.216e-07, Loss_0: 1.128e-11, Loss_r: 6.216e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 6.083e-07, Loss_0: 5.729e-12, Loss_r: 6.083e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 5.954e-07, Loss_0: 6.873e-12, Loss_r: 5.954e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 5.830e-07, Loss_0: 4.872e-12, Loss_r: 5.830e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 5.711e-07, Loss_0: 6.183e-12, Loss_r: 5.711e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 5.595e-07, Loss_0: 1.454e-11, Loss_r: 5.595e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 5.483e-07, Loss_0: 1.104e-11, Loss_r: 5.483e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 5.376e-07, Loss_0: 3.407e-12, Loss_r: 5.376e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 5.271e-07, Loss_0: 1.228e-11, Loss_r: 5.271e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 5.170e-07, Loss_0: 6.474e-12, Loss_r: 5.170e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 5.071e-07, Loss_0: 1.720e-12, Loss_r: 5.071e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 4.976e-07, Loss_0: 3.312e-12, Loss_r: 4.976e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 4.883e-07, Loss_0: 1.160e-11, Loss_r: 4.882e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 4.792e-07, Loss_0: 1.210e-11, Loss_r: 4.792e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 4.704e-07, Loss_0: 3.636e-11, Loss_r: 4.703e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.628e-07, Loss_0: 1.155e-09, Loss_r: 4.616e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 6.312e-07, Loss_0: 1.776e-07, Loss_r: 4.535e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 6.880e-05, Loss_0: 6.744e-05, Loss_r: 1.363e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.205e-04, Loss_0: 2.154e-04, Loss_r: 5.058e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.169e-04, Loss_0: 1.133e-04, Loss_r: 3.617e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.890e-05, Loss_0: 2.629e-05, Loss_r: 2.610e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 3.208e-06, Loss_0: 1.962e-06, Loss_r: 1.246e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.210e-06, Loss_0: 1.833e-07, Loss_r: 1.027e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.728e-06, Loss_0: 8.310e-07, Loss_r: 8.968e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.409e-06, Loss_0: 6.182e-07, Loss_r: 7.908e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 8.560e-07, Loss_0: 1.113e-07, Loss_r: 7.446e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 7.286e-07, Loss_0: 7.508e-09, Loss_r: 7.211e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 7.245e-07, Loss_0: 2.552e-08, Loss_r: 6.989e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 6.745e-07, Loss_0: 1.285e-09, Loss_r: 6.732e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 6.562e-07, Loss_0: 3.432e-09, Loss_r: 6.527e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 6.339e-07, Loss_0: 3.097e-11, Loss_r: 6.339e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 6.162e-07, Loss_0: 3.077e-10, Loss_r: 6.159e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 5.987e-07, Loss_0: 8.716e-12, Loss_r: 5.987e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 5.825e-07, Loss_0: 4.439e-11, Loss_r: 5.825e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 5.671e-07, Loss_0: 5.487e-11, Loss_r: 5.671e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 5.525e-07, Loss_0: 3.570e-11, Loss_r: 5.524e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 5.385e-07, Loss_0: 2.438e-11, Loss_r: 5.385e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 5.251e-07, Loss_0: 1.200e-11, Loss_r: 5.251e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.123e-07, Loss_0: 1.047e-11, Loss_r: 5.123e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 5.000e-07, Loss_0: 1.207e-11, Loss_r: 5.000e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 4.883e-07, Loss_0: 8.422e-12, Loss_r: 4.882e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 4.769e-07, Loss_0: 6.829e-12, Loss_r: 4.769e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 4.660e-07, Loss_0: 6.045e-12, Loss_r: 4.660e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.555e-07, Loss_0: 9.122e-12, Loss_r: 4.555e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.453e-07, Loss_0: 8.481e-12, Loss_r: 4.453e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 4.355e-07, Loss_0: 4.137e-12, Loss_r: 4.355e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 4.261e-07, Loss_0: 4.673e-12, Loss_r: 4.261e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 4.168e-07, Loss_0: 7.651e-12, Loss_r: 4.168e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 4.080e-07, Loss_0: 4.137e-12, Loss_r: 4.080e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 3.993e-07, Loss_0: 4.375e-12, Loss_r: 3.993e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 3.909e-07, Loss_0: 4.697e-12, Loss_r: 3.909e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 3.828e-07, Loss_0: 4.043e-12, Loss_r: 3.828e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 3.748e-07, Loss_0: 1.252e-15, Loss_r: 3.748e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 3.671e-07, Loss_0: 1.475e-12, Loss_r: 3.671e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 3.597e-07, Loss_0: 8.530e-11, Loss_r: 3.596e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.558e-07, Loss_0: 3.252e-09, Loss_r: 3.525e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 6.245e-07, Loss_0: 2.748e-07, Loss_r: 3.496e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 5.341e-05, Loss_0: 5.269e-05, Loss_r: 7.179e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 3.062e-05, Loss_0: 2.989e-05, Loss_r: 7.329e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.934e-05, Loss_0: 2.878e-05, Loss_r: 5.615e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.807e-05, Loss_0: 2.633e-05, Loss_r: 1.740e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.357e-06, Loss_0: 6.292e-07, Loss_r: 7.280e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.417e-06, Loss_0: 3.479e-06, Loss_r: 9.387e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 5.941e-07, Loss_0: 9.757e-09, Loss_r: 5.843e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.023e-06, Loss_0: 4.408e-07, Loss_r: 5.819e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 5.834e-07, Loss_0: 4.603e-08, Loss_r: 5.374e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 5.311e-07, Loss_0: 1.156e-08, Loss_r: 5.195e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 5.208e-07, Loss_0: 1.906e-08, Loss_r: 5.017e-07, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 4.906e-07, Loss_0: 5.554e-09, Loss_r: 4.850e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 4.696e-07, Loss_0: 9.341e-10, Loss_r: 4.686e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 4.539e-07, Loss_0: 9.639e-11, Loss_r: 4.538e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 4.399e-07, Loss_0: 6.077e-12, Loss_r: 4.399e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 4.268e-07, Loss_0: 3.581e-12, Loss_r: 4.268e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 4.144e-07, Loss_0: 5.836e-12, Loss_r: 4.144e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 4.027e-07, Loss_0: 1.520e-12, Loss_r: 4.027e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 3.915e-07, Loss_0: 5.023e-13, Loss_r: 3.915e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 3.808e-07, Loss_0: 8.503e-12, Loss_r: 3.808e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 3.707e-07, Loss_0: 7.615e-12, Loss_r: 3.707e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 3.610e-07, Loss_0: 4.794e-12, Loss_r: 3.610e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 3.517e-07, Loss_0: 6.868e-12, Loss_r: 3.517e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 3.428e-07, Loss_0: 4.971e-12, Loss_r: 3.428e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 3.343e-07, Loss_0: 4.244e-12, Loss_r: 3.343e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 3.261e-07, Loss_0: 2.832e-12, Loss_r: 3.261e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 3.182e-07, Loss_0: 6.151e-12, Loss_r: 3.182e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 3.106e-07, Loss_0: 2.539e-12, Loss_r: 3.106e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 3.033e-07, Loss_0: 4.259e-12, Loss_r: 3.033e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 2.962e-07, Loss_0: 3.778e-13, Loss_r: 2.962e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 2.893e-07, Loss_0: 5.408e-12, Loss_r: 2.893e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 2.827e-07, Loss_0: 1.360e-13, Loss_r: 2.827e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 2.763e-07, Loss_0: 1.092e-11, Loss_r: 2.763e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2370, Loss: 2.712e-07, Loss_0: 9.349e-10, Loss_r: 2.702e-07, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2380, Loss: 4.068e-07, Loss_0: 1.403e-07, Loss_r: 2.664e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 2.638e-07, Loss_0: 4.708e-09, Loss_r: 2.591e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 2.650e-07, Loss_0: 1.148e-08, Loss_r: 2.535e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 2.563e-07, Loss_0: 7.727e-09, Loss_r: 2.486e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 2.443e-07, Loss_0: 2.832e-10, Loss_r: 2.440e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 2.401e-07, Loss_0: 7.831e-10, Loss_r: 2.393e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 2.349e-07, Loss_0: 3.322e-10, Loss_r: 2.345e-07, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 2.301e-07, Loss_0: 4.397e-13, Loss_r: 2.301e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 2.257e-07, Loss_0: 3.774e-12, Loss_r: 2.257e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 2.214e-07, Loss_0: 2.644e-11, Loss_r: 2.214e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 2.172e-07, Loss_0: 1.824e-12, Loss_r: 2.172e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 2.131e-07, Loss_0: 5.945e-12, Loss_r: 2.131e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 2.091e-07, Loss_0: 1.120e-11, Loss_r: 2.091e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 2.052e-07, Loss_0: 8.622e-13, Loss_r: 2.052e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 2.014e-07, Loss_0: 1.851e-13, Loss_r: 2.014e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 1.977e-07, Loss_0: 1.249e-11, Loss_r: 1.977e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 1.947e-07, Loss_0: 6.508e-10, Loss_r: 1.941e-07, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 2.492e-07, Loss_0: 5.782e-08, Loss_r: 1.914e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 1.923e-07, Loss_0: 4.747e-09, Loss_r: 1.875e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 1.859e-07, Loss_0: 1.677e-09, Loss_r: 1.842e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 1.847e-07, Loss_0: 3.543e-09, Loss_r: 1.811e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 1.785e-07, Loss_0: 1.521e-10, Loss_r: 1.783e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 1.758e-07, Loss_0: 3.658e-10, Loss_r: 1.755e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 1.726e-07, Loss_0: 2.868e-13, Loss_r: 1.726e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 1.699e-07, Loss_0: 7.453e-11, Loss_r: 1.698e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 1.671e-07, Loss_0: 1.686e-11, Loss_r: 1.671e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 1.645e-07, Loss_0: 8.079e-12, Loss_r: 1.644e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 1.618e-07, Loss_0: 4.471e-13, Loss_r: 1.618e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 1.593e-07, Loss_0: 2.157e-12, Loss_r: 1.593e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 1.568e-07, Loss_0: 8.016e-14, Loss_r: 1.568e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 1.543e-07, Loss_0: 1.688e-12, Loss_r: 1.543e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 1.519e-07, Loss_0: 4.364e-12, Loss_r: 1.519e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 1.496e-07, Loss_0: 1.702e-12, Loss_r: 1.496e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 1.473e-07, Loss_0: 2.477e-12, Loss_r: 1.472e-07, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 1.452e-07, Loss_0: 1.803e-10, Loss_r: 1.450e-07, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 1.580e-07, Loss_0: 1.494e-08, Loss_r: 1.431e-07, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 2.658e-06, Loss_0: 2.504e-06, Loss_r: 1.541e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 2.179e-07, Loss_0: 7.800e-08, Loss_r: 1.399e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 3.287e-07, Loss_0: 1.912e-07, Loss_r: 1.374e-07, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 2.735e-07, Loss_0: 1.380e-07, Loss_r: 1.355e-07, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 1.359e-07, Loss_0: 1.997e-09, Loss_r: 1.339e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 1.517e-07, Loss_0: 1.925e-08, Loss_r: 1.324e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 1.327e-07, Loss_0: 2.423e-09, Loss_r: 1.302e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 1.290e-07, Loss_0: 4.851e-10, Loss_r: 1.285e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 1.278e-07, Loss_0: 8.187e-10, Loss_r: 1.269e-07, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 1.256e-07, Loss_0: 4.177e-10, Loss_r: 1.252e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 1.237e-07, Loss_0: 1.300e-10, Loss_r: 1.236e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 1.220e-07, Loss_0: 8.686e-11, Loss_r: 1.220e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 1.204e-07, Loss_0: 6.961e-12, Loss_r: 1.204e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 1.189e-07, Loss_0: 1.628e-12, Loss_r: 1.189e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 1.173e-07, Loss_0: 4.255e-12, Loss_r: 1.173e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 1.158e-07, Loss_0: 3.102e-12, Loss_r: 1.158e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 1.144e-07, Loss_0: 2.081e-12, Loss_r: 1.144e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 1.129e-07, Loss_0: 2.141e-12, Loss_r: 1.129e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 1.116e-07, Loss_0: 4.654e-11, Loss_r: 1.115e-07, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 1.164e-07, Loss_0: 6.349e-09, Loss_r: 1.100e-07, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 2.280e-06, Loss_0: 2.165e-06, Loss_r: 1.151e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 1.079e-07, Loss_0: 9.566e-11, Loss_r: 1.078e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 4.598e-07, Loss_0: 3.510e-07, Loss_r: 1.089e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 1.423e-07, Loss_0: 3.611e-08, Loss_r: 1.062e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 1.480e-07, Loss_0: 4.351e-08, Loss_r: 1.045e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 1.040e-07, Loss_0: 5.598e-10, Loss_r: 1.035e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 1.087e-07, Loss_0: 6.191e-09, Loss_r: 1.026e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 1.037e-07, Loss_0: 2.481e-09, Loss_r: 1.012e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 1.007e-07, Loss_0: 4.529e-10, Loss_r: 1.003e-07, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 9.932e-08, Loss_0: 1.411e-10, Loss_r: 9.918e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 9.822e-08, Loss_0: 6.609e-11, Loss_r: 9.816e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 9.719e-08, Loss_0: 7.418e-11, Loss_r: 9.712e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 9.612e-08, Loss_0: 2.785e-12, Loss_r: 9.612e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 9.515e-08, Loss_0: 1.701e-11, Loss_r: 9.513e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 9.416e-08, Loss_0: 3.539e-12, Loss_r: 9.415e-08, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 9.322e-08, Loss_0: 2.811e-11, Loss_r: 9.319e-08, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 9.286e-08, Loss_0: 5.875e-10, Loss_r: 9.227e-08, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 1.288e-07, Loss_0: 3.713e-08, Loss_r: 9.170e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 9.520e-08, Loss_0: 4.582e-09, Loss_r: 9.061e-08, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 8.993e-08, Loss_0: 2.578e-10, Loss_r: 8.967e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 9.068e-08, Loss_0: 1.832e-09, Loss_r: 8.885e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 8.853e-08, Loss_0: 4.296e-10, Loss_r: 8.810e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 8.745e-08, Loss_0: 8.431e-11, Loss_r: 8.736e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 8.666e-08, Loss_0: 6.072e-11, Loss_r: 8.660e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 8.587e-08, Loss_0: 3.029e-11, Loss_r: 8.584e-08, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 8.510e-08, Loss_0: 2.933e-12, Loss_r: 8.510e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 8.438e-08, Loss_0: 2.988e-12, Loss_r: 8.438e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 8.367e-08, Loss_0: 4.069e-12, Loss_r: 8.366e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 8.293e-08, Loss_0: 1.774e-14, Loss_r: 8.293e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 8.226e-08, Loss_0: 1.604e-15, Loss_r: 8.226e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 8.156e-08, Loss_0: 4.403e-12, Loss_r: 8.155e-08, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 8.088e-08, Loss_0: 8.901e-13, Loss_r: 8.088e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 8.021e-08, Loss_0: 1.326e-13, Loss_r: 8.021e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 7.955e-08, Loss_0: 3.954e-12, Loss_r: 7.955e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 7.892e-08, Loss_0: 9.915e-12, Loss_r: 7.891e-08, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 7.854e-08, Loss_0: 2.641e-10, Loss_r: 7.828e-08, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 9.125e-08, Loss_0: 1.345e-08, Loss_r: 7.780e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 7.911e-08, Loss_0: 1.963e-09, Loss_r: 7.714e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 7.653e-08, Loss_0: 1.459e-11, Loss_r: 7.652e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 7.649e-08, Loss_0: 5.433e-10, Loss_r: 7.595e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 7.563e-08, Loss_0: 2.122e-10, Loss_r: 7.542e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 7.492e-08, Loss_0: 2.820e-12, Loss_r: 7.491e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 7.442e-08, Loss_0: 2.895e-11, Loss_r: 7.439e-08, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 7.386e-08, Loss_0: 2.851e-12, Loss_r: 7.386e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 7.335e-08, Loss_0: 7.822e-12, Loss_r: 7.334e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 7.284e-08, Loss_0: 2.959e-12, Loss_r: 7.284e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 7.233e-08, Loss_0: 3.191e-12, Loss_r: 7.233e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 7.184e-08, Loss_0: 2.779e-14, Loss_r: 7.184e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 7.135e-08, Loss_0: 7.599e-13, Loss_r: 7.135e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 7.087e-08, Loss_0: 1.239e-13, Loss_r: 7.087e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 7.040e-08, Loss_0: 3.597e-13, Loss_r: 7.040e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 6.991e-08, Loss_0: 4.360e-13, Loss_r: 6.991e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 6.948e-08, Loss_0: 1.318e-11, Loss_r: 6.947e-08, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 6.904e-08, Loss_0: 2.969e-11, Loss_r: 6.901e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 6.877e-08, Loss_0: 2.217e-10, Loss_r: 6.855e-08, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 7.510e-08, Loss_0: 6.921e-09, Loss_r: 6.818e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 6.908e-08, Loss_0: 1.358e-09, Loss_r: 6.772e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 6.732e-08, Loss_0: 2.160e-11, Loss_r: 6.729e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 6.703e-08, Loss_0: 1.375e-10, Loss_r: 6.689e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 6.664e-08, Loss_0: 1.445e-10, Loss_r: 6.650e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 6.614e-08, Loss_0: 1.081e-11, Loss_r: 6.613e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 6.576e-08, Loss_0: 1.013e-11, Loss_r: 6.575e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 6.537e-08, Loss_0: 1.245e-12, Loss_r: 6.537e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 6.499e-08, Loss_0: 5.279e-12, Loss_r: 6.498e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 6.463e-08, Loss_0: 1.451e-13, Loss_r: 6.463e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 6.427e-08, Loss_0: 2.176e-12, Loss_r: 6.427e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 6.389e-08, Loss_0: 8.016e-14, Loss_r: 6.389e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 6.355e-08, Loss_0: 8.094e-13, Loss_r: 6.354e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 6.317e-08, Loss_0: 9.363e-13, Loss_r: 6.317e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 6.283e-08, Loss_0: 6.276e-14, Loss_r: 6.283e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 6.247e-08, Loss_0: 3.597e-13, Loss_r: 6.247e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 6.212e-08, Loss_0: 7.045e-14, Loss_r: 6.212e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 6.177e-08, Loss_0: 8.848e-13, Loss_r: 6.177e-08, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 6.146e-08, Loss_0: 8.047e-12, Loss_r: 6.145e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 6.118e-08, Loss_0: 7.820e-11, Loss_r: 6.110e-08, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 6.363e-08, Loss_0: 2.813e-09, Loss_r: 6.082e-08, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 3.123e-07, Loss_0: 2.510e-07, Loss_r: 6.132e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 8.241e-08, Loss_0: 2.208e-08, Loss_r: 6.033e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 6.538e-08, Loss_0: 5.571e-09, Loss_r: 5.981e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 7.362e-08, Loss_0: 1.410e-08, Loss_r: 5.952e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 6.049e-08, Loss_0: 1.209e-09, Loss_r: 5.928e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 6.022e-08, Loss_0: 1.179e-09, Loss_r: 5.905e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 5.893e-08, Loss_0: 1.893e-10, Loss_r: 5.874e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 5.866e-08, Loss_0: 2.409e-10, Loss_r: 5.842e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 5.818e-08, Loss_0: 1.052e-11, Loss_r: 5.817e-08, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 5.788e-08, Loss_0: 3.380e-12, Loss_r: 5.788e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 5.760e-08, Loss_0: 1.323e-11, Loss_r: 5.759e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 5.733e-08, Loss_0: 7.989e-12, Loss_r: 5.732e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 5.705e-08, Loss_0: 3.800e-12, Loss_r: 5.705e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 5.678e-08, Loss_0: 1.494e-13, Loss_r: 5.678e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 5.650e-08, Loss_0: 6.535e-13, Loss_r: 5.650e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 5.625e-08, Loss_0: 1.509e-12, Loss_r: 5.625e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 5.597e-08, Loss_0: 1.306e-11, Loss_r: 5.596e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 5.577e-08, Loss_0: 6.500e-11, Loss_r: 5.570e-08, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 5.633e-08, Loss_0: 9.027e-10, Loss_r: 5.543e-08, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 8.812e-08, Loss_0: 3.302e-08, Loss_r: 5.510e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 6.068e-08, Loss_0: 5.776e-09, Loss_r: 5.491e-08, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 5.471e-08, Loss_0: 3.836e-12, Loss_r: 5.471e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 5.548e-08, Loss_0: 9.673e-10, Loss_r: 5.451e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 5.483e-08, Loss_0: 5.676e-10, Loss_r: 5.427e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 5.402e-08, Loss_0: 1.702e-13, Loss_r: 5.402e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 5.388e-08, Loss_0: 9.819e-11, Loss_r: 5.378e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 5.357e-08, Loss_0: 1.247e-12, Loss_r: 5.357e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 5.335e-08, Loss_0: 1.895e-11, Loss_r: 5.333e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 5.312e-08, Loss_0: 4.244e-12, Loss_r: 5.312e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 5.289e-08, Loss_0: 1.124e-13, Loss_r: 5.289e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 5.265e-08, Loss_0: 3.334e-15, Loss_r: 5.265e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 5.244e-08, Loss_0: 2.407e-12, Loss_r: 5.243e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 5.222e-08, Loss_0: 1.355e-12, Loss_r: 5.222e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 5.200e-08, Loss_0: 3.599e-12, Loss_r: 5.200e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 5.178e-08, Loss_0: 1.025e-12, Loss_r: 5.178e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 5.157e-08, Loss_0: 1.031e-11, Loss_r: 5.156e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 5.134e-08, Loss_0: 1.523e-13, Loss_r: 5.134e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 5.114e-08, Loss_0: 1.364e-11, Loss_r: 5.113e-08, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 5.106e-08, Loss_0: 1.616e-10, Loss_r: 5.089e-08, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 5.611e-08, Loss_0: 5.458e-09, Loss_r: 5.065e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 5.144e-08, Loss_0: 9.655e-10, Loss_r: 5.047e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 5.032e-08, Loss_0: 5.556e-12, Loss_r: 5.031e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 5.025e-08, Loss_0: 1.339e-10, Loss_r: 5.011e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 5.004e-08, Loss_0: 1.120e-10, Loss_r: 4.993e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 4.973e-08, Loss_0: 4.913e-14, Loss_r: 4.973e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 4.955e-08, Loss_0: 1.762e-11, Loss_r: 4.954e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 4.936e-08, Loss_0: 7.405e-13, Loss_r: 4.936e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 4.917e-08, Loss_0: 2.062e-12, Loss_r: 4.917e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 4.898e-08, Loss_0: 4.054e-12, Loss_r: 4.897e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 4.879e-08, Loss_0: 3.987e-13, Loss_r: 4.879e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 4.861e-08, Loss_0: 4.070e-13, Loss_r: 4.861e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 4.841e-08, Loss_0: 2.745e-12, Loss_r: 4.841e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 4.823e-08, Loss_0: 2.087e-12, Loss_r: 4.823e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 4.804e-08, Loss_0: 1.685e-12, Loss_r: 4.804e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 4.787e-08, Loss_0: 4.489e-12, Loss_r: 4.787e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 4.767e-08, Loss_0: 1.824e-12, Loss_r: 4.767e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 4.750e-08, Loss_0: 1.127e-11, Loss_r: 4.749e-08, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 4.735e-08, Loss_0: 4.580e-11, Loss_r: 4.730e-08, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 4.765e-08, Loss_0: 5.362e-10, Loss_r: 4.711e-08, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 6.179e-08, Loss_0: 1.491e-08, Loss_r: 4.688e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 4.963e-08, Loss_0: 2.887e-09, Loss_r: 4.675e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 4.666e-08, Loss_0: 5.246e-11, Loss_r: 4.661e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 4.675e-08, Loss_0: 2.928e-10, Loss_r: 4.646e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 4.660e-08, Loss_0: 2.969e-10, Loss_r: 4.630e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 4.613e-08, Loss_0: 1.059e-11, Loss_r: 4.612e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 4.599e-08, Loss_0: 2.910e-11, Loss_r: 4.596e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 4.580e-08, Loss_0: 2.364e-12, Loss_r: 4.579e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 4.564e-08, Loss_0: 1.954e-12, Loss_r: 4.564e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 4.547e-08, Loss_0: 8.313e-13, Loss_r: 4.547e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 4.530e-08, Loss_0: 3.154e-13, Loss_r: 4.530e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 4.516e-08, Loss_0: 6.341e-13, Loss_r: 4.516e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 4.499e-08, Loss_0: 3.917e-13, Loss_r: 4.499e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 4.482e-08, Loss_0: 3.543e-12, Loss_r: 4.482e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 4.467e-08, Loss_0: 9.446e-14, Loss_r: 4.467e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 4.450e-08, Loss_0: 4.484e-13, Loss_r: 4.449e-08, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 4.434e-08, Loss_0: 3.475e-13, Loss_r: 4.434e-08, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 4.419e-08, Loss_0: 5.464e-12, Loss_r: 4.419e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 4.406e-08, Loss_0: 2.576e-11, Loss_r: 4.403e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 4.397e-08, Loss_0: 9.879e-11, Loss_r: 4.387e-08, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 4.589e-08, Loss_0: 2.158e-09, Loss_r: 4.373e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 4.402e-08, Loss_0: 4.524e-10, Loss_r: 4.357e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 4.344e-08, Loss_0: 1.718e-11, Loss_r: 4.342e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 4.330e-08, Loss_0: 2.859e-11, Loss_r: 4.327e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 4.318e-08, Loss_0: 5.327e-11, Loss_r: 4.313e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 4.300e-08, Loss_0: 7.300e-12, Loss_r: 4.299e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 4.284e-08, Loss_0: 9.327e-13, Loss_r: 4.284e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 4.270e-08, Loss_0: 2.359e-12, Loss_r: 4.270e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 4.257e-08, Loss_0: 4.165e-13, Loss_r: 4.257e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 4.242e-08, Loss_0: 6.763e-13, Loss_r: 4.241e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 4.228e-08, Loss_0: 6.565e-15, Loss_r: 4.228e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 4.213e-08, Loss_0: 2.529e-15, Loss_r: 4.213e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 4.198e-08, Loss_0: 3.231e-14, Loss_r: 4.198e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 4.185e-08, Loss_0: 7.026e-17, Loss_r: 4.185e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 4.170e-08, Loss_0: 1.143e-13, Loss_r: 4.170e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 4.156e-08, Loss_0: 6.898e-14, Loss_r: 4.156e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 4.141e-08, Loss_0: 2.296e-12, Loss_r: 4.141e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 4.128e-08, Loss_0: 4.372e-12, Loss_r: 4.127e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 4.114e-08, Loss_0: 2.274e-13, Loss_r: 4.114e-08, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 4.098e-08, Loss_0: 3.241e-12, Loss_r: 4.098e-08, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 4.095e-08, Loss_0: 1.066e-10, Loss_r: 4.084e-08, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 4.554e-08, Loss_0: 4.870e-09, Loss_r: 4.067e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 4.138e-08, Loss_0: 8.165e-10, Loss_r: 4.057e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 4.046e-08, Loss_0: 1.756e-15, Loss_r: 4.046e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 4.048e-08, Loss_0: 1.514e-10, Loss_r: 4.033e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 4.030e-08, Loss_0: 9.501e-11, Loss_r: 4.020e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 4.007e-08, Loss_0: 2.333e-14, Loss_r: 4.007e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 3.996e-08, Loss_0: 2.584e-11, Loss_r: 3.993e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 3.982e-08, Loss_0: 4.046e-13, Loss_r: 3.982e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 3.969e-08, Loss_0: 2.105e-12, Loss_r: 3.969e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 3.956e-08, Loss_0: 2.823e-12, Loss_r: 3.956e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 3.943e-08, Loss_0: 4.372e-13, Loss_r: 3.943e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 3.929e-08, Loss_0: 4.238e-13, Loss_r: 3.929e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 3.917e-08, Loss_0: 4.853e-13, Loss_r: 3.917e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 3.905e-08, Loss_0: 2.151e-13, Loss_r: 3.905e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 3.892e-08, Loss_0: 1.952e-14, Loss_r: 3.892e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 3.880e-08, Loss_0: 9.909e-14, Loss_r: 3.880e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 3.867e-08, Loss_0: 6.355e-13, Loss_r: 3.867e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 3.855e-08, Loss_0: 1.122e-11, Loss_r: 3.854e-08, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 3.848e-08, Loss_0: 8.114e-11, Loss_r: 3.840e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 3.947e-08, Loss_0: 1.203e-09, Loss_r: 3.827e-08, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 7.207e-08, Loss_0: 3.397e-08, Loss_r: 3.810e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 4.486e-08, Loss_0: 6.854e-09, Loss_r: 3.800e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 3.808e-08, Loss_0: 1.664e-10, Loss_r: 3.791e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 3.838e-08, Loss_0: 5.583e-10, Loss_r: 3.782e-08, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 3.832e-08, Loss_0: 6.194e-10, Loss_r: 3.770e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 3.761e-08, Loss_0: 3.386e-11, Loss_r: 3.758e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 3.752e-08, Loss_0: 6.034e-11, Loss_r: 3.746e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 3.735e-08, Loss_0: 1.066e-11, Loss_r: 3.734e-08, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 3.724e-08, Loss_0: 7.115e-12, Loss_r: 3.723e-08, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 3.712e-08, Loss_0: 3.217e-13, Loss_r: 3.712e-08, Time: 0.06, Learning Rate: 0.00023\n",
            "Training time: 23.0550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CSol[idx] = predict_CSol[idx].reshape(exact_C.shape)\n",
        "\n",
        "  error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "  print('Error C Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_C) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec33aa6b-0859-45bc-8414-7731ae77bd99",
        "id": "03QJAdUAotoh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Sol [1, 64, 64, 64, 1] : 6.926940e-04\n",
            "Error C Sol [1, 128, 128, 128, 1] : 7.304154e-04\n",
            "Error C Sol [1, 256, 256, 256, 1] : 9.402685e-04\n",
            "Error C Sol [1, 64, 64, 64, 64, 1] : 5.085711e-04\n",
            "Error C Sol [1, 256, 256, 256, 256, 1] : 1.673594e-03\n",
            "Error C Sol [1, 128, 128, 128, 128, 1] : 6.821085e-04\n",
            "Error C Sol [1, 128, 128, 64, 64, 1] : 6.004315e-04\n",
            "Error C Sol [1, 256, 128, 64, 32, 1] : 7.411866e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [3,5,6]:\n",
        "  plt.plot(t, predict_CSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_C.flatten(), 'c', label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e535234-a504-43d7-84d1-1cfca281deed",
        "id": "xETJ4YQtotoi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6c802ad300>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjk0lEQVR4nO3deVhUZf8G8PvMsA3bgCCLyOqOKy6YmKmJmhpq9aaVKWnZhqlR/tLU0Exxy900zeXtTVzKXCrTDHFLTXNX3EVxAzSWYV9mzu+PkckJ1Bmc4cBwf7rmgnnOc87znQGcb+fZBFEURRARERFZCJnUARARERGZEpMbIiIisihMboiIiMiiMLkhIiIii8LkhoiIiCwKkxsiIiKyKExuiIiIyKJYSR1AZdNoNLh9+zacnJwgCILU4RAREZEBRFFEdnY26tSpA5ns0fdmalxyc/v2bfj6+kodBhEREVXAjRs3ULdu3UfWqXHJjZOTEwDtm+Ps7CxxNERERGQIlUoFX19f3ef4o9S45Ka0K8rZ2ZnJDRERUTVjyJASDigmIiIii8LkhoiIiCwKkxsiIiKyKExuiIiIyKIwuSEiIiKLwuSGiIiILAqTGyIiIrIoTG6IiIjIojC5ISIiIovC5IaIiIgsCpMbIiIisihVIrlZvHgxAgICYGdnh/bt2+Pw4cMPrbt69WoIgqD3sLOzq8RoiYiIqCqTfOPM9evXIzo6GkuXLkX79u0xb9489OzZExcuXICHh0e55zg7O+PChQu654ZsokVExhFF8f5D+71GI5Yp09bT1tdoxAfOffCrqP1P/OerrvQhZYBQphwQ7tcHhPulACAK/8T74L8FIsQy/zb8u472f5DKvvZ/l1W0jkymXyigbKUH6zx4/ME4ZUL55eXVF+6/T2XriQ/UefAcsdzzHoy09FKl1wH+OU9L1Pv64O9C+fX++b70Z1zedR6sX34948sfXla2jv41yj439Dplr/vYKkbUKe/zT3hsHb2/lTJ1BL0yQRAeWwcANKV/u6IIudwari4uj38RZiJ5cjNnzhwMHz4cQ4cOBQAsXboUv/zyC1auXImxY8eWe44gCPDy8jLo+oWFhSgsLNQ9V6lUTx400SOIogi1Oh9ZWdlIz1Qh7e8MZKjykJlbgPTcAmTklUBVoEF2kRrZhUBeiYD8EgHFagGFGhmKRRmKNTKUQI5iUQ41ZCiBDGpRBo0ggwYyqCGDKAjQCIK27P5XURAgCtB+xT/fo7Ts/nEIgAhtOWT3/zkurXv/e+0x8YHn4j/lpd+XHsMjyh58/qB/13lYvX+XG1L/YUz1/0GCAZ88RDWYY1oRsj/uJFn7kiY3RUVFOHr0KMaNG6crk8lkCA8Px8GDBx96Xk5ODvz9/aHRaNC6dWtMmzYNTZs2LbdubGwsJk+ebPLYyfIVFeXhZspNXLh+C+dTM3EtIx+3szW4mydHZokNskUb5Au2KJJZo1hujWJrOUqsZVDbyKCxBaBQ//OwAaC00z5MQmOi6xARmV5JibSjXiRNbu7duwe1Wg1PT0+9ck9PT5w/f77ccxo1aoSVK1eiRYsWyMrKwuzZsxEWFoazZ8+ibt26ZeqPGzcO0dHRuucqlQq+vr6mfSFU7eTkZ+KvxNP443IKEtMKkJwjx91iBTJlDsi1U6DA0RolzgCciwFHAXB31T4equT+wzBCESCUiJCVPjQiZGoNZKKofWg0kEOEXNRALoqQC6L2eelXAHJBhJUg3P8KWMkEyAVALgPkgqA9JhNgJQByuUz3XC4IsJLJIJMBVjIZ5LpyGWQyAVaCDLL7ZTJBpv0K7XkymbaOXBAgEwCZIIcM2m4TQQbIoL2eIAjarhTh/rH7HRq67+/fQZHd/15bJtwvA3S3w+8fF8V/umb0ulTu37vX3jYXH+imEfFgp03pV42o38VTbjdDma6IslU04r9uApVbRz8BLbdDQyM+0J9VtltPe51/OuIe2sXzQFfeQ2uJ+s8fVkcQ/hVr6XkPdP/pd0v98/0/1xHv3x3855x/x6E7tbRMKHtYFB9e9q+m79cr/46atvzRd9vKPfrv3wUDupyA8rsoy6lVoev8u2tTkJUtFx74+3loHeFfdcv7+yr929TV+eeYftk/f2+l3ay1Gtk89vWZk+TdUsbq0KEDOnTooHseFhaGJk2a4Ouvv8aUKVPK1Le1tYWtrW1lhkhVyOWbF7Hpz1PYl5yLyzkOSLNygUppj2J3AG5qwLM24Pnvs9T3H/pkuYA8X4RNoRq2xcVQqEvgIKhhLxPhZAU428rhqrCCu8IW3kon+CidUdfFEbXtHKG0toO9XA47mQy2MpneBywREZmWpMmNu7s75HI5UlNT9cpTU1MNHlNjbW2NkJAQXL582RwhUjWh0Wiw/a+9+ObAdZzNdsJdGyWyXexRUkcNuLlrHzpF/3xbAlipALucEjgWFsMVRahjBzSqZY8WdTzR0scbQUpn1LK2hpWsSkwuJCKix5A0ubGxsUGbNm0QHx+P/v37A9B+SMXHx2PEiBEGXUOtVuP06dPo3bu3GSOlqqa4pAQ/7P8d/zuSilOF7khzq4XiegLQyv+BWvcHkqsB678BpaoQdcRCtKhlgx4N/NG5XgB87O0g510UIiKLInm3VHR0NCIjI9G2bVuEhoZi3rx5yM3N1c2eGjJkCHx8fBAbGwsA+Pzzz/HUU0+hfv36yMzMxKxZs3D9+nW89dZbUr4MMjNRFLFubzy+OXAbiSUeuOvhAnU9BdCuNJnRJjLyLAG17ubDVyhCazcFXmjWEM8G1oWdXC5d8EREVKkkT24GDhyIu3fv4rPPPkNKSgpatWqF7du36wYZJycnQ/ZAd0BGRgaGDx+OlJQUuLq6ok2bNjhw4ACCg4OleglkRofOncKnm07iiFUAclrYAB387h8pAABYpQOe6XkIcwY+7NgGT9Xx4LpHREQ1nCCWvyqRxVKpVFAqlcjKyoKzs7PU4VA5bt9Lwcf/24GdKh/ca6YA3Ip1x6z/BnwyctHZ3RofPR2K5u4u0gVKRESVxpjPb8nv3BABQH5hAT5fswlrk5yQHFgbYkhpd1MxZDkCGqapEPNUQwzs3JB3ZoiI6JGY3JCkioqL8dbCddigaYDCEG8gCADygWLA+2Ye3q6nxKe9O8CGM5WIiMhATG5IEqIo4tNVG7DoVl3khPkCcu0YGudbRXjeSYN5Pbqith3XJyIiIuMxuaFKt/Sn7Zh4yBb3OnkDQdrxNHWSc7H22ZZ4pkvZVaaJiIiMweSGKs32Q4fxzqZ0JIc5At1LAGigvF2IRa188HqXLlKHR0REFoLJDZnd6UuX8Mrys0hs5QX0sgNQArt7aoyro8DEVztzgDAREZkUkxsyq0n/24QvVD5Q93YBUACrbBGDbYqx7MVwbmdARERmweSGzEKjEdHn83XY3swf8M2DUACE52Xjx7694GjFXzsiIjIffsqQyWVmZ6NdzF5cfq4OYFMExd0SJHRqgvZePlKHRkRENQCTGzKpQ2cS8dy6DGT1dQAgwud2Ns681BMutjZSh0ZERDUEBz2Qycz/4Vd0is9BVrh2enfXv7OQ/OrzTGyIiKhSMbkhk3h12jqMLnRFScs8CIXAp1YCdr3UDzLOhCIiokrGbil6Irl5BegwfgdOd/cB7Atgk6HBltb+eM6/ntShERFRDcXkhiosKzsHDSYfxd1+SgBquN/Jw6n+XeCtsJc6NCIiqsHYLUUVotGIaPP5Xtx9XgQAtL6bidsDn2NiQ0REkmNyQxXS6/MNuNLDAQDQ9V4mjr7cH9ZclI+IiKoAfhqR0cZ9sxm/tawLWIvwu6lC/Ev9pA6JiIhIh8kNGeXHPQcxQ+YFuBbDIaUYZwb04t5QRERUpTC5IYNduXETr/0pgxhUALkKONi9BZxsrKUOi4iISA+TGzJIQWERnvrqMgpD84FiYHmAM5q7eUodFhERURlMbsggoZ/9ins9td8PV+djaIvW0gZERET0EExu6LEGTN+A011dAQBt76Rj2XO9JI6IiIjo4Zjc0CPNXL8N3wfVBew08LiZg0OvvCB1SERERI/E5IYeatfRExiX4QZ4FMEuTY0z/+kGOWdGERFRFcfkhsqVm1eAvr/mQ9M4H7Jc4Lew+qhtp5A6LCIiosdickPl+s/8Lch9uhBQA9PdrNGprr/UIRERERmEyQ2VkXwnBb95apOZxtfTMeapjhJHREREZDgmN1RGv68PQBNUACFXwLaXukodDhERkVGY3JCefafO4ERTLwDAMxl3Eah0lTgiIiIi4zC5IT2DN94AahfBKh34eSA3xCQiouqHyQ3prPwtAddDHQEAg+TZcLTmvlFERFT9MLkhAIAoihhzWAAc1LC/rcbKvs9LHRIREVGFMLkhAMDY/21G+lPaBfom+FpBxsX6iIiommJyQyguLsHCex6AlQi3pHyM69RZ6pCIiIgqjMkN4bVFG5HfuhhQAyvCfKUOh4iI6Ikwuanh7mWkY5OjHwAgKCkT/Ro1kzgiIiKiJ8Pkpobruzge6gaFQIGALf2ekjocIiKiJ8bkpgY7cfkyDtX3AQCE3rmHZrW9JI6IiIjoyTG5qcEGxCVC9CqCPFPAr6/2ljocIiIik2ByU0P9+McBXGpVCwDQrzAdtewUEkdERERkGkxuaqj3E3IB5xLY3dFg/cvcZoGIiCwHk5saaMP+A0gNtQEAjHIrgZWMvwZERGQ5+KlWA03ZnwLYiFDc0GB6jx5Sh0NERGRSTG5qmJISNc55amdFtS/6W+JoiIiITI/JTQ0z8fufoQ4sAooFfNW3k9ThEBERmRyTmxrmf8nasTZuSflownVtiIjIAjG5qUFu/30Xtxo4AQD61SqSOBoiIiLzYHJTg0St2wXUKoGQLcPCfr2kDoeIiMgsmNzUILuKPAAAATcyYG9tLXE0RERE5sHkpobYdeokVE21P+7RLd0kjoaIiMh8mNzUEGN3XARsRNjcAT4I6yh1OERERGbD5KYG0GhEnFTWAQC0VN2DIAgSR0RERGQ+TG5qgLnbdqCoYTGgBub1DJE6HCIiIrNiclMDLDpbDABwTipGWEA9iaMhIiIyLyY3Fk6Vm4PrAbUAAN3tsiWOhoiIyPyY3Fi4keu2QfQsBvJkWPICN8kkIiLLVyWSm8WLFyMgIAB2dnZo3749Dh8+bNB569atgyAI6N+/v3kDrMZ+ztTetalzTYXaDo4SR0NERGR+kic369evR3R0NGJiYnDs2DG0bNkSPXv2RFpa2iPPu3btGj7++GN06sTNHx/mZNJl/N1Eu5fUm/VsJY6GiIiockie3MyZMwfDhw/H0KFDERwcjKVLl8Le3h4rV6586DlqtRqDBg3C5MmTERQU9MjrFxYWQqVS6T1qipGbjwL2GsjvyjCpezepwyEiIqoUkiY3RUVFOHr0KMLDw3VlMpkM4eHhOHjw4EPP+/zzz+Hh4YE333zzsW3ExsZCqVTqHr6+viaJvTo4YusNAGhy9x5kMsnzWCIiokoh6SfevXv3oFar4enpqVfu6emJlJSUcs/Zv38/VqxYgeXLlxvUxrhx45CVlaV73Lhx44njrg6+3bMH+Y01AIAvnmkgcTRERESVx0rqAIyRnZ2NwYMHY/ny5XB3dzfoHFtbW9ja1rzxJrF/pgOhrrC/qkG/Ls2lDoeIiKjSSJrcuLu7Qy6XIzU1Va88NTUVXl5eZepfuXIF165dQ0REhK5Mo9HenbCyssKFCxdQrx4XqSsqLsKlOh4AivG0mCF1OERERJVK0m4pGxsbtGnTBvHx8boyjUaD+Ph4dOjQoUz9xo0b4/Tp0zhx4oTu0bdvX3Tt2hUnTpyoUeNpHuX/NvwEdd1ioFDAkhc6Sx0OERFRpZK8Wyo6OhqRkZFo27YtQkNDMW/ePOTm5mLo0KEAgCFDhsDHxwexsbGws7NDs2bN9M53cXEBgDLlNdn6Ow6AD1D7Sj6CehrWfUdERGQpJE9uBg4ciLt37+Kzzz5DSkoKWrVqhe3bt+sGGScnJ3OmjxHuZNxDSkMHAGq87FkidThERESVThBFUZQ6iMqkUqmgVCqRlZUFZ2dnqcMxuQ/ifsSiOrUgqGTI7x0GWyvJ81ciIqInZsznN2+JWJj429qvLjcLmNgQEVGNxOTGwlx3cAMANEbNWYmZiIjoQUxuLEhK5t/ICxAAAIOalZ1KT0REVBMwubEg03/dAyg0ELJleDus7FR6IiKimoDJjQX57bb2ro3yZiGsreQSR0NERCQNJjcWJFnhCgBoJGZJHAkREZF0mNxYiLTMDOQGaH+crwbXljgaIiIi6TC5sRAzd+wG7LXjbd5/+mmpwyEiIpIMkxsLsf2Gdi1GjrchIqKajsmNhbhmXwsA0FDD8TZERFSzMbmxAH9nZyHXX/ujHBjMjTKJiKhmY3JjAWZsTwAcNECODCM6dZI6HCIiIkkxubEAvyarAQDKm0Ww4XgbIiKq4ZjcWIBrdtr9pBqUcLwNERERk5tqLiNHhZwA7d2alxvXkjgaIiIi6TG5qeZmbk8AHNRArgyjunC8DREREZObam7b9RIAgPONIthaWUkcDRERkfSY3FRzSbbariiOtyEiItJiclONZeXmIDtAe7fmxUZKiaMhIiKqGpjcVGOzdiQAjmogT4YPu3SROhwiIqIqgclNNfbztUIAgPONYiisOd6GiIgIYHJTrV210a5vU684U9pAiIiIqhAmN9VUdl4esv3vj7epz/E2REREpZjcVFOzduwCnLTjbT7q1lnqcIiIiKoMJjfV1M9JBQAApxvFUFhbSxwNERFR1cHkppq6YqNd36ZeUaa0gRAREVUxTG6qoZz8PKj8tHdr+tdzkjgaIiKiqoXJTTX05c4EwFkN5Mvw0bMcb0NERPQgJjfV0NbLeQAAxxslcLSzlTgaIiKiqoXJTTV02Vq7vk1QYaa0gRAREVVBTG6qmfzCAqj8bAAAEYEOEkdDRERU9TC5qWbm/LYLUJYABTJ8Es7xNkRERP/G5Kaa2XQpF4B2vI2TnZ3E0RAREVU9TG6qmctW2vVtAgsypQ2EiIioimJyU40UFBYiy087O+r5AHuJoyEiIqqarKQOgAy3+o8DgIsAFAkcb0NERPQQvHNTjey8chcAYJMiQKlQSBwNERFR1cTkpho5r9LeaHNW5UscCRERUdXF5KYaSbHS7iPlrcmVOBIiIqKqi8lNNZLtop363cxF2jiIiIiqMiY31URWbi6KvUQAQO/GdSWOhoiIqOpiclNNrP3zMGCrAQoF/CektdThEBERVVlMbqqJnZfvAQBsUgXYWXMGPxER0cMwuakmzmfLAXCmFBER0eMwuakmOFOKiIjIMExuqolspXbRvqauEgdCRERUxTG5qQaycnNR7K0BADzPmVJERESPxOSmGtDNlCoS8FIrzpQiIiJ6FCY31QBnShERERmOyU01cK50plQWZ0oRERE9DpObaiC1dKaUmCNxJERERFUfk5tqQDdTykWQOBIiIqKqj8lNFafKzUWxl3amVERjX4mjISIiqvqMHp2q0WiwZ88e7Nu3D9evX0deXh5q166NkJAQhIeHw9eXH8CmtPbPw4CdoJ0p9XSI1OEQERFVeQbfucnPz8cXX3wBX19f9O7dG7/++isyMzMhl8tx+fJlxMTEIDAwEL1798ahQ4fMGXON8uBMKVsrzpQiIiJ6HIM/LRs2bIgOHTpg+fLl6N69O6ytrcvUuX79OuLi4vDKK69g/PjxGD58uEmDrYkSuacUERGRUQy+c/Pbb79hw4YN6N27d7mJDQD4+/tj3LhxuHTpEp599lmDg1i8eDECAgJgZ2eH9u3b4/Dhww+t++OPP6Jt27ZwcXGBg4MDWrVqhf/9738Gt1XdpMlL95TiTCkiIiJDGJzcNGnSxOCLWltbo169egbVXb9+PaKjoxETE4Njx46hZcuW6NmzJ9LS0sqtX6tWLYwfPx4HDx7EqVOnMHToUAwdOhQ7duwwOL7qROXCmVJERETGEERRFI05QRRFXLt2Db6+vrCyskJRURE2bdqEwsJC9O7dG+7u7kYF0L59e7Rr1w6LFi0CoB2w7Ovriw8++ABjx4416BqtW7dGnz59MGXKlDLHCgsLUVhYqHuuUqng6+uLrKwsODs7GxVrZVPl5kK57yhgp8F3Dg4Y1K6d1CERERFJQqVSQalUGvT5bdRU8AsXLiAwMBD169dHkyZNkJSUhLCwMLz55pt477330KRJE1y6dMng6xUVFeHo0aMIDw//JyCZDOHh4Th48OBjzxdFEfHx8bhw4QKeeeaZcuvExsZCqVTqHtVpNlfcocOA3f09pUI4U4qIiMgQRiU3n3zyCVq2bIkTJ07g+eefR58+fVC3bl1kZGQgPT0dHTp0wOeff27w9e7duwe1Wg1PT0+9ck9PT6SkpDz0vKysLDg6OsLGxgZ9+vTBwoUL0b1793Lrjhs3DllZWbrHjRs3DI5Par9fuT9TKk2AHWdKERERGcSoT8wDBw7gt99+Q/PmzfHFF19g/vz5WLZsmW6A8dixY/Hqq6+aJdAHOTk54cSJE8jJyUF8fDyio6MRFBSELl26lKlra2sLW1tbs8dkDv/sKZUncSRERETVh1HJTU5ODmrVqgUAcHBwgIODA7y9vXXHfX19kZqaavD13N3dIZfLy5yTmpoKLy+vh54nk8lQv359AECrVq1w7tw5xMbGlpvcVGcpuplSuRJHQkREVH0Y1S1Vp04dJCcn657PnDkTHh4euud3796Fq6urwdezsbFBmzZtEB8fryvTaDSIj49Hhw4dDL6ORqPRGzRsKbinFBERkfGMunMTHh6O8+fP4+mnnwYAvPfee3rHf/vtN7Ru3dqoAKKjoxEZGYm2bdsiNDQU8+bNQ25uLoYOHQoAGDJkCHx8fBAbGwtAO0C4bdu2qFevHgoLC7Ft2zb873//w5IlS4xqt6rT31OqrsTREBERVR9GJTdLly595PGBAwciMjLSqAAGDhyIu3fv4rPPPkNKSgpatWqF7du36wYZJycnQyb75wZTbm4u3n//fdy8eRMKhQKNGzfGd999h4EDBxrVblW39tBhQCEAxQJeDDEuYSQiIqrJjF7nprozZp68lP6zbAM2NvSAzU0ZCl8vf5o7ERFRTWHM53eF5xcfOXIECQkJSEtLg0aj0Ts2Z86cil6W7juXrf3RcE8pIiIi41QouZk2bRomTJiARo0awdPTE4Lwz4DXB7+nikuRcU8pIiKiiqhQcjN//nysXLkSb7zxhonDoVLZLgoAJWiqZLJIRERkDKOmgutOksnQsWNHU8dC9z04U+p5zpQiIiIySoWSmw8//BCLFy82dSx0n3amlAYoFvASZ0oREREZpULdUh9//DH69OmDevXqITg4WLf9Qqkff/zRJMHVVL9dvgs08oBNKrinFBERkZEq9Mk5cuRIJCQkoGvXrnBzc+MgYhM7r5spVSBxJERERNVPhZKb//73v9i4cSP69Olj6ngID+4pxZlSRERExqrQmJtatWqhXr16po6F7vtnTylp4yAiIqqOKpTcTJo0CTExMcjLyzN1PDWeKjcXxd73Z0o18pU4GiIiouqnQt1SCxYswJUrV+Dp6YmAgIAyA4qPHTtmkuBqonUP7Cn1n5A2UodDRERU7VQouenfv7+Jw6BSD86UsrWSSx0OERFRtWNUcnP16lUEBQUhJibGXPHUeOc4U4qIiOiJGDXmpkWLFmjWrBk+/fRTHD582Fwx1WipuplSuRJHQkREVD0Zldzcu3cPsbGxSEtLQ9++feHt7Y3hw4fjp59+QkEB7zSYgko3U0qUOBIiIqLqyajkxs7ODhEREfjmm29w584dbNy4EW5ubvjkk0/g7u6O/v37Y+XKlbh796654rVoWTn/zJTq05gzpYiIiCqiQlPBAUAQBISFhWH69OlITEzE8ePH0alTJ6xevRp169bl3lMVsP7Pf/aUerkVZ0oRERFVRIWTm39r0KABPvroI+zduxe3b99Gjx49THXpGuO3y9o7XpwpRUREVHEGz5baunWrQfUEQUBERATc3NwqHFRNdU5VOlMqX+JIiIiIqi+DkxtD17YRBAFqtbqi8dRoqValM6W48jMREVFFGZzcaDQac8ZBKJ0pVcKZUkRERE/AZGNu6MlwphQREZFpVDi52bNnDyIiIlC/fn3Ur18fffv2xb59+0wZW42y5fgJzpQiIiIygQolN9999x3Cw8Nhb2+PkSNHYuTIkVAoFOjWrRvi4uJMHWONcDg5DQAgT5dxphQREdETqNDGmVOnTsXMmTPx4Ycf6spGjhyJOXPmYMqUKXjttddMFmBNcSmzGPABbHNKpA6FiIioWqvQnZurV68iIiKiTHnfvn2RlJT0xEHVRLcLtHmmQ36hxJEQERFVbxVKbnx9fREfH1+m/Pfff4evLwfDVkQ67AAASg336CIiInoSFeqW+uijjzBy5EicOHECYWFhAIA//vgDq1evxvz5800aYE2RY6NNbjytiiWOhIiIqHqrUHLz3nvvwcvLC19++SU2bNgAAGjSpAnWr1+Pfv36mTTAmqLAwRqAGgHOnJ1PRET0JCqU3ADACy+8gBdeeMGUsdRoxS4CAKCll4u0gRAREVVzFU5uSuXk5JRZvdjZ2flJL1ujJN9NheiinSXVtWGQxNEQERFVbxXqA0lKSkKfPn3g4OAApVIJV1dXuLq6wsXFBa6urqaO0eL9nnhB+5MoEtC6rr/U4RAREVVrFbpz8/rrr0MURaxcuRKenp4QBMHUcdUof938G/BxhTxDBpmMY26IiIieRIWSm5MnT+Lo0aNo1KiRqeOpkS5nFHEBPyIiIhOp0G2Cdu3a4caNG6aOpca6XWgNgAv4ERERmUKF7tx88803ePfdd3Hr1i00a9YM1tbWesdbtGhhkuBqioz7C/i5aPIljoSIiKj6q1Byc/fuXVy5cgVDhw7VlQmCAFEUIQgC1Gq1yQKsCbJttcmNhzW7pYiIiJ5UhZKbYcOGISQkBGvXruWAYhMocLABUIIgLuBHRET0xCqU3Fy/fh1bt25F/fr1TR1PjVTiov3awovT6ImIiJ5UhW4VPPvsszh58qSpY6mRrqWmQnTVdkc927CexNEQERFVfxW6cxMREYEPP/wQp0+fRvPmzcsMKO7bt69JgqsJdp47r/2mSEBIXT9pgyEiIrIAFUpu3n33XQDA559/XuYYBxQb59jNv4G6tSDPEDh2iYiIyAQqlNz8ey8pqrjLmcVAXcAumwkhERGRKVRozM3NmzcfeuzQoUMVDqYmunN/AT/7ggKJIyEiIrIMFUpuevTogfT09DLlf/zxB5577rknDqomSb+/gJ+rhskNERGRKVQouXnqqafQo0cPZGdn68r27t2L3r17IyYmxmTB1QQ5tgoAgCcX8CMiIjKJCiU333zzDfz8/BAREYHCwkIkJCSgT58++Pzzz/Hhhx+aOkaLVuCg7ZYK4AJ+REREJlGhT1SZTIZ169bB2toazz77LPr27YvY2FiMGjXK1PFZvGIX7deQOrUkjYOIiMhSGDxb6tSpU2XKJk2ahFdffRWvv/46nnnmGV0dbpxpmCspt4H7C/h1bcjVnomIiExBEEVRNKSiTCbTbY6pO/mB59Vl40yVSgWlUomsrCw4OztLGsuyhN14RwBQKEDT4xmuc0NERPQQxnx+G3znJikp6YkDI31Hb6YDvrVglckF/IiIiEzF4OTG39/fnHHUSJezSgBfwJYL+BEREZmMwQOKjVmcLy8vD2fPnq1QQDVJSqE2t3TgAn5EREQmY3ByM3jwYPTs2RPff/89cnNzy62TmJiITz/9FPXq1cPRo0dNFqSlyri/gJ+LplDiSIiIiCyHwd1SiYmJWLJkCSZMmIDXXnsNDRs2RJ06dWBnZ4eMjAycP38eOTk5eOGFF/Dbb7+hefPm5ozbIugW8LMpljgSIiIiy2HwbKkH/fXXX9i/fz+uX7+O/Px8uLu7IyQkBF27dkWtWlV7vZaqNFvKZvV+FAeUIPJ2Ola/9qKksRAREVVlZpkt9aC2bduibdu2FQquPIsXL8asWbOQkpKCli1bYuHChQgNDS237vLly/Htt9/izJkzAIA2bdpg2rRpD61flXEBPyIiItOTfM3/9evXIzo6GjExMTh27BhatmyJnj17Ii0trdz6u3fvxquvvoqEhAQcPHgQvr6+6NGjB27dulXJkT+Zi7duAi7aBfyebdhA4miIiIgsR4W6pUypffv2aNeuHRYtWgQA0Gg08PX1xQcffICxY8c+9ny1Wg1XV1csWrQIQ4YMeWz9qtIttSQ+Ae/LBS7gR0REZABjPr8lvXNTVFSEo0ePIjw8XFcmk8kQHh6OgwcPGnSNvLw8FBcXP3SsT2FhIVQqld6jKjh2Ox0AYJXBBfyIiIhMSdLk5t69e1Cr1fD09NQr9/T0REpKikHX+OSTT1CnTh29BOlBsbGxUCqVuoevr+8Tx20KVzO1C/fZZpdIHAkREZFlkXzMzZOYPn061q1bh02bNsHOzq7cOuPGjUNWVpbucePGjUqOsnx3iqwBAA6FXOOGiIjIlIxKbnbt2oXg4OByu3aysrLQtGlT7Nu3z+Drubu7Qy6XIzU1Va88NTUVXl5ejzx39uzZmD59On777bdH7kJua2sLZ2dnvUdVkClokzFXDVcnJiIiMiWjkpt58+Zh+PDh5SYISqUS77zzDubMmWPw9WxsbNCmTRvEx8fryjQaDeLj49GhQ4eHnjdz5kxMmTIF27dvN+mU9MqUY6tNbjxt2C1FRERkSkYlNydPnsRzzz330OM9evQwetuF6OhoLF++HP/9739x7tw5vPfee8jNzcXQoUMBAEOGDMG4ceN09WfMmIGJEydi5cqVCAgIQEpKClJSUpCTk2NUu1IrcNB2SwU5V2ipISIiInoIoz5ZU1NTYW1t/fCLWVnh7t27RgUwcOBA3L17F5999hlSUlLQqlUrbN++XTfIODk5GTLZPznYkiVLUFRUhP/85z9614mJicGkSZOMaltKxa7aryF1XKUNhIiIyMIYldz4+PjgzJkzqF+/frnHT506BW9vb6ODGDFiBEaMGFHusd27d+s9v3btmtHXr2ou3LwJKLXdUd0aN5Q4GiIiIstiVLdU7969MXHiRBQUlB0Em5+fj5iYGDz//PMmC85S/X7+kvabAgHBnsYng0RERPRwRt25mTBhAn788Uc0bNgQI0aMQKNGjQAA58+fx+LFi6FWqzF+/HizBGpJjt/OAPxqwSqTC/gRERGZmlHJjaenJw4cOID33nsP48aNQ+nODYIgoGfPnli8eHGZBfmorCtZ2i4pLuBHRERkekZP1fH398e2bduQkZGBy5cvQxRFNGjQAK6uHBhrqNRC7aBsRy7gR0REZHIVnofs6uqKdu3amTKWGiNDpl3jxoUL+BEREZlctd5+obrKtdEmN15cwI+IiMjkmNxIoMDp/gJ+SrnEkRAREVkeJjeVTKMRUeyiHYgd4lNL4miIiIgsD5ObSnbp9g1AqQYAhDduJHE0RERElofJTSXbef6y9pt8GRrXfvTO50RERGQ8JjeV7PitTACAVSa4gB8REZEZMLmpZFdV2hlSdjmcKUVERGQOTG4qWcr9BfwcCrnGDRERkTkwualkmfcX8HPVcHViIiIic2ByU8lybBUAAC/bYokjISIiskxMbipZgaN2x4t6ztYSR0JERGSZmNxUIo1GRImrdgG/1j5uEkdDRERkmZjcVKJzN68DztoF/Lo1bihxNERERJaJyU0lij9/RftNvgwNa3tKGwwREZGFYnJTiU7czgQAWGVwAT8iIiJzYXJTia6qtF1SXMCPiIjIfJjcVKKUIu0MKcciLuBHRERkLkxuKlGmTLvGDRfwIyIiMh8mN5Uo11a7OrGXLbuliIiIzIXJTSUqcLq/gJ8LF/AjIiIyFyY3lUSt1nABPyIiokrA5KaSnL2ZDDhpZ0v1aNxI4miIiIgsF5ObSrLrwiXtN/kyBLnVljYYIiIiC8bkppKcuK0CAFhzAT8iIiKzYnJTSZLuL+BnywX8iIiIzIrJTSVJLbIBADgWcgE/IiIic2JyU0ky5No1blzBBfyIiIjMiclNJcm10SY33jbsliIiIjInJjeVpJAL+BEREVUKJjeV4MEF/Nr4uEscDRERkWVjclMJTidf+2cBv2Au4EdERGROTG4qwb7LSdpv8mQIrMUF/IiIiMyJyU0luPR3NgBAns3F+4iIiMyNyU0luKkqAgBY52kkjoSIiMjyMbmpBKn31+2zKeQ0cCIiInNjclMJMou108AVxUUSR0JERGT5mNxUgmxBu/WCg4bJDRERkbkxuakEeXJtcqOUsVuKiIjI3JjcVIICG21y42atljgSIiIiy8fkphIUK+QAAG9HK4kjISIisnxMbipBiYN2fZtAV3uJIyEiIrJ8TG7MTK3WQOOkXd8m2LOWxNEQERFZPiY3Znbpzk3ATpvctAsIkDYYIiKiGoCDQMzsrxs3td8UCwh0475SRFWZWq1GcXGx1GEQ1Vg2NjaQyZ78vguTGzM7l5IOuDpCppJBELi3FFFVJIoiUlJSkJmZKXUoRDWaTCZDYGAgbO7PMq4oJjdmdj2jAHB1hFWuKHUoRPQQpYmNh4cH7O3t+T8iRBLQaDS4ffs27ty5Az8/vyf6O2RyY2Yp+drxNtYFXOOGqCpSq9W6xMbNzU3qcIhqtNq1a+P27dsoKSmBtbV1ha/DAcVm9neRdo0buyJuvUBUFZWOsbG351INRFIr7Y5Sq5/shgCTGzNTidrM017N5IaoKmNXFJH0TPV3yOTGzHLub5rpJHAGBhERUWVgcmNmpftK1ZJzzA0RmU6XLl0gCAIEQcCJEyekDoeqoGvXrul+R1q1aiV1OJVK8uRm8eLFCAgIgJ2dHdq3b4/Dhw8/tO7Zs2fx0ksvISAgAIIgYN68eZUXaAUV2WnHbHsqJA6EiCzO8OHDcefOHTRr1kxXNnLkSLRp0wa2trZP9IGWmZmJqKgoeHt7w9bWFg0bNsS2bdvKrTt9+nQIgoDRo0dXqK3Vq1ejRYsWsLOzg4eHB6Kiosqtd/nyZTg5OcHFxaVC7Zw7dw59+/aFUqmEg4MD2rVrh+Tk5DL1RFFEr169IAgCNm/ebHQ7hYWFGD9+PPz9/WFra4uAgACsXLmy3Lrr1q2DIAjo37+/0e1MnToVYWFhsLe3L/c98fX1xZ07d/DRRx8Zfe3qTtLZUuvXr0d0dDSWLl2K9u3bY968eejZsycuXLgADw+PMvXz8vIQFBSEl19+GR9++KEEERuvWCEDoIGv0lbqUIjIwtjb28PLy6tM+bBhw/Dnn3/i1KlTFbpuUVERunfvDg8PD/zwww/w8fHB9evXy/0APXLkCL7++mu0aNGiQm3NmTMHX375JWbNmoX27dsjNzcX165dK1OvuLgYr776Kjp16oQDBw4Y3c6VK1fw9NNP480338TkyZPh7OyMs2fPws7OrkzdefPmPdHYjwEDBiA1NRUrVqxA/fr1cefOHWg0mjL1rl27ho8//hidOnWqUDtFRUV4+eWX0aFDB6xYsaLMcblcDi8vLzg6Olbo+tWZpMnNnDlzMHz4cAwdOhQAsHTpUvzyyy9YuXIlxo4dW6Z+u3bt0K5dOwAo93hVpHbSrm/TsLZS4kiIqCZYsGABAODu3bsVTm5WrlyJ9PR0HDhwQDcdN6Cc7WNycnIwaNAgLF++HF988YXR7WRkZGDChAn46aef0K1bN115eYnShAkT0LhxY3Tr1q1Cyc348ePRu3dvzJw5U1dWr169MvVOnDiBL7/8En/99Re8vb2Nbmf79u3Ys2cPrl69ilq1tPsJlvfeqdVqDBo0CJMnT8a+ffsqtIDk5MmTAWjvfJE+ybqlioqKcPToUYSHh/8TjEyG8PBwHDx40GTtFBYWQqVS6T0qyz2VCnDSjrVpVbdupbVLRBUniiJyi3IleYhi1Vjsc+vWrejQoQOioqLg6emJZs2aYdq0aWWm50ZFRaFPnz56/44bY+fOndBoNLh16xaaNGmCunXrYsCAAbhx44ZevV27duH777/H4sWLK9SORqPBL7/8goYNG6Jnz57w8PBA+/bty3Q55eXl4bXXXsPixYvLvSNmiK1bt6Jt27aYOXMmfHx80LBhQ3z88cfIz8/Xq/f555/Dw8MDb775ZoXaoUeT7M7NvXv3oFar4enpqVfu6emJ8+fPm6yd2NhYXXZb2Q5fu6r9RgOE+PpJEgMRGSevOA+OsdLcxs8ZlwMHGwdJ2n7Q1atXsWvXLgwaNAjbtm3D5cuX8f7776O4uBgxMTEAtGNFjh07hiNHjjxROxqNBtOmTcP8+fOhVCoxYcIEdO/eHadOnYKNjQ3+/vtvvPHGG/juu+/g7OxcoXbS0tKQk5OD6dOn44svvsCMGTOwfft2vPjii0hISEDnzp0BAB9++CHCwsLQr1+/J3pN+/fvh52dHTZt2oR79+7h/fffx99//41Vq1YBAPbv348VK1ZwILgZWfwKxePGjUN0dLTuuUqlgq+vb6W0feZWGqCwgZAjh90TrLRIRFSZNBoNPDw8sGzZMsjlcrRp0wa3bt3CrFmzEBMTgxs3bmDUqFHYuXNnuWNWjGmnuLgYCxYsQI8ePQAAa9euhZeXFxISEtCzZ08MHz4cr732Gp555pknagcA+vXrpxuv2apVKxw4cABLly5F586dsXXrVuzatQvHjx+vcDulbQmCgDVr1kCp1A5HmDNnDv7zn//gq6++QklJCQYPHozly5fD3d39idqih5MsuXF3d4dcLkdqaqpeeWpqaoVvB5bH1tYWtrbSDOa9kp4D+NSCPEeS5omoAuyt7ZEzTpo/WnvrqrFKsre3N6ytrSGXy3VlTZo0QUpKim5IQVpaGlq3bq07rlarsXfvXixatAiFhYV65z6qHQAIDg7WldWuXRvu7u66WUy7du3C1q1bMXv2bADabkONRgMrKyssW7YMw4YNe2w77u7usLKy0mun9DXt379f186VK1fKDJp+6aWX0KlTJ+zevfux7ZS+Jh8fH11iU9qOKIq4efOmbsB0RESE7nhp8mVlZYULFy6UOxaIjCNZcmNjY4M2bdogPj5eNwVOo9EgPj4eI0aMkCosk7qdXQIAsM7jGjdE1YUgCFWia0hKHTt2RFxcHDQaDWQy7dDMixcvwtvbGzY2NujWrRtOnz6td87QoUPRuHFjfPLJJwYlNqXtAMCFCxdQ9/64xPT0dNy7dw/+/v4AgIMHD+qN9dmyZQtmzJiBAwcOwMfHx6B2bGxs0K5dO1y4cEGv/OLFi7p2xo4di7feekvvePPmzTF37ly9RMSQ1/T9998jJydHN0vp4sWLkMlkqFu3LgRBKPPeTZgwAdnZ2Zg/f36l9SxYOkm7paKjoxEZGYm2bdsiNDQU8+bNQ25urm721JAhQ+Dj44PY2FgA2kHIiYmJuu9v3bqFEydOwNHREfXr15fsdTxMWqF2KqFtEVcnJqLKcfnyZeTk5CAlJQX5+fm6cR3BwcG6fXse57333sOiRYswatQofPDBB7h06RKmTZuGkSNHAgCcnJz01tYBAAcHB7i5uZUpf5SGDRuiX79+GDVqFJYtWwZnZ2eMGzcOjRs3RteuXQFo73o86K+//oJMJjOqHQAYM2YMBg4ciGeeeQZdu3bF9u3b8dNPP+nuyHh5eZXba+Dn54fAwECD23nttdcwZcoUDB06FJMnT8a9e/cwZswYDBs2DAqFdsGzf8deerfI2NeUnJyM9PR0JCcnQ61W637W9evXr5HTv/WIElu4cKHo5+cn2tjYiKGhoeKhQ4d0xzp37ixGRkbqniclJYkAyjw6d+5scHtZWVkiADErK8uEr6J8jb78UURCgug97xezt0VEFZOfny8mJiaK+fn5UodilM6dO4ujRo0qt7y8fyeTkpJ0dQCIq1ateuT1Dxw4ILZv3160tbUVg4KCxKlTp4olJSVGxRMZGfnYf5+zsrLEYcOGiS4uLmKtWrXEF154QUxOTn5o/VWrVolKpVKvLCEhocxrLM+KFSvE+vXri3Z2dmLLli3FzZs3P7I+AHHTpk16Zf7+/mJMTMwjzzt37pwYHh4uKhQKsW7dumJ0dLSYl5f30PqRkZFiv3799MpiYmJEf3//R7YTGRlZ7s86ISGhzLVatmz5yGtVFY/6ezTm81sQxSoy97CSqFQqKJVKZGVlVXjkvaF85v6C2yEOaHAyExdH9TdrW0RUMQUFBUhKSkJgYOATDY6tbF26dEGrVq2MXqk9KSkJDRs2RGJiIho0aGCe4O7r3LkzunbtikmTJpm1nVWrVmHatGlITEzUrctjDnl5eXBzc8Ovv/6KLl26mK0dAIiMjIQgCCZZw2bSpEnYvHlztZid9ai/R2M+vy1+tpSU8q20t4CVshKJIyEiS/TVV1/hm2++wcGDB9G8eXODztm2bRvefvttsyc2WVlZuHLlCn755ReztgNoX9O0adPMmtgAQEJCAp599lmzJzaiKGL37t26wc4VlZycjODgYBQVFZUZTG3peOfGjBy+3oO8RiJ6XUzDtrcHmLUtIqqY6nrn5tatW7qF4fz8/AweT0M1R0lJiW4rC1tb22oxWJl3bqqBYnsZADV8nPg2E5FpGTpTiGouKyurKjnZpjJIviu4JStx0M6WCnSp4aPWiYiIKhGTGzMpKi6GeH9fqeZ1akscDRERUc3B5MZMTt5IBqy1w5lCA4MkjoaIiKjmYHJjJsdv3NR+kyeDp7Py0ZWJiIjIZJjcmMmFtCwAgCxHkDgSIiKimoXJjZncUBUCAKzzatRMeyIiIskxuTGTVO3yE7DJ5wJ+RGR6Xbp0gSAIEAShWqw8S/TGG2/ofmc3b95s1raY3JhJerH2rbUrKZI4EiKyVMOHD8edO3f0NlwcOXIk2rRpA1tbW7Rq1apC1z179ixeeuklBAQEQBCEcrd4iI2NRbt27eDk5AQPDw/079+/zK7bKSkpGDx4MLy8vODg4IDWrVtj48aNRsVSUFCAN954A82bN4eVlRX69+9fps6PP/6I7t27o3bt2nB2dkaHDh2wY8cOvTpqtRoTJ05EYGAgFAoF6tWrhylTpsDYdWynTp2KsLAw2Nvb6za8fNDJkyfx6quvwtfXFwqFAk2aNMH8+fPL1FuzZg1atmwJe3t7eHt7Y9iwYfj777+NiuXHH39Ejx494ObmVm6Sm56ejg8++ACNGjWCQqGAn58fRo4ciaysLL16R44cQbdu3eDi4gJXV1f07NkTJ0+eNCoWQ35n5s+fjzt37hh13YpicmMm2dCuFuqgZnJDROZhb28PLy8vWFnpLxQ6bNgwDBw4sMLXzcvLQ1BQEKZPn17uTtkAsGfPHkRFReHQoUPYuXMniouL0aNHD+Tm5urqDBkyBBcuXMDWrVtx+vRpvPjiixgwYACOHz9ucCxqtRoKhQIjR45EeHh4uXX27t2L7t27Y9u2bTh69Ci6du2KiIgIvXZmzJiBJUuWYNGiRTh37hxmzJiBmTNnYuHChQbHAgBFRUV4+eWX8d5775V7/OjRo/Dw8MB3332Hs2fPYvz48Rg3bhwWLVqkq/PHH39gyJAhePPNN3H27Fl8//33OHz4MIYPH25ULLm5uXj66acxY8aMco/fvn0bt2/fxuzZs3HmzBmsXr0a27dvx5tvvqmrk5OTg+eeew5+fn74888/sX//fjg5OaFnz54oLi42OBZDfmeUSuVDj5mcaffzrPoqa1dw93nbRSQkiC3m/WDWdojoyfx7F2KNRiOWlORI8tBoNAbH/bBdwUuZaidof39/ce7cuY+tl5aWJgIQ9+zZoytzcHAQv/32W716tWrVEpcvX16hWMrbPfthgoODxcmTJ+ue9+nTRxw2bJhenRdffFEcNGhQhWIpb3fyh3n//ffFrl276p7PmjVLDAoK0quzYMEC0cfHp0KxJCUliQDE48ePP7buhg0bRBsbG7G4uFgURVE8cuSICEBvJ/ZTp06JAMRLly5VKJ7H/c6gnN3WS5lqV3DuC2AmBff3eallrZE4EiIyhkaTh337pFlVvFOnHMjlDpK0/aRKuzpq1aqlKwsLC8P69evRp08fuLi4YMOGDSgoKDD7xpMajQbZ2dllYlm2bBkuXryIhg0b4uTJk9i/fz/mzJlj1lgA7XvzYCwdOnTAp59+im3btqFXr15IS0vDDz/8gN69e1dKLM7Ozrq7fY0aNYKbmxtWrFiBTz/9FGq1GitWrECTJk0QEBBg9njMhcmNmRTZyQFo4GXPqeBEZNk0Gg1Gjx6Njh076o3/2bBhAwYOHAg3NzdYWVnB3t4emzZtMvt+R7Nnz0ZOTg4GDPhnw+KxY8dCpVKhcePGkMvlUKvVmDp1KgYNGmTWWA4cOID169fr7Y7esWNHrFmzBgMHDkRBQQFKSkoQERGBxYsXmzWWe/fuYcqUKXj77bd1ZU5OTti9ezf69++PKVOmAAAaNGiAHTt2lOnurE6qb+RVXImDDIAG/i4KqUMhIiPIZPbo1ClHsraro6ioKJw5cwb79+/XK584cSIyMzPx+++/w93dHZs3b8aAAQOwb98+NG/e3CyxxMXFYfLkydiyZQs8PDx05Rs2bMCaNWsQFxeHpk2b4sSJExg9ejTq1KmDyMhIs8Ry5swZ9OvXDzExMejRo4euPDExEaNGjcJnn32Gnj174s6dOxgzZgzeffddrFixwiyxqFQq9OnTB8HBwZg0aZKuPD8/H2+++SY6duyItWvXQq1WY/bs2ejTpw+OHDkChaJ6foYxuTETjaO2O6pxbRdpAyEiowiCUG27hqQwYsQI/Pzzz9i7dy/q1q2rK79y5QoWLVqEM2fOoGnTpgCAli1bYt++fVi8eDGWLl1q8ljWrVuHt956C99//32ZwcdjxozB2LFj8corrwAAmjdvjuvXryM2NtYsyU1iYiK6deuGt99+GxMmTNA7Fhsbi44dO2LMmDEAgBYtWsDBwQGdOnXCF198AW9vb5PGkp2djeeeew5OTk7YtGkTrK2tdcfi4uJw7do1HDx4EDKZTFfm6uqKLVu26N6v6oazpczg+r00wF6b3LT295U4GiIi0xNFESNGjMCmTZuwa9cuBAYG6h3Py8sDAN0HZim5XA6NxvRjEdeuXYuhQ4di7dq16NOnT5njeXl5lRbL2bNn0bVrV0RGRmLq1KkGxwLA6Knpj6NSqdCjRw/Y2Nhg69atsLOzKzcWQfhnCEXpc3O8N5WFd27M4Mj1a9pvigUEe/lIGgsR1SyXL19GTk4OUlJSkJ+fr1v7JDg4GDb3Jzo8TlFRERITE3Xf37p1CydOnICjo6NuvExUVBTi4uKwZcsWODk5ISUlBYB2uq9CoUDjxo1Rv359vPPOO5g9ezbc3NywefNm7Ny5Ez///LNRrykxMRFFRUVIT09Hdna27jWVruMTFxeHyMhIzJ8/H+3bt9fFolAooFRq9/aLiIjA1KlT4efnh6ZNm+L48eOYM2cOhg0bZlQsycnJSE9PR3JyMtRqtS6W+vXrw9HREWfOnMGzzz6Lnj17Ijo6WheLXC5H7dq1dbEMHz4cS5Ys0XVLjR49GqGhoahTp47BsZTGcfv2bQDQrTPk5eUFLy8vXWKTl5eH7777DiqVCiqVCgBQu3ZtyOVydO/eHWPGjEFUVBQ++OADaDQaTJ8+HVZWVujatavBsRjyO1OpDJvYZTkqYyr45K3bRCQkiMLGvWZrg4hM41FTT6uyh00F79y5swigzCMpKUlXB4C4atWqh167dGrxvx+dO3fWu0Z5jweve/HiRfHFF18UPTw8RHt7e7FFixZlpoZ37txZjIyMfORr9ff3L7etx73mB6+rUqnEUaNGiX5+fqKdnZ0YFBQkjh8/XiwsLNTViYmJEf39/R8ZS2RkZLltJSQk6K5R3vF/X3fBggVicHCwqFAoRG9vb3HQoEHizZs3dccTEhLK/Nz+bdWqVeW2FRMTo3eNx/0+/Pbbb2LHjh1FpVIpurq6is8++6x48OBBvbZM8Tvz4LXMPRVcuN9QjaFSqaBUKnXT4czhjW834r9+brC+IUfR4E5maYOITKOgoABJSUkIDAwsc8u+KuvSpQtatWpV7kqwj5KUlISGDRsiMTERDRo0ME9wRvD398fkyZPxxhtvSB0KIiMjIQgCVq9eLXUoWLVqFaZNm4bExES9MTJSMPXvjCAI2LRpU7mrTT/q79GYz2+OuTGDO3lqAIB1vlriSIjIkn311VdwdHTE6dOnDT5n27ZtePvtt6tEYnP27FkolUoMGTJE6lAgiiJ2796tmw4ttW3btmHatGmSJzalsZjid+bdd9+Fo2PlrCHFOzdm0Gb+DzjW0h21zhTh7xE9Hn8CEUmmut65uXXrFvLztTv0+vn5GTyehkgqaWlpujE/3t7ecHAoOyvRVHduOKDYDFRqbaatUBu+LwcRkTF8fDhZgaoXDw8PvbWHzIndUmaQI9P+H5STyE0ziYiIKhuTGzPIt9YmNy5WJRJHQkREVPMwuTGDIlttb59H9em+JyIishhMbsygWKF9W+s6cYAfERFRZWNyYwZqJ+3X+m5O0gZCRERUAzG5MbGcgnyIjtr1bVrUqZxR4URERPQPJjcm9te1JN272i4wSNpgiMhidenSBYIgQBAE3f5GROawevVq3e/a6NGjpQ7HIExuTOzETe0maciWw1lhL20wRGTRhg8fjjt37qBZs2a6spEjR6JNmzawtbXVbSxprLNnz+Kll15CQEAABEEod4uH2NhYtGvXDk5OTvDw8ED//v11GzeWSklJweDBg+Hl5QUHBwe0bt0aGzduNCqWgoICvPHGG2jevDmsrKzKXbL/xx9/RPfu3VG7dm04OzujQ4cO2LFjh14dtVqNiRMnIjAwEAqFAvXq1cOUKVMqtAv3L7/8gvbt20OhUMDV1bXcmADg77//Rt26dSEIAjIzM41u59atW3j99dfh5uYGhUKB5s2b46+//iq37rvvvvvQn9WjGPL+Dhw4EHfu3EGHDh2Mfg1SYXJjYpf/1q6+KM+ROBAisnj29vbw8vKClZX+eqzDhg3DwIEDK3zdvLw8BAUFYfr06fDy8iq3zp49exAVFYVDhw5h586dKC4uRo8ePZCbm6urM2TIEFy4cAFbt27F6dOn8eKLL2LAgAE4fvy4wbGo1WooFAqMHDkS4eHh5dbZu3cvunfvjm3btuHo0aPo2rUrIiIi9NqZMWMGlixZgkWLFuHcuXOYMWMGZs6ciYULFxocCwBs3LgRgwcPxtChQ3Hy5En88ccfeO2118qt++abb6JFixZGXb9URkYGOnbsCGtra/z6669ITEzEl19+CVdX1zJ1N23ahEOHDhm1o3gpQ95fhUIBLy+varUKNlcoNrGbqmLAE7DO00gdChFVgCgCeXnStG1vDwjCk11jwYIFAIC7d+/i1KlTFbpGu3bt0K5dOwDA2LFjy62zfft2veerV6+Gh4cHjh49imeeeQYAcODAASxZsgShoaEAgAkTJmDu3Lk4evQoQkJCDIrFwcEBS5YsAQD88ccf5d4B+ffdimnTpmHLli346aefdO0cOHAA/fr1Q58+fQAAAQEBWLt2LQ4fPmxQHABQUlKCUaNGYdasWXjzzTd15cHBwWXqLlmyBJmZmfjss8/w66+/GtxGqRkzZsDX1xerVq3SlQUGBpapd+vWLXzwwQfYsWOH7rUZw5D3tzrinRsTu1ug/WpTyAX8iKqjvDzA0VGah1RJlSlkZWUBAGrVqqUrCwsLw/r165Geng6NRoN169ahoKAAXbp0MWssGo0G2dnZZWKJj4/HxYsXAQAnT57E/v370atXL4Ove+zYMdy6dQsymQwhISHw9vZGr169cObMGb16iYmJ+Pzzz/Htt99CJqvYx+zWrVvRtm1bvPzyy/Dw8EBISAiWL19e5nUOHjwYY8aMQdOmTSvUjqVicmNiGSXam2GKYm69QEQ1g0ajwejRo9GxY0e98T8bNmxAcXEx3NzcYGtri3feeQebNm1C/fr1zRrP7NmzkZOTgwEDBujKxo4di1deeQWNGzeGtbU1QkJCMHr0aAwaNMjg6169ehUAMGnSJEyYMAE///wzXF1d0aVLF6SnpwMACgsL8eqrr2LWrFnw8/Or8Gu4evUqlixZggYNGmDHjh147733MHLkSPz3v//V1ZkxYwasrKwwcuTICrdjqdgtZWLZ0G6a6aBhckNUHdnbAzkSjZmzr6ZzEKKionDmzBns379fr3zixInIzMzE77//Dnd3d2zevBkDBgzAvn370Lx5c7PEEhcXh8mTJ2PLli16mzRu2LABa9asQVxcHJo2bYoTJ05g9OjRqFOnDiIjIw26tkajHW4wfvx4vPTSSwCAVatWoW7duvj+++/xzjvvYNy4cWjSpAlef/31J3odGo0Gbdu2xbRp0wAAISEhOHPmDJYuXYrIyEgcPXoU8+fPx7FjxyA8aV+mBWJyY2J5VtoBV0oZu6WIqiNBABwcpI6i+hgxYgR+/vln7N27F3Xr1tWVX7lyBYsWLcKZM2d0XSYtW7bEvn37sHjxYixdutTksaxbtw5vvfUWvv/++zKDY8eMGaO7ewMAzZs3x/Xr1xEbG2twcuPt7Q1Af4yNra0tgoKCkJycDADYtWsXTp8+jR9++AEAdLOx3N3dMX78eEyePNngtv49lqdJkya62Wb79u1DWlqa3t0htVqNjz76CPPmzcO1a9cMasdSMbkxscL7o8ndbTigmIgslyiK+OCDD7Bp0ybs3r27zGDXvPsDiP495kQul+vugJjS2rVrMWzYMKxbt67cgbV5eXlPHEvpFPsLFy7g6aefBgAUFxfj2rVr8Pf3B6CdTZWfn68758iRIxg2bBj27duHevXqGdxWx44dy0ytv3jxoq6dwYMHl0ngevbsqZvJVdMxuTGxIoUcgBpeDhzORESV7/Lly8jJyUFKSgry8/N1C/wFBwcbPJW3qKgIiYmJuu9v3bqFEydOwNHRUTdeJioqCnFxcdiyZQucnJyQkqJd40upVEKhUKBx48aoX78+3nnnHcyePRtubm7YvHkzdu7ciZ9//tmo15SYmIiioiKkp6cjOztb95pK1/GJi4tDZGQk5s+fj/bt2+tiUSgUUCqVAICIiAhMnToVfn5+aNq0KY4fP445c+Zg2LBhBsfh7OyMd999FzExMfD19YW/vz9mzZoFAHj55ZcBoEwCc+/ePQDauy4uLi4Gt/Xhhx8iLCwM06ZNw4ABA3D48GEsW7YMy5YtAwC4ubnBzc1N7xxra2t4eXmhUaNGBrcDPP79rZbEGiYrK0sEIGZlZZnl+rINe0UkJIifbv7ZLNcnItPKz88XExMTxfz8fKlDMUrnzp3FUaNGlVsOoMwjKSlJVweAuGrVqodeOykpqdxrdO7cWe8a5T0evO7FixfFF198UfTw8BDt7e3FFi1aiN9++22ZeCMjIx/5Wv39/ctt63Gv+cHrqlQqcdSoUaKfn59oZ2cnBgUFiePHjxcLCwt1dWJiYkR/f/9HxlJUVCR+9NFHooeHh+jk5CSGh4eLZ86ceWj9hIQEEYCYkZGhKyt9fxMSEh7Z1k8//SQ2a9ZMtLW1FRs3biwuW7bskfX9/f3FuXPn6pWZ4v198Frl/c6Z0qP+Ho35/OadGxPTOGj7V+u7u0gbCBHVSLt3737k8aSkJFhZWaFjx44PrRMQEPDYlXsfdxwAGjRo8NgViZOSkvDGG288ss7jxo887jUDgJOTE+bNm/fIFXyTkpIeO03d2toas2fPxuzZsx/bJqDdJuPf71VSUhJcXFzQsmXLR577/PPP4/nnnzeoHaD898kU7291xL4TE0rPUQH22v7bYG9PiaMhIkv31VdfwdHREadPnzb4nG3btuHtt99GgwYNzBiZYc6ePQulUokhQ4ZIHQpEUcTu3bsxZcoUs7e1bds2fPrpp+WuNmxKpnp/16xZA0dHR+zbt89EkZmfIBqSflsQlUoFpVKJrKwsODs7m/TaCefO4tnUuwCA3A4dYG9ra9LrE5HpFRQUICkpCYGBgbCzs5M6HIPdunVLN3DVz8+vWi2NT9VLdnY2UlNTAQAuLi5wd3c3W1uP+ns05vOb3VImdO5+YoMcORMbIjIrHx8fqUOgGsLJyQlOTk5Sh2EUdkuZ0NX0bACAPPcxFYmIiMhsmNyY0G1VIQBAnl+jevqIiIiqFCY3JnQ3XzuY2LpQLXEkRERENReTGxPKLNa+nbZFxRJHQkREVHMxuTEhlVoOALBTM7khIiKSCpMbE8oVtDuC24vcNJOIiEgqTG5MqECuXWfCScY7N0REVcG1a9cgCIJuvySqGZjcmFChtfbOjas1dwQnIvNLSUnBBx98gKCgINja2sLX1xcRERGIj4+XOrQnwoSEnhQX8TOhYls5AA3cFYLUoRCRhbt27Ro6duwIFxcXzJo1C82bN0dxcTF27NiBqKgonD9/XuoQiSTDOzcmVHI/qfFx4urERNWVKIrIVasleRizG877778PQRBw+PBhvPTSS2jYsCGaNm2K6OhoHDp06JHnfvPNN2jSpAns7OzQuHFjfPXVV7pjw4YNQ4sWLVBYqF23q6ioCCEhIbr9iUrvqqxbtw5hYWGws7NDs2bNsGfPHr02zpw5g169esHR0RGenp4YPHgw7t27pzuu0Wgwc+ZM1K9fH7a2tvDz88PUqVMBAIGBgQCAkJAQCIKgt5nlo2IHgMOHDyMkJAR2dnZo27Ytjh8/bvB7SpaDd25MSGOv/ern4ihtIERUYXkaDRwl2iAwp1MnOMjlj62Xnp6O7du3Y+rUqXBwcChz3MXF5aHnrlmzBp999hkWLVqEkJAQHD9+HMOHD4eDgwMiIyOxYMECtGzZEmPHjsXcuXMxfvx4ZGZmYtGiRXrXGTNmDObNm4fg4GDMmTMHERERSEpKgpubGzIzM/Hss8/irbfewty5c5Gfn49PPvkEAwYMwK5duwAA48aNw/LlyzF37lw8/fTTuHPnju5u0+HDhxEaGorff/8dTZs21e2b9bjYc3Jy8Pzzz6N79+747rvvkJSUhFGjRhn69pMFqRLJzeLFizFr1iykpKSgZcuWWLhwIUJDQx9a//vvv8fEiRNx7do1NGjQADNmzEDv3r0rMeKyCouLITpoF+9r7Gm+TcWIiC5fvgxRFNG4cWOjz42JicGXX36JF198EYD2LkliYiK+/vprREZGwtHREd999x06d+4MJycnzJs3DwkJCWU2KhwxYgReeuklAMCSJUuwfft2rFixAv/3f/+nSz6mTZumq79y5Ur4+vri4sWL8Pb2xvz587Fo0SJERkYCAOrVq4enn34aAFC7dm0AgJubG7y8vAyOPS4uDhqNBitWrICdnR2aNm2Kmzdv4r333jP6faLqTfLkZv369YiOjsbSpUvRvn17zJs3Dz179sSFCxfg4eFRpv6BAwfw6quvIjY2Fs8//zzi4uLQv39/HDt2DM2aNZPgFWhduHMLuP8/XC1860oWBxE9GXuZDDmdOknWtiGM6b56UG5uLq5cuYI333wTw4cP15WXlJRAqVTqnnfo0AEff/wxpkyZgk8++USXdDyoQ4cOuu+trKzQtm1bnDt3DgBw8uRJJCQkwNGx7F3sK1euIDMzE4WFhejWrZtJYz937hxatGiht5v0g3FSzSF5cjNnzhwMHz4cQ4cOBQAsXboUv/zyC1auXImxY8eWqT9//nw899xzGDNmDABgypQp2LlzJxYtWoSlS5dWauwPOn3njvabAhnquNSSLA4iejKCIBjUNSSlBg0aQBAEowcN5+TkAACWL1+O9u3b6x2TP/CaNRoN/vjjD8jlcly+fNno+HJychAREYEZM2aUOebt7Y2rV69W6JrA42MnAiQeUFxUVISjR48iPDxcVyaTyRAeHo6DBw+We87Bgwf16gNAz549H1q/sLAQKpVK72EOV+9lAgCEXM6UIiLzqlWrFnr27InFixcjNze3zPHMzMxyz/P09ESdOnVw9epV1K9fX+9ROogXAGbNmoXz589jz5492L59O1atWlXmWg8OWi4pKcHRo0fRpEkTAEDr1q1x9uxZBAQElGnHwcEBDRo0gEKheOiU9dIxNmr1P/v0GRJ7kyZNcOrUKRQUFJQbJ9UckiY39+7dg1qthqenp165p6cnUlJSyj0nJSXFqPqxsbFQKpW6h6+vr2mC/5fM/GJAJYdV2X9niIhMbvHixVCr1QgNDcXGjRtx6dIlnDt3DgsWLHhkV8zkyZMRGxuLBQsW4OLFizh9+jRWrVqFOXPmAACOHz+Ozz77DN988w06duyIOXPmYNSoUWXutixevBibNm3C+fPnERUVhYyMDAwbNgwAEBUVhfT0dLz66qs4cuQIrly5gh07dmDo0KFQq9Wws7PDJ598gv/7v//Dt99+iytXruDQoUNYsWIFAMDDwwMKhQLbt29HamoqsrKyDIr9tddegyAIGD58OBITE7Ft2zbMnj3b5O89VQOihG7duiUCEA8cOKBXPmbMGDE0NLTcc6ytrcW4uDi9ssWLF4seHh7l1i8oKBCzsrJ0jxs3bogAxKysLNO8iH8pKioxy3WJyDzy8/PFxMREMT8/X+pQjHb79m0xKipK9Pf3F21sbEQfHx+xb9++YkJCwiPPW7NmjdiqVSvRxsZGdHV1FZ955hnxxx9/FPPz88Xg4GDx7bff1qvft29fMSwsTCwpKRGTkpJEAGJcXJwYGhoq2tjYiMHBweKuXbv0zrl48aL4wgsviC4uLqJCoRAbN24sjh49WtRoNKIoiqJarRa/+OIL0d/fX7S2thb9/PzEadOm6c5fvny56OvrK8pkMrFz586Pjb3UwYMHxZYtW4o2NjZiq1atxI0bN4oAxOPHj1fsTaZK9ai/x6ysLIM/vwVRrODINBMoKiqCvb09fvjhB/Tv319XHhkZiczMTGzZsqXMOX5+foiOjsbo0aN1ZTExMdi8eTNOnjz52DZVKhWUSiWysrLKjP4nopqnoKAASUlJCAwM1BuISuW7du0aAgMDcfz4cbRq1UrqcMjCPOrv0ZjPb0m7pWxsbNCmTRu9fleNRoP4+PiH3lbt0KFDmX7anTt3ckQ8ERERAagCs6Wio6MRGRmJtm3bIjQ0FPPmzUNubq5u9tSQIUPg4+OD2NhYAMCoUaPQuXNnfPnll+jTpw/WrVuHv/76C8uWLZPyZRAREVEVIXlyM3DgQNy9exefffYZUlJS0KpVK2zfvl03aDg5ORmyB9Z+CAsLQ1xcHCZMmIBPP/0UDRo0wObNmyVd44aIqKYICAio8Do7RJVF0jE3UuCYGyJ6EMfcEFUdFjHmhoioqqhh/59HVCWZ6u+QyQ0R1WjW1tYAgLy8PIkjIaKioiIAT77qtORjboiIpCSXy+Hi4oK0tDQAgL29PQSBK40TVTaNRoO7d+/C3t4eVlZPlp4wuSGiGq905+nSBIeIpCGTyeDn5/fE/4PB5IaIajxBEODt7Q0PDw8UFxdLHQ5RjWVjY6M3Q7qimNwQEd0nl8u5wzSRBeCAYiIiIrIoTG6IiIjIojC5ISIiIotS48bclC4QpFKpJI6EiIiIDFX6uW3IQn81LrnJzs4GAPj6+kocCRERERkrOzsbSqXykXVq3N5SGo0Gt2/fhpOTk8kX6lKpVPD19cWNGze4b5UZ8X2uHHyfKwff58rD97pymOt9FkUR2dnZqFOnzmOni9e4OzcymQx169Y1axvOzs78w6kEfJ8rB9/nysH3ufLwva4c5nifH3fHphQHFBMREZFFYXJDREREFoXJjQnZ2toiJiYGtra2Uodi0fg+Vw6+z5WD73Pl4XtdOarC+1zjBhQTERGRZeOdGyIiIrIoTG6IiIjIojC5ISIiIovC5IaIiIgsCpMbE1m8eDECAgJgZ2eH9u3b4/Dhw1KHZHFiY2PRrl07ODk5wcPDA/3798eFCxekDsuiTZ8+HYIgYPTo0VKHYpFu3bqF119/HW5ublAoFGjevDn++usvqcOyKGq1GhMnTkRgYCAUCgXq1auHKVOmGLQ/ET3c3r17ERERgTp16kAQBGzevFnvuCiK+Oyzz+Dt7Q2FQoHw8HBcunSp0uJjcmMC69evR3R0NGJiYnDs2DG0bNkSPXv2RFpamtShWZQ9e/YgKioKhw4dws6dO1FcXIwePXogNzdX6tAs0pEjR/D111+jRYsWUodikTIyMtCxY0dYW1vj119/RWJiIr788ku4urpKHZpFmTFjBpYsWYJFixbh3LlzmDFjBmbOnImFCxdKHVq1lpubi5YtW2Lx4sXlHp85cyYWLFiApUuX4s8//4SDgwN69uyJgoKCyglQpCcWGhoqRkVF6Z6r1WqxTp06YmxsrIRRWb60tDQRgLhnzx6pQ7E42dnZYoMGDcSdO3eKnTt3FkeNGiV1SBbnk08+EZ9++mmpw7B4ffr0EYcNG6ZX9uKLL4qDBg2SKCLLA0DctGmT7rlGoxG9vLzEWbNm6coyMzNFW1tbce3atZUSE+/cPKGioiIcPXoU4eHhujKZTIbw8HAcPHhQwsgsX1ZWFgCgVq1aEkdieaKiotCnTx+932syra1bt6Jt27Z4+eWX4eHhgZCQECxfvlzqsCxOWFgY4uPjcfHiRQDAyZMnsX//fvTq1UviyCxXUlISUlJS9P79UCqVaN++faV9Lta4jTNN7d69e1Cr1fD09NQr9/T0xPnz5yWKyvJpNBqMHj0aHTt2RLNmzaQOx6KsW7cOx44dw5EjR6QOxaJdvXoVS5YsQXR0ND799FMcOXIEI0eOhI2NDSIjI6UOz2KMHTsWKpUKjRs3hlwuh1qtxtSpUzFo0CCpQ7NYKSkpAFDu52LpMXNjckPVUlRUFM6cOYP9+/dLHYpFuXHjBkaNGoWdO3fCzs5O6nAsmkajQdu2bTFt2jQAQEhICM6cOYOlS5cyuTGhDRs2YM2aNYiLi0PTpk1x4sQJjB49GnXq1OH7bMHYLfWE3N3dIZfLkZqaqleempoKLy8viaKybCNGjMDPP/+MhIQE1K1bV+pwLMrRo0eRlpaG1q1bw8rKClZWVtizZw8WLFgAKysrqNVqqUO0GN7e3ggODtYra9KkCZKTkyWKyDKNGTMGY8eOxSuvvILmzZtj8ODB+PDDDxEbGyt1aBar9LNPys9FJjdPyMbGBm3atEF8fLyuTKPRID4+Hh06dJAwMssjiiJGjBiBTZs2YdeuXQgMDJQ6JIvTrVs3nD59GidOnNA92rZti0GDBuHEiROQy+VSh2gxOnbsWGYpg4sXL8Lf31+iiCxTXl4eZDL9jzq5XA6NRiNRRJYvMDAQXl5eep+LKpUKf/75Z6V9LrJbygSio6MRGRmJtm3bIjQ0FPPmzUNubi6GDh0qdWgWJSoqCnFxcdiyZQucnJx0fbdKpRIKhULi6CyDk5NTmTFMDg4OcHNz49gmE/vwww8RFhaGadOmYcCAATh8+DCWLVuGZcuWSR2aRYmIiMDUqVPh5+eHpk2b4vjx45gzZw6GDRsmdWjVWk5ODi5fvqx7npSUhBMnTqBWrVrw8/PD6NGj8cUXX6BBgwYIDAzExIkTUadOHfTv379yAqyUOVk1wMKFC0U/Pz/RxsZGDA0NFQ8dOiR1SBYHQLmPVatWSR2aReNUcPP56aefxGbNmom2trZi48aNxWXLlkkdksVRqVTiqFGjRD8/P9HOzk4MCgoSx48fLxYWFkodWrWWkJBQ7r/HkZGRoihqp4NPnDhR9PT0FG1tbcVu3bqJFy5cqLT4BFHkMo1ERERkOTjmhoiIiCwKkxsiIiKyKExuiIiIyKIwuSEiIiKLwuSGiIiILAqTGyIiIrIoTG6IiIjIojC5ISIiIovC5IaIqoTdu3dDEARkZmY+sl5AQADmzZtXKTEZ6plnnkFcXJxBdZ966ils3LjRzBER1WxMboioSggLC8OdO3egVCoBAKtXr4aLi0uZekeOHMHbb79t1lge1nZ5tm7ditTUVLzyyisG1Z8wYQLGjh3LjRuJzIjJDRFVCTY2NvDy8oIgCI+sV7t2bdjb21dSVI+3YMECDB06tMzO0w/Tq1cvZGdn49dffzVzZEQ1F5MbIjJIly5dMGLECIwYMQJKpRLu7u6YOHEiHtyeLiMjA0OGDIGrqyvs7e3Rq1cvXLp0SXf8+vXriIiIgKurKxwcHNC0aVNs27YNgH631O7duzF06FBkZWVBEAQIgoBJkyYBKNstlZycjH79+sHR0RHOzs4YMGAAUlNTdccnTZqEVq1a4X//+x8CAgKgVCrxyiuvIDs7u9zX+ai2/+3u3bvYtWsXIiIidGWiKGLSpEnw8/ODra0t6tSpg5EjR+qOy+Vy9O7dG+vWrTP4vSci4zC5ISKD/fe//4WVlRUOHz6M+fPnY86cOfjmm290x9944w389ddf2Lp1Kw4ePAhRFNG7d28UFxcDAKKiolBYWIi9e/fi9OnTmDFjBhwdHcu0ExYWhnnz5sHZ2Rl37tzBnTt38PHHH5epp9Fo0K9fP6Snp2PPnj3YuXMnrl69ioEDB+rVu3LlCjZv3oyff/4ZP//8M/bs2YPp06eX+xoNbRsA9u/fD3t7ezRp0kRXtnHjRsydOxdff/01Ll26hM2bN6N58+Z654WGhmLfvn0PeZeJ6ElZSR0AEVUfvr6+mDt3LgRBQKNGjXD69GnMnTsXw4cPx6VLl7B161b88ccfCAsLAwCsWbMGvr6+2Lx5M15++WUkJyfjpZde0n3YBwUFlduOjY0NlEolBEGAl5fXQ+OJj4/H6dOnkZSUBF9fXwDAt99+i6ZNm+LIkSNo164dAG0StHr1ajg5OQEABg8ejPj4eEydOrXCbQPaO1Genp56XVLJycnw8vJCeHg4rK2t4efnh9DQUL3z6tSpgxs3bkCj0RjcnUVEhuNfFREZ7KmnntIbE9OhQwdcunQJarUa586dg5WVFdq3b6877ubmhkaNGuHcuXMAgJEjR+KLL75Ax44dERMTg1OnTj1RPOfOnYOvr68usQGA4OBguLi46NoEtF1ZpYkNAHh7eyMtLe2J2gaA/Px82NnZ6ZW9/PLLyM/PR1BQEIYPH45NmzahpKREr45CoYBGo0FhYeETx0BEZTG5IaJK89Zbb+Hq1asYPHgwTp8+jbZt22LhwoVmb9fa2lrvuSAIJpmt5O7ujoyMDL0yX19fXLhwAV999RUUCgXef/99PPPMM7quOQBIT0+Hg4MDFArFE8dARGUxuSEig/355596zw8dOoQGDRpALpejSZMmKCkp0avz999/48KFCwgODtaV+fr64t1338WPP/6Ijz76CMuXLy+3LRsbG6jV6kfG06RJE9y4cQM3btzQlSUmJiIzM1OvTWMZ0jYAhISEICUlpUyCo1AoEBERgQULFmD37t04ePAgTp8+rTt+5swZhISEVDg+Ino0JjdEZLDk5GRER0fjwoULWLt2LRYuXIhRo0YBABo0aIB+/fph+PDh2L9/P06ePInXX38dPj4+6NevHwBg9OjR2LFjB5KSknDs2DEkJCToDcZ9UEBAAHJychAfH4979+4hLy+vTJ3w8HA0b94cgwYNwrFjx3D48GEMGTIEnTt3Rtu2bSv8Og1pG9AmN+7u7vjjjz90ZatXr8aKFStw5swZXL16Fd999x0UCgX8/f11dfbt24cePXpUOD4iejQmN0RksCFDhiA/Px+hoaGIiorCqFGj9BbUW7VqFdq0aYPnn38eHTp0gCiK2LZtm65bSK1WIyoqCk2aNMFzzz2Hhg0b4quvviq3rbCwMLz77rsYOHAgateujZkzZ5apIwgCtmzZAldXVzzzzDMIDw9HUFAQ1q9f/0Sv05C2Ae207qFDh2LNmjW6MhcXFyxfvhwdO3ZEixYt8Pvvv+Onn36Cm5sbAODWrVs4cOAAhg4d+kQxEtHDCeKDi1QQET1Ely5d0KpVqyq39YHUUlJS0LRpUxw7dkzv7szDfPLJJ8jIyMCyZcsqITqimol3boiInoCXlxdWrFiB5ORkg+p7eHhgypQpZo6KqGbjOjdERE+of//+Btf96KOPzBcIEQFgtxQRERFZGHZLERERkUVhckNEREQWhckNERERWRQmN0RERGRRmNwQERGRRWFyQ0RERBaFyQ0RERFZFCY3REREZFH+H1xUWCcpzy1ZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN simpler with ODE"
      ],
      "metadata": {
        "id": "0e92_S5jp333"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,1],[1,128,128,128,1],[1,256,256,256,1],[1,64,64,64,64,1],[1,256,256,256,256,1],[1,128,128,128,128,1],[1,128,128,64,64,1],[1,256,128,64,32,1]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = ReactionPINN_ODESimpler(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ceg3yVyTp9KX",
        "outputId": "b49b494b-f838-476f-c361-07e9d7bae972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.012e+00, Loss_0: 3.923e-04, Loss_r: 1.012e+00, Time: 0.92, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.520e-01, Loss_0: 3.809e-02, Loss_r: 2.139e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.268e-01, Loss_0: 2.366e-02, Loss_r: 1.031e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.306e-01, Loss_0: 2.841e-02, Loss_r: 1.022e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.214e-01, Loss_0: 4.872e-02, Loss_r: 7.266e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.154e-01, Loss_0: 4.835e-02, Loss_r: 6.703e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.154e-01, Loss_0: 5.129e-02, Loss_r: 6.410e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.153e-01, Loss_0: 5.592e-02, Loss_r: 5.939e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.151e-01, Loss_0: 5.385e-02, Loss_r: 6.120e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.149e-01, Loss_0: 5.443e-02, Loss_r: 6.050e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.149e-01, Loss_0: 5.377e-02, Loss_r: 6.109e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.148e-01, Loss_0: 5.324e-02, Loss_r: 6.155e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.147e-01, Loss_0: 5.326e-02, Loss_r: 6.146e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.146e-01, Loss_0: 5.304e-02, Loss_r: 6.159e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.145e-01, Loss_0: 5.314e-02, Loss_r: 6.140e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.144e-01, Loss_0: 5.307e-02, Loss_r: 6.137e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.143e-01, Loss_0: 5.308e-02, Loss_r: 6.125e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.142e-01, Loss_0: 5.301e-02, Loss_r: 6.120e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.141e-01, Loss_0: 5.295e-02, Loss_r: 6.112e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.139e-01, Loss_0: 5.287e-02, Loss_r: 6.104e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.137e-01, Loss_0: 5.279e-02, Loss_r: 6.094e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.135e-01, Loss_0: 5.270e-02, Loss_r: 6.082e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.133e-01, Loss_0: 5.260e-02, Loss_r: 6.067e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.130e-01, Loss_0: 5.249e-02, Loss_r: 6.050e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.127e-01, Loss_0: 5.236e-02, Loss_r: 6.029e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.122e-01, Loss_0: 5.221e-02, Loss_r: 6.003e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.118e-01, Loss_0: 5.204e-02, Loss_r: 5.972e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.111e-01, Loss_0: 5.182e-02, Loss_r: 5.932e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.104e-01, Loss_0: 5.157e-02, Loss_r: 5.881e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.094e-01, Loss_0: 5.124e-02, Loss_r: 5.815e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.081e-01, Loss_0: 5.083e-02, Loss_r: 5.727e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.064e-01, Loss_0: 5.029e-02, Loss_r: 5.608e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.040e-01, Loss_0: 4.957e-02, Loss_r: 5.442e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.007e-01, Loss_0: 4.857e-02, Loss_r: 5.211e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 9.611e-02, Loss_0: 4.714e-02, Loss_r: 4.897e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 9.000e-02, Loss_0: 4.497e-02, Loss_r: 4.503e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 8.232e-02, Loss_0: 4.150e-02, Loss_r: 4.082e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 370, Loss: 7.339e-02, Loss_0: 3.608e-02, Loss_r: 3.731e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.391e-02, Loss_0: 2.881e-02, Loss_r: 3.510e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.508e-02, Loss_0: 2.125e-02, Loss_r: 3.383e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.817e-02, Loss_0: 1.532e-02, Loss_r: 3.284e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 4.337e-02, Loss_0: 1.148e-02, Loss_r: 3.190e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.948e-02, Loss_0: 9.141e-03, Loss_r: 3.034e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.545e-02, Loss_0: 7.808e-03, Loss_r: 2.765e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.131e-02, Loss_0: 6.791e-03, Loss_r: 2.452e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.735e-02, Loss_0: 5.770e-03, Loss_r: 2.158e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.359e-02, Loss_0: 4.657e-03, Loss_r: 1.894e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 2.009e-02, Loss_0: 3.601e-03, Loss_r: 1.649e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.700e-02, Loss_0: 2.748e-03, Loss_r: 1.425e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.442e-02, Loss_0: 2.107e-03, Loss_r: 1.231e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.235e-02, Loss_0: 1.618e-03, Loss_r: 1.073e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.073e-02, Loss_0: 1.247e-03, Loss_r: 9.487e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 9.468e-03, Loss_0: 9.739e-04, Loss_r: 8.494e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 8.447e-03, Loss_0: 7.740e-04, Loss_r: 7.673e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 7.582e-03, Loss_0: 6.247e-04, Loss_r: 6.957e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 6.816e-03, Loss_0: 5.116e-04, Loss_r: 6.304e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 6.121e-03, Loss_0: 4.264e-04, Loss_r: 5.695e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.682e-03, Loss_0: 3.286e-04, Loss_r: 5.353e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.021e-03, Loss_0: 2.825e-04, Loss_r: 4.738e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 4.444e-03, Loss_0: 2.562e-04, Loss_r: 4.188e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.987e-03, Loss_0: 2.191e-04, Loss_r: 3.768e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.553e-03, Loss_0: 1.827e-04, Loss_r: 3.370e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 3.170e-03, Loss_0: 1.498e-04, Loss_r: 3.020e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.824e-03, Loss_0: 1.316e-04, Loss_r: 2.692e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.595e-03, Loss_0: 1.207e-04, Loss_r: 2.474e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.376e-03, Loss_0: 7.940e-05, Loss_r: 2.297e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.003e-03, Loss_0: 7.929e-05, Loss_r: 1.924e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.831e-03, Loss_0: 7.249e-05, Loss_r: 1.759e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.603e-03, Loss_0: 5.662e-05, Loss_r: 1.547e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.439e-03, Loss_0: 4.695e-05, Loss_r: 1.392e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.302e-03, Loss_0: 3.803e-05, Loss_r: 1.264e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.292e-03, Loss_0: 2.652e-05, Loss_r: 1.266e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.086e-03, Loss_0: 3.437e-05, Loss_r: 1.051e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.072e-03, Loss_0: 1.888e-05, Loss_r: 1.053e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 8.771e-04, Loss_0: 2.181e-05, Loss_r: 8.553e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 8.152e-04, Loss_0: 2.163e-05, Loss_r: 7.935e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 7.435e-04, Loss_0: 1.551e-05, Loss_r: 7.280e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 6.860e-04, Loss_0: 1.378e-05, Loss_r: 6.722e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 6.395e-04, Loss_0: 1.154e-05, Loss_r: 6.280e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.268e-03, Loss_0: 2.290e-06, Loss_r: 1.266e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 7.848e-04, Loss_0: 1.705e-05, Loss_r: 7.677e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.571e-04, Loss_0: 1.536e-05, Loss_r: 6.417e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.224e-04, Loss_0: 1.029e-05, Loss_r: 5.121e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 4.614e-04, Loss_0: 7.208e-06, Loss_r: 4.542e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 4.441e-04, Loss_0: 5.450e-06, Loss_r: 4.387e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 4.144e-04, Loss_0: 6.217e-06, Loss_r: 4.082e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 3.929e-04, Loss_0: 5.258e-06, Loss_r: 3.876e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 3.739e-04, Loss_0: 4.967e-06, Loss_r: 3.690e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 3.565e-04, Loss_0: 4.185e-06, Loss_r: 3.523e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 3.424e-04, Loss_0: 3.524e-06, Loss_r: 3.389e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 4.857e-04, Loss_0: 9.033e-07, Loss_r: 4.848e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 3.233e-04, Loss_0: 4.596e-06, Loss_r: 3.187e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 4.485e-04, Loss_0: 5.075e-07, Loss_r: 4.480e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 3.637e-04, Loss_0: 1.192e-06, Loss_r: 3.625e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.787e-04, Loss_0: 2.292e-06, Loss_r: 2.764e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 2.750e-04, Loss_0: 3.247e-06, Loss_r: 2.717e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.555e-04, Loss_0: 2.513e-06, Loss_r: 2.530e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.468e-04, Loss_0: 1.892e-06, Loss_r: 2.449e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.368e-04, Loss_0: 2.216e-06, Loss_r: 2.346e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.273e-04, Loss_0: 1.814e-06, Loss_r: 2.255e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.187e-04, Loss_0: 1.712e-06, Loss_r: 2.170e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.105e-04, Loss_0: 1.670e-06, Loss_r: 2.088e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.029e-04, Loss_0: 1.656e-06, Loss_r: 2.012e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.266e-04, Loss_0: 2.778e-06, Loss_r: 2.238e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.748e-03, Loss_0: 1.830e-05, Loss_r: 1.730e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 7.332e-04, Loss_0: 1.063e-05, Loss_r: 7.226e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 3.525e-04, Loss_0: 5.538e-06, Loss_r: 3.470e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 2.138e-04, Loss_0: 2.505e-06, Loss_r: 2.113e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.766e-04, Loss_0: 1.723e-06, Loss_r: 1.749e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.655e-04, Loss_0: 1.507e-06, Loss_r: 1.640e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.591e-04, Loss_0: 1.310e-06, Loss_r: 1.578e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.531e-04, Loss_0: 1.151e-06, Loss_r: 1.519e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.472e-04, Loss_0: 9.867e-07, Loss_r: 1.463e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.426e-04, Loss_0: 8.345e-07, Loss_r: 1.417e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.381e-04, Loss_0: 7.961e-07, Loss_r: 1.373e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.339e-04, Loss_0: 8.072e-07, Loss_r: 1.331e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.299e-04, Loss_0: 7.365e-07, Loss_r: 1.291e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.260e-04, Loss_0: 7.157e-07, Loss_r: 1.253e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.223e-04, Loss_0: 6.714e-07, Loss_r: 1.216e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.188e-04, Loss_0: 6.475e-07, Loss_r: 1.181e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.154e-04, Loss_0: 6.182e-07, Loss_r: 1.148e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.121e-04, Loss_0: 5.984e-07, Loss_r: 1.115e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.095e-04, Loss_0: 6.564e-07, Loss_r: 1.088e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 2.043e-04, Loss_0: 2.657e-06, Loss_r: 2.017e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.193e-04, Loss_0: 4.172e-06, Loss_r: 3.151e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.308e-04, Loss_0: 2.286e-06, Loss_r: 2.285e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.008e-04, Loss_0: 3.326e-07, Loss_r: 1.004e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.147e-04, Loss_0: 9.941e-08, Loss_r: 1.146e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.118e-04, Loss_0: 1.189e-07, Loss_r: 1.117e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.015e-04, Loss_0: 1.824e-07, Loss_r: 1.013e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 9.465e-05, Loss_0: 2.402e-07, Loss_r: 9.441e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 9.070e-05, Loss_0: 3.010e-07, Loss_r: 9.039e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 8.818e-05, Loss_0: 3.577e-07, Loss_r: 8.782e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 8.636e-05, Loss_0: 3.941e-07, Loss_r: 8.596e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 8.459e-05, Loss_0: 3.843e-07, Loss_r: 8.421e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 8.284e-05, Loss_0: 3.416e-07, Loss_r: 8.249e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 8.122e-05, Loss_0: 3.182e-07, Loss_r: 8.090e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 7.965e-05, Loss_0: 3.195e-07, Loss_r: 7.933e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 7.816e-05, Loss_0: 3.021e-07, Loss_r: 7.786e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 7.673e-05, Loss_0: 2.911e-07, Loss_r: 7.644e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 7.537e-05, Loss_0: 2.801e-07, Loss_r: 7.509e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 7.406e-05, Loss_0: 2.715e-07, Loss_r: 7.379e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 7.280e-05, Loss_0: 2.599e-07, Loss_r: 7.254e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 7.161e-05, Loss_0: 2.495e-07, Loss_r: 7.136e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 7.046e-05, Loss_0: 2.351e-07, Loss_r: 7.023e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 6.974e-05, Loss_0: 1.781e-07, Loss_r: 6.956e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.415e-04, Loss_0: 1.495e-07, Loss_r: 1.414e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 1.338e-03, Loss_0: 1.167e-05, Loss_r: 1.326e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 6.586e-04, Loss_0: 5.507e-06, Loss_r: 6.531e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.803e-04, Loss_0: 7.048e-07, Loss_r: 1.796e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 7.116e-05, Loss_0: 2.481e-08, Loss_r: 7.113e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 6.680e-05, Loss_0: 2.989e-07, Loss_r: 6.651e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.705e-05, Loss_0: 3.754e-07, Loss_r: 6.668e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 6.512e-05, Loss_0: 3.130e-07, Loss_r: 6.481e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 6.355e-05, Loss_0: 2.423e-07, Loss_r: 6.330e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 6.261e-05, Loss_0: 1.890e-07, Loss_r: 6.242e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 6.196e-05, Loss_0: 1.587e-07, Loss_r: 6.181e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 6.129e-05, Loss_0: 1.514e-07, Loss_r: 6.114e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 6.057e-05, Loss_0: 1.609e-07, Loss_r: 6.041e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 5.993e-05, Loss_0: 1.692e-07, Loss_r: 5.976e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 5.932e-05, Loss_0: 1.602e-07, Loss_r: 5.916e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 5.873e-05, Loss_0: 1.499e-07, Loss_r: 5.858e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 5.816e-05, Loss_0: 1.496e-07, Loss_r: 5.801e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 5.762e-05, Loss_0: 1.439e-07, Loss_r: 5.747e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 5.709e-05, Loss_0: 1.402e-07, Loss_r: 5.695e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 5.658e-05, Loss_0: 1.359e-07, Loss_r: 5.644e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 5.609e-05, Loss_0: 1.330e-07, Loss_r: 5.595e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 5.561e-05, Loss_0: 1.291e-07, Loss_r: 5.548e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 5.515e-05, Loss_0: 1.253e-07, Loss_r: 5.503e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 5.471e-05, Loss_0: 1.204e-07, Loss_r: 5.459e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 5.432e-05, Loss_0: 1.046e-07, Loss_r: 5.421e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 5.922e-05, Loss_0: 8.059e-09, Loss_r: 5.921e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.352e-03, Loss_0: 1.352e-05, Loss_r: 1.338e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.251e-03, Loss_0: 1.568e-05, Loss_r: 1.235e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.932e-04, Loss_0: 4.767e-06, Loss_r: 3.885e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 9.460e-05, Loss_0: 7.638e-07, Loss_r: 9.384e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 5.334e-05, Loss_0: 5.142e-08, Loss_r: 5.329e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 5.605e-05, Loss_0: 6.372e-09, Loss_r: 5.604e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 5.531e-05, Loss_0: 1.498e-08, Loss_r: 5.529e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 5.344e-05, Loss_0: 4.168e-08, Loss_r: 5.340e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 5.226e-05, Loss_0: 7.120e-08, Loss_r: 5.219e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 5.166e-05, Loss_0: 9.656e-08, Loss_r: 5.156e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 5.132e-05, Loss_0: 1.123e-07, Loss_r: 5.121e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 5.103e-05, Loss_0: 1.150e-07, Loss_r: 5.092e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 5.070e-05, Loss_0: 1.058e-07, Loss_r: 5.060e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 5.039e-05, Loss_0: 9.333e-08, Loss_r: 5.030e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 5.010e-05, Loss_0: 8.883e-08, Loss_r: 5.001e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 4.982e-05, Loss_0: 9.078e-08, Loss_r: 4.972e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 4.953e-05, Loss_0: 8.938e-08, Loss_r: 4.945e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 4.926e-05, Loss_0: 8.601e-08, Loss_r: 4.918e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.899e-05, Loss_0: 8.545e-08, Loss_r: 4.891e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.873e-05, Loss_0: 8.330e-08, Loss_r: 4.865e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 4.847e-05, Loss_0: 8.221e-08, Loss_r: 4.839e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 4.822e-05, Loss_0: 8.051e-08, Loss_r: 4.814e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 4.797e-05, Loss_0: 7.913e-08, Loss_r: 4.789e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 4.772e-05, Loss_0: 7.778e-08, Loss_r: 4.765e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 4.748e-05, Loss_0: 7.672e-08, Loss_r: 4.741e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 4.725e-05, Loss_0: 7.752e-08, Loss_r: 4.717e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.714e-05, Loss_0: 9.859e-08, Loss_r: 4.705e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 6.964e-05, Loss_0: 6.857e-07, Loss_r: 6.895e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 3.509e-03, Loss_0: 4.914e-05, Loss_r: 3.460e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.981e-04, Loss_0: 3.870e-06, Loss_r: 1.943e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.142e-04, Loss_0: 4.173e-06, Loss_r: 2.100e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.709e-04, Loss_0: 3.014e-06, Loss_r: 1.679e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 8.664e-05, Loss_0: 1.040e-06, Loss_r: 8.560e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 5.339e-05, Loss_0: 2.870e-07, Loss_r: 5.310e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 4.706e-05, Loss_0: 1.134e-07, Loss_r: 4.695e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 4.619e-05, Loss_0: 8.749e-08, Loss_r: 4.610e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 4.593e-05, Loss_0: 8.980e-08, Loss_r: 4.584e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.573e-05, Loss_0: 9.037e-08, Loss_r: 4.564e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 4.553e-05, Loss_0: 8.668e-08, Loss_r: 4.544e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 4.530e-05, Loss_0: 7.987e-08, Loss_r: 4.522e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 4.507e-05, Loss_0: 7.185e-08, Loss_r: 4.500e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 4.486e-05, Loss_0: 6.478e-08, Loss_r: 4.480e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 4.466e-05, Loss_0: 6.219e-08, Loss_r: 4.460e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 4.446e-05, Loss_0: 6.353e-08, Loss_r: 4.440e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 4.427e-05, Loss_0: 6.338e-08, Loss_r: 4.420e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 4.407e-05, Loss_0: 6.107e-08, Loss_r: 4.401e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 4.388e-05, Loss_0: 6.069e-08, Loss_r: 4.382e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 4.369e-05, Loss_0: 6.017e-08, Loss_r: 4.363e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 4.350e-05, Loss_0: 5.884e-08, Loss_r: 4.344e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 4.331e-05, Loss_0: 5.805e-08, Loss_r: 4.326e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 4.313e-05, Loss_0: 5.749e-08, Loss_r: 4.307e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 4.294e-05, Loss_0: 5.678e-08, Loss_r: 4.288e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 4.276e-05, Loss_0: 5.587e-08, Loss_r: 4.270e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 4.257e-05, Loss_0: 5.533e-08, Loss_r: 4.252e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 4.239e-05, Loss_0: 5.516e-08, Loss_r: 4.234e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 4.221e-05, Loss_0: 5.767e-08, Loss_r: 4.216e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 4.243e-05, Loss_0: 9.579e-08, Loss_r: 4.233e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 1.215e-04, Loss_0: 1.804e-06, Loss_r: 1.197e-04, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 4.394e-05, Loss_0: 1.774e-07, Loss_r: 4.376e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 4.749e-05, Loss_0: 6.326e-09, Loss_r: 4.749e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 4.594e-05, Loss_0: 1.926e-09, Loss_r: 4.594e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 4.125e-05, Loss_0: 5.915e-08, Loss_r: 4.119e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 4.173e-05, Loss_0: 1.063e-07, Loss_r: 4.162e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 4.096e-05, Loss_0: 3.829e-08, Loss_r: 4.092e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 4.078e-05, Loss_0: 4.033e-08, Loss_r: 4.074e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 4.063e-05, Loss_0: 5.893e-08, Loss_r: 4.057e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 4.045e-05, Loss_0: 4.191e-08, Loss_r: 4.041e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 4.028e-05, Loss_0: 5.153e-08, Loss_r: 4.023e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 4.012e-05, Loss_0: 4.463e-08, Loss_r: 4.008e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 3.996e-05, Loss_0: 4.767e-08, Loss_r: 3.992e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 3.981e-05, Loss_0: 4.694e-08, Loss_r: 3.976e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 3.965e-05, Loss_0: 4.532e-08, Loss_r: 3.960e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 3.949e-05, Loss_0: 4.425e-08, Loss_r: 3.945e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 3.934e-05, Loss_0: 4.149e-08, Loss_r: 3.930e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 3.933e-05, Loss_0: 2.618e-08, Loss_r: 3.931e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 5.259e-05, Loss_0: 6.608e-08, Loss_r: 5.253e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 4.003e-05, Loss_0: 5.499e-09, Loss_r: 4.003e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.909e-05, Loss_0: 8.002e-08, Loss_r: 3.901e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 3.940e-05, Loss_0: 1.027e-07, Loss_r: 3.930e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.851e-05, Loss_0: 5.428e-08, Loss_r: 3.846e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.841e-05, Loss_0: 2.844e-08, Loss_r: 3.839e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 3.819e-05, Loss_0: 3.933e-08, Loss_r: 3.815e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 3.806e-05, Loss_0: 4.823e-08, Loss_r: 3.801e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 3.791e-05, Loss_0: 3.868e-08, Loss_r: 3.787e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 3.777e-05, Loss_0: 4.158e-08, Loss_r: 3.773e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 3.763e-05, Loss_0: 4.075e-08, Loss_r: 3.759e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 3.749e-05, Loss_0: 4.038e-08, Loss_r: 3.745e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 3.735e-05, Loss_0: 3.993e-08, Loss_r: 3.731e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 3.721e-05, Loss_0: 4.025e-08, Loss_r: 3.717e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 3.707e-05, Loss_0: 3.957e-08, Loss_r: 3.703e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 3.693e-05, Loss_0: 3.901e-08, Loss_r: 3.689e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 3.679e-05, Loss_0: 3.885e-08, Loss_r: 3.675e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 3.665e-05, Loss_0: 3.842e-08, Loss_r: 3.661e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 3.651e-05, Loss_0: 3.722e-08, Loss_r: 3.647e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 3.641e-05, Loss_0: 2.795e-08, Loss_r: 3.639e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 4.776e-05, Loss_0: 5.750e-08, Loss_r: 4.771e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 3.618e-05, Loss_0: 2.535e-08, Loss_r: 3.615e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 3.743e-05, Loss_0: 1.205e-07, Loss_r: 3.731e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 3.625e-05, Loss_0: 7.475e-08, Loss_r: 3.617e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 3.585e-05, Loss_0: 2.160e-08, Loss_r: 3.583e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 3.563e-05, Loss_0: 2.801e-08, Loss_r: 3.560e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 3.551e-05, Loss_0: 4.620e-08, Loss_r: 3.546e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 3.535e-05, Loss_0: 3.297e-08, Loss_r: 3.532e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 3.522e-05, Loss_0: 3.606e-08, Loss_r: 3.518e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 3.509e-05, Loss_0: 3.603e-08, Loss_r: 3.506e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 3.497e-05, Loss_0: 3.522e-08, Loss_r: 3.493e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 3.484e-05, Loss_0: 3.487e-08, Loss_r: 3.480e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 3.471e-05, Loss_0: 3.536e-08, Loss_r: 3.468e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 3.459e-05, Loss_0: 3.463e-08, Loss_r: 3.455e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 3.446e-05, Loss_0: 3.425e-08, Loss_r: 3.442e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 3.433e-05, Loss_0: 3.394e-08, Loss_r: 3.430e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 3.421e-05, Loss_0: 3.286e-08, Loss_r: 3.417e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 3.410e-05, Loss_0: 2.705e-08, Loss_r: 3.407e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 3.644e-05, Loss_0: 4.022e-10, Loss_r: 3.644e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 3.400e-05, Loss_0: 1.729e-08, Loss_r: 3.398e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 3.383e-05, Loss_0: 5.014e-08, Loss_r: 3.378e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 3.376e-05, Loss_0: 5.361e-08, Loss_r: 3.370e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 3.350e-05, Loss_0: 3.470e-08, Loss_r: 3.346e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 3.340e-05, Loss_0: 2.642e-08, Loss_r: 3.337e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 3.326e-05, Loss_0: 3.257e-08, Loss_r: 3.323e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 3.315e-05, Loss_0: 3.451e-08, Loss_r: 3.312e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 3.304e-05, Loss_0: 3.071e-08, Loss_r: 3.301e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 3.292e-05, Loss_0: 3.236e-08, Loss_r: 3.289e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 3.281e-05, Loss_0: 3.140e-08, Loss_r: 3.277e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 3.269e-05, Loss_0: 3.171e-08, Loss_r: 3.266e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 3.258e-05, Loss_0: 3.124e-08, Loss_r: 3.254e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 3.246e-05, Loss_0: 3.124e-08, Loss_r: 3.243e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 3.235e-05, Loss_0: 3.100e-08, Loss_r: 3.231e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 3.223e-05, Loss_0: 3.095e-08, Loss_r: 3.220e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 3.211e-05, Loss_0: 3.068e-08, Loss_r: 3.208e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 3.200e-05, Loss_0: 3.066e-08, Loss_r: 3.197e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 3.189e-05, Loss_0: 3.171e-08, Loss_r: 3.185e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 3.187e-05, Loss_0: 4.595e-08, Loss_r: 3.182e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 5.159e-05, Loss_0: 5.642e-07, Loss_r: 5.103e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 3.201e-05, Loss_0: 6.830e-08, Loss_r: 3.194e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 3.316e-05, Loss_0: 4.414e-12, Loss_r: 3.316e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 3.240e-05, Loss_0: 1.428e-09, Loss_r: 3.240e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 3.127e-05, Loss_0: 3.693e-08, Loss_r: 3.123e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 3.129e-05, Loss_0: 4.893e-08, Loss_r: 3.124e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 3.105e-05, Loss_0: 2.309e-08, Loss_r: 3.103e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 3.093e-05, Loss_0: 2.611e-08, Loss_r: 3.091e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 3.083e-05, Loss_0: 3.248e-08, Loss_r: 3.080e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 3.073e-05, Loss_0: 2.606e-08, Loss_r: 3.070e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 3.062e-05, Loss_0: 2.985e-08, Loss_r: 3.059e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 3.051e-05, Loss_0: 2.726e-08, Loss_r: 3.049e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 3.041e-05, Loss_0: 2.852e-08, Loss_r: 3.038e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 3.031e-05, Loss_0: 2.791e-08, Loss_r: 3.028e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 3.020e-05, Loss_0: 2.763e-08, Loss_r: 3.017e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 3.010e-05, Loss_0: 2.746e-08, Loss_r: 3.007e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 2.999e-05, Loss_0: 2.742e-08, Loss_r: 2.997e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 2.989e-05, Loss_0: 2.717e-08, Loss_r: 2.986e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 2.978e-05, Loss_0: 2.690e-08, Loss_r: 2.976e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 2.968e-05, Loss_0: 2.426e-08, Loss_r: 2.966e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 3.052e-05, Loss_0: 1.317e-09, Loss_r: 3.052e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 4.017e-04, Loss_0: 5.658e-06, Loss_r: 3.961e-04, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.344e-04, Loss_0: 1.347e-06, Loss_r: 1.331e-04, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 7.519e-05, Loss_0: 5.229e-07, Loss_r: 7.467e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 4.807e-05, Loss_0: 1.760e-07, Loss_r: 4.790e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 3.617e-05, Loss_0: 4.083e-08, Loss_r: 3.613e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 3.151e-05, Loss_0: 2.807e-09, Loss_r: 3.151e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 2.972e-05, Loss_0: 2.201e-09, Loss_r: 2.971e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 2.903e-05, Loss_0: 1.308e-08, Loss_r: 2.901e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 2.880e-05, Loss_0: 2.509e-08, Loss_r: 2.877e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 2.872e-05, Loss_0: 3.190e-08, Loss_r: 2.869e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 2.863e-05, Loss_0: 3.139e-08, Loss_r: 2.860e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 2.852e-05, Loss_0: 2.668e-08, Loss_r: 2.850e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 2.843e-05, Loss_0: 2.372e-08, Loss_r: 2.841e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 2.834e-05, Loss_0: 2.471e-08, Loss_r: 2.831e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 2.825e-05, Loss_0: 2.583e-08, Loss_r: 2.822e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 2.815e-05, Loss_0: 2.503e-08, Loss_r: 2.813e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 2.806e-05, Loss_0: 2.474e-08, Loss_r: 2.804e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 2.797e-05, Loss_0: 2.487e-08, Loss_r: 2.794e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 2.788e-05, Loss_0: 2.453e-08, Loss_r: 2.785e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 2.779e-05, Loss_0: 2.454e-08, Loss_r: 2.776e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 2.769e-05, Loss_0: 2.422e-08, Loss_r: 2.767e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 2.760e-05, Loss_0: 2.408e-08, Loss_r: 2.758e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 2.751e-05, Loss_0: 2.403e-08, Loss_r: 2.748e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 2.742e-05, Loss_0: 2.384e-08, Loss_r: 2.739e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 2.732e-05, Loss_0: 2.382e-08, Loss_r: 2.730e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 2.723e-05, Loss_0: 2.368e-08, Loss_r: 2.721e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 2.714e-05, Loss_0: 2.352e-08, Loss_r: 2.712e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 2.705e-05, Loss_0: 2.336e-08, Loss_r: 2.702e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 2.696e-05, Loss_0: 2.326e-08, Loss_r: 2.693e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 2.686e-05, Loss_0: 2.318e-08, Loss_r: 2.684e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3560, Loss: 2.677e-05, Loss_0: 2.310e-08, Loss_r: 2.675e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3570, Loss: 2.669e-05, Loss_0: 2.817e-08, Loss_r: 2.667e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3580, Loss: 5.506e-05, Loss_0: 7.373e-07, Loss_r: 5.432e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 3.364e-05, Loss_0: 4.161e-08, Loss_r: 3.360e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 2.984e-05, Loss_0: 9.174e-09, Loss_r: 2.983e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 2.818e-05, Loss_0: 1.089e-07, Loss_r: 2.807e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 2.626e-05, Loss_0: 2.214e-08, Loss_r: 2.624e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 2.635e-05, Loss_0: 8.808e-09, Loss_r: 2.634e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 2.621e-05, Loss_0: 3.795e-08, Loss_r: 2.618e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 2.606e-05, Loss_0: 1.424e-08, Loss_r: 2.605e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 2.595e-05, Loss_0: 2.762e-08, Loss_r: 2.592e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 2.585e-05, Loss_0: 1.887e-08, Loss_r: 2.583e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 2.576e-05, Loss_0: 2.177e-08, Loss_r: 2.574e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 2.568e-05, Loss_0: 2.306e-08, Loss_r: 2.566e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 2.560e-05, Loss_0: 2.163e-08, Loss_r: 2.558e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 2.552e-05, Loss_0: 2.078e-08, Loss_r: 2.550e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 2.544e-05, Loss_0: 1.919e-08, Loss_r: 2.542e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 2.545e-05, Loss_0: 1.081e-08, Loss_r: 2.543e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 3.291e-05, Loss_0: 5.064e-08, Loss_r: 3.286e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 2.602e-05, Loss_0: 5.612e-10, Loss_r: 2.601e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 2.520e-05, Loss_0: 3.321e-08, Loss_r: 2.517e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 2.544e-05, Loss_0: 5.154e-08, Loss_r: 2.538e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 2.505e-05, Loss_0: 3.228e-08, Loss_r: 2.501e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 2.491e-05, Loss_0: 1.547e-08, Loss_r: 2.490e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 2.484e-05, Loss_0: 1.609e-08, Loss_r: 2.482e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 2.475e-05, Loss_0: 2.301e-08, Loss_r: 2.473e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 2.467e-05, Loss_0: 2.065e-08, Loss_r: 2.465e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 2.460e-05, Loss_0: 1.898e-08, Loss_r: 2.458e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 2.452e-05, Loss_0: 2.081e-08, Loss_r: 2.450e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 2.445e-05, Loss_0: 1.936e-08, Loss_r: 2.443e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 2.437e-05, Loss_0: 1.999e-08, Loss_r: 2.435e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 2.430e-05, Loss_0: 1.943e-08, Loss_r: 2.428e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 2.422e-05, Loss_0: 1.973e-08, Loss_r: 2.420e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 2.415e-05, Loss_0: 1.955e-08, Loss_r: 2.413e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 2.407e-05, Loss_0: 1.940e-08, Loss_r: 2.405e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 2.400e-05, Loss_0: 1.938e-08, Loss_r: 2.398e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 2.392e-05, Loss_0: 1.937e-08, Loss_r: 2.391e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 2.385e-05, Loss_0: 1.989e-08, Loss_r: 2.383e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3940, Loss: 2.381e-05, Loss_0: 2.690e-08, Loss_r: 2.379e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3950, Loss: 2.937e-05, Loss_0: 2.089e-07, Loss_r: 2.916e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 2.389e-05, Loss_0: 4.201e-08, Loss_r: 2.384e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 2.389e-05, Loss_0: 3.645e-09, Loss_r: 2.389e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 2.384e-05, Loss_0: 3.418e-09, Loss_r: 2.383e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 2.343e-05, Loss_0: 1.762e-08, Loss_r: 2.342e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 2.341e-05, Loss_0: 2.700e-08, Loss_r: 2.338e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 2.330e-05, Loss_0: 1.790e-08, Loss_r: 2.328e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 2.324e-05, Loss_0: 1.583e-08, Loss_r: 2.322e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 2.317e-05, Loss_0: 1.983e-08, Loss_r: 2.315e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 2.310e-05, Loss_0: 1.755e-08, Loss_r: 2.308e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 2.303e-05, Loss_0: 1.814e-08, Loss_r: 2.301e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 2.296e-05, Loss_0: 1.781e-08, Loss_r: 2.294e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 2.289e-05, Loss_0: 1.791e-08, Loss_r: 2.288e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 2.283e-05, Loss_0: 1.753e-08, Loss_r: 2.281e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 2.276e-05, Loss_0: 1.766e-08, Loss_r: 2.274e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 2.269e-05, Loss_0: 1.755e-08, Loss_r: 2.267e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 2.262e-05, Loss_0: 1.732e-08, Loss_r: 2.261e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 2.256e-05, Loss_0: 1.701e-08, Loss_r: 2.254e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 2.249e-05, Loss_0: 1.555e-08, Loss_r: 2.248e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4140, Loss: 2.256e-05, Loss_0: 6.718e-09, Loss_r: 2.255e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4150, Loss: 3.846e-05, Loss_0: 1.695e-07, Loss_r: 3.829e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 2.373e-05, Loss_0: 8.976e-10, Loss_r: 2.373e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 2.248e-05, Loss_0: 3.905e-08, Loss_r: 2.244e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 2.303e-05, Loss_0: 6.486e-08, Loss_r: 2.296e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 2.228e-05, Loss_0: 3.406e-08, Loss_r: 2.225e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 2.209e-05, Loss_0: 1.073e-08, Loss_r: 2.208e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 2.203e-05, Loss_0: 1.085e-08, Loss_r: 2.202e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 2.194e-05, Loss_0: 1.970e-08, Loss_r: 2.192e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 2.187e-05, Loss_0: 1.763e-08, Loss_r: 2.186e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 2.182e-05, Loss_0: 1.487e-08, Loss_r: 2.180e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 2.175e-05, Loss_0: 1.732e-08, Loss_r: 2.174e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 2.169e-05, Loss_0: 1.580e-08, Loss_r: 2.168e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 2.163e-05, Loss_0: 1.656e-08, Loss_r: 2.161e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 2.157e-05, Loss_0: 1.583e-08, Loss_r: 2.155e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 2.151e-05, Loss_0: 1.623e-08, Loss_r: 2.149e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 2.145e-05, Loss_0: 1.591e-08, Loss_r: 2.143e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 2.139e-05, Loss_0: 1.581e-08, Loss_r: 2.137e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 2.133e-05, Loss_0: 1.573e-08, Loss_r: 2.131e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 2.127e-05, Loss_0: 1.546e-08, Loss_r: 2.125e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4340, Loss: 2.121e-05, Loss_0: 1.458e-08, Loss_r: 2.119e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4350, Loss: 2.121e-05, Loss_0: 8.484e-09, Loss_r: 2.120e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4360, Loss: 2.782e-05, Loss_0: 5.201e-08, Loss_r: 2.777e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 2.155e-05, Loss_0: 7.132e-10, Loss_r: 2.155e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 2.115e-05, Loss_0: 3.243e-08, Loss_r: 2.111e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 2.131e-05, Loss_0: 4.350e-08, Loss_r: 2.127e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 2.091e-05, Loss_0: 2.226e-08, Loss_r: 2.088e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 2.085e-05, Loss_0: 9.781e-09, Loss_r: 2.084e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 2.077e-05, Loss_0: 1.248e-08, Loss_r: 2.075e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 2.071e-05, Loss_0: 1.785e-08, Loss_r: 2.069e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 2.065e-05, Loss_0: 1.476e-08, Loss_r: 2.064e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 2.060e-05, Loss_0: 1.437e-08, Loss_r: 2.058e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 2.054e-05, Loss_0: 1.544e-08, Loss_r: 2.053e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 2.049e-05, Loss_0: 1.437e-08, Loss_r: 2.047e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 2.043e-05, Loss_0: 1.504e-08, Loss_r: 2.042e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 2.038e-05, Loss_0: 1.454e-08, Loss_r: 2.036e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 2.032e-05, Loss_0: 1.454e-08, Loss_r: 2.031e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 2.027e-05, Loss_0: 1.455e-08, Loss_r: 2.025e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 2.021e-05, Loss_0: 1.442e-08, Loss_r: 2.020e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4530, Loss: 2.016e-05, Loss_0: 1.430e-08, Loss_r: 2.014e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4540, Loss: 2.010e-05, Loss_0: 1.406e-08, Loss_r: 2.009e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4550, Loss: 2.005e-05, Loss_0: 1.285e-08, Loss_r: 2.004e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4560, Loss: 2.013e-05, Loss_0: 4.881e-09, Loss_r: 2.012e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4570, Loss: 3.876e-05, Loss_0: 2.244e-07, Loss_r: 3.853e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 2.173e-05, Loss_0: 4.237e-09, Loss_r: 2.172e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 1.998e-05, Loss_0: 2.876e-08, Loss_r: 1.996e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 2.064e-05, Loss_0: 5.917e-08, Loss_r: 2.058e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 2.005e-05, Loss_0: 3.702e-08, Loss_r: 2.001e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 1.970e-05, Loss_0: 1.185e-08, Loss_r: 1.969e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 1.970e-05, Loss_0: 7.377e-09, Loss_r: 1.970e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 1.960e-05, Loss_0: 1.419e-08, Loss_r: 1.959e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 1.956e-05, Loss_0: 1.629e-08, Loss_r: 1.954e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 1.950e-05, Loss_0: 1.231e-08, Loss_r: 1.949e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 1.945e-05, Loss_0: 1.365e-08, Loss_r: 1.944e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 1.941e-05, Loss_0: 1.381e-08, Loss_r: 1.939e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 1.936e-05, Loss_0: 1.312e-08, Loss_r: 1.934e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 1.931e-05, Loss_0: 1.352e-08, Loss_r: 1.929e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 1.926e-05, Loss_0: 1.323e-08, Loss_r: 1.924e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 1.921e-05, Loss_0: 1.318e-08, Loss_r: 1.920e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 1.916e-05, Loss_0: 1.334e-08, Loss_r: 1.915e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4740, Loss: 1.911e-05, Loss_0: 1.317e-08, Loss_r: 1.910e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4750, Loss: 1.906e-05, Loss_0: 1.315e-08, Loss_r: 1.905e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4760, Loss: 1.901e-05, Loss_0: 1.290e-08, Loss_r: 1.900e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4770, Loss: 1.896e-05, Loss_0: 1.231e-08, Loss_r: 1.895e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4780, Loss: 1.894e-05, Loss_0: 8.375e-09, Loss_r: 1.893e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4790, Loss: 2.256e-05, Loss_0: 2.254e-08, Loss_r: 2.253e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 1.901e-05, Loss_0: 2.872e-09, Loss_r: 1.901e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 1.895e-05, Loss_0: 2.897e-08, Loss_r: 1.893e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 1.896e-05, Loss_0: 3.129e-08, Loss_r: 1.893e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 1.869e-05, Loss_0: 1.442e-08, Loss_r: 1.868e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 1.867e-05, Loss_0: 7.987e-09, Loss_r: 1.866e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 1.860e-05, Loss_0: 1.194e-08, Loss_r: 1.859e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 1.856e-05, Loss_0: 1.439e-08, Loss_r: 1.855e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 1.851e-05, Loss_0: 1.155e-08, Loss_r: 1.850e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 1.847e-05, Loss_0: 1.242e-08, Loss_r: 1.846e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 1.842e-05, Loss_0: 1.243e-08, Loss_r: 1.841e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 1.838e-05, Loss_0: 1.210e-08, Loss_r: 1.837e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 1.834e-05, Loss_0: 1.219e-08, Loss_r: 1.832e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 1.829e-05, Loss_0: 1.202e-08, Loss_r: 1.828e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 1.825e-05, Loss_0: 1.195e-08, Loss_r: 1.824e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 1.820e-05, Loss_0: 1.210e-08, Loss_r: 1.819e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4950, Loss: 1.816e-05, Loss_0: 1.198e-08, Loss_r: 1.815e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4960, Loss: 1.812e-05, Loss_0: 1.201e-08, Loss_r: 1.810e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4970, Loss: 1.807e-05, Loss_0: 1.246e-08, Loss_r: 1.806e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4980, Loss: 1.804e-05, Loss_0: 1.568e-08, Loss_r: 1.803e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4990, Loss: 1.929e-05, Loss_0: 7.063e-08, Loss_r: 1.922e-05, Time: 0.03, Learning Rate: 0.00023\n",
            "Training time: 18.0242\n",
            "[1, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 4.988e-01, Loss_0: 9.508e-04, Loss_r: 4.978e-01, Time: 0.60, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.535e-01, Loss_0: 1.259e-02, Loss_r: 1.409e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.216e-01, Loss_0: 3.991e-02, Loss_r: 8.165e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.168e-01, Loss_0: 5.118e-02, Loss_r: 6.561e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.167e-01, Loss_0: 5.968e-02, Loss_r: 5.700e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.166e-01, Loss_0: 5.960e-02, Loss_r: 5.703e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.164e-01, Loss_0: 5.638e-02, Loss_r: 6.003e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.163e-01, Loss_0: 5.380e-02, Loss_r: 6.255e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.163e-01, Loss_0: 5.373e-02, Loss_r: 6.259e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.163e-01, Loss_0: 5.352e-02, Loss_r: 6.276e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.162e-01, Loss_0: 5.459e-02, Loss_r: 6.163e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.162e-01, Loss_0: 5.440e-02, Loss_r: 6.176e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.161e-01, Loss_0: 5.441e-02, Loss_r: 6.169e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.160e-01, Loss_0: 5.426e-02, Loss_r: 6.177e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.160e-01, Loss_0: 5.414e-02, Loss_r: 6.182e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.159e-01, Loss_0: 5.416e-02, Loss_r: 6.170e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.158e-01, Loss_0: 5.414e-02, Loss_r: 6.162e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.156e-01, Loss_0: 5.407e-02, Loss_r: 6.156e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.155e-01, Loss_0: 5.400e-02, Loss_r: 6.148e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.153e-01, Loss_0: 5.393e-02, Loss_r: 6.138e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.151e-01, Loss_0: 5.385e-02, Loss_r: 6.124e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.148e-01, Loss_0: 5.374e-02, Loss_r: 6.108e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.145e-01, Loss_0: 5.362e-02, Loss_r: 6.087e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.141e-01, Loss_0: 5.346e-02, Loss_r: 6.059e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.135e-01, Loss_0: 5.325e-02, Loss_r: 6.022e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.127e-01, Loss_0: 5.297e-02, Loss_r: 5.971e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.115e-01, Loss_0: 5.258e-02, Loss_r: 5.896e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.098e-01, Loss_0: 5.200e-02, Loss_r: 5.782e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.071e-01, Loss_0: 5.111e-02, Loss_r: 5.598e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.025e-01, Loss_0: 4.968e-02, Loss_r: 5.284e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 9.483e-02, Loss_0: 4.722e-02, Loss_r: 4.761e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 8.304e-02, Loss_0: 4.253e-02, Loss_r: 4.051e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 6.833e-02, Loss_0: 3.313e-02, Loss_r: 3.520e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.511e-02, Loss_0: 1.974e-02, Loss_r: 3.536e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.808e-02, Loss_0: 1.136e-02, Loss_r: 3.672e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.315e-02, Loss_0: 8.581e-03, Loss_r: 3.457e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.740e-02, Loss_0: 8.151e-03, Loss_r: 2.925e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.204e-02, Loss_0: 6.976e-03, Loss_r: 2.506e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.028e-02, Loss_0: 3.393e-03, Loss_r: 3.689e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.366e-02, Loss_0: 3.616e-03, Loss_r: 2.004e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.029e-02, Loss_0: 3.070e-03, Loss_r: 1.722e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.521e-02, Loss_0: 2.141e-03, Loss_r: 1.306e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.192e-02, Loss_0: 1.554e-03, Loss_r: 1.037e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 9.656e-03, Loss_0: 1.126e-03, Loss_r: 8.530e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 7.937e-03, Loss_0: 8.057e-04, Loss_r: 7.131e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.720e-03, Loss_0: 5.781e-04, Loss_r: 6.142e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.769e-03, Loss_0: 4.412e-04, Loss_r: 5.328e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.990e-03, Loss_0: 3.407e-04, Loss_r: 4.650e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.318e-03, Loss_0: 2.676e-04, Loss_r: 4.051e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.732e-03, Loss_0: 2.137e-04, Loss_r: 3.519e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.221e-03, Loss_0: 1.707e-04, Loss_r: 3.050e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.777e-03, Loss_0: 1.384e-04, Loss_r: 2.639e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.600e-03, Loss_0: 1.235e-04, Loss_r: 2.477e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.034e-02, Loss_0: 2.691e-05, Loss_r: 1.031e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.083e-03, Loss_0: 1.420e-04, Loss_r: 3.942e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 3.005e-03, Loss_0: 4.963e-05, Loss_r: 2.955e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.872e-03, Loss_0: 6.452e-05, Loss_r: 1.808e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.417e-03, Loss_0: 4.096e-05, Loss_r: 1.376e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.214e-03, Loss_0: 4.532e-05, Loss_r: 1.168e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.062e-03, Loss_0: 3.109e-05, Loss_r: 1.031e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 600, Loss: 9.217e-04, Loss_0: 2.845e-05, Loss_r: 8.933e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 8.158e-04, Loss_0: 2.440e-05, Loss_r: 7.914e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 7.249e-04, Loss_0: 1.975e-05, Loss_r: 7.051e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 630, Loss: 6.490e-04, Loss_0: 1.658e-05, Loss_r: 6.325e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 5.841e-04, Loss_0: 1.427e-05, Loss_r: 5.698e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 650, Loss: 5.287e-04, Loss_0: 1.221e-05, Loss_r: 5.165e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 4.815e-04, Loss_0: 1.057e-05, Loss_r: 4.710e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 4.410e-04, Loss_0: 9.045e-06, Loss_r: 4.320e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 4.063e-04, Loss_0: 7.897e-06, Loss_r: 3.984e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 3.763e-04, Loss_0: 6.864e-06, Loss_r: 3.694e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 700, Loss: 3.503e-04, Loss_0: 6.007e-06, Loss_r: 3.443e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 3.296e-04, Loss_0: 5.019e-06, Loss_r: 3.245e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.524e-03, Loss_0: 1.445e-07, Loss_r: 1.523e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 8.692e-03, Loss_0: 6.475e-05, Loss_r: 8.628e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 3.229e-04, Loss_0: 8.609e-07, Loss_r: 3.221e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.878e-04, Loss_0: 2.446e-06, Loss_r: 5.853e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 5.608e-04, Loss_0: 7.075e-06, Loss_r: 5.538e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 3.719e-04, Loss_0: 2.209e-06, Loss_r: 3.697e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.605e-04, Loss_0: 3.668e-06, Loss_r: 2.568e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.549e-04, Loss_0: 3.650e-06, Loss_r: 2.512e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.368e-04, Loss_0: 2.619e-06, Loss_r: 2.342e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.258e-04, Loss_0: 2.283e-06, Loss_r: 2.235e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.151e-04, Loss_0: 2.392e-06, Loss_r: 2.127e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.067e-04, Loss_0: 2.188e-06, Loss_r: 2.045e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.988e-04, Loss_0: 2.028e-06, Loss_r: 1.968e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.915e-04, Loss_0: 1.866e-06, Loss_r: 1.896e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.846e-04, Loss_0: 1.708e-06, Loss_r: 1.829e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.783e-04, Loss_0: 1.585e-06, Loss_r: 1.767e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.723e-04, Loss_0: 1.469e-06, Loss_r: 1.709e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.667e-04, Loss_0: 1.371e-06, Loss_r: 1.654e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.615e-04, Loss_0: 1.290e-06, Loss_r: 1.602e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.565e-04, Loss_0: 1.209e-06, Loss_r: 1.553e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.518e-04, Loss_0: 1.136e-06, Loss_r: 1.507e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.473e-04, Loss_0: 1.070e-06, Loss_r: 1.463e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.431e-04, Loss_0: 1.011e-06, Loss_r: 1.421e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.392e-04, Loss_0: 9.556e-07, Loss_r: 1.382e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.354e-04, Loss_0: 9.064e-07, Loss_r: 1.345e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.319e-04, Loss_0: 8.829e-07, Loss_r: 1.310e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.498e-04, Loss_0: 1.361e-06, Loss_r: 1.485e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.379e-02, Loss_0: 4.529e-05, Loss_r: 1.375e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 4.150e-03, Loss_0: 2.527e-05, Loss_r: 4.124e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.510e-04, Loss_0: 3.663e-11, Loss_r: 1.510e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 3.556e-04, Loss_0: 2.826e-08, Loss_r: 3.556e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 3.118e-04, Loss_0: 4.855e-06, Loss_r: 3.069e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.125e-04, Loss_0: 9.981e-08, Loss_r: 2.124e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.427e-04, Loss_0: 1.212e-06, Loss_r: 1.415e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.229e-04, Loss_0: 9.624e-07, Loss_r: 1.219e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.229e-04, Loss_0: 6.910e-07, Loss_r: 1.222e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.176e-04, Loss_0: 8.145e-07, Loss_r: 1.168e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.146e-04, Loss_0: 7.372e-07, Loss_r: 1.139e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.120e-04, Loss_0: 6.483e-07, Loss_r: 1.113e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.096e-04, Loss_0: 6.202e-07, Loss_r: 1.090e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.074e-04, Loss_0: 5.981e-07, Loss_r: 1.068e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.054e-04, Loss_0: 5.715e-07, Loss_r: 1.048e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.034e-04, Loss_0: 5.450e-07, Loss_r: 1.029e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.016e-04, Loss_0: 5.198e-07, Loss_r: 1.011e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 9.986e-05, Loss_0: 4.955e-07, Loss_r: 9.936e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 9.822e-05, Loss_0: 4.728e-07, Loss_r: 9.774e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 9.666e-05, Loss_0: 4.518e-07, Loss_r: 9.621e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 9.518e-05, Loss_0: 4.329e-07, Loss_r: 9.475e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 9.377e-05, Loss_0: 4.155e-07, Loss_r: 9.336e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 9.243e-05, Loss_0: 3.995e-07, Loss_r: 9.204e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 9.116e-05, Loss_0: 3.843e-07, Loss_r: 9.078e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 8.995e-05, Loss_0: 3.698e-07, Loss_r: 8.958e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 8.879e-05, Loss_0: 3.567e-07, Loss_r: 8.843e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 8.769e-05, Loss_0: 3.439e-07, Loss_r: 8.734e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 8.663e-05, Loss_0: 3.322e-07, Loss_r: 8.630e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 8.562e-05, Loss_0: 3.207e-07, Loss_r: 8.530e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 8.466e-05, Loss_0: 3.100e-07, Loss_r: 8.435e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 8.374e-05, Loss_0: 2.999e-07, Loss_r: 8.344e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 8.286e-05, Loss_0: 2.888e-07, Loss_r: 8.257e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 8.209e-05, Loss_0: 2.636e-07, Loss_r: 8.182e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.011e-04, Loss_0: 5.445e-08, Loss_r: 1.010e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 9.663e-03, Loss_0: 4.164e-05, Loss_r: 9.621e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.263e-04, Loss_0: 9.446e-08, Loss_r: 1.262e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.983e-03, Loss_0: 7.454e-06, Loss_r: 1.976e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 5.247e-04, Loss_0: 2.148e-06, Loss_r: 5.225e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.001e-04, Loss_0: 5.840e-10, Loss_r: 1.001e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.084e-04, Loss_0: 6.322e-09, Loss_r: 1.084e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.145e-04, Loss_0: 7.713e-07, Loss_r: 1.138e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 8.886e-05, Loss_0: 1.808e-07, Loss_r: 8.868e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.419e-05, Loss_0: 2.682e-07, Loss_r: 8.392e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 8.338e-05, Loss_0: 4.316e-07, Loss_r: 8.295e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 8.117e-05, Loss_0: 3.408e-07, Loss_r: 8.083e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 8.030e-05, Loss_0: 2.827e-07, Loss_r: 8.001e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 7.930e-05, Loss_0: 2.811e-07, Loss_r: 7.902e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 7.842e-05, Loss_0: 2.833e-07, Loss_r: 7.814e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 7.764e-05, Loss_0: 2.743e-07, Loss_r: 7.736e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 7.688e-05, Loss_0: 2.618e-07, Loss_r: 7.662e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 7.617e-05, Loss_0: 2.502e-07, Loss_r: 7.592e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 7.548e-05, Loss_0: 2.403e-07, Loss_r: 7.524e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.483e-05, Loss_0: 2.316e-07, Loss_r: 7.460e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 7.420e-05, Loss_0: 2.240e-07, Loss_r: 7.397e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 7.359e-05, Loss_0: 2.168e-07, Loss_r: 7.338e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 7.301e-05, Loss_0: 2.102e-07, Loss_r: 7.280e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 7.244e-05, Loss_0: 2.040e-07, Loss_r: 7.223e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 7.189e-05, Loss_0: 1.984e-07, Loss_r: 7.169e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 7.135e-05, Loss_0: 1.932e-07, Loss_r: 7.116e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 7.083e-05, Loss_0: 1.881e-07, Loss_r: 7.065e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 7.033e-05, Loss_0: 1.832e-07, Loss_r: 7.014e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 6.983e-05, Loss_0: 1.787e-07, Loss_r: 6.965e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 6.935e-05, Loss_0: 1.744e-07, Loss_r: 6.917e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 6.888e-05, Loss_0: 1.703e-07, Loss_r: 6.871e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 6.841e-05, Loss_0: 1.667e-07, Loss_r: 6.825e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 6.796e-05, Loss_0: 1.631e-07, Loss_r: 6.780e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 6.751e-05, Loss_0: 1.592e-07, Loss_r: 6.735e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 6.707e-05, Loss_0: 1.555e-07, Loss_r: 6.692e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 6.664e-05, Loss_0: 1.512e-07, Loss_r: 6.649e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 6.625e-05, Loss_0: 1.400e-07, Loss_r: 6.611e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 7.024e-05, Loss_0: 5.477e-08, Loss_r: 7.018e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.442e-03, Loss_0: 5.402e-06, Loss_r: 1.437e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.013e-03, Loss_0: 2.316e-05, Loss_r: 2.990e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.509e-03, Loss_0: 2.381e-05, Loss_r: 2.486e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 7.184e-05, Loss_0: 1.195e-08, Loss_r: 7.183e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.149e-04, Loss_0: 1.409e-06, Loss_r: 3.135e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.176e-04, Loss_0: 5.839e-07, Loss_r: 1.171e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 7.862e-05, Loss_0: 5.534e-07, Loss_r: 7.807e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 7.923e-05, Loss_0: 1.231e-07, Loss_r: 7.910e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 6.837e-05, Loss_0: 2.131e-07, Loss_r: 6.815e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 6.795e-05, Loss_0: 2.791e-07, Loss_r: 6.768e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 6.655e-05, Loss_0: 2.178e-07, Loss_r: 6.633e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 6.553e-05, Loss_0: 1.805e-07, Loss_r: 6.535e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 6.498e-05, Loss_0: 1.718e-07, Loss_r: 6.481e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 6.441e-05, Loss_0: 1.663e-07, Loss_r: 6.424e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 6.386e-05, Loss_0: 1.603e-07, Loss_r: 6.370e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 6.336e-05, Loss_0: 1.565e-07, Loss_r: 6.320e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 6.287e-05, Loss_0: 1.541e-07, Loss_r: 6.272e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.240e-05, Loss_0: 1.499e-07, Loss_r: 6.225e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 6.195e-05, Loss_0: 1.451e-07, Loss_r: 6.180e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 6.151e-05, Loss_0: 1.411e-07, Loss_r: 6.136e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 6.107e-05, Loss_0: 1.378e-07, Loss_r: 6.093e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 6.064e-05, Loss_0: 1.345e-07, Loss_r: 6.051e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 6.023e-05, Loss_0: 1.313e-07, Loss_r: 6.009e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 5.981e-05, Loss_0: 1.283e-07, Loss_r: 5.969e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 5.941e-05, Loss_0: 1.254e-07, Loss_r: 5.928e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 5.901e-05, Loss_0: 1.226e-07, Loss_r: 5.888e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 5.861e-05, Loss_0: 1.201e-07, Loss_r: 5.849e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 5.822e-05, Loss_0: 1.177e-07, Loss_r: 5.810e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 5.783e-05, Loss_0: 1.151e-07, Loss_r: 5.772e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 5.745e-05, Loss_0: 1.130e-07, Loss_r: 5.734e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 5.707e-05, Loss_0: 1.110e-07, Loss_r: 5.696e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 5.670e-05, Loss_0: 1.087e-07, Loss_r: 5.659e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 5.632e-05, Loss_0: 1.067e-07, Loss_r: 5.622e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 5.595e-05, Loss_0: 1.047e-07, Loss_r: 5.585e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 5.558e-05, Loss_0: 1.023e-07, Loss_r: 5.548e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 5.523e-05, Loss_0: 9.591e-08, Loss_r: 5.513e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 5.628e-05, Loss_0: 5.052e-08, Loss_r: 5.623e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 4.128e-04, Loss_0: 1.333e-06, Loss_r: 4.114e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 5.170e-03, Loss_0: 3.449e-05, Loss_r: 5.136e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.725e-04, Loss_0: 1.423e-06, Loss_r: 4.711e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 8.843e-04, Loss_0: 1.338e-05, Loss_r: 8.709e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 6.478e-05, Loss_0: 7.178e-09, Loss_r: 6.477e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.711e-04, Loss_0: 5.557e-08, Loss_r: 1.711e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 6.908e-05, Loss_0: 6.267e-07, Loss_r: 6.846e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 6.620e-05, Loss_0: 2.898e-07, Loss_r: 6.591e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 5.878e-05, Loss_0: 1.264e-07, Loss_r: 5.866e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 5.713e-05, Loss_0: 7.214e-08, Loss_r: 5.706e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 5.519e-05, Loss_0: 1.579e-07, Loss_r: 5.503e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 5.481e-05, Loss_0: 1.744e-07, Loss_r: 5.463e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 5.410e-05, Loss_0: 1.365e-07, Loss_r: 5.396e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 5.356e-05, Loss_0: 1.289e-07, Loss_r: 5.343e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 5.310e-05, Loss_0: 1.134e-07, Loss_r: 5.299e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 5.266e-05, Loss_0: 1.133e-07, Loss_r: 5.254e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 5.222e-05, Loss_0: 1.083e-07, Loss_r: 5.212e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 5.181e-05, Loss_0: 1.067e-07, Loss_r: 5.170e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 5.140e-05, Loss_0: 1.034e-07, Loss_r: 5.129e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 5.100e-05, Loss_0: 1.014e-07, Loss_r: 5.090e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 5.060e-05, Loss_0: 9.876e-08, Loss_r: 5.050e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 5.022e-05, Loss_0: 9.646e-08, Loss_r: 5.012e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 4.983e-05, Loss_0: 9.425e-08, Loss_r: 4.974e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 4.946e-05, Loss_0: 9.208e-08, Loss_r: 4.936e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 4.908e-05, Loss_0: 8.990e-08, Loss_r: 4.899e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 4.871e-05, Loss_0: 8.809e-08, Loss_r: 4.863e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 4.835e-05, Loss_0: 8.625e-08, Loss_r: 4.826e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 4.799e-05, Loss_0: 8.454e-08, Loss_r: 4.790e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 4.763e-05, Loss_0: 8.276e-08, Loss_r: 4.754e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 4.727e-05, Loss_0: 8.129e-08, Loss_r: 4.719e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2370, Loss: 4.692e-05, Loss_0: 7.979e-08, Loss_r: 4.684e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2380, Loss: 4.657e-05, Loss_0: 7.816e-08, Loss_r: 4.649e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2390, Loss: 4.622e-05, Loss_0: 7.673e-08, Loss_r: 4.614e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2400, Loss: 4.587e-05, Loss_0: 7.557e-08, Loss_r: 4.579e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2410, Loss: 4.552e-05, Loss_0: 7.432e-08, Loss_r: 4.545e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2420, Loss: 4.518e-05, Loss_0: 7.318e-08, Loss_r: 4.511e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2430, Loss: 4.484e-05, Loss_0: 7.271e-08, Loss_r: 4.477e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2440, Loss: 4.454e-05, Loss_0: 7.958e-08, Loss_r: 4.446e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2450, Loss: 4.944e-05, Loss_0: 2.063e-07, Loss_r: 4.923e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 4.407e-05, Loss_0: 8.985e-08, Loss_r: 4.398e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 4.392e-05, Loss_0: 4.372e-08, Loss_r: 4.387e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 4.355e-05, Loss_0: 4.490e-08, Loss_r: 4.350e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 4.296e-05, Loss_0: 6.815e-08, Loss_r: 4.289e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 4.269e-05, Loss_0: 7.300e-08, Loss_r: 4.262e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 4.236e-05, Loss_0: 6.018e-08, Loss_r: 4.230e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 4.205e-05, Loss_0: 6.152e-08, Loss_r: 4.199e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 4.176e-05, Loss_0: 6.322e-08, Loss_r: 4.169e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 4.146e-05, Loss_0: 5.963e-08, Loss_r: 4.140e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 4.116e-05, Loss_0: 6.062e-08, Loss_r: 4.110e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2560, Loss: 4.086e-05, Loss_0: 5.870e-08, Loss_r: 4.080e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2570, Loss: 4.057e-05, Loss_0: 5.841e-08, Loss_r: 4.051e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2580, Loss: 4.027e-05, Loss_0: 5.759e-08, Loss_r: 4.021e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2590, Loss: 3.997e-05, Loss_0: 5.677e-08, Loss_r: 3.992e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2600, Loss: 3.968e-05, Loss_0: 5.586e-08, Loss_r: 3.962e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2610, Loss: 3.939e-05, Loss_0: 5.466e-08, Loss_r: 3.933e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2620, Loss: 3.910e-05, Loss_0: 5.162e-08, Loss_r: 3.905e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2630, Loss: 3.934e-05, Loss_0: 2.895e-08, Loss_r: 3.931e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2640, Loss: 1.583e-04, Loss_0: 4.803e-07, Loss_r: 1.578e-04, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 4.005e-05, Loss_0: 1.344e-08, Loss_r: 4.003e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 5.008e-05, Loss_0: 2.709e-07, Loss_r: 4.981e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 4.346e-05, Loss_0: 1.813e-07, Loss_r: 4.328e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 3.793e-05, Loss_0: 2.930e-08, Loss_r: 3.790e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 3.800e-05, Loss_0: 2.264e-08, Loss_r: 3.798e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 3.720e-05, Loss_0: 6.949e-08, Loss_r: 3.713e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 3.671e-05, Loss_0: 4.991e-08, Loss_r: 3.666e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 3.647e-05, Loss_0: 4.316e-08, Loss_r: 3.643e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 3.621e-05, Loss_0: 5.270e-08, Loss_r: 3.615e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 3.594e-05, Loss_0: 4.454e-08, Loss_r: 3.589e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2750, Loss: 3.567e-05, Loss_0: 4.813e-08, Loss_r: 3.563e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2760, Loss: 3.541e-05, Loss_0: 4.644e-08, Loss_r: 3.537e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2770, Loss: 3.516e-05, Loss_0: 4.480e-08, Loss_r: 3.511e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2780, Loss: 3.490e-05, Loss_0: 4.475e-08, Loss_r: 3.485e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2790, Loss: 3.464e-05, Loss_0: 4.468e-08, Loss_r: 3.460e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2800, Loss: 3.439e-05, Loss_0: 4.462e-08, Loss_r: 3.434e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2810, Loss: 3.414e-05, Loss_0: 4.685e-08, Loss_r: 3.409e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2820, Loss: 3.464e-05, Loss_0: 7.859e-08, Loss_r: 3.456e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2830, Loss: 1.785e-04, Loss_0: 1.453e-06, Loss_r: 1.770e-04, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 3.781e-05, Loss_0: 1.438e-07, Loss_r: 3.767e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 4.344e-05, Loss_0: 3.839e-09, Loss_r: 4.343e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 4.134e-05, Loss_0: 1.679e-09, Loss_r: 4.134e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 3.275e-05, Loss_0: 4.437e-08, Loss_r: 3.270e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 3.372e-05, Loss_0: 8.593e-08, Loss_r: 3.363e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 3.232e-05, Loss_0: 3.435e-08, Loss_r: 3.229e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 3.215e-05, Loss_0: 3.057e-08, Loss_r: 3.211e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 3.190e-05, Loss_0: 4.799e-08, Loss_r: 3.185e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 3.163e-05, Loss_0: 3.463e-08, Loss_r: 3.159e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 3.139e-05, Loss_0: 4.057e-08, Loss_r: 3.135e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 3.116e-05, Loss_0: 3.636e-08, Loss_r: 3.112e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2950, Loss: 3.094e-05, Loss_0: 3.870e-08, Loss_r: 3.090e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2960, Loss: 3.071e-05, Loss_0: 3.641e-08, Loss_r: 3.067e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2970, Loss: 3.049e-05, Loss_0: 3.609e-08, Loss_r: 3.045e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2980, Loss: 3.026e-05, Loss_0: 3.624e-08, Loss_r: 3.023e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2990, Loss: 3.004e-05, Loss_0: 3.598e-08, Loss_r: 3.001e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 3000, Loss: 2.982e-05, Loss_0: 3.632e-08, Loss_r: 2.979e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 3010, Loss: 2.965e-05, Loss_0: 4.199e-08, Loss_r: 2.960e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 3020, Loss: 3.460e-05, Loss_0: 1.424e-07, Loss_r: 3.446e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 2.948e-05, Loss_0: 5.315e-08, Loss_r: 2.943e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 2.923e-05, Loss_0: 2.024e-08, Loss_r: 2.921e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 2.910e-05, Loss_0: 1.848e-08, Loss_r: 2.908e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 2.859e-05, Loss_0: 3.225e-08, Loss_r: 2.856e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 2.844e-05, Loss_0: 3.936e-08, Loss_r: 2.840e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 2.820e-05, Loss_0: 3.186e-08, Loss_r: 2.817e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 2.801e-05, Loss_0: 3.029e-08, Loss_r: 2.798e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 2.781e-05, Loss_0: 3.313e-08, Loss_r: 2.778e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 2.761e-05, Loss_0: 3.079e-08, Loss_r: 2.758e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3120, Loss: 2.742e-05, Loss_0: 3.136e-08, Loss_r: 2.739e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3130, Loss: 2.723e-05, Loss_0: 3.048e-08, Loss_r: 2.719e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3140, Loss: 2.703e-05, Loss_0: 3.058e-08, Loss_r: 2.700e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3150, Loss: 2.684e-05, Loss_0: 2.989e-08, Loss_r: 2.681e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3160, Loss: 2.665e-05, Loss_0: 2.970e-08, Loss_r: 2.662e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3170, Loss: 2.646e-05, Loss_0: 2.958e-08, Loss_r: 2.643e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3180, Loss: 2.627e-05, Loss_0: 2.931e-08, Loss_r: 2.624e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3190, Loss: 2.608e-05, Loss_0: 2.978e-08, Loss_r: 2.605e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3200, Loss: 2.594e-05, Loss_0: 3.535e-08, Loss_r: 2.590e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3210, Loss: 3.116e-05, Loss_0: 1.321e-07, Loss_r: 3.103e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 2.590e-05, Loss_0: 4.760e-08, Loss_r: 2.585e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 2.556e-05, Loss_0: 1.649e-08, Loss_r: 2.554e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 2.551e-05, Loss_0: 1.369e-08, Loss_r: 2.550e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 2.503e-05, Loss_0: 2.487e-08, Loss_r: 2.500e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 2.489e-05, Loss_0: 3.242e-08, Loss_r: 2.486e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 2.469e-05, Loss_0: 2.684e-08, Loss_r: 2.466e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 2.452e-05, Loss_0: 2.429e-08, Loss_r: 2.450e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 2.435e-05, Loss_0: 2.699e-08, Loss_r: 2.433e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 2.419e-05, Loss_0: 2.529e-08, Loss_r: 2.416e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3310, Loss: 2.402e-05, Loss_0: 2.540e-08, Loss_r: 2.399e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3320, Loss: 2.385e-05, Loss_0: 2.505e-08, Loss_r: 2.383e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3330, Loss: 2.369e-05, Loss_0: 2.496e-08, Loss_r: 2.367e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3340, Loss: 2.353e-05, Loss_0: 2.442e-08, Loss_r: 2.350e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3350, Loss: 2.336e-05, Loss_0: 2.428e-08, Loss_r: 2.334e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3360, Loss: 2.320e-05, Loss_0: 2.407e-08, Loss_r: 2.318e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3370, Loss: 2.304e-05, Loss_0: 2.373e-08, Loss_r: 2.301e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3380, Loss: 2.287e-05, Loss_0: 2.355e-08, Loss_r: 2.285e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3390, Loss: 2.271e-05, Loss_0: 2.326e-08, Loss_r: 2.269e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3400, Loss: 2.255e-05, Loss_0: 2.299e-08, Loss_r: 2.253e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3410, Loss: 2.239e-05, Loss_0: 2.298e-08, Loss_r: 2.237e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3420, Loss: 2.226e-05, Loss_0: 2.669e-08, Loss_r: 2.223e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3430, Loss: 3.850e-05, Loss_0: 2.389e-07, Loss_r: 3.826e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 2.259e-05, Loss_0: 6.391e-09, Loss_r: 2.259e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 2.503e-05, Loss_0: 1.315e-11, Loss_r: 2.503e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 2.170e-05, Loss_0: 2.754e-08, Loss_r: 2.167e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 2.185e-05, Loss_0: 3.805e-08, Loss_r: 2.181e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 2.150e-05, Loss_0: 1.336e-08, Loss_r: 2.148e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 2.123e-05, Loss_0: 2.292e-08, Loss_r: 2.121e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 2.109e-05, Loss_0: 2.124e-08, Loss_r: 2.107e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 2.095e-05, Loss_0: 1.983e-08, Loss_r: 2.093e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 2.081e-05, Loss_0: 2.044e-08, Loss_r: 2.079e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 2.067e-05, Loss_0: 2.050e-08, Loss_r: 2.065e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 2.053e-05, Loss_0: 1.925e-08, Loss_r: 2.051e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 2.040e-05, Loss_0: 1.970e-08, Loss_r: 2.038e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3560, Loss: 2.026e-05, Loss_0: 1.971e-08, Loss_r: 2.024e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3570, Loss: 2.012e-05, Loss_0: 1.987e-08, Loss_r: 2.010e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3580, Loss: 2.000e-05, Loss_0: 2.140e-08, Loss_r: 1.997e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3590, Loss: 2.030e-05, Loss_0: 3.754e-08, Loss_r: 2.027e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3600, Loss: 6.792e-05, Loss_0: 5.134e-07, Loss_r: 6.741e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 2.355e-05, Loss_0: 9.113e-08, Loss_r: 2.346e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 2.059e-05, Loss_0: 2.231e-09, Loss_r: 2.059e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 2.212e-05, Loss_0: 2.282e-11, Loss_r: 2.212e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 1.952e-05, Loss_0: 8.074e-09, Loss_r: 1.951e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 1.935e-05, Loss_0: 3.000e-08, Loss_r: 1.932e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 1.905e-05, Loss_0: 2.275e-08, Loss_r: 1.903e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 1.893e-05, Loss_0: 1.301e-08, Loss_r: 1.892e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 1.877e-05, Loss_0: 1.791e-08, Loss_r: 1.875e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 1.865e-05, Loss_0: 1.798e-08, Loss_r: 1.864e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 1.854e-05, Loss_0: 1.594e-08, Loss_r: 1.852e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 1.842e-05, Loss_0: 1.734e-08, Loss_r: 1.840e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 1.830e-05, Loss_0: 1.609e-08, Loss_r: 1.828e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 1.818e-05, Loss_0: 1.650e-08, Loss_r: 1.817e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 1.807e-05, Loss_0: 1.625e-08, Loss_r: 1.805e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3750, Loss: 1.795e-05, Loss_0: 1.586e-08, Loss_r: 1.794e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3760, Loss: 1.784e-05, Loss_0: 1.566e-08, Loss_r: 1.782e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3770, Loss: 1.772e-05, Loss_0: 1.564e-08, Loss_r: 1.771e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3780, Loss: 1.761e-05, Loss_0: 1.528e-08, Loss_r: 1.760e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3790, Loss: 1.750e-05, Loss_0: 1.405e-08, Loss_r: 1.749e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3800, Loss: 1.777e-05, Loss_0: 5.040e-09, Loss_r: 1.776e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3810, Loss: 9.365e-05, Loss_0: 3.809e-07, Loss_r: 9.327e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 1.985e-05, Loss_0: 2.608e-10, Loss_r: 1.984e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 2.174e-05, Loss_0: 9.209e-08, Loss_r: 2.165e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 2.159e-05, Loss_0: 9.082e-08, Loss_r: 2.150e-05, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 1.691e-05, Loss_0: 1.800e-08, Loss_r: 1.690e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 1.739e-05, Loss_0: 2.998e-09, Loss_r: 1.739e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 1.669e-05, Loss_0: 1.363e-08, Loss_r: 1.668e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 1.667e-05, Loss_0: 2.016e-08, Loss_r: 1.665e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 1.651e-05, Loss_0: 1.116e-08, Loss_r: 1.650e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 1.639e-05, Loss_0: 1.437e-08, Loss_r: 1.638e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 1.630e-05, Loss_0: 1.373e-08, Loss_r: 1.628e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 1.620e-05, Loss_0: 1.337e-08, Loss_r: 1.619e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 1.610e-05, Loss_0: 1.325e-08, Loss_r: 1.609e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3940, Loss: 1.601e-05, Loss_0: 1.342e-08, Loss_r: 1.599e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3950, Loss: 1.591e-05, Loss_0: 1.286e-08, Loss_r: 1.590e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3960, Loss: 1.581e-05, Loss_0: 1.288e-08, Loss_r: 1.580e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3970, Loss: 1.572e-05, Loss_0: 1.277e-08, Loss_r: 1.571e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3980, Loss: 1.562e-05, Loss_0: 1.264e-08, Loss_r: 1.561e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3990, Loss: 1.553e-05, Loss_0: 1.262e-08, Loss_r: 1.552e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 4000, Loss: 1.544e-05, Loss_0: 1.347e-08, Loss_r: 1.543e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 4010, Loss: 1.572e-05, Loss_0: 2.637e-08, Loss_r: 1.570e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 4020, Loss: 1.029e-04, Loss_0: 7.973e-07, Loss_r: 1.021e-04, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 1.795e-05, Loss_0: 6.239e-08, Loss_r: 1.789e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 2.040e-05, Loss_0: 7.228e-09, Loss_r: 2.040e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 2.037e-05, Loss_0: 7.465e-09, Loss_r: 2.036e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 1.501e-05, Loss_0: 7.219e-09, Loss_r: 1.500e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 1.550e-05, Loss_0: 3.106e-08, Loss_r: 1.547e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 1.478e-05, Loss_0: 1.354e-08, Loss_r: 1.477e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 1.478e-05, Loss_0: 6.496e-09, Loss_r: 1.478e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 1.462e-05, Loss_0: 1.355e-08, Loss_r: 1.461e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 1.452e-05, Loss_0: 1.118e-08, Loss_r: 1.451e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 1.444e-05, Loss_0: 1.044e-08, Loss_r: 1.443e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 1.436e-05, Loss_0: 1.135e-08, Loss_r: 1.435e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4140, Loss: 1.428e-05, Loss_0: 1.062e-08, Loss_r: 1.427e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4150, Loss: 1.420e-05, Loss_0: 1.069e-08, Loss_r: 1.419e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4160, Loss: 1.412e-05, Loss_0: 1.071e-08, Loss_r: 1.411e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4170, Loss: 1.404e-05, Loss_0: 1.038e-08, Loss_r: 1.403e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4180, Loss: 1.397e-05, Loss_0: 1.024e-08, Loss_r: 1.395e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4190, Loss: 1.389e-05, Loss_0: 1.010e-08, Loss_r: 1.388e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4200, Loss: 1.381e-05, Loss_0: 9.863e-09, Loss_r: 1.380e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4210, Loss: 1.374e-05, Loss_0: 8.351e-09, Loss_r: 1.373e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4220, Loss: 1.460e-05, Loss_0: 2.994e-10, Loss_r: 1.460e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 1.366e-05, Loss_0: 5.665e-09, Loss_r: 1.366e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 1.354e-05, Loss_0: 1.251e-08, Loss_r: 1.353e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 1.350e-05, Loss_0: 1.395e-08, Loss_r: 1.349e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 1.338e-05, Loss_0: 1.058e-08, Loss_r: 1.337e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 1.331e-05, Loss_0: 8.281e-09, Loss_r: 1.330e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 1.324e-05, Loss_0: 8.984e-09, Loss_r: 1.323e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 1.317e-05, Loss_0: 9.800e-09, Loss_r: 1.316e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 1.310e-05, Loss_0: 8.982e-09, Loss_r: 1.309e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 1.304e-05, Loss_0: 9.049e-09, Loss_r: 1.303e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 1.297e-05, Loss_0: 9.068e-09, Loss_r: 1.296e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 1.290e-05, Loss_0: 8.834e-09, Loss_r: 1.289e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4340, Loss: 1.284e-05, Loss_0: 8.843e-09, Loss_r: 1.283e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4350, Loss: 1.277e-05, Loss_0: 8.709e-09, Loss_r: 1.276e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4360, Loss: 1.270e-05, Loss_0: 8.576e-09, Loss_r: 1.269e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4370, Loss: 1.264e-05, Loss_0: 8.536e-09, Loss_r: 1.263e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4380, Loss: 1.257e-05, Loss_0: 8.425e-09, Loss_r: 1.256e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4390, Loss: 1.250e-05, Loss_0: 8.266e-09, Loss_r: 1.250e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4400, Loss: 1.244e-05, Loss_0: 7.946e-09, Loss_r: 1.243e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4410, Loss: 1.240e-05, Loss_0: 6.110e-09, Loss_r: 1.239e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4420, Loss: 1.419e-05, Loss_0: 6.514e-10, Loss_r: 1.419e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 1.242e-05, Loss_0: 3.008e-09, Loss_r: 1.242e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 1.223e-05, Loss_0: 1.118e-08, Loss_r: 1.222e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 1.224e-05, Loss_0: 1.356e-08, Loss_r: 1.223e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 1.209e-05, Loss_0: 9.424e-09, Loss_r: 1.208e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 1.203e-05, Loss_0: 6.421e-09, Loss_r: 1.202e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 1.197e-05, Loss_0: 7.073e-09, Loss_r: 1.196e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 1.191e-05, Loss_0: 8.293e-09, Loss_r: 1.190e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 1.185e-05, Loss_0: 7.421e-09, Loss_r: 1.184e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 1.180e-05, Loss_0: 7.355e-09, Loss_r: 1.179e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 1.174e-05, Loss_0: 7.541e-09, Loss_r: 1.173e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4530, Loss: 1.168e-05, Loss_0: 7.245e-09, Loss_r: 1.168e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4540, Loss: 1.163e-05, Loss_0: 7.311e-09, Loss_r: 1.162e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4550, Loss: 1.157e-05, Loss_0: 7.141e-09, Loss_r: 1.156e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4560, Loss: 1.152e-05, Loss_0: 7.139e-09, Loss_r: 1.151e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4570, Loss: 1.146e-05, Loss_0: 7.038e-09, Loss_r: 1.145e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4580, Loss: 1.141e-05, Loss_0: 6.960e-09, Loss_r: 1.140e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4590, Loss: 1.135e-05, Loss_0: 6.903e-09, Loss_r: 1.134e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4600, Loss: 1.130e-05, Loss_0: 6.844e-09, Loss_r: 1.129e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4610, Loss: 1.124e-05, Loss_0: 6.749e-09, Loss_r: 1.123e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4620, Loss: 1.119e-05, Loss_0: 6.886e-09, Loss_r: 1.118e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4630, Loss: 1.115e-05, Loss_0: 8.620e-09, Loss_r: 1.114e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4640, Loss: 1.655e-05, Loss_0: 7.745e-08, Loss_r: 1.647e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 1.104e-05, Loss_0: 7.615e-09, Loss_r: 1.103e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 1.178e-05, Loss_0: 2.480e-11, Loss_r: 1.178e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 1.108e-05, Loss_0: 2.308e-09, Loss_r: 1.107e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 1.097e-05, Loss_0: 1.074e-08, Loss_r: 1.096e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 1.085e-05, Loss_0: 7.668e-09, Loss_r: 1.084e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 1.081e-05, Loss_0: 4.565e-09, Loss_r: 1.081e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 1.075e-05, Loss_0: 6.837e-09, Loss_r: 1.074e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 1.070e-05, Loss_0: 6.058e-09, Loss_r: 1.069e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 1.065e-05, Loss_0: 5.954e-09, Loss_r: 1.065e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4740, Loss: 1.060e-05, Loss_0: 6.053e-09, Loss_r: 1.060e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4750, Loss: 1.056e-05, Loss_0: 5.931e-09, Loss_r: 1.055e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4760, Loss: 1.051e-05, Loss_0: 5.855e-09, Loss_r: 1.050e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4770, Loss: 1.046e-05, Loss_0: 5.819e-09, Loss_r: 1.046e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4780, Loss: 1.042e-05, Loss_0: 5.779e-09, Loss_r: 1.041e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4790, Loss: 1.037e-05, Loss_0: 5.730e-09, Loss_r: 1.037e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4800, Loss: 1.033e-05, Loss_0: 5.628e-09, Loss_r: 1.032e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4810, Loss: 1.028e-05, Loss_0: 5.598e-09, Loss_r: 1.027e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4820, Loss: 1.023e-05, Loss_0: 5.230e-09, Loss_r: 1.023e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4830, Loss: 1.026e-05, Loss_0: 2.689e-09, Loss_r: 1.025e-05, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4840, Loss: 2.575e-05, Loss_0: 6.826e-08, Loss_r: 2.568e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 1.039e-05, Loss_0: 8.086e-10, Loss_r: 1.039e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 1.149e-05, Loss_0: 3.036e-08, Loss_r: 1.146e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 1.084e-05, Loss_0: 2.231e-08, Loss_r: 1.082e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 1.000e-05, Loss_0: 3.601e-09, Loss_r: 1.000e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 1.006e-05, Loss_0: 1.828e-09, Loss_r: 1.006e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 9.914e-06, Loss_0: 6.647e-09, Loss_r: 9.907e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 9.866e-06, Loss_0: 5.973e-09, Loss_r: 9.860e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 9.828e-06, Loss_0: 4.204e-09, Loss_r: 9.824e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 9.784e-06, Loss_0: 5.592e-09, Loss_r: 9.778e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 9.742e-06, Loss_0: 4.725e-09, Loss_r: 9.738e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4950, Loss: 9.702e-06, Loss_0: 5.149e-09, Loss_r: 9.697e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4960, Loss: 9.663e-06, Loss_0: 4.714e-09, Loss_r: 9.658e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4970, Loss: 9.623e-06, Loss_0: 4.989e-09, Loss_r: 9.618e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4980, Loss: 9.584e-06, Loss_0: 4.788e-09, Loss_r: 9.579e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4990, Loss: 9.544e-06, Loss_0: 4.687e-09, Loss_r: 9.540e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "Training time: 17.9882\n",
            "[1, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.361e-01, Loss_0: 2.013e-03, Loss_r: 2.340e-01, Time: 0.76, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.407e-01, Loss_0: 2.155e-02, Loss_r: 1.191e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.253e-01, Loss_0: 4.516e-02, Loss_r: 8.019e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.243e-01, Loss_0: 7.122e-02, Loss_r: 5.304e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.202e-01, Loss_0: 5.523e-02, Loss_r: 6.497e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.177e-01, Loss_0: 5.626e-02, Loss_r: 6.141e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.172e-01, Loss_0: 5.321e-02, Loss_r: 6.403e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.173e-01, Loss_0: 5.371e-02, Loss_r: 6.360e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.172e-01, Loss_0: 5.614e-02, Loss_r: 6.109e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.172e-01, Loss_0: 5.547e-02, Loss_r: 6.171e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.172e-01, Loss_0: 5.465e-02, Loss_r: 6.252e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.172e-01, Loss_0: 5.487e-02, Loss_r: 6.229e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.171e-01, Loss_0: 5.513e-02, Loss_r: 6.201e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.171e-01, Loss_0: 5.507e-02, Loss_r: 6.206e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.171e-01, Loss_0: 5.495e-02, Loss_r: 6.216e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.171e-01, Loss_0: 5.495e-02, Loss_r: 6.214e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.171e-01, Loss_0: 5.497e-02, Loss_r: 6.210e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.170e-01, Loss_0: 5.496e-02, Loss_r: 6.208e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.170e-01, Loss_0: 5.494e-02, Loss_r: 6.208e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.170e-01, Loss_0: 5.492e-02, Loss_r: 6.207e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.170e-01, Loss_0: 5.491e-02, Loss_r: 6.205e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.169e-01, Loss_0: 5.489e-02, Loss_r: 6.204e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.169e-01, Loss_0: 5.486e-02, Loss_r: 6.202e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.168e-01, Loss_0: 5.484e-02, Loss_r: 6.199e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.168e-01, Loss_0: 5.482e-02, Loss_r: 6.196e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.167e-01, Loss_0: 5.478e-02, Loss_r: 6.193e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.166e-01, Loss_0: 5.475e-02, Loss_r: 6.188e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.165e-01, Loss_0: 5.471e-02, Loss_r: 6.183e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.164e-01, Loss_0: 5.466e-02, Loss_r: 6.177e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.163e-01, Loss_0: 5.459e-02, Loss_r: 6.168e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.161e-01, Loss_0: 5.451e-02, Loss_r: 6.158e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.158e-01, Loss_0: 5.441e-02, Loss_r: 6.143e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.155e-01, Loss_0: 5.427e-02, Loss_r: 6.124e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.150e-01, Loss_0: 5.408e-02, Loss_r: 6.096e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.143e-01, Loss_0: 5.379e-02, Loss_r: 6.053e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.132e-01, Loss_0: 5.334e-02, Loss_r: 5.983e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.111e-01, Loss_0: 5.257e-02, Loss_r: 5.856e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.070e-01, Loss_0: 5.108e-02, Loss_r: 5.596e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 380, Loss: 9.812e-02, Loss_0: 4.788e-02, Loss_r: 5.024e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 8.013e-02, Loss_0: 4.007e-02, Loss_r: 4.007e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.604e-02, Loss_0: 2.169e-02, Loss_r: 3.435e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.565e-01, Loss_0: 5.922e-03, Loss_r: 1.506e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 8.874e-02, Loss_0: 1.710e-02, Loss_r: 7.165e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.366e-02, Loss_0: 1.311e-02, Loss_r: 4.055e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 440, Loss: 4.863e-02, Loss_0: 1.334e-02, Loss_r: 3.530e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.366e-02, Loss_0: 8.770e-03, Loss_r: 3.489e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 3.821e-02, Loss_0: 7.911e-03, Loss_r: 3.030e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 470, Loss: 3.340e-02, Loss_0: 5.676e-03, Loss_r: 2.773e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.890e-02, Loss_0: 4.886e-03, Loss_r: 2.401e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 490, Loss: 2.463e-02, Loss_0: 3.793e-03, Loss_r: 2.084e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.050e-02, Loss_0: 2.848e-03, Loss_r: 1.765e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.657e-02, Loss_0: 2.139e-03, Loss_r: 1.443e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.302e-02, Loss_0: 1.535e-03, Loss_r: 1.148e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.004e-02, Loss_0: 1.077e-03, Loss_r: 8.967e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 7.768e-03, Loss_0: 7.522e-04, Loss_r: 7.016e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 6.165e-03, Loss_0: 5.302e-04, Loss_r: 5.635e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.082e-03, Loss_0: 3.810e-04, Loss_r: 4.701e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 4.323e-03, Loss_0: 2.819e-04, Loss_r: 4.041e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 3.736e-03, Loss_0: 2.146e-04, Loss_r: 3.521e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.240e-03, Loss_0: 1.675e-04, Loss_r: 3.072e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 2.804e-03, Loss_0: 1.333e-04, Loss_r: 2.670e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 4.084e-03, Loss_0: 1.195e-04, Loss_r: 3.965e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 9.925e-03, Loss_0: 9.999e-05, Loss_r: 9.825e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 5.029e-03, Loss_0: 9.619e-05, Loss_r: 4.933e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.224e-03, Loss_0: 6.026e-05, Loss_r: 2.164e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.712e-03, Loss_0: 7.748e-05, Loss_r: 1.634e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.500e-03, Loss_0: 5.623e-05, Loss_r: 1.444e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.313e-03, Loss_0: 4.521e-05, Loss_r: 1.268e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.158e-03, Loss_0: 3.730e-05, Loss_r: 1.120e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 690, Loss: 9.754e-04, Loss_0: 2.965e-05, Loss_r: 9.458e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 8.425e-04, Loss_0: 2.583e-05, Loss_r: 8.167e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 7.258e-04, Loss_0: 2.051e-05, Loss_r: 7.053e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 6.261e-04, Loss_0: 1.706e-05, Loss_r: 6.091e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.400e-04, Loss_0: 1.401e-05, Loss_r: 5.260e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 4.666e-04, Loss_0: 1.175e-05, Loss_r: 4.549e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 4.043e-04, Loss_0: 9.829e-06, Loss_r: 3.945e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 3.971e-04, Loss_0: 9.262e-06, Loss_r: 3.878e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.094e-02, Loss_0: 2.739e-05, Loss_r: 4.091e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.749e-03, Loss_0: 1.966e-05, Loss_r: 3.730e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.354e-03, Loss_0: 1.150e-05, Loss_r: 1.343e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.118e-03, Loss_0: 1.136e-05, Loss_r: 1.107e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 7.755e-04, Loss_0: 3.475e-06, Loss_r: 7.720e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.693e-04, Loss_0: 1.289e-05, Loss_r: 3.564e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 3.783e-04, Loss_0: 7.945e-06, Loss_r: 3.703e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.888e-04, Loss_0: 6.456e-06, Loss_r: 2.824e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.544e-04, Loss_0: 5.742e-06, Loss_r: 2.487e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.268e-04, Loss_0: 4.834e-06, Loss_r: 2.219e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.016e-04, Loss_0: 4.064e-06, Loss_r: 1.975e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.809e-04, Loss_0: 3.726e-06, Loss_r: 1.772e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.638e-04, Loss_0: 3.032e-06, Loss_r: 1.607e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.494e-04, Loss_0: 2.718e-06, Loss_r: 1.467e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.373e-04, Loss_0: 2.391e-06, Loss_r: 1.349e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.271e-04, Loss_0: 2.083e-06, Loss_r: 1.251e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.185e-04, Loss_0: 1.848e-06, Loss_r: 1.167e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.112e-04, Loss_0: 1.649e-06, Loss_r: 1.095e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.049e-04, Loss_0: 1.476e-06, Loss_r: 1.034e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 9.952e-05, Loss_0: 1.325e-06, Loss_r: 9.820e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 9.488e-05, Loss_0: 1.197e-06, Loss_r: 9.369e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 9.085e-05, Loss_0: 1.087e-06, Loss_r: 8.977e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 8.733e-05, Loss_0: 9.914e-07, Loss_r: 8.634e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 8.423e-05, Loss_0: 9.085e-07, Loss_r: 8.332e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 8.147e-05, Loss_0: 8.357e-07, Loss_r: 8.064e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 7.900e-05, Loss_0: 7.719e-07, Loss_r: 7.823e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 7.677e-05, Loss_0: 7.157e-07, Loss_r: 7.606e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 7.474e-05, Loss_0: 6.659e-07, Loss_r: 7.408e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 7.288e-05, Loss_0: 6.218e-07, Loss_r: 7.226e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 7.116e-05, Loss_0: 5.823e-07, Loss_r: 7.058e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 6.956e-05, Loss_0: 5.472e-07, Loss_r: 6.901e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 6.806e-05, Loss_0: 5.157e-07, Loss_r: 6.754e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 6.665e-05, Loss_0: 4.877e-07, Loss_r: 6.616e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 6.535e-05, Loss_0: 4.706e-07, Loss_r: 6.488e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 8.483e-05, Loss_0: 6.681e-07, Loss_r: 8.416e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.142e-02, Loss_0: 1.835e-05, Loss_r: 2.141e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.135e-02, Loss_0: 3.106e-05, Loss_r: 1.132e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.299e-03, Loss_0: 3.904e-06, Loss_r: 3.295e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 9.291e-04, Loss_0: 1.527e-06, Loss_r: 9.276e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.538e-04, Loss_0: 7.672e-07, Loss_r: 1.530e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.317e-04, Loss_0: 3.077e-06, Loss_r: 2.287e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.831e-04, Loss_0: 3.676e-06, Loss_r: 1.794e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.197e-04, Loss_0: 2.371e-06, Loss_r: 1.173e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.156e-04, Loss_0: 1.660e-06, Loss_r: 1.139e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.015e-04, Loss_0: 1.524e-06, Loss_r: 1.000e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 9.508e-05, Loss_0: 1.437e-06, Loss_r: 9.364e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 8.846e-05, Loss_0: 1.142e-06, Loss_r: 8.732e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 8.369e-05, Loss_0: 9.825e-07, Loss_r: 8.271e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 7.959e-05, Loss_0: 9.086e-07, Loss_r: 7.869e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 7.610e-05, Loss_0: 7.918e-07, Loss_r: 7.530e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 7.311e-05, Loss_0: 7.239e-07, Loss_r: 7.239e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 7.052e-05, Loss_0: 6.557e-07, Loss_r: 6.986e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 6.824e-05, Loss_0: 6.005e-07, Loss_r: 6.764e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 6.621e-05, Loss_0: 5.517e-07, Loss_r: 6.566e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 6.439e-05, Loss_0: 5.105e-07, Loss_r: 6.388e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 6.273e-05, Loss_0: 4.734e-07, Loss_r: 6.225e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 6.120e-05, Loss_0: 4.418e-07, Loss_r: 6.076e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 5.979e-05, Loss_0: 4.131e-07, Loss_r: 5.938e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 5.847e-05, Loss_0: 3.881e-07, Loss_r: 5.808e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 5.723e-05, Loss_0: 3.658e-07, Loss_r: 5.686e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 5.605e-05, Loss_0: 3.457e-07, Loss_r: 5.570e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 5.493e-05, Loss_0: 3.275e-07, Loss_r: 5.460e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 5.386e-05, Loss_0: 3.110e-07, Loss_r: 5.355e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 5.284e-05, Loss_0: 2.960e-07, Loss_r: 5.254e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 5.186e-05, Loss_0: 2.823e-07, Loss_r: 5.157e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 5.091e-05, Loss_0: 2.697e-07, Loss_r: 5.064e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 4.999e-05, Loss_0: 2.581e-07, Loss_r: 4.973e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 4.911e-05, Loss_0: 2.474e-07, Loss_r: 4.886e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 4.825e-05, Loss_0: 2.371e-07, Loss_r: 4.801e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 4.742e-05, Loss_0: 2.279e-07, Loss_r: 4.719e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 4.661e-05, Loss_0: 2.192e-07, Loss_r: 4.640e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 4.583e-05, Loss_0: 2.109e-07, Loss_r: 4.562e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 4.507e-05, Loss_0: 2.033e-07, Loss_r: 4.487e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 4.433e-05, Loss_0: 1.959e-07, Loss_r: 4.414e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 4.361e-05, Loss_0: 1.891e-07, Loss_r: 4.342e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 4.291e-05, Loss_0: 1.825e-07, Loss_r: 4.273e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 4.223e-05, Loss_0: 1.763e-07, Loss_r: 4.205e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 4.157e-05, Loss_0: 1.704e-07, Loss_r: 4.140e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.092e-05, Loss_0: 1.648e-07, Loss_r: 4.075e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.029e-05, Loss_0: 1.596e-07, Loss_r: 4.013e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 3.967e-05, Loss_0: 1.544e-07, Loss_r: 3.952e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 3.907e-05, Loss_0: 1.493e-07, Loss_r: 3.892e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 3.848e-05, Loss_0: 1.446e-07, Loss_r: 3.834e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 3.791e-05, Loss_0: 1.396e-07, Loss_r: 3.777e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.739e-05, Loss_0: 1.295e-07, Loss_r: 3.726e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.459e-05, Loss_0: 5.783e-08, Loss_r: 4.453e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.382e-03, Loss_0: 5.734e-06, Loss_r: 3.377e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.284e-02, Loss_0: 1.351e-05, Loss_r: 1.283e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.750e-03, Loss_0: 1.245e-05, Loss_r: 1.738e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.225e-03, Loss_0: 1.538e-05, Loss_r: 1.209e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.384e-04, Loss_0: 9.479e-07, Loss_r: 2.374e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.124e-04, Loss_0: 2.160e-08, Loss_r: 1.124e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.247e-04, Loss_0: 1.783e-06, Loss_r: 1.230e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 7.791e-05, Loss_0: 7.063e-07, Loss_r: 7.720e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 6.228e-05, Loss_0: 5.179e-07, Loss_r: 6.176e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 5.731e-05, Loss_0: 5.644e-07, Loss_r: 5.675e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 5.349e-05, Loss_0: 4.579e-07, Loss_r: 5.303e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 5.033e-05, Loss_0: 3.911e-07, Loss_r: 4.994e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 4.783e-05, Loss_0: 3.517e-07, Loss_r: 4.748e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 4.584e-05, Loss_0: 3.121e-07, Loss_r: 4.553e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 4.421e-05, Loss_0: 2.805e-07, Loss_r: 4.393e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 4.283e-05, Loss_0: 2.531e-07, Loss_r: 4.258e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 4.163e-05, Loss_0: 2.332e-07, Loss_r: 4.140e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 4.057e-05, Loss_0: 2.147e-07, Loss_r: 4.036e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 3.961e-05, Loss_0: 1.974e-07, Loss_r: 3.942e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 3.873e-05, Loss_0: 1.858e-07, Loss_r: 3.855e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 3.792e-05, Loss_0: 1.734e-07, Loss_r: 3.774e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 3.715e-05, Loss_0: 1.632e-07, Loss_r: 3.699e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.642e-05, Loss_0: 1.539e-07, Loss_r: 3.627e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 3.572e-05, Loss_0: 1.460e-07, Loss_r: 3.558e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 3.505e-05, Loss_0: 1.388e-07, Loss_r: 3.491e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 3.441e-05, Loss_0: 1.322e-07, Loss_r: 3.428e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 3.379e-05, Loss_0: 1.262e-07, Loss_r: 3.366e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 3.318e-05, Loss_0: 1.208e-07, Loss_r: 3.306e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.260e-05, Loss_0: 1.157e-07, Loss_r: 3.248e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 3.203e-05, Loss_0: 1.110e-07, Loss_r: 3.192e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.148e-05, Loss_0: 1.066e-07, Loss_r: 3.137e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 3.094e-05, Loss_0: 1.025e-07, Loss_r: 3.084e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 3.042e-05, Loss_0: 9.875e-08, Loss_r: 3.032e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 2.991e-05, Loss_0: 9.514e-08, Loss_r: 2.982e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 2.942e-05, Loss_0: 9.175e-08, Loss_r: 2.932e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 2.893e-05, Loss_0: 8.847e-08, Loss_r: 2.884e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 2.846e-05, Loss_0: 8.548e-08, Loss_r: 2.837e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.800e-05, Loss_0: 8.244e-08, Loss_r: 2.792e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.755e-05, Loss_0: 7.979e-08, Loss_r: 2.747e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.711e-05, Loss_0: 7.700e-08, Loss_r: 2.703e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.668e-05, Loss_0: 7.448e-08, Loss_r: 2.660e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.626e-05, Loss_0: 7.209e-08, Loss_r: 2.618e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.584e-05, Loss_0: 6.980e-08, Loss_r: 2.577e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.544e-05, Loss_0: 6.749e-08, Loss_r: 2.537e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.505e-05, Loss_0: 6.521e-08, Loss_r: 2.498e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.466e-05, Loss_0: 6.267e-08, Loss_r: 2.460e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 2.432e-05, Loss_0: 5.598e-08, Loss_r: 2.426e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 2.972e-05, Loss_0: 1.120e-08, Loss_r: 2.971e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 2.371e-05, Loss_0: 4.856e-08, Loss_r: 2.366e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 2.386e-05, Loss_0: 7.904e-08, Loss_r: 2.378e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 2.319e-05, Loss_0: 6.862e-08, Loss_r: 2.312e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.269e-05, Loss_0: 4.755e-08, Loss_r: 2.264e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 2.235e-05, Loss_0: 4.841e-08, Loss_r: 2.230e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 2.205e-05, Loss_0: 5.395e-08, Loss_r: 2.200e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 2.174e-05, Loss_0: 4.742e-08, Loss_r: 2.169e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.145e-05, Loss_0: 4.842e-08, Loss_r: 2.140e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 2.116e-05, Loss_0: 4.601e-08, Loss_r: 2.111e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 2.088e-05, Loss_0: 4.565e-08, Loss_r: 2.083e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 2.060e-05, Loss_0: 4.381e-08, Loss_r: 2.056e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 2.033e-05, Loss_0: 4.270e-08, Loss_r: 2.028e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 2.006e-05, Loss_0: 4.186e-08, Loss_r: 2.002e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.979e-05, Loss_0: 4.105e-08, Loss_r: 1.975e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.954e-05, Loss_0: 4.173e-08, Loss_r: 1.950e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.999e-05, Loss_0: 5.965e-08, Loss_r: 1.993e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.589e-04, Loss_0: 7.284e-07, Loss_r: 1.582e-04, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2280, Loss: 2.110e-05, Loss_0: 7.574e-08, Loss_r: 2.103e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 3.286e-05, Loss_0: 4.297e-10, Loss_r: 3.286e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 2.481e-05, Loss_0: 2.064e-09, Loss_r: 2.481e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 1.895e-05, Loss_0: 5.604e-08, Loss_r: 1.889e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 1.873e-05, Loss_0: 5.502e-08, Loss_r: 1.867e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 1.815e-05, Loss_0: 2.197e-08, Loss_r: 1.813e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 1.759e-05, Loss_0: 3.556e-08, Loss_r: 1.755e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 1.736e-05, Loss_0: 3.288e-08, Loss_r: 1.733e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 1.716e-05, Loss_0: 3.016e-08, Loss_r: 1.713e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 1.695e-05, Loss_0: 3.065e-08, Loss_r: 1.692e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 1.676e-05, Loss_0: 3.043e-08, Loss_r: 1.672e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 1.656e-05, Loss_0: 2.813e-08, Loss_r: 1.653e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.636e-05, Loss_0: 2.795e-08, Loss_r: 1.633e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.617e-05, Loss_0: 2.754e-08, Loss_r: 1.614e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.598e-05, Loss_0: 2.686e-08, Loss_r: 1.595e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.579e-05, Loss_0: 2.526e-08, Loss_r: 1.577e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.604e-05, Loss_0: 1.524e-08, Loss_r: 1.603e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 1.156e-04, Loss_0: 1.701e-07, Loss_r: 1.154e-04, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 1.609e-05, Loss_0: 1.103e-08, Loss_r: 1.608e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 2.722e-05, Loss_0: 1.255e-07, Loss_r: 2.709e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 1.872e-05, Loss_0: 6.966e-08, Loss_r: 1.865e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 1.577e-05, Loss_0: 9.898e-09, Loss_r: 1.576e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 1.500e-05, Loss_0: 1.442e-08, Loss_r: 1.498e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 1.483e-05, Loss_0: 3.347e-08, Loss_r: 1.479e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.439e-05, Loss_0: 1.917e-08, Loss_r: 1.437e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 1.420e-05, Loss_0: 2.229e-08, Loss_r: 1.418e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.405e-05, Loss_0: 2.147e-08, Loss_r: 1.403e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 1.390e-05, Loss_0: 2.125e-08, Loss_r: 1.388e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 1.375e-05, Loss_0: 1.986e-08, Loss_r: 1.373e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.361e-05, Loss_0: 2.070e-08, Loss_r: 1.359e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.346e-05, Loss_0: 2.003e-08, Loss_r: 1.344e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.332e-05, Loss_0: 1.937e-08, Loss_r: 1.330e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.318e-05, Loss_0: 1.886e-08, Loss_r: 1.316e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.304e-05, Loss_0: 1.839e-08, Loss_r: 1.302e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.291e-05, Loss_0: 1.681e-08, Loss_r: 1.289e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.436e-05, Loss_0: 3.864e-09, Loss_r: 1.436e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2640, Loss: 1.266e-05, Loss_0: 1.541e-08, Loss_r: 1.264e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 1.270e-05, Loss_0: 2.404e-08, Loss_r: 1.268e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 1.246e-05, Loss_0: 2.075e-08, Loss_r: 1.244e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 1.229e-05, Loss_0: 1.487e-08, Loss_r: 1.228e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 1.216e-05, Loss_0: 1.527e-08, Loss_r: 1.215e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.205e-05, Loss_0: 1.703e-08, Loss_r: 1.203e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.193e-05, Loss_0: 1.540e-08, Loss_r: 1.191e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 1.181e-05, Loss_0: 1.546e-08, Loss_r: 1.180e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.170e-05, Loss_0: 1.519e-08, Loss_r: 1.168e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 1.158e-05, Loss_0: 1.489e-08, Loss_r: 1.157e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.147e-05, Loss_0: 1.459e-08, Loss_r: 1.146e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.136e-05, Loss_0: 1.440e-08, Loss_r: 1.135e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.125e-05, Loss_0: 1.416e-08, Loss_r: 1.124e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.114e-05, Loss_0: 1.394e-08, Loss_r: 1.113e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.103e-05, Loss_0: 1.370e-08, Loss_r: 1.102e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.093e-05, Loss_0: 1.388e-08, Loss_r: 1.092e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.092e-05, Loss_0: 1.742e-08, Loss_r: 1.090e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 2.563e-05, Loss_0: 1.082e-07, Loss_r: 2.552e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 1.105e-05, Loss_0: 2.220e-08, Loss_r: 1.103e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 1.178e-05, Loss_0: 2.475e-09, Loss_r: 1.178e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 1.121e-05, Loss_0: 3.843e-09, Loss_r: 1.121e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 1.038e-05, Loss_0: 1.456e-08, Loss_r: 1.037e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 1.036e-05, Loss_0: 1.620e-08, Loss_r: 1.035e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 1.019e-05, Loss_0: 9.953e-09, Loss_r: 1.018e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 1.008e-05, Loss_0: 1.142e-08, Loss_r: 1.007e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 9.995e-06, Loss_0: 1.210e-08, Loss_r: 9.983e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 9.906e-06, Loss_0: 1.067e-08, Loss_r: 9.895e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 9.817e-06, Loss_0: 1.135e-08, Loss_r: 9.806e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 9.730e-06, Loss_0: 1.071e-08, Loss_r: 9.719e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 9.644e-06, Loss_0: 1.066e-08, Loss_r: 9.633e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 9.559e-06, Loss_0: 1.060e-08, Loss_r: 9.548e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 9.474e-06, Loss_0: 1.036e-08, Loss_r: 9.464e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 9.390e-06, Loss_0: 1.019e-08, Loss_r: 9.380e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 9.307e-06, Loss_0: 1.000e-08, Loss_r: 9.297e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 9.224e-06, Loss_0: 9.824e-09, Loss_r: 9.215e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 9.143e-06, Loss_0: 9.627e-09, Loss_r: 9.133e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 9.070e-06, Loss_0: 8.545e-09, Loss_r: 9.061e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.237e-05, Loss_0: 3.958e-11, Loss_r: 1.237e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 8.928e-06, Loss_0: 1.077e-08, Loss_r: 8.917e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 9.464e-06, Loss_0: 1.946e-08, Loss_r: 9.445e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 8.777e-06, Loss_0: 1.004e-08, Loss_r: 8.767e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 8.784e-06, Loss_0: 6.027e-09, Loss_r: 8.778e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 8.633e-06, Loss_0: 9.498e-09, Loss_r: 8.623e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 8.560e-06, Loss_0: 9.069e-09, Loss_r: 8.550e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 8.492e-06, Loss_0: 7.883e-09, Loss_r: 8.484e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 8.421e-06, Loss_0: 8.745e-09, Loss_r: 8.413e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 8.352e-06, Loss_0: 7.952e-09, Loss_r: 8.344e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 8.284e-06, Loss_0: 8.192e-09, Loss_r: 8.276e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 8.217e-06, Loss_0: 7.975e-09, Loss_r: 8.209e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 8.150e-06, Loss_0: 7.781e-09, Loss_r: 8.143e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 8.084e-06, Loss_0: 7.664e-09, Loss_r: 8.076e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 8.018e-06, Loss_0: 7.524e-09, Loss_r: 8.011e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 7.954e-06, Loss_0: 7.197e-09, Loss_r: 7.947e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 7.931e-06, Loss_0: 5.529e-09, Loss_r: 7.926e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.207e-05, Loss_0: 9.032e-10, Loss_r: 1.207e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3190, Loss: 8.071e-06, Loss_0: 2.906e-09, Loss_r: 8.068e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3200, Loss: 7.859e-06, Loss_0: 1.119e-08, Loss_r: 7.848e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 7.906e-06, Loss_0: 1.244e-08, Loss_r: 7.894e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 7.603e-06, Loss_0: 7.642e-09, Loss_r: 7.595e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 7.571e-06, Loss_0: 5.271e-09, Loss_r: 7.566e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 7.485e-06, Loss_0: 6.614e-09, Loss_r: 7.478e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 7.433e-06, Loss_0: 7.195e-09, Loss_r: 7.426e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 7.375e-06, Loss_0: 6.200e-09, Loss_r: 7.369e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 7.319e-06, Loss_0: 6.534e-09, Loss_r: 7.313e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 7.265e-06, Loss_0: 6.313e-09, Loss_r: 7.258e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 7.211e-06, Loss_0: 6.281e-09, Loss_r: 7.204e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 7.157e-06, Loss_0: 6.137e-09, Loss_r: 7.151e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 7.103e-06, Loss_0: 6.106e-09, Loss_r: 7.097e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 7.050e-06, Loss_0: 6.018e-09, Loss_r: 7.044e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 6.997e-06, Loss_0: 5.925e-09, Loss_r: 6.991e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 6.945e-06, Loss_0: 5.802e-09, Loss_r: 6.939e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 6.892e-06, Loss_0: 5.727e-09, Loss_r: 6.887e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 6.840e-06, Loss_0: 5.595e-09, Loss_r: 6.835e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 6.796e-06, Loss_0: 4.878e-09, Loss_r: 6.792e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 8.132e-06, Loss_0: 7.123e-11, Loss_r: 8.132e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3390, Loss: 6.719e-06, Loss_0: 4.168e-09, Loss_r: 6.715e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3400, Loss: 6.783e-06, Loss_0: 8.808e-09, Loss_r: 6.774e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3410, Loss: 6.666e-06, Loss_0: 7.579e-09, Loss_r: 6.658e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 6.562e-06, Loss_0: 4.621e-09, Loss_r: 6.557e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 6.519e-06, Loss_0: 4.475e-09, Loss_r: 6.515e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 6.469e-06, Loss_0: 5.541e-09, Loss_r: 6.464e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 6.422e-06, Loss_0: 5.029e-09, Loss_r: 6.417e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 6.378e-06, Loss_0: 4.846e-09, Loss_r: 6.373e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 6.334e-06, Loss_0: 5.018e-09, Loss_r: 6.329e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 6.290e-06, Loss_0: 4.785e-09, Loss_r: 6.285e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 6.246e-06, Loss_0: 4.804e-09, Loss_r: 6.242e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 6.203e-06, Loss_0: 4.714e-09, Loss_r: 6.198e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 6.160e-06, Loss_0: 4.638e-09, Loss_r: 6.155e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 6.117e-06, Loss_0: 4.587e-09, Loss_r: 6.112e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 6.075e-06, Loss_0: 4.512e-09, Loss_r: 6.070e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 6.032e-06, Loss_0: 4.397e-09, Loss_r: 6.028e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 5.993e-06, Loss_0: 4.038e-09, Loss_r: 5.989e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 6.122e-06, Loss_0: 1.825e-09, Loss_r: 6.120e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 2.988e-05, Loss_0: 4.330e-08, Loss_r: 2.984e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3580, Loss: 6.996e-06, Loss_0: 3.330e-11, Loss_r: 6.996e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3590, Loss: 7.174e-06, Loss_0: 1.679e-08, Loss_r: 7.157e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3600, Loss: 7.216e-06, Loss_0: 1.702e-08, Loss_r: 7.199e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3610, Loss: 5.764e-06, Loss_0: 4.413e-09, Loss_r: 5.760e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 5.921e-06, Loss_0: 1.551e-09, Loss_r: 5.919e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 5.691e-06, Loss_0: 4.314e-09, Loss_r: 5.687e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 5.671e-06, Loss_0: 4.973e-09, Loss_r: 5.666e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 5.626e-06, Loss_0: 3.281e-09, Loss_r: 5.623e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 5.582e-06, Loss_0: 4.158e-09, Loss_r: 5.578e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 5.545e-06, Loss_0: 3.716e-09, Loss_r: 5.541e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 5.509e-06, Loss_0: 3.852e-09, Loss_r: 5.505e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 5.474e-06, Loss_0: 3.647e-09, Loss_r: 5.470e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 5.438e-06, Loss_0: 3.759e-09, Loss_r: 5.434e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 5.403e-06, Loss_0: 3.651e-09, Loss_r: 5.399e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 5.368e-06, Loss_0: 3.561e-09, Loss_r: 5.364e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 5.333e-06, Loss_0: 3.505e-09, Loss_r: 5.329e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 5.298e-06, Loss_0: 3.345e-09, Loss_r: 5.295e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 5.278e-06, Loss_0: 2.712e-09, Loss_r: 5.275e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 6.318e-06, Loss_0: 2.083e-13, Loss_r: 6.318e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3770, Loss: 5.308e-06, Loss_0: 1.565e-09, Loss_r: 5.306e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3780, Loss: 5.184e-06, Loss_0: 4.237e-09, Loss_r: 5.179e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3790, Loss: 5.196e-06, Loss_0: 5.037e-09, Loss_r: 5.191e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 5.113e-06, Loss_0: 3.835e-09, Loss_r: 5.109e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 5.080e-06, Loss_0: 2.814e-09, Loss_r: 5.077e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 5.046e-06, Loss_0: 2.971e-09, Loss_r: 5.043e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 5.016e-06, Loss_0: 3.352e-09, Loss_r: 5.012e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 4.985e-06, Loss_0: 3.084e-09, Loss_r: 4.981e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 4.955e-06, Loss_0: 3.034e-09, Loss_r: 4.952e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 4.925e-06, Loss_0: 3.075e-09, Loss_r: 4.922e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 4.895e-06, Loss_0: 2.986e-09, Loss_r: 4.892e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 4.865e-06, Loss_0: 2.971e-09, Loss_r: 4.862e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 4.836e-06, Loss_0: 2.944e-09, Loss_r: 4.833e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 4.806e-06, Loss_0: 2.876e-09, Loss_r: 4.803e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 4.777e-06, Loss_0: 2.874e-09, Loss_r: 4.774e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 4.748e-06, Loss_0: 2.833e-09, Loss_r: 4.745e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 4.719e-06, Loss_0: 2.788e-09, Loss_r: 4.716e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 4.690e-06, Loss_0: 2.770e-09, Loss_r: 4.687e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 4.661e-06, Loss_0: 2.738e-09, Loss_r: 4.658e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 4.632e-06, Loss_0: 2.792e-09, Loss_r: 4.629e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 4.659e-06, Loss_0: 4.185e-09, Loss_r: 4.655e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 2.578e-05, Loss_0: 9.340e-08, Loss_r: 2.569e-05, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 3990, Loss: 4.569e-06, Loss_0: 1.882e-09, Loss_r: 4.567e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 8.116e-06, Loss_0: 2.937e-09, Loss_r: 8.113e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 4.781e-06, Loss_0: 4.582e-10, Loss_r: 4.780e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 4.930e-06, Loss_0: 7.704e-09, Loss_r: 4.922e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 4.453e-06, Loss_0: 2.747e-09, Loss_r: 4.450e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 4.485e-06, Loss_0: 1.335e-09, Loss_r: 4.483e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 4.426e-06, Loss_0: 3.401e-09, Loss_r: 4.422e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 4.381e-06, Loss_0: 2.070e-09, Loss_r: 4.379e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 4.352e-06, Loss_0: 2.589e-09, Loss_r: 4.350e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 4.327e-06, Loss_0: 2.233e-09, Loss_r: 4.325e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 4.302e-06, Loss_0: 2.487e-09, Loss_r: 4.299e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 4.277e-06, Loss_0: 2.256e-09, Loss_r: 4.275e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 4.252e-06, Loss_0: 2.253e-09, Loss_r: 4.250e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 4.227e-06, Loss_0: 2.302e-09, Loss_r: 4.225e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 4.203e-06, Loss_0: 2.323e-09, Loss_r: 4.201e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 4.181e-06, Loss_0: 2.499e-09, Loss_r: 4.179e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 4.249e-06, Loss_0: 4.088e-09, Loss_r: 4.245e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 1.221e-05, Loss_0: 4.147e-08, Loss_r: 1.217e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4170, Loss: 4.862e-06, Loss_0: 8.903e-09, Loss_r: 4.853e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4180, Loss: 4.238e-06, Loss_0: 6.031e-10, Loss_r: 4.237e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4190, Loss: 4.515e-06, Loss_0: 7.577e-11, Loss_r: 4.515e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4200, Loss: 4.094e-06, Loss_0: 1.101e-09, Loss_r: 4.093e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4210, Loss: 4.058e-06, Loss_0: 3.131e-09, Loss_r: 4.055e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 4.010e-06, Loss_0: 2.518e-09, Loss_r: 4.007e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 3.987e-06, Loss_0: 1.625e-09, Loss_r: 3.985e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 3.958e-06, Loss_0: 2.037e-09, Loss_r: 3.956e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 3.937e-06, Loss_0: 2.083e-09, Loss_r: 3.935e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 3.916e-06, Loss_0: 1.857e-09, Loss_r: 3.914e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 3.895e-06, Loss_0: 2.018e-09, Loss_r: 3.892e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 3.873e-06, Loss_0: 1.873e-09, Loss_r: 3.871e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 3.852e-06, Loss_0: 1.907e-09, Loss_r: 3.850e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 3.831e-06, Loss_0: 1.886e-09, Loss_r: 3.829e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 3.810e-06, Loss_0: 1.848e-09, Loss_r: 3.808e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 3.789e-06, Loss_0: 1.820e-09, Loss_r: 3.787e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 3.768e-06, Loss_0: 1.791e-09, Loss_r: 3.766e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 3.747e-06, Loss_0: 1.697e-09, Loss_r: 3.746e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 3.739e-06, Loss_0: 1.297e-09, Loss_r: 3.738e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 4.729e-06, Loss_0: 2.085e-10, Loss_r: 4.728e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 3.787e-06, Loss_0: 5.857e-10, Loss_r: 3.786e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 3.686e-06, Loss_0: 2.392e-09, Loss_r: 3.684e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 3.706e-06, Loss_0: 2.965e-09, Loss_r: 3.703e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 3.638e-06, Loss_0: 2.086e-09, Loss_r: 3.636e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 3.617e-06, Loss_0: 1.388e-09, Loss_r: 3.616e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 3.596e-06, Loss_0: 1.519e-09, Loss_r: 3.594e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 3.577e-06, Loss_0: 1.781e-09, Loss_r: 3.575e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 3.558e-06, Loss_0: 1.605e-09, Loss_r: 3.556e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 3.539e-06, Loss_0: 1.580e-09, Loss_r: 3.538e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 3.521e-06, Loss_0: 1.622e-09, Loss_r: 3.520e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 3.503e-06, Loss_0: 1.556e-09, Loss_r: 3.501e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 3.485e-06, Loss_0: 1.568e-09, Loss_r: 3.483e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 3.466e-06, Loss_0: 1.534e-09, Loss_r: 3.465e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 3.448e-06, Loss_0: 1.533e-09, Loss_r: 3.447e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 3.430e-06, Loss_0: 1.514e-09, Loss_r: 3.429e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 3.412e-06, Loss_0: 1.486e-09, Loss_r: 3.411e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 3.394e-06, Loss_0: 1.488e-09, Loss_r: 3.393e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 3.376e-06, Loss_0: 1.456e-09, Loss_r: 3.375e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 3.358e-06, Loss_0: 1.463e-09, Loss_r: 3.357e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 3.340e-06, Loss_0: 1.453e-09, Loss_r: 3.339e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 3.327e-06, Loss_0: 1.716e-09, Loss_r: 3.325e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 4.649e-06, Loss_0: 1.037e-08, Loss_r: 4.639e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 3.289e-06, Loss_0: 1.459e-09, Loss_r: 3.288e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 3.481e-06, Loss_0: 1.397e-10, Loss_r: 3.481e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 3.284e-06, Loss_0: 7.792e-10, Loss_r: 3.283e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 3.265e-06, Loss_0: 2.066e-09, Loss_r: 3.263e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 3.226e-06, Loss_0: 1.465e-09, Loss_r: 3.225e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 3.213e-06, Loss_0: 1.088e-09, Loss_r: 3.212e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 3.195e-06, Loss_0: 1.458e-09, Loss_r: 3.193e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 3.178e-06, Loss_0: 1.270e-09, Loss_r: 3.176e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 3.162e-06, Loss_0: 1.300e-09, Loss_r: 3.160e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 3.146e-06, Loss_0: 1.268e-09, Loss_r: 3.145e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 3.130e-06, Loss_0: 1.276e-09, Loss_r: 3.129e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 3.115e-06, Loss_0: 1.241e-09, Loss_r: 3.113e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 3.099e-06, Loss_0: 1.247e-09, Loss_r: 3.098e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 3.083e-06, Loss_0: 1.235e-09, Loss_r: 3.082e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 3.067e-06, Loss_0: 1.247e-09, Loss_r: 3.066e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 3.052e-06, Loss_0: 1.239e-09, Loss_r: 3.051e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 3.038e-06, Loss_0: 1.363e-09, Loss_r: 3.037e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 3.141e-06, Loss_0: 2.891e-09, Loss_r: 3.139e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 1.983e-05, Loss_0: 6.802e-08, Loss_r: 1.976e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 3.920e-06, Loss_0: 7.634e-09, Loss_r: 3.912e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 3.718e-06, Loss_0: 1.817e-10, Loss_r: 3.718e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 3.979e-06, Loss_0: 4.897e-10, Loss_r: 3.978e-06, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 2.975e-06, Loss_0: 6.412e-10, Loss_r: 2.975e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 3.057e-06, Loss_0: 2.789e-09, Loss_r: 3.055e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 2.927e-06, Loss_0: 1.320e-09, Loss_r: 2.925e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 2.928e-06, Loss_0: 6.668e-10, Loss_r: 2.928e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 2.899e-06, Loss_0: 1.305e-09, Loss_r: 2.898e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 2.883e-06, Loss_0: 1.092e-09, Loss_r: 2.881e-06, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 2.869e-06, Loss_0: 1.029e-09, Loss_r: 2.868e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 2.855e-06, Loss_0: 1.108e-09, Loss_r: 2.854e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 2.841e-06, Loss_0: 1.050e-09, Loss_r: 2.840e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 2.827e-06, Loss_0: 1.035e-09, Loss_r: 2.826e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 2.814e-06, Loss_0: 1.063e-09, Loss_r: 2.813e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 2.800e-06, Loss_0: 1.009e-09, Loss_r: 2.799e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 2.786e-06, Loss_0: 1.005e-09, Loss_r: 2.785e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 2.773e-06, Loss_0: 9.744e-10, Loss_r: 2.772e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 2.760e-06, Loss_0: 8.942e-10, Loss_r: 2.759e-06, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 2.780e-06, Loss_0: 4.490e-10, Loss_r: 2.779e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 5.696e-06, Loss_0: 4.158e-09, Loss_r: 5.692e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 2.998e-06, Loss_0: 3.394e-12, Loss_r: 2.998e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 2.761e-06, Loss_0: 1.945e-09, Loss_r: 2.759e-06, Time: 0.03, Learning Rate: 0.00019\n",
            "Training time: 18.4213\n",
            "[1, 64, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 9.994e-01, Loss_0: 1.073e-03, Loss_r: 9.983e-01, Time: 0.70, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.220e-01, Loss_0: 3.832e-02, Loss_r: 1.836e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.657e-01, Loss_0: 2.127e-02, Loss_r: 1.444e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.240e-01, Loss_0: 5.115e-02, Loss_r: 7.281e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.148e-01, Loss_0: 4.894e-02, Loss_r: 6.586e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.148e-01, Loss_0: 5.231e-02, Loss_r: 6.248e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.148e-01, Loss_0: 5.697e-02, Loss_r: 5.787e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.145e-01, Loss_0: 5.276e-02, Loss_r: 6.173e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.142e-01, Loss_0: 5.418e-02, Loss_r: 6.002e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.140e-01, Loss_0: 5.201e-02, Loss_r: 6.197e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.138e-01, Loss_0: 5.252e-02, Loss_r: 6.125e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.136e-01, Loss_0: 5.197e-02, Loss_r: 6.159e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.133e-01, Loss_0: 5.228e-02, Loss_r: 6.102e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.130e-01, Loss_0: 5.207e-02, Loss_r: 6.093e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.126e-01, Loss_0: 5.205e-02, Loss_r: 6.057e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.121e-01, Loss_0: 5.182e-02, Loss_r: 6.032e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.115e-01, Loss_0: 5.161e-02, Loss_r: 5.992e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.107e-01, Loss_0: 5.136e-02, Loss_r: 5.936e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.096e-01, Loss_0: 5.103e-02, Loss_r: 5.858e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.080e-01, Loss_0: 5.059e-02, Loss_r: 5.746e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.057e-01, Loss_0: 4.995e-02, Loss_r: 5.580e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.022e-01, Loss_0: 4.893e-02, Loss_r: 5.329e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.677e-02, Loss_0: 4.724e-02, Loss_r: 4.953e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 8.860e-02, Loss_0: 4.422e-02, Loss_r: 4.438e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 7.748e-02, Loss_0: 3.872e-02, Loss_r: 3.876e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 6.404e-02, Loss_0: 2.972e-02, Loss_r: 3.432e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.025e-02, Loss_0: 1.870e-02, Loss_r: 3.155e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 3.956e-02, Loss_0: 1.039e-02, Loss_r: 2.917e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.239e-02, Loss_0: 6.652e-03, Loss_r: 2.574e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 2.560e-02, Loss_0: 4.848e-03, Loss_r: 2.075e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.968e-02, Loss_0: 3.674e-03, Loss_r: 1.600e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.478e-02, Loss_0: 2.414e-03, Loss_r: 1.236e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.079e-02, Loss_0: 1.551e-03, Loss_r: 9.242e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 7.932e-03, Loss_0: 1.019e-03, Loss_r: 6.914e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.995e-03, Loss_0: 6.811e-04, Loss_r: 5.314e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.700e-03, Loss_0: 4.428e-04, Loss_r: 4.258e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.303e-03, Loss_0: 3.229e-04, Loss_r: 4.980e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.248e-03, Loss_0: 2.467e-04, Loss_r: 3.001e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 2.701e-03, Loss_0: 1.905e-04, Loss_r: 2.511e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.172e-03, Loss_0: 1.431e-04, Loss_r: 2.029e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.790e-03, Loss_0: 1.059e-04, Loss_r: 1.684e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.459e-03, Loss_0: 8.353e-05, Loss_r: 1.375e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.199e-03, Loss_0: 6.324e-05, Loss_r: 1.136e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 9.891e-04, Loss_0: 5.006e-05, Loss_r: 9.391e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 8.196e-04, Loss_0: 3.905e-05, Loss_r: 7.805e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 450, Loss: 6.835e-04, Loss_0: 3.062e-05, Loss_r: 6.529e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.739e-04, Loss_0: 2.423e-05, Loss_r: 5.496e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.858e-04, Loss_0: 1.933e-05, Loss_r: 4.664e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.151e-04, Loss_0: 1.558e-05, Loss_r: 3.996e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.585e-04, Loss_0: 1.263e-05, Loss_r: 3.459e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.130e-04, Loss_0: 1.030e-05, Loss_r: 3.027e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.762e-04, Loss_0: 8.491e-06, Loss_r: 2.677e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.464e-04, Loss_0: 7.065e-06, Loss_r: 2.393e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.231e-04, Loss_0: 6.061e-06, Loss_r: 2.170e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.454e-03, Loss_0: 1.080e-05, Loss_r: 1.443e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.315e-04, Loss_0: 2.387e-06, Loss_r: 2.291e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 8.772e-04, Loss_0: 8.699e-06, Loss_r: 8.685e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.826e-04, Loss_0: 2.187e-06, Loss_r: 2.804e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.664e-04, Loss_0: 2.731e-06, Loss_r: 1.637e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.805e-04, Loss_0: 3.468e-06, Loss_r: 1.771e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.391e-04, Loss_0: 2.457e-06, Loss_r: 1.367e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.350e-04, Loss_0: 1.933e-06, Loss_r: 1.331e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.257e-04, Loss_0: 1.808e-06, Loss_r: 1.239e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.184e-04, Loss_0: 1.656e-06, Loss_r: 1.167e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.126e-04, Loss_0: 1.525e-06, Loss_r: 1.111e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.075e-04, Loss_0: 1.365e-06, Loss_r: 1.062e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.028e-04, Loss_0: 1.242e-06, Loss_r: 1.016e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 9.852e-05, Loss_0: 1.134e-06, Loss_r: 9.739e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 9.456e-05, Loss_0: 1.045e-06, Loss_r: 9.351e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 9.089e-05, Loss_0: 9.559e-07, Loss_r: 8.994e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 700, Loss: 8.750e-05, Loss_0: 8.743e-07, Loss_r: 8.662e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 710, Loss: 8.434e-05, Loss_0: 8.080e-07, Loss_r: 8.353e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 720, Loss: 8.140e-05, Loss_0: 7.452e-07, Loss_r: 8.066e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 7.867e-05, Loss_0: 6.883e-07, Loss_r: 7.798e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.611e-05, Loss_0: 6.364e-07, Loss_r: 7.548e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 7.386e-05, Loss_0: 5.687e-07, Loss_r: 7.330e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.259e-04, Loss_0: 1.733e-07, Loss_r: 1.258e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 2.845e-03, Loss_0: 4.929e-06, Loss_r: 2.840e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.989e-04, Loss_0: 3.381e-09, Loss_r: 1.989e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 4.699e-04, Loss_0: 3.914e-06, Loss_r: 4.660e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 9.092e-05, Loss_0: 9.012e-07, Loss_r: 9.002e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.153e-04, Loss_0: 7.106e-08, Loss_r: 1.153e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 7.350e-05, Loss_0: 2.967e-07, Loss_r: 7.320e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 6.391e-05, Loss_0: 5.565e-07, Loss_r: 6.335e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 6.380e-05, Loss_0: 5.105e-07, Loss_r: 6.329e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 850, Loss: 6.084e-05, Loss_0: 4.255e-07, Loss_r: 6.041e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 5.877e-05, Loss_0: 3.830e-07, Loss_r: 5.839e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 5.730e-05, Loss_0: 3.539e-07, Loss_r: 5.694e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 5.602e-05, Loss_0: 3.252e-07, Loss_r: 5.569e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 5.483e-05, Loss_0: 2.984e-07, Loss_r: 5.453e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 5.371e-05, Loss_0: 2.749e-07, Loss_r: 5.344e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 5.266e-05, Loss_0: 2.567e-07, Loss_r: 5.241e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 5.166e-05, Loss_0: 2.447e-07, Loss_r: 5.142e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 5.071e-05, Loss_0: 2.334e-07, Loss_r: 5.047e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 4.979e-05, Loss_0: 2.199e-07, Loss_r: 4.957e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 4.891e-05, Loss_0: 2.096e-07, Loss_r: 4.871e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 4.807e-05, Loss_0: 1.986e-07, Loss_r: 4.788e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 4.727e-05, Loss_0: 1.894e-07, Loss_r: 4.708e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 4.649e-05, Loss_0: 1.804e-07, Loss_r: 4.631e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 4.574e-05, Loss_0: 1.718e-07, Loss_r: 4.557e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 4.502e-05, Loss_0: 1.638e-07, Loss_r: 4.486e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 4.433e-05, Loss_0: 1.533e-07, Loss_r: 4.418e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 4.692e-05, Loss_0: 8.271e-08, Loss_r: 4.684e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 3.321e-03, Loss_0: 8.841e-06, Loss_r: 3.312e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 6.107e-05, Loss_0: 8.164e-08, Loss_r: 6.099e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 9.954e-04, Loss_0: 3.011e-06, Loss_r: 9.924e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 3.311e-04, Loss_0: 1.399e-06, Loss_r: 3.297e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 5.561e-05, Loss_0: 5.714e-10, Loss_r: 5.561e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 5.681e-05, Loss_0: 7.640e-09, Loss_r: 5.681e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 5.930e-05, Loss_0: 3.727e-07, Loss_r: 5.893e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.290e-05, Loss_0: 1.017e-07, Loss_r: 4.280e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 4.294e-05, Loss_0: 8.995e-08, Loss_r: 4.285e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 4.104e-05, Loss_0: 1.608e-07, Loss_r: 4.088e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 4.045e-05, Loss_0: 1.550e-07, Loss_r: 4.029e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.965e-05, Loss_0: 1.257e-07, Loss_r: 3.952e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.912e-05, Loss_0: 1.129e-07, Loss_r: 3.901e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 3.858e-05, Loss_0: 1.091e-07, Loss_r: 3.848e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 3.807e-05, Loss_0: 1.064e-07, Loss_r: 3.796e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 3.757e-05, Loss_0: 1.030e-07, Loss_r: 3.747e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 3.709e-05, Loss_0: 9.913e-08, Loss_r: 3.699e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 3.662e-05, Loss_0: 9.528e-08, Loss_r: 3.653e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 3.616e-05, Loss_0: 9.164e-08, Loss_r: 3.607e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.572e-05, Loss_0: 8.830e-08, Loss_r: 3.563e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.528e-05, Loss_0: 8.517e-08, Loss_r: 3.520e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.485e-05, Loss_0: 8.221e-08, Loss_r: 3.477e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.444e-05, Loss_0: 7.927e-08, Loss_r: 3.436e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.403e-05, Loss_0: 7.654e-08, Loss_r: 3.395e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 3.363e-05, Loss_0: 7.389e-08, Loss_r: 3.355e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 3.323e-05, Loss_0: 7.148e-08, Loss_r: 3.316e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 3.285e-05, Loss_0: 6.910e-08, Loss_r: 3.278e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 3.247e-05, Loss_0: 6.702e-08, Loss_r: 3.240e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 3.209e-05, Loss_0: 6.486e-08, Loss_r: 3.203e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 3.173e-05, Loss_0: 6.287e-08, Loss_r: 3.167e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 3.137e-05, Loss_0: 6.093e-08, Loss_r: 3.131e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 3.102e-05, Loss_0: 5.910e-08, Loss_r: 3.096e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 3.067e-05, Loss_0: 5.728e-08, Loss_r: 3.061e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 3.033e-05, Loss_0: 5.385e-08, Loss_r: 3.028e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 3.122e-05, Loss_0: 2.460e-08, Loss_r: 3.120e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 9.069e-04, Loss_0: 3.419e-06, Loss_r: 9.035e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 4.371e-03, Loss_0: 2.147e-05, Loss_r: 4.349e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 2.716e-04, Loss_0: 2.066e-07, Loss_r: 2.714e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 5.143e-04, Loss_0: 3.029e-06, Loss_r: 5.112e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 8.878e-05, Loss_0: 8.540e-07, Loss_r: 8.793e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.996e-05, Loss_0: 4.901e-07, Loss_r: 3.947e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 5.182e-05, Loss_0: 7.535e-10, Loss_r: 5.181e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 3.099e-05, Loss_0: 1.114e-07, Loss_r: 3.088e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 3.187e-05, Loss_0: 1.184e-07, Loss_r: 3.175e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.948e-05, Loss_0: 4.751e-08, Loss_r: 2.944e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.928e-05, Loss_0: 4.738e-08, Loss_r: 2.923e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.860e-05, Loss_0: 6.067e-08, Loss_r: 2.854e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.830e-05, Loss_0: 6.149e-08, Loss_r: 2.823e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.797e-05, Loss_0: 5.774e-08, Loss_r: 2.791e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.765e-05, Loss_0: 5.442e-08, Loss_r: 2.760e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.735e-05, Loss_0: 5.101e-08, Loss_r: 2.730e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 2.706e-05, Loss_0: 4.803e-08, Loss_r: 2.701e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.677e-05, Loss_0: 4.634e-08, Loss_r: 2.672e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.649e-05, Loss_0: 4.500e-08, Loss_r: 2.644e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.621e-05, Loss_0: 4.352e-08, Loss_r: 2.617e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.594e-05, Loss_0: 4.209e-08, Loss_r: 2.590e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.567e-05, Loss_0: 4.082e-08, Loss_r: 2.563e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.541e-05, Loss_0: 3.955e-08, Loss_r: 2.537e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.515e-05, Loss_0: 3.840e-08, Loss_r: 2.511e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 2.490e-05, Loss_0: 3.743e-08, Loss_r: 2.486e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.465e-05, Loss_0: 3.640e-08, Loss_r: 2.461e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.440e-05, Loss_0: 3.541e-08, Loss_r: 2.436e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.416e-05, Loss_0: 3.447e-08, Loss_r: 2.412e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.392e-05, Loss_0: 3.356e-08, Loss_r: 2.388e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.368e-05, Loss_0: 3.277e-08, Loss_r: 2.365e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 2.345e-05, Loss_0: 3.191e-08, Loss_r: 2.342e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 2.322e-05, Loss_0: 3.109e-08, Loss_r: 2.319e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.299e-05, Loss_0: 3.028e-08, Loss_r: 2.296e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 2.277e-05, Loss_0: 2.947e-08, Loss_r: 2.274e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.255e-05, Loss_0: 2.853e-08, Loss_r: 2.252e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.234e-05, Loss_0: 2.567e-08, Loss_r: 2.231e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 2.344e-05, Loss_0: 4.946e-09, Loss_r: 2.343e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 4.531e-04, Loss_0: 2.537e-06, Loss_r: 4.506e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.348e-04, Loss_0: 3.193e-06, Loss_r: 2.316e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 5.582e-04, Loss_0: 6.662e-06, Loss_r: 5.516e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 4.376e-04, Loss_0: 2.803e-06, Loss_r: 4.348e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 2.797e-05, Loss_0: 1.491e-08, Loss_r: 2.796e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.048e-05, Loss_0: 1.332e-07, Loss_r: 8.035e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.520e-05, Loss_0: 2.918e-09, Loss_r: 2.520e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.608e-05, Loss_0: 2.163e-07, Loss_r: 2.586e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.477e-05, Loss_0: 9.919e-08, Loss_r: 2.467e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 2.228e-05, Loss_0: 6.260e-08, Loss_r: 2.222e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 2.169e-05, Loss_0: 3.778e-08, Loss_r: 2.165e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 2.148e-05, Loss_0: 2.589e-08, Loss_r: 2.145e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 2.126e-05, Loss_0: 2.899e-08, Loss_r: 2.123e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.104e-05, Loss_0: 2.588e-08, Loss_r: 2.101e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 2.083e-05, Loss_0: 2.791e-08, Loss_r: 2.080e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 2.063e-05, Loss_0: 2.667e-08, Loss_r: 2.060e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 2.044e-05, Loss_0: 2.692e-08, Loss_r: 2.041e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.025e-05, Loss_0: 2.586e-08, Loss_r: 2.022e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 2.006e-05, Loss_0: 2.508e-08, Loss_r: 2.003e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.987e-05, Loss_0: 2.391e-08, Loss_r: 1.985e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.969e-05, Loss_0: 2.337e-08, Loss_r: 1.966e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.951e-05, Loss_0: 2.278e-08, Loss_r: 1.949e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.933e-05, Loss_0: 2.222e-08, Loss_r: 1.931e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.916e-05, Loss_0: 2.155e-08, Loss_r: 1.913e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.898e-05, Loss_0: 2.113e-08, Loss_r: 1.896e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.881e-05, Loss_0: 2.057e-08, Loss_r: 1.879e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.865e-05, Loss_0: 2.008e-08, Loss_r: 1.863e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.848e-05, Loss_0: 1.962e-08, Loss_r: 1.846e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.832e-05, Loss_0: 1.904e-08, Loss_r: 1.830e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.816e-05, Loss_0: 1.857e-08, Loss_r: 1.814e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.800e-05, Loss_0: 1.812e-08, Loss_r: 1.798e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.784e-05, Loss_0: 1.756e-08, Loss_r: 1.782e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.769e-05, Loss_0: 1.572e-08, Loss_r: 1.767e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.805e-05, Loss_0: 3.609e-09, Loss_r: 1.804e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.418e-04, Loss_0: 9.496e-07, Loss_r: 1.409e-04, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 1.883e-05, Loss_0: 1.212e-11, Loss_r: 1.883e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 3.010e-05, Loss_0: 2.346e-07, Loss_r: 2.987e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 2.290e-05, Loss_0: 1.330e-07, Loss_r: 2.276e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 1.741e-05, Loss_0: 2.951e-09, Loss_r: 1.741e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 1.752e-05, Loss_0: 1.358e-09, Loss_r: 1.752e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 1.691e-05, Loss_0: 3.079e-08, Loss_r: 1.687e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 1.650e-05, Loss_0: 1.392e-08, Loss_r: 1.649e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 1.639e-05, Loss_0: 1.184e-08, Loss_r: 1.638e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 1.626e-05, Loss_0: 1.673e-08, Loss_r: 1.625e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 1.613e-05, Loss_0: 1.255e-08, Loss_r: 1.612e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.601e-05, Loss_0: 1.382e-08, Loss_r: 1.599e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 1.589e-05, Loss_0: 1.406e-08, Loss_r: 1.587e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.577e-05, Loss_0: 1.270e-08, Loss_r: 1.576e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.565e-05, Loss_0: 1.232e-08, Loss_r: 1.564e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.554e-05, Loss_0: 1.150e-08, Loss_r: 1.553e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.547e-05, Loss_0: 7.720e-09, Loss_r: 1.547e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.875e-05, Loss_0: 5.940e-09, Loss_r: 1.874e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2270, Loss: 1.559e-05, Loss_0: 2.147e-09, Loss_r: 1.559e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2280, Loss: 1.515e-05, Loss_0: 1.674e-08, Loss_r: 1.513e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 1.519e-05, Loss_0: 2.289e-08, Loss_r: 1.517e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 1.493e-05, Loss_0: 1.494e-08, Loss_r: 1.491e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 1.482e-05, Loss_0: 8.702e-09, Loss_r: 1.481e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 1.471e-05, Loss_0: 9.947e-09, Loss_r: 1.470e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 1.461e-05, Loss_0: 1.220e-08, Loss_r: 1.460e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 1.452e-05, Loss_0: 1.030e-08, Loss_r: 1.450e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 1.442e-05, Loss_0: 1.053e-08, Loss_r: 1.441e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 1.433e-05, Loss_0: 1.047e-08, Loss_r: 1.431e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 1.423e-05, Loss_0: 1.016e-08, Loss_r: 1.422e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 1.414e-05, Loss_0: 1.002e-08, Loss_r: 1.413e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 1.405e-05, Loss_0: 9.911e-09, Loss_r: 1.404e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.395e-05, Loss_0: 9.689e-09, Loss_r: 1.394e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.386e-05, Loss_0: 9.475e-09, Loss_r: 1.385e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.377e-05, Loss_0: 9.220e-09, Loss_r: 1.376e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.369e-05, Loss_0: 8.384e-09, Loss_r: 1.368e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 1.370e-05, Loss_0: 3.917e-09, Loss_r: 1.369e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 2.571e-05, Loss_0: 7.119e-08, Loss_r: 2.563e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 1.411e-05, Loss_0: 8.137e-11, Loss_r: 1.411e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 1.396e-05, Loss_0: 3.054e-08, Loss_r: 1.393e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 1.400e-05, Loss_0: 3.280e-08, Loss_r: 1.396e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 1.320e-05, Loss_0: 9.166e-09, Loss_r: 1.319e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 1.322e-05, Loss_0: 3.494e-09, Loss_r: 1.322e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 1.305e-05, Loss_0: 9.150e-09, Loss_r: 1.304e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.298e-05, Loss_0: 9.905e-09, Loss_r: 1.297e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 1.290e-05, Loss_0: 6.747e-09, Loss_r: 1.289e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.282e-05, Loss_0: 8.750e-09, Loss_r: 1.281e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 1.274e-05, Loss_0: 7.432e-09, Loss_r: 1.274e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 1.267e-05, Loss_0: 8.009e-09, Loss_r: 1.266e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.260e-05, Loss_0: 7.468e-09, Loss_r: 1.259e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.252e-05, Loss_0: 7.569e-09, Loss_r: 1.251e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.245e-05, Loss_0: 7.538e-09, Loss_r: 1.244e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.238e-05, Loss_0: 7.386e-09, Loss_r: 1.237e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.231e-05, Loss_0: 7.265e-09, Loss_r: 1.230e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.223e-05, Loss_0: 7.395e-09, Loss_r: 1.223e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.217e-05, Loss_0: 9.032e-09, Loss_r: 1.217e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.359e-05, Loss_0: 4.508e-08, Loss_r: 1.355e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 1.209e-05, Loss_0: 1.180e-08, Loss_r: 1.208e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 1.207e-05, Loss_0: 2.359e-09, Loss_r: 1.207e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 1.199e-05, Loss_0: 2.634e-09, Loss_r: 1.199e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 1.184e-05, Loss_0: 7.095e-09, Loss_r: 1.184e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.179e-05, Loss_0: 8.491e-09, Loss_r: 1.178e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.172e-05, Loss_0: 5.903e-09, Loss_r: 1.171e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 1.166e-05, Loss_0: 6.046e-09, Loss_r: 1.165e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.160e-05, Loss_0: 6.714e-09, Loss_r: 1.159e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 1.154e-05, Loss_0: 5.982e-09, Loss_r: 1.153e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.147e-05, Loss_0: 6.295e-09, Loss_r: 1.147e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.141e-05, Loss_0: 5.967e-09, Loss_r: 1.141e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.135e-05, Loss_0: 6.043e-09, Loss_r: 1.135e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.129e-05, Loss_0: 5.994e-09, Loss_r: 1.129e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.123e-05, Loss_0: 5.847e-09, Loss_r: 1.123e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.118e-05, Loss_0: 5.779e-09, Loss_r: 1.117e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.112e-05, Loss_0: 5.560e-09, Loss_r: 1.111e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.106e-05, Loss_0: 4.936e-09, Loss_r: 1.105e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.112e-05, Loss_0: 1.386e-09, Loss_r: 1.112e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 2.722e-05, Loss_0: 1.291e-07, Loss_r: 2.709e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 1.169e-05, Loss_0: 5.096e-10, Loss_r: 1.169e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 1.171e-05, Loss_0: 3.034e-08, Loss_r: 1.168e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 1.176e-05, Loss_0: 3.196e-08, Loss_r: 1.172e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 1.074e-05, Loss_0: 5.884e-09, Loss_r: 1.073e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 1.082e-05, Loss_0: 1.089e-09, Loss_r: 1.082e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 1.063e-05, Loss_0: 5.754e-09, Loss_r: 1.063e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 1.059e-05, Loss_0: 6.909e-09, Loss_r: 1.059e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 1.054e-05, Loss_0: 3.887e-09, Loss_r: 1.053e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.048e-05, Loss_0: 5.585e-09, Loss_r: 1.048e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 1.043e-05, Loss_0: 4.684e-09, Loss_r: 1.042e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.038e-05, Loss_0: 5.060e-09, Loss_r: 1.037e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 1.033e-05, Loss_0: 4.687e-09, Loss_r: 1.032e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 1.028e-05, Loss_0: 4.855e-09, Loss_r: 1.027e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.023e-05, Loss_0: 4.781e-09, Loss_r: 1.022e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.018e-05, Loss_0: 4.652e-09, Loss_r: 1.017e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.013e-05, Loss_0: 4.615e-09, Loss_r: 1.012e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.008e-05, Loss_0: 4.588e-09, Loss_r: 1.007e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.003e-05, Loss_0: 4.520e-09, Loss_r: 1.002e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 9.980e-06, Loss_0: 4.512e-09, Loss_r: 9.976e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 9.932e-06, Loss_0: 4.762e-09, Loss_r: 9.927e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.006e-05, Loss_0: 1.261e-08, Loss_r: 1.005e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.302e-04, Loss_0: 1.580e-06, Loss_r: 1.287e-04, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 1.034e-05, Loss_0: 2.290e-10, Loss_r: 1.034e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 2.995e-05, Loss_0: 1.825e-07, Loss_r: 2.976e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.296e-05, Loss_0: 1.756e-08, Loss_r: 1.294e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.121e-05, Loss_0: 4.073e-08, Loss_r: 1.117e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.008e-05, Loss_0: 1.907e-08, Loss_r: 1.006e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 9.956e-06, Loss_0: 3.871e-12, Loss_r: 9.956e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 9.555e-06, Loss_0: 5.535e-09, Loss_r: 9.550e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 9.516e-06, Loss_0: 5.803e-09, Loss_r: 9.510e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 9.474e-06, Loss_0: 2.550e-09, Loss_r: 9.471e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 9.423e-06, Loss_0: 5.123e-09, Loss_r: 9.418e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 9.376e-06, Loss_0: 3.386e-09, Loss_r: 9.373e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 9.331e-06, Loss_0: 3.973e-09, Loss_r: 9.328e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 9.289e-06, Loss_0: 4.136e-09, Loss_r: 9.285e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 9.246e-06, Loss_0: 3.671e-09, Loss_r: 9.243e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 9.205e-06, Loss_0: 3.528e-09, Loss_r: 9.201e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 9.163e-06, Loss_0: 3.279e-09, Loss_r: 9.160e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 9.137e-06, Loss_0: 2.127e-09, Loss_r: 9.134e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 9.914e-06, Loss_0: 1.682e-09, Loss_r: 9.913e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 9.164e-06, Loss_0: 4.282e-10, Loss_r: 9.163e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 9.002e-06, Loss_0: 3.980e-09, Loss_r: 8.998e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 8.996e-06, Loss_0: 6.324e-09, Loss_r: 8.990e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 8.939e-06, Loss_0: 5.158e-09, Loss_r: 8.934e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 8.889e-06, Loss_0: 3.226e-09, Loss_r: 8.886e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 8.853e-06, Loss_0: 2.872e-09, Loss_r: 8.850e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 8.813e-06, Loss_0: 3.539e-09, Loss_r: 8.810e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 8.776e-06, Loss_0: 3.541e-09, Loss_r: 8.772e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 8.738e-06, Loss_0: 3.221e-09, Loss_r: 8.735e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 8.701e-06, Loss_0: 3.363e-09, Loss_r: 8.697e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 8.663e-06, Loss_0: 3.264e-09, Loss_r: 8.660e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 8.626e-06, Loss_0: 3.271e-09, Loss_r: 8.623e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 8.589e-06, Loss_0: 3.193e-09, Loss_r: 8.586e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 8.552e-06, Loss_0: 3.204e-09, Loss_r: 8.548e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 8.515e-06, Loss_0: 3.233e-09, Loss_r: 8.511e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 8.477e-06, Loss_0: 3.159e-09, Loss_r: 8.474e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 8.440e-06, Loss_0: 3.140e-09, Loss_r: 8.437e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 8.403e-06, Loss_0: 3.115e-09, Loss_r: 8.400e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 8.367e-06, Loss_0: 3.125e-09, Loss_r: 8.364e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 8.333e-06, Loss_0: 3.771e-09, Loss_r: 8.329e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 8.716e-06, Loss_0: 1.626e-08, Loss_r: 8.699e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 8.281e-06, Loss_0: 4.988e-09, Loss_r: 8.276e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 8.250e-06, Loss_0: 1.374e-09, Loss_r: 8.248e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 8.218e-06, Loss_0: 1.330e-09, Loss_r: 8.217e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 8.161e-06, Loss_0: 2.814e-09, Loss_r: 8.158e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 8.131e-06, Loss_0: 3.619e-09, Loss_r: 8.128e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 8.095e-06, Loss_0: 2.793e-09, Loss_r: 8.092e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 8.062e-06, Loss_0: 2.592e-09, Loss_r: 8.060e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 8.029e-06, Loss_0: 2.977e-09, Loss_r: 8.026e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 7.996e-06, Loss_0: 2.688e-09, Loss_r: 7.993e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 7.963e-06, Loss_0: 2.774e-09, Loss_r: 7.961e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 7.931e-06, Loss_0: 2.700e-09, Loss_r: 7.928e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 7.898e-06, Loss_0: 2.710e-09, Loss_r: 7.895e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 7.865e-06, Loss_0: 2.663e-09, Loss_r: 7.862e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 7.832e-06, Loss_0: 2.669e-09, Loss_r: 7.830e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 7.800e-06, Loss_0: 2.581e-09, Loss_r: 7.797e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 7.767e-06, Loss_0: 2.553e-09, Loss_r: 7.764e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 7.734e-06, Loss_0: 2.520e-09, Loss_r: 7.732e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 7.703e-06, Loss_0: 2.224e-09, Loss_r: 7.700e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 7.728e-06, Loss_0: 5.272e-10, Loss_r: 7.727e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.541e-05, Loss_0: 7.129e-08, Loss_r: 1.534e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 8.014e-06, Loss_0: 5.041e-10, Loss_r: 8.014e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 7.960e-06, Loss_0: 1.440e-08, Loss_r: 7.946e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 8.014e-06, Loss_0: 1.619e-08, Loss_r: 7.998e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 7.525e-06, Loss_0: 3.212e-09, Loss_r: 7.521e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 7.552e-06, Loss_0: 4.522e-10, Loss_r: 7.551e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 7.462e-06, Loss_0: 2.347e-09, Loss_r: 7.460e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 7.441e-06, Loss_0: 3.448e-09, Loss_r: 7.437e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 7.406e-06, Loss_0: 1.866e-09, Loss_r: 7.405e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 7.375e-06, Loss_0: 2.499e-09, Loss_r: 7.373e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 7.346e-06, Loss_0: 2.320e-09, Loss_r: 7.343e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 7.317e-06, Loss_0: 2.304e-09, Loss_r: 7.314e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 7.287e-06, Loss_0: 2.284e-09, Loss_r: 7.285e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 7.258e-06, Loss_0: 2.305e-09, Loss_r: 7.256e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 7.229e-06, Loss_0: 2.235e-09, Loss_r: 7.227e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 7.200e-06, Loss_0: 2.216e-09, Loss_r: 7.198e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 7.171e-06, Loss_0: 2.150e-09, Loss_r: 7.169e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 7.142e-06, Loss_0: 2.117e-09, Loss_r: 7.140e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 7.114e-06, Loss_0: 1.860e-09, Loss_r: 7.112e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 7.130e-06, Loss_0: 4.714e-10, Loss_r: 7.130e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.205e-05, Loss_0: 4.421e-08, Loss_r: 1.200e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 7.394e-06, Loss_0: 5.446e-10, Loss_r: 7.394e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 7.159e-06, Loss_0: 8.387e-09, Loss_r: 7.151e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 7.271e-06, Loss_0: 1.169e-08, Loss_r: 7.260e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 6.967e-06, Loss_0: 3.592e-09, Loss_r: 6.963e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 6.955e-06, Loss_0: 6.633e-10, Loss_r: 6.955e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 6.901e-06, Loss_0: 1.598e-09, Loss_r: 6.900e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 6.878e-06, Loss_0: 2.876e-09, Loss_r: 6.876e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 6.848e-06, Loss_0: 1.807e-09, Loss_r: 6.846e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 6.821e-06, Loss_0: 1.960e-09, Loss_r: 6.819e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 6.795e-06, Loss_0: 2.118e-09, Loss_r: 6.793e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 6.769e-06, Loss_0: 1.895e-09, Loss_r: 6.767e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 6.743e-06, Loss_0: 2.012e-09, Loss_r: 6.741e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 6.717e-06, Loss_0: 1.952e-09, Loss_r: 6.715e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 6.691e-06, Loss_0: 1.915e-09, Loss_r: 6.689e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 6.665e-06, Loss_0: 1.975e-09, Loss_r: 6.663e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 6.639e-06, Loss_0: 1.916e-09, Loss_r: 6.637e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 6.613e-06, Loss_0: 1.939e-09, Loss_r: 6.611e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 6.587e-06, Loss_0: 1.889e-09, Loss_r: 6.585e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 6.561e-06, Loss_0: 1.900e-09, Loss_r: 6.559e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 6.535e-06, Loss_0: 1.967e-09, Loss_r: 6.533e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 6.524e-06, Loss_0: 3.259e-09, Loss_r: 6.521e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 1.146e-05, Loss_0: 9.076e-08, Loss_r: 1.137e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 6.461e-06, Loss_0: 2.172e-09, Loss_r: 6.459e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 7.193e-06, Loss_0: 3.407e-09, Loss_r: 7.190e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 6.527e-06, Loss_0: 9.953e-12, Loss_r: 6.527e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 6.471e-06, Loss_0: 5.650e-09, Loss_r: 6.465e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 6.373e-06, Loss_0: 2.651e-09, Loss_r: 6.371e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 6.360e-06, Loss_0: 7.250e-10, Loss_r: 6.359e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 6.323e-06, Loss_0: 2.328e-09, Loss_r: 6.321e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 6.297e-06, Loss_0: 1.645e-09, Loss_r: 6.295e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 6.273e-06, Loss_0: 1.687e-09, Loss_r: 6.272e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 6.250e-06, Loss_0: 1.724e-09, Loss_r: 6.248e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 6.227e-06, Loss_0: 1.727e-09, Loss_r: 6.225e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 6.203e-06, Loss_0: 1.638e-09, Loss_r: 6.202e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 6.180e-06, Loss_0: 1.738e-09, Loss_r: 6.178e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 6.156e-06, Loss_0: 1.707e-09, Loss_r: 6.155e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 6.133e-06, Loss_0: 1.664e-09, Loss_r: 6.131e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 6.110e-06, Loss_0: 1.702e-09, Loss_r: 6.108e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 6.087e-06, Loss_0: 1.873e-09, Loss_r: 6.085e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 6.094e-06, Loss_0: 3.723e-09, Loss_r: 6.091e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 9.687e-06, Loss_0: 6.937e-08, Loss_r: 9.617e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 6.258e-06, Loss_0: 9.457e-09, Loss_r: 6.249e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 6.131e-06, Loss_0: 9.428e-12, Loss_r: 6.131e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 6.194e-06, Loss_0: 2.232e-10, Loss_r: 6.194e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 5.963e-06, Loss_0: 8.448e-10, Loss_r: 5.963e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 5.958e-06, Loss_0: 3.323e-09, Loss_r: 5.955e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 5.914e-06, Loss_0: 1.868e-09, Loss_r: 5.913e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 5.896e-06, Loss_0: 1.011e-09, Loss_r: 5.895e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 5.872e-06, Loss_0: 1.750e-09, Loss_r: 5.870e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 5.850e-06, Loss_0: 1.539e-09, Loss_r: 5.849e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 5.829e-06, Loss_0: 1.452e-09, Loss_r: 5.828e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 5.808e-06, Loss_0: 1.533e-09, Loss_r: 5.807e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 5.787e-06, Loss_0: 1.483e-09, Loss_r: 5.785e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 5.766e-06, Loss_0: 1.474e-09, Loss_r: 5.764e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 5.745e-06, Loss_0: 1.470e-09, Loss_r: 5.743e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 5.724e-06, Loss_0: 1.483e-09, Loss_r: 5.722e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 5.703e-06, Loss_0: 1.452e-09, Loss_r: 5.701e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.681e-06, Loss_0: 1.447e-09, Loss_r: 5.680e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 5.660e-06, Loss_0: 1.453e-09, Loss_r: 5.659e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.639e-06, Loss_0: 1.496e-09, Loss_r: 5.638e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 5.622e-06, Loss_0: 2.054e-09, Loss_r: 5.620e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 6.277e-06, Loss_0: 1.811e-08, Loss_r: 6.259e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 5.601e-06, Loss_0: 3.038e-09, Loss_r: 5.598e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 5.609e-06, Loss_0: 1.194e-10, Loss_r: 5.608e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 5.578e-06, Loss_0: 1.998e-10, Loss_r: 5.577e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 5.521e-06, Loss_0: 1.522e-09, Loss_r: 5.520e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 5.507e-06, Loss_0: 2.084e-09, Loss_r: 5.505e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 5.483e-06, Loss_0: 1.212e-09, Loss_r: 5.482e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 5.464e-06, Loss_0: 1.188e-09, Loss_r: 5.463e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 5.445e-06, Loss_0: 1.514e-09, Loss_r: 5.443e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 5.426e-06, Loss_0: 1.247e-09, Loss_r: 5.424e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 5.407e-06, Loss_0: 1.384e-09, Loss_r: 5.405e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 5.387e-06, Loss_0: 1.273e-09, Loss_r: 5.386e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 5.368e-06, Loss_0: 1.355e-09, Loss_r: 5.367e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 5.349e-06, Loss_0: 1.309e-09, Loss_r: 5.348e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 5.330e-06, Loss_0: 1.305e-09, Loss_r: 5.329e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 5.311e-06, Loss_0: 1.326e-09, Loss_r: 5.310e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 5.292e-06, Loss_0: 1.282e-09, Loss_r: 5.291e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 5.273e-06, Loss_0: 1.310e-09, Loss_r: 5.271e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 5.254e-06, Loss_0: 1.456e-09, Loss_r: 5.253e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 5.254e-06, Loss_0: 2.666e-09, Loss_r: 5.251e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 7.389e-06, Loss_0: 4.428e-08, Loss_r: 7.345e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 5.347e-06, Loss_0: 6.564e-09, Loss_r: 5.340e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 5.255e-06, Loss_0: 7.661e-12, Loss_r: 5.255e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 5.293e-06, Loss_0: 5.599e-11, Loss_r: 5.293e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 5.153e-06, Loss_0: 6.507e-10, Loss_r: 5.152e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 5.143e-06, Loss_0: 2.384e-09, Loss_r: 5.141e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 5.113e-06, Loss_0: 1.446e-09, Loss_r: 5.111e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 5.097e-06, Loss_0: 8.301e-10, Loss_r: 5.096e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 5.078e-06, Loss_0: 1.324e-09, Loss_r: 5.076e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 5.060e-06, Loss_0: 1.221e-09, Loss_r: 5.059e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 5.043e-06, Loss_0: 1.116e-09, Loss_r: 5.042e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 5.025e-06, Loss_0: 1.223e-09, Loss_r: 5.024e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 5.008e-06, Loss_0: 1.138e-09, Loss_r: 5.007e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 4.991e-06, Loss_0: 1.173e-09, Loss_r: 4.990e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 4.974e-06, Loss_0: 1.153e-09, Loss_r: 4.972e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 4.956e-06, Loss_0: 1.124e-09, Loss_r: 4.955e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 4.939e-06, Loss_0: 1.105e-09, Loss_r: 4.938e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 4.921e-06, Loss_0: 1.098e-09, Loss_r: 4.920e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 4.904e-06, Loss_0: 1.048e-09, Loss_r: 4.903e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 4.889e-06, Loss_0: 7.308e-10, Loss_r: 4.889e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 5.055e-06, Loss_0: 3.351e-10, Loss_r: 5.055e-06, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 4.875e-06, Loss_0: 2.489e-10, Loss_r: 4.875e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 4.840e-06, Loss_0: 1.417e-09, Loss_r: 4.838e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 4.832e-06, Loss_0: 1.986e-09, Loss_r: 4.830e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 4.809e-06, Loss_0: 1.463e-09, Loss_r: 4.807e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 4.791e-06, Loss_0: 9.341e-10, Loss_r: 4.791e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 4.776e-06, Loss_0: 9.362e-10, Loss_r: 4.775e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 4.760e-06, Loss_0: 1.137e-09, Loss_r: 4.759e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 4.744e-06, Loss_0: 1.090e-09, Loss_r: 4.743e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 4.728e-06, Loss_0: 1.031e-09, Loss_r: 4.727e-06, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 4.713e-06, Loss_0: 1.072e-09, Loss_r: 4.711e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 4.697e-06, Loss_0: 1.044e-09, Loss_r: 4.696e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 4.681e-06, Loss_0: 1.047e-09, Loss_r: 4.680e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 4.665e-06, Loss_0: 1.028e-09, Loss_r: 4.664e-06, Time: 0.05, Learning Rate: 0.00021\n",
            "Training time: 20.1126\n",
            "[1, 256, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.054e-01, Loss_0: 2.903e-03, Loss_r: 2.024e-01, Time: 0.84, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.420e-01, Loss_0: 3.163e-02, Loss_r: 1.104e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.360e-01, Loss_0: 4.360e-02, Loss_r: 9.239e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.180e-01, Loss_0: 6.605e-02, Loss_r: 5.193e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.187e-01, Loss_0: 6.485e-02, Loss_r: 5.383e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.179e-01, Loss_0: 5.241e-02, Loss_r: 6.546e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.170e-01, Loss_0: 5.243e-02, Loss_r: 6.456e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.171e-01, Loss_0: 5.476e-02, Loss_r: 6.229e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.169e-01, Loss_0: 5.514e-02, Loss_r: 6.174e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.169e-01, Loss_0: 5.475e-02, Loss_r: 6.212e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.168e-01, Loss_0: 5.442e-02, Loss_r: 6.238e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.167e-01, Loss_0: 5.442e-02, Loss_r: 6.232e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.167e-01, Loss_0: 5.456e-02, Loss_r: 6.212e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.166e-01, Loss_0: 5.455e-02, Loss_r: 6.204e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.165e-01, Loss_0: 5.441e-02, Loss_r: 6.206e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.163e-01, Loss_0: 5.430e-02, Loss_r: 6.202e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.161e-01, Loss_0: 5.423e-02, Loss_r: 6.187e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.158e-01, Loss_0: 5.413e-02, Loss_r: 6.166e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.153e-01, Loss_0: 5.395e-02, Loss_r: 6.133e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.144e-01, Loss_0: 5.364e-02, Loss_r: 6.074e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.126e-01, Loss_0: 5.307e-02, Loss_r: 5.956e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.085e-01, Loss_0: 5.181e-02, Loss_r: 5.671e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.732e-02, Loss_0: 4.845e-02, Loss_r: 4.886e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 7.102e-02, Loss_0: 3.662e-02, Loss_r: 3.440e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 2.335e-01, Loss_0: 1.108e-02, Loss_r: 2.224e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 8.117e-02, Loss_0: 1.179e-02, Loss_r: 6.938e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.991e-02, Loss_0: 1.902e-02, Loss_r: 4.089e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.025e-02, Loss_0: 1.257e-02, Loss_r: 3.768e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.874e-02, Loss_0: 9.114e-03, Loss_r: 2.963e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.359e-02, Loss_0: 6.223e-03, Loss_r: 2.737e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 2.746e-02, Loss_0: 4.706e-03, Loss_r: 2.275e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 2.209e-02, Loss_0: 3.813e-03, Loss_r: 1.828e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.739e-02, Loss_0: 2.653e-03, Loss_r: 1.474e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.331e-02, Loss_0: 1.728e-03, Loss_r: 1.158e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 9.970e-03, Loss_0: 1.108e-03, Loss_r: 8.862e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 350, Loss: 7.472e-03, Loss_0: 7.158e-04, Loss_r: 6.756e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.762e-03, Loss_0: 4.580e-04, Loss_r: 5.304e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.610e-02, Loss_0: 1.658e-04, Loss_r: 2.593e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.335e-03, Loss_0: 2.333e-04, Loss_r: 5.102e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.784e-03, Loss_0: 2.368e-04, Loss_r: 3.547e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.086e-03, Loss_0: 1.594e-04, Loss_r: 2.927e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.643e-03, Loss_0: 1.191e-04, Loss_r: 2.524e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.195e-03, Loss_0: 9.658e-05, Loss_r: 2.098e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 1.862e-03, Loss_0: 7.327e-05, Loss_r: 1.789e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.543e-03, Loss_0: 5.503e-05, Loss_r: 1.488e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.283e-03, Loss_0: 4.354e-05, Loss_r: 1.240e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 1.070e-03, Loss_0: 3.443e-05, Loss_r: 1.035e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 8.958e-04, Loss_0: 2.579e-05, Loss_r: 8.700e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.894e-03, Loss_0: 1.061e-05, Loss_r: 1.883e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.038e-02, Loss_0: 2.495e-06, Loss_r: 1.038e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.461e-03, Loss_0: 8.608e-05, Loss_r: 3.374e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.242e-03, Loss_0: 1.441e-05, Loss_r: 1.227e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.189e-03, Loss_0: 1.280e-05, Loss_r: 1.176e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 8.907e-04, Loss_0: 1.529e-05, Loss_r: 8.754e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 6.978e-04, Loss_0: 1.723e-05, Loss_r: 6.806e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 6.057e-04, Loss_0: 1.900e-05, Loss_r: 5.867e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.215e-04, Loss_0: 1.397e-05, Loss_r: 5.075e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 4.558e-04, Loss_0: 9.526e-06, Loss_r: 4.463e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 4.026e-04, Loss_0: 8.475e-06, Loss_r: 3.941e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.612e-04, Loss_0: 7.019e-06, Loss_r: 3.541e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.281e-04, Loss_0: 5.693e-06, Loss_r: 3.224e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.018e-04, Loss_0: 4.863e-06, Loss_r: 2.969e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.807e-04, Loss_0: 4.160e-06, Loss_r: 2.766e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.638e-04, Loss_0: 3.534e-06, Loss_r: 2.603e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.501e-04, Loss_0: 3.095e-06, Loss_r: 2.470e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.389e-04, Loss_0: 2.722e-06, Loss_r: 2.361e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.295e-04, Loss_0: 2.409e-06, Loss_r: 2.271e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.217e-04, Loss_0: 2.153e-06, Loss_r: 2.195e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.150e-04, Loss_0: 1.944e-06, Loss_r: 2.130e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 2.091e-04, Loss_0: 1.769e-06, Loss_r: 2.074e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 2.040e-04, Loss_0: 1.622e-06, Loss_r: 2.023e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.993e-04, Loss_0: 1.498e-06, Loss_r: 1.978e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.950e-04, Loss_0: 1.391e-06, Loss_r: 1.937e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.911e-04, Loss_0: 1.299e-06, Loss_r: 1.898e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.874e-04, Loss_0: 1.219e-06, Loss_r: 1.862e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.839e-04, Loss_0: 1.149e-06, Loss_r: 1.827e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.805e-04, Loss_0: 1.088e-06, Loss_r: 1.794e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.773e-04, Loss_0: 1.033e-06, Loss_r: 1.763e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.742e-04, Loss_0: 9.842e-07, Loss_r: 1.732e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.712e-04, Loss_0: 9.401e-07, Loss_r: 1.703e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.684e-04, Loss_0: 8.999e-07, Loss_r: 1.675e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.656e-04, Loss_0: 8.633e-07, Loss_r: 1.647e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.628e-04, Loss_0: 8.295e-07, Loss_r: 1.620e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.602e-04, Loss_0: 7.981e-07, Loss_r: 1.594e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.576e-04, Loss_0: 7.695e-07, Loss_r: 1.569e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.552e-04, Loss_0: 7.424e-07, Loss_r: 1.544e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.527e-04, Loss_0: 7.168e-07, Loss_r: 1.520e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.504e-04, Loss_0: 6.928e-07, Loss_r: 1.497e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.481e-04, Loss_0: 6.700e-07, Loss_r: 1.474e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.459e-04, Loss_0: 6.484e-07, Loss_r: 1.453e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.438e-04, Loss_0: 6.278e-07, Loss_r: 1.431e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.417e-04, Loss_0: 6.084e-07, Loss_r: 1.411e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.397e-04, Loss_0: 5.896e-07, Loss_r: 1.391e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.377e-04, Loss_0: 5.714e-07, Loss_r: 1.372e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.358e-04, Loss_0: 5.539e-07, Loss_r: 1.353e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.340e-04, Loss_0: 5.362e-07, Loss_r: 1.335e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.323e-04, Loss_0: 4.942e-07, Loss_r: 1.318e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.026e-04, Loss_0: 3.722e-09, Loss_r: 2.026e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 6.009e-04, Loss_0: 4.889e-06, Loss_r: 5.960e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 4.921e-04, Loss_0: 1.867e-05, Loss_r: 4.735e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.621e-04, Loss_0: 3.343e-05, Loss_r: 2.287e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.664e-04, Loss_0: 1.349e-05, Loss_r: 2.529e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.904e-04, Loss_0: 5.102e-06, Loss_r: 2.853e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.389e-04, Loss_0: 1.561e-05, Loss_r: 2.233e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.051e-04, Loss_0: 1.567e-08, Loss_r: 2.051e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.902e-04, Loss_0: 3.480e-06, Loss_r: 1.867e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.816e-04, Loss_0: 1.274e-06, Loss_r: 1.803e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.752e-04, Loss_0: 1.511e-06, Loss_r: 1.737e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.700e-04, Loss_0: 9.594e-07, Loss_r: 1.690e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.656e-04, Loss_0: 1.299e-06, Loss_r: 1.643e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.618e-04, Loss_0: 8.714e-07, Loss_r: 1.609e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.584e-04, Loss_0: 9.341e-07, Loss_r: 1.574e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.553e-04, Loss_0: 8.480e-07, Loss_r: 1.544e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.524e-04, Loss_0: 7.653e-07, Loss_r: 1.517e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.497e-04, Loss_0: 7.394e-07, Loss_r: 1.490e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.472e-04, Loss_0: 7.026e-07, Loss_r: 1.465e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.448e-04, Loss_0: 6.599e-07, Loss_r: 1.441e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.425e-04, Loss_0: 6.275e-07, Loss_r: 1.419e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.403e-04, Loss_0: 6.020e-07, Loss_r: 1.397e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.381e-04, Loss_0: 5.773e-07, Loss_r: 1.376e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.361e-04, Loss_0: 5.541e-07, Loss_r: 1.356e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.341e-04, Loss_0: 5.325e-07, Loss_r: 1.336e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.323e-04, Loss_0: 5.123e-07, Loss_r: 1.317e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.304e-04, Loss_0: 4.940e-07, Loss_r: 1.299e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.287e-04, Loss_0: 4.766e-07, Loss_r: 1.282e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.270e-04, Loss_0: 4.599e-07, Loss_r: 1.265e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.253e-04, Loss_0: 4.447e-07, Loss_r: 1.249e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.238e-04, Loss_0: 4.298e-07, Loss_r: 1.233e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.222e-04, Loss_0: 4.159e-07, Loss_r: 1.218e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.208e-04, Loss_0: 4.028e-07, Loss_r: 1.204e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.193e-04, Loss_0: 3.900e-07, Loss_r: 1.189e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.179e-04, Loss_0: 3.779e-07, Loss_r: 1.176e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.166e-04, Loss_0: 3.663e-07, Loss_r: 1.162e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.153e-04, Loss_0: 3.552e-07, Loss_r: 1.149e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.140e-04, Loss_0: 3.446e-07, Loss_r: 1.137e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.128e-04, Loss_0: 3.343e-07, Loss_r: 1.124e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.116e-04, Loss_0: 3.248e-07, Loss_r: 1.113e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.104e-04, Loss_0: 3.154e-07, Loss_r: 1.101e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.093e-04, Loss_0: 3.062e-07, Loss_r: 1.090e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.081e-04, Loss_0: 2.978e-07, Loss_r: 1.078e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.070e-04, Loss_0: 2.893e-07, Loss_r: 1.068e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.060e-04, Loss_0: 2.814e-07, Loss_r: 1.057e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.049e-04, Loss_0: 2.739e-07, Loss_r: 1.046e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.039e-04, Loss_0: 2.665e-07, Loss_r: 1.036e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.029e-04, Loss_0: 2.593e-07, Loss_r: 1.026e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.019e-04, Loss_0: 2.525e-07, Loss_r: 1.016e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.009e-04, Loss_0: 2.459e-07, Loss_r: 1.007e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 9.993e-05, Loss_0: 2.398e-07, Loss_r: 9.969e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 9.898e-05, Loss_0: 2.337e-07, Loss_r: 9.875e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 9.804e-05, Loss_0: 2.279e-07, Loss_r: 9.781e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 9.712e-05, Loss_0: 2.222e-07, Loss_r: 9.689e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 9.620e-05, Loss_0: 2.165e-07, Loss_r: 9.598e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 9.529e-05, Loss_0: 2.113e-07, Loss_r: 9.508e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 9.440e-05, Loss_0: 2.064e-07, Loss_r: 9.419e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 9.351e-05, Loss_0: 2.015e-07, Loss_r: 9.331e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 9.263e-05, Loss_0: 1.972e-07, Loss_r: 9.243e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 9.176e-05, Loss_0: 1.926e-07, Loss_r: 9.157e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 9.089e-05, Loss_0: 1.881e-07, Loss_r: 9.071e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 9.004e-05, Loss_0: 1.842e-07, Loss_r: 8.985e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 8.919e-05, Loss_0: 1.875e-07, Loss_r: 8.900e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 9.038e-05, Loss_0: 5.381e-07, Loss_r: 8.984e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 6.452e-03, Loss_0: 3.460e-04, Loss_r: 6.106e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.835e-04, Loss_0: 1.281e-04, Loss_r: 2.555e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.957e-04, Loss_0: 4.234e-08, Loss_r: 1.956e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 3.943e-04, Loss_0: 7.878e-05, Loss_r: 3.156e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.624e-04, Loss_0: 5.759e-07, Loss_r: 1.618e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.683e-04, Loss_0: 3.552e-05, Loss_r: 1.328e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.323e-04, Loss_0: 9.362e-10, Loss_r: 1.323e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.285e-04, Loss_0: 7.967e-07, Loss_r: 1.277e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.215e-04, Loss_0: 2.949e-07, Loss_r: 1.212e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.185e-04, Loss_0: 8.733e-07, Loss_r: 1.176e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.162e-04, Loss_0: 8.691e-07, Loss_r: 1.154e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.143e-04, Loss_0: 6.163e-07, Loss_r: 1.137e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.126e-04, Loss_0: 3.750e-07, Loss_r: 1.123e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.111e-04, Loss_0: 2.904e-07, Loss_r: 1.108e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.097e-04, Loss_0: 2.869e-07, Loss_r: 1.094e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.083e-04, Loss_0: 2.917e-07, Loss_r: 1.080e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.070e-04, Loss_0: 2.858e-07, Loss_r: 1.067e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.056e-04, Loss_0: 2.755e-07, Loss_r: 1.054e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.044e-04, Loss_0: 2.666e-07, Loss_r: 1.041e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.031e-04, Loss_0: 2.587e-07, Loss_r: 1.028e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.019e-04, Loss_0: 2.516e-07, Loss_r: 1.016e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.007e-04, Loss_0: 2.441e-07, Loss_r: 1.004e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 9.946e-05, Loss_0: 2.366e-07, Loss_r: 9.923e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 9.829e-05, Loss_0: 2.292e-07, Loss_r: 9.806e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 9.713e-05, Loss_0: 2.222e-07, Loss_r: 9.690e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 9.598e-05, Loss_0: 2.161e-07, Loss_r: 9.577e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 9.485e-05, Loss_0: 2.100e-07, Loss_r: 9.464e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 9.374e-05, Loss_0: 2.039e-07, Loss_r: 9.353e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 9.263e-05, Loss_0: 1.985e-07, Loss_r: 9.243e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 9.154e-05, Loss_0: 1.930e-07, Loss_r: 9.135e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 9.046e-05, Loss_0: 1.880e-07, Loss_r: 9.027e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 8.939e-05, Loss_0: 1.831e-07, Loss_r: 8.921e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 8.833e-05, Loss_0: 1.783e-07, Loss_r: 8.815e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 8.728e-05, Loss_0: 1.737e-07, Loss_r: 8.711e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 8.624e-05, Loss_0: 1.693e-07, Loss_r: 8.607e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 8.521e-05, Loss_0: 1.650e-07, Loss_r: 8.504e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 8.418e-05, Loss_0: 1.609e-07, Loss_r: 8.402e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 8.317e-05, Loss_0: 1.569e-07, Loss_r: 8.301e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 8.217e-05, Loss_0: 1.533e-07, Loss_r: 8.201e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 8.117e-05, Loss_0: 1.497e-07, Loss_r: 8.102e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 8.018e-05, Loss_0: 1.456e-07, Loss_r: 8.003e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 7.920e-05, Loss_0: 1.424e-07, Loss_r: 7.906e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 7.823e-05, Loss_0: 1.391e-07, Loss_r: 7.809e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 7.726e-05, Loss_0: 1.361e-07, Loss_r: 7.713e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 7.631e-05, Loss_0: 1.330e-07, Loss_r: 7.618e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 7.536e-05, Loss_0: 1.298e-07, Loss_r: 7.523e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 7.442e-05, Loss_0: 1.269e-07, Loss_r: 7.430e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 7.349e-05, Loss_0: 1.239e-07, Loss_r: 7.337e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 7.257e-05, Loss_0: 1.213e-07, Loss_r: 7.245e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 7.166e-05, Loss_0: 1.184e-07, Loss_r: 7.154e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 7.075e-05, Loss_0: 1.161e-07, Loss_r: 7.064e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 6.986e-05, Loss_0: 1.142e-07, Loss_r: 6.974e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 6.898e-05, Loss_0: 1.249e-07, Loss_r: 6.885e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 8.267e-05, Loss_0: 1.362e-06, Loss_r: 8.131e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 7.687e-05, Loss_0: 1.171e-07, Loss_r: 7.675e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 6.812e-05, Loss_0: 3.400e-07, Loss_r: 6.778e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 6.682e-05, Loss_0: 1.277e-08, Loss_r: 6.681e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 6.636e-05, Loss_0: 3.010e-07, Loss_r: 6.606e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 6.638e-05, Loss_0: 3.662e-07, Loss_r: 6.601e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.184e-04, Loss_0: 3.314e-06, Loss_r: 1.151e-04, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 1.643e-03, Loss_0: 1.125e-04, Loss_r: 1.531e-03, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 2.458e-04, Loss_0: 4.120e-05, Loss_r: 2.046e-04, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.717e-04, Loss_0: 1.683e-05, Loss_r: 1.549e-04, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 7.424e-05, Loss_0: 3.170e-08, Loss_r: 7.421e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 8.509e-05, Loss_0: 4.649e-06, Loss_r: 8.044e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 7.092e-05, Loss_0: 2.231e-08, Loss_r: 7.090e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 7.015e-05, Loss_0: 4.656e-08, Loss_r: 7.010e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 6.723e-05, Loss_0: 6.895e-08, Loss_r: 6.716e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 6.624e-05, Loss_0: 1.630e-07, Loss_r: 6.608e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 6.536e-05, Loss_0: 1.537e-07, Loss_r: 6.521e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 6.450e-05, Loss_0: 1.240e-07, Loss_r: 6.437e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 6.367e-05, Loss_0: 9.283e-08, Loss_r: 6.358e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 6.287e-05, Loss_0: 7.724e-08, Loss_r: 6.280e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 6.208e-05, Loss_0: 8.295e-08, Loss_r: 6.200e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 6.130e-05, Loss_0: 9.350e-08, Loss_r: 6.120e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 6.052e-05, Loss_0: 8.370e-08, Loss_r: 6.044e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 5.976e-05, Loss_0: 8.634e-08, Loss_r: 5.967e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 5.901e-05, Loss_0: 8.101e-08, Loss_r: 5.892e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 5.826e-05, Loss_0: 7.879e-08, Loss_r: 5.818e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 5.753e-05, Loss_0: 6.979e-08, Loss_r: 5.746e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 5.747e-05, Loss_0: 3.589e-10, Loss_r: 5.747e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 6.227e-04, Loss_0: 5.093e-05, Loss_r: 5.718e-04, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 1.026e-04, Loss_0: 4.929e-06, Loss_r: 9.771e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 9.813e-05, Loss_0: 4.441e-06, Loss_r: 9.369e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 8.260e-05, Loss_0: 2.279e-06, Loss_r: 8.032e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 6.741e-05, Loss_0: 7.733e-07, Loss_r: 6.664e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 5.996e-05, Loss_0: 1.687e-07, Loss_r: 5.979e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 5.628e-05, Loss_0: 1.223e-09, Loss_r: 5.628e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 5.469e-05, Loss_0: 9.339e-08, Loss_r: 5.460e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 5.426e-05, Loss_0: 1.889e-07, Loss_r: 5.407e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 5.339e-05, Loss_0: 5.824e-08, Loss_r: 5.334e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 5.276e-05, Loss_0: 6.161e-08, Loss_r: 5.270e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 5.214e-05, Loss_0: 7.222e-08, Loss_r: 5.207e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 5.154e-05, Loss_0: 8.127e-08, Loss_r: 5.146e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 5.093e-05, Loss_0: 6.791e-08, Loss_r: 5.086e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 5.034e-05, Loss_0: 6.277e-08, Loss_r: 5.028e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 4.981e-05, Loss_0: 3.476e-08, Loss_r: 4.977e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 8.838e-05, Loss_0: 2.245e-06, Loss_r: 8.614e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 5.536e-05, Loss_0: 9.103e-07, Loss_r: 5.445e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 5.293e-05, Loss_0: 7.023e-07, Loss_r: 5.223e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 5.137e-05, Loss_0: 6.864e-08, Loss_r: 5.130e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 4.855e-05, Loss_0: 2.894e-07, Loss_r: 4.826e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 4.756e-05, Loss_0: 7.200e-10, Loss_r: 4.756e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 4.651e-05, Loss_0: 1.054e-07, Loss_r: 4.640e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 4.670e-05, Loss_0: 2.176e-07, Loss_r: 4.648e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 6.377e-05, Loss_0: 1.761e-06, Loss_r: 6.201e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.554e-03, Loss_0: 1.144e-04, Loss_r: 1.439e-03, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.111e-04, Loss_0: 6.733e-06, Loss_r: 1.043e-04, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.843e-04, Loss_0: 2.678e-05, Loss_r: 1.575e-04, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 7.661e-05, Loss_0: 2.553e-06, Loss_r: 7.405e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 5.750e-05, Loss_0: 4.712e-07, Loss_r: 5.702e-05, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 5.717e-05, Loss_0: 1.926e-06, Loss_r: 5.524e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 5.021e-05, Loss_0: 3.073e-07, Loss_r: 4.990e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 4.915e-05, Loss_0: 3.370e-09, Loss_r: 4.914e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 4.844e-05, Loss_0: 1.149e-09, Loss_r: 4.844e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 4.763e-05, Loss_0: 1.199e-08, Loss_r: 4.762e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 4.699e-05, Loss_0: 2.702e-08, Loss_r: 4.696e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 4.641e-05, Loss_0: 3.678e-08, Loss_r: 4.638e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 4.587e-05, Loss_0: 4.683e-08, Loss_r: 4.582e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 4.534e-05, Loss_0: 5.729e-08, Loss_r: 4.528e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 4.482e-05, Loss_0: 5.651e-08, Loss_r: 4.477e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 4.431e-05, Loss_0: 4.942e-08, Loss_r: 4.426e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 4.381e-05, Loss_0: 5.185e-08, Loss_r: 4.376e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 4.332e-05, Loss_0: 4.969e-08, Loss_r: 4.327e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 4.283e-05, Loss_0: 5.063e-08, Loss_r: 4.278e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 4.235e-05, Loss_0: 4.893e-08, Loss_r: 4.230e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 4.188e-05, Loss_0: 4.760e-08, Loss_r: 4.183e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 4.142e-05, Loss_0: 4.137e-08, Loss_r: 4.138e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 4.173e-05, Loss_0: 1.056e-09, Loss_r: 4.173e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 7.982e-04, Loss_0: 5.580e-05, Loss_r: 7.424e-04, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 2.672e-04, Loss_0: 2.927e-05, Loss_r: 2.379e-04, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.189e-04, Loss_0: 8.975e-06, Loss_r: 1.100e-04, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 4.459e-05, Loss_0: 4.826e-07, Loss_r: 4.411e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 5.506e-05, Loss_0: 2.579e-06, Loss_r: 5.248e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 4.429e-05, Loss_0: 6.777e-07, Loss_r: 4.361e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 4.122e-05, Loss_0: 9.902e-08, Loss_r: 4.112e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 4.067e-05, Loss_0: 3.113e-08, Loss_r: 4.064e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 4.020e-05, Loss_0: 3.616e-08, Loss_r: 4.016e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 3.977e-05, Loss_0: 5.979e-08, Loss_r: 3.971e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 3.937e-05, Loss_0: 6.931e-08, Loss_r: 3.930e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 3.894e-05, Loss_0: 5.460e-08, Loss_r: 3.889e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 3.854e-05, Loss_0: 3.653e-08, Loss_r: 3.850e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 3.814e-05, Loss_0: 3.915e-08, Loss_r: 3.810e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 3.775e-05, Loss_0: 4.370e-08, Loss_r: 3.771e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 3.737e-05, Loss_0: 3.845e-08, Loss_r: 3.733e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 3.699e-05, Loss_0: 3.990e-08, Loss_r: 3.695e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 3.662e-05, Loss_0: 3.996e-08, Loss_r: 3.658e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 3.625e-05, Loss_0: 3.923e-08, Loss_r: 3.622e-05, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 3.589e-05, Loss_0: 4.234e-08, Loss_r: 3.585e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 3.578e-05, Loss_0: 1.081e-07, Loss_r: 3.567e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 1.684e-04, Loss_0: 1.115e-05, Loss_r: 1.573e-04, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 3.542e-05, Loss_0: 1.044e-09, Loss_r: 3.542e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 5.183e-05, Loss_0: 9.649e-07, Loss_r: 5.086e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 4.116e-05, Loss_0: 2.634e-07, Loss_r: 4.089e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 3.509e-05, Loss_0: 1.766e-07, Loss_r: 3.491e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 3.495e-05, Loss_0: 1.944e-07, Loss_r: 3.476e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 3.435e-05, Loss_0: 1.096e-11, Loss_r: 3.435e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 3.371e-05, Loss_0: 8.415e-08, Loss_r: 3.362e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 3.334e-05, Loss_0: 1.109e-08, Loss_r: 3.333e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 3.299e-05, Loss_0: 5.121e-08, Loss_r: 3.294e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 3.271e-05, Loss_0: 5.357e-08, Loss_r: 3.266e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 3.256e-05, Loss_0: 8.078e-08, Loss_r: 3.248e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 4.012e-05, Loss_0: 8.047e-07, Loss_r: 3.931e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 1.104e-03, Loss_0: 7.875e-05, Loss_r: 1.026e-03, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.657e-04, Loss_0: 2.045e-05, Loss_r: 1.453e-04, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.450e-04, Loss_0: 1.190e-05, Loss_r: 1.331e-04, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 5.547e-05, Loss_0: 3.535e-06, Loss_r: 5.193e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 3.445e-05, Loss_0: 2.261e-08, Loss_r: 3.442e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 3.658e-05, Loss_0: 1.792e-07, Loss_r: 3.640e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 3.458e-05, Loss_0: 3.536e-07, Loss_r: 3.423e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 3.282e-05, Loss_0: 7.754e-08, Loss_r: 3.274e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 3.259e-05, Loss_0: 1.888e-09, Loss_r: 3.259e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 3.216e-05, Loss_0: 1.155e-08, Loss_r: 3.215e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 3.183e-05, Loss_0: 3.084e-08, Loss_r: 3.180e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 3.156e-05, Loss_0: 3.697e-08, Loss_r: 3.152e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 3.129e-05, Loss_0: 3.533e-08, Loss_r: 3.126e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 3.103e-05, Loss_0: 3.251e-08, Loss_r: 3.100e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 3.077e-05, Loss_0: 3.014e-08, Loss_r: 3.074e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 3.051e-05, Loss_0: 2.879e-08, Loss_r: 3.049e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 3.026e-05, Loss_0: 2.759e-08, Loss_r: 3.024e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 3.001e-05, Loss_0: 2.677e-08, Loss_r: 2.999e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 2.977e-05, Loss_0: 2.669e-08, Loss_r: 2.974e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 2.953e-05, Loss_0: 2.677e-08, Loss_r: 2.950e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 2.929e-05, Loss_0: 2.624e-08, Loss_r: 2.926e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 2.905e-05, Loss_0: 2.577e-08, Loss_r: 2.903e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 2.882e-05, Loss_0: 2.560e-08, Loss_r: 2.879e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 2.859e-05, Loss_0: 2.510e-08, Loss_r: 2.856e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 2.836e-05, Loss_0: 2.488e-08, Loss_r: 2.834e-05, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 2.814e-05, Loss_0: 2.460e-08, Loss_r: 2.811e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 2.792e-05, Loss_0: 2.420e-08, Loss_r: 2.789e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 2.770e-05, Loss_0: 2.375e-08, Loss_r: 2.767e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 2.748e-05, Loss_0: 2.087e-08, Loss_r: 2.746e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 2.793e-05, Loss_0: 3.036e-09, Loss_r: 2.793e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 7.872e-04, Loss_0: 4.644e-05, Loss_r: 7.407e-04, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 3.218e-04, Loss_0: 3.658e-05, Loss_r: 2.852e-04, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 3.389e-05, Loss_0: 2.338e-07, Loss_r: 3.365e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 6.485e-05, Loss_0: 5.773e-06, Loss_r: 5.908e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 2.892e-05, Loss_0: 8.243e-08, Loss_r: 2.884e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 3.277e-05, Loss_0: 3.531e-07, Loss_r: 3.242e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 2.815e-05, Loss_0: 2.510e-10, Loss_r: 2.815e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 2.772e-05, Loss_0: 8.860e-08, Loss_r: 2.763e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 2.751e-05, Loss_0: 8.775e-08, Loss_r: 2.742e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 2.716e-05, Loss_0: 5.548e-08, Loss_r: 2.711e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 2.690e-05, Loss_0: 3.990e-08, Loss_r: 2.686e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 2.668e-05, Loss_0: 3.140e-08, Loss_r: 2.665e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 2.648e-05, Loss_0: 2.666e-08, Loss_r: 2.645e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 2.628e-05, Loss_0: 2.364e-08, Loss_r: 2.625e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 2.608e-05, Loss_0: 2.066e-08, Loss_r: 2.606e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 2.589e-05, Loss_0: 1.940e-08, Loss_r: 2.587e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 2.570e-05, Loss_0: 2.014e-08, Loss_r: 2.568e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 2.551e-05, Loss_0: 2.060e-08, Loss_r: 2.549e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 2.533e-05, Loss_0: 1.962e-08, Loss_r: 2.531e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 2.514e-05, Loss_0: 1.974e-08, Loss_r: 2.512e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 2.496e-05, Loss_0: 1.932e-08, Loss_r: 2.494e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3750, Loss: 2.478e-05, Loss_0: 1.938e-08, Loss_r: 2.477e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3760, Loss: 2.461e-05, Loss_0: 1.893e-08, Loss_r: 2.459e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3770, Loss: 2.443e-05, Loss_0: 1.872e-08, Loss_r: 2.441e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3780, Loss: 2.426e-05, Loss_0: 1.825e-08, Loss_r: 2.424e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3790, Loss: 2.409e-05, Loss_0: 1.677e-08, Loss_r: 2.407e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3800, Loss: 2.398e-05, Loss_0: 4.684e-09, Loss_r: 2.398e-05, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3810, Loss: 4.806e-05, Loss_0: 1.335e-06, Loss_r: 4.672e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 2.381e-05, Loss_0: 5.287e-08, Loss_r: 2.375e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 2.811e-05, Loss_0: 4.879e-07, Loss_r: 2.763e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 2.338e-05, Loss_0: 3.075e-08, Loss_r: 2.335e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 2.385e-05, Loss_0: 5.582e-09, Loss_r: 2.384e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 2.323e-05, Loss_0: 5.333e-08, Loss_r: 2.317e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 2.295e-05, Loss_0: 9.448e-09, Loss_r: 2.294e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 2.279e-05, Loss_0: 2.147e-08, Loss_r: 2.277e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 2.266e-05, Loss_0: 1.055e-08, Loss_r: 2.265e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 2.252e-05, Loss_0: 2.192e-08, Loss_r: 2.249e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 2.237e-05, Loss_0: 1.962e-08, Loss_r: 2.235e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 2.224e-05, Loss_0: 1.955e-08, Loss_r: 2.222e-05, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 2.217e-05, Loss_0: 3.589e-08, Loss_r: 2.213e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3940, Loss: 2.791e-05, Loss_0: 5.217e-07, Loss_r: 2.739e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 2.222e-05, Loss_0: 7.372e-08, Loss_r: 2.215e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 2.203e-05, Loss_0: 1.380e-10, Loss_r: 2.203e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 2.196e-05, Loss_0: 4.186e-10, Loss_r: 2.196e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 2.150e-05, Loss_0: 1.920e-08, Loss_r: 2.148e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 2.142e-05, Loss_0: 2.833e-08, Loss_r: 2.139e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 2.128e-05, Loss_0: 8.106e-09, Loss_r: 2.127e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 2.115e-05, Loss_0: 1.614e-08, Loss_r: 2.113e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 2.103e-05, Loss_0: 1.304e-08, Loss_r: 2.102e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 2.092e-05, Loss_0: 1.465e-08, Loss_r: 2.090e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 2.080e-05, Loss_0: 1.221e-08, Loss_r: 2.079e-05, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 2.069e-05, Loss_0: 1.370e-08, Loss_r: 2.068e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 2.058e-05, Loss_0: 1.416e-08, Loss_r: 2.056e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 2.047e-05, Loss_0: 1.574e-08, Loss_r: 2.045e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 2.044e-05, Loss_0: 3.280e-08, Loss_r: 2.041e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 3.056e-05, Loss_0: 7.296e-07, Loss_r: 2.983e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 2.060e-05, Loss_0: 6.869e-08, Loss_r: 2.053e-05, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 2.076e-05, Loss_0: 6.526e-09, Loss_r: 2.075e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 2.054e-05, Loss_0: 3.601e-09, Loss_r: 2.053e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 1.990e-05, Loss_0: 2.105e-08, Loss_r: 1.988e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 1.984e-05, Loss_0: 2.825e-08, Loss_r: 1.982e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 1.971e-05, Loss_0: 5.090e-09, Loss_r: 1.971e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 1.959e-05, Loss_0: 1.436e-08, Loss_r: 1.958e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 1.949e-05, Loss_0: 1.110e-08, Loss_r: 1.948e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 1.940e-05, Loss_0: 1.226e-08, Loss_r: 1.939e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 1.931e-05, Loss_0: 1.017e-08, Loss_r: 1.930e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 1.921e-05, Loss_0: 1.194e-08, Loss_r: 1.920e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 1.912e-05, Loss_0: 1.184e-08, Loss_r: 1.911e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 1.903e-05, Loss_0: 1.216e-08, Loss_r: 1.902e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 1.895e-05, Loss_0: 1.689e-08, Loss_r: 1.893e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 1.987e-05, Loss_0: 1.069e-07, Loss_r: 1.976e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 1.884e-05, Loss_0: 2.681e-08, Loss_r: 1.881e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.872e-05, Loss_0: 3.423e-09, Loss_r: 1.872e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 1.866e-05, Loss_0: 2.339e-09, Loss_r: 1.866e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 1.852e-05, Loss_0: 1.045e-08, Loss_r: 1.851e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 1.845e-05, Loss_0: 1.461e-08, Loss_r: 1.843e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 1.836e-05, Loss_0: 8.692e-09, Loss_r: 1.835e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 1.828e-05, Loss_0: 9.818e-09, Loss_r: 1.827e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 1.820e-05, Loss_0: 1.043e-08, Loss_r: 1.819e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 1.812e-05, Loss_0: 9.529e-09, Loss_r: 1.811e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 1.804e-05, Loss_0: 9.746e-09, Loss_r: 1.803e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 1.796e-05, Loss_0: 9.839e-09, Loss_r: 1.795e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 1.788e-05, Loss_0: 9.445e-09, Loss_r: 1.787e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 1.780e-05, Loss_0: 9.226e-09, Loss_r: 1.779e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 1.773e-05, Loss_0: 8.988e-09, Loss_r: 1.772e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 1.765e-05, Loss_0: 7.331e-09, Loss_r: 1.764e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 1.787e-05, Loss_0: 3.990e-10, Loss_r: 1.787e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 1.026e-04, Loss_0: 3.409e-06, Loss_r: 9.916e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 2.371e-05, Loss_0: 2.292e-07, Loss_r: 2.348e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 1.772e-05, Loss_0: 3.805e-08, Loss_r: 1.768e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 2.015e-05, Loss_0: 2.150e-07, Loss_r: 1.994e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 1.914e-05, Loss_0: 1.556e-07, Loss_r: 1.899e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 1.736e-05, Loss_0: 2.627e-08, Loss_r: 1.733e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 1.736e-05, Loss_0: 2.098e-11, Loss_r: 1.736e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 1.715e-05, Loss_0: 3.301e-09, Loss_r: 1.714e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 1.709e-05, Loss_0: 1.741e-08, Loss_r: 1.707e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 1.699e-05, Loss_0: 6.562e-09, Loss_r: 1.698e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 1.691e-05, Loss_0: 8.096e-09, Loss_r: 1.691e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 1.685e-05, Loss_0: 8.618e-09, Loss_r: 1.684e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 1.678e-05, Loss_0: 8.210e-09, Loss_r: 1.677e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 1.671e-05, Loss_0: 7.661e-09, Loss_r: 1.670e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 1.664e-05, Loss_0: 8.428e-09, Loss_r: 1.663e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 1.657e-05, Loss_0: 8.310e-09, Loss_r: 1.656e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 1.651e-05, Loss_0: 8.465e-09, Loss_r: 1.650e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 1.644e-05, Loss_0: 1.033e-08, Loss_r: 1.643e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 1.664e-05, Loss_0: 3.893e-08, Loss_r: 1.660e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 6.399e-05, Loss_0: 2.420e-06, Loss_r: 6.157e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 1.993e-05, Loss_0: 2.290e-07, Loss_r: 1.970e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 1.676e-05, Loss_0: 6.051e-09, Loss_r: 1.675e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 1.848e-05, Loss_0: 5.704e-08, Loss_r: 1.843e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 1.681e-05, Loss_0: 8.287e-09, Loss_r: 1.680e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 1.610e-05, Loss_0: 1.689e-08, Loss_r: 1.608e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 1.613e-05, Loss_0: 2.622e-08, Loss_r: 1.610e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 1.596e-05, Loss_0: 3.326e-09, Loss_r: 1.595e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 1.588e-05, Loss_0: 5.397e-09, Loss_r: 1.588e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 1.582e-05, Loss_0: 1.013e-08, Loss_r: 1.581e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 1.576e-05, Loss_0: 5.388e-09, Loss_r: 1.576e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 1.570e-05, Loss_0: 8.215e-09, Loss_r: 1.569e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 1.564e-05, Loss_0: 6.414e-09, Loss_r: 1.563e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.558e-05, Loss_0: 6.600e-09, Loss_r: 1.557e-05, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.552e-05, Loss_0: 7.221e-09, Loss_r: 1.551e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.546e-05, Loss_0: 7.468e-09, Loss_r: 1.545e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 1.541e-05, Loss_0: 9.305e-09, Loss_r: 1.540e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 1.555e-05, Loss_0: 3.128e-08, Loss_r: 1.552e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 4.027e-05, Loss_0: 1.272e-06, Loss_r: 3.900e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 1.741e-05, Loss_0: 1.452e-07, Loss_r: 1.727e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 1.556e-05, Loss_0: 2.492e-09, Loss_r: 1.556e-05, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 1.649e-05, Loss_0: 2.622e-08, Loss_r: 1.646e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 1.532e-05, Loss_0: 3.896e-10, Loss_r: 1.532e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 1.513e-05, Loss_0: 1.977e-08, Loss_r: 1.511e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 1.503e-05, Loss_0: 1.399e-08, Loss_r: 1.501e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 1.496e-05, Loss_0: 2.183e-09, Loss_r: 1.496e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 1.489e-05, Loss_0: 7.497e-09, Loss_r: 1.488e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 1.483e-05, Loss_0: 6.537e-09, Loss_r: 1.482e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 1.478e-05, Loss_0: 5.698e-09, Loss_r: 1.477e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 1.472e-05, Loss_0: 6.198e-09, Loss_r: 1.472e-05, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 1.467e-05, Loss_0: 6.365e-09, Loss_r: 1.467e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 1.462e-05, Loss_0: 5.563e-09, Loss_r: 1.461e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 1.457e-05, Loss_0: 5.544e-09, Loss_r: 1.456e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 1.451e-05, Loss_0: 5.385e-09, Loss_r: 1.451e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 1.446e-05, Loss_0: 3.883e-09, Loss_r: 1.446e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.469e-05, Loss_0: 1.110e-09, Loss_r: 1.469e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 5.812e-05, Loss_0: 1.610e-06, Loss_r: 5.651e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 1.934e-05, Loss_0: 1.598e-07, Loss_r: 1.918e-05, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 1.431e-05, Loss_0: 8.902e-09, Loss_r: 1.430e-05, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 1.549e-05, Loss_0: 9.573e-08, Loss_r: 1.540e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 21.4597\n",
            "[1, 128, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 4.430e-01, Loss_0: 1.243e-03, Loss_r: 4.418e-01, Time: 0.74, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.012e-01, Loss_0: 1.099e-02, Loss_r: 1.902e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.404e-01, Loss_0: 5.330e-02, Loss_r: 8.707e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.231e-01, Loss_0: 4.976e-02, Loss_r: 7.337e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.184e-01, Loss_0: 6.555e-02, Loss_r: 5.288e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.166e-01, Loss_0: 5.702e-02, Loss_r: 5.962e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.161e-01, Loss_0: 5.516e-02, Loss_r: 6.091e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.160e-01, Loss_0: 5.356e-02, Loss_r: 6.249e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.160e-01, Loss_0: 5.211e-02, Loss_r: 6.387e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.158e-01, Loss_0: 5.355e-02, Loss_r: 6.229e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.157e-01, Loss_0: 5.386e-02, Loss_r: 6.186e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.156e-01, Loss_0: 5.367e-02, Loss_r: 6.190e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.154e-01, Loss_0: 5.357e-02, Loss_r: 6.180e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.151e-01, Loss_0: 5.340e-02, Loss_r: 6.171e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.147e-01, Loss_0: 5.323e-02, Loss_r: 6.149e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.141e-01, Loss_0: 5.309e-02, Loss_r: 6.104e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.131e-01, Loss_0: 5.287e-02, Loss_r: 6.025e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.112e-01, Loss_0: 5.246e-02, Loss_r: 5.871e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.067e-01, Loss_0: 5.168e-02, Loss_r: 5.506e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 9.563e-02, Loss_0: 4.967e-02, Loss_r: 4.596e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 200, Loss: 7.325e-02, Loss_0: 4.084e-02, Loss_r: 3.242e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.531e-02, Loss_0: 1.402e-02, Loss_r: 3.128e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 3.213e-02, Loss_0: 5.379e-03, Loss_r: 2.675e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 230, Loss: 3.507e-02, Loss_0: 2.371e-03, Loss_r: 3.270e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.594e-02, Loss_0: 2.317e-03, Loss_r: 1.362e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.004e-02, Loss_0: 1.535e-03, Loss_r: 8.500e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 260, Loss: 6.786e-03, Loss_0: 8.207e-04, Loss_r: 5.965e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 4.402e-03, Loss_0: 4.232e-04, Loss_r: 3.978e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 280, Loss: 2.970e-03, Loss_0: 2.038e-04, Loss_r: 2.766e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 290, Loss: 2.194e-03, Loss_0: 1.212e-04, Loss_r: 2.073e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.638e-03, Loss_0: 9.666e-05, Loss_r: 1.541e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.240e-03, Loss_0: 5.833e-05, Loss_r: 1.182e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 9.515e-04, Loss_0: 4.661e-05, Loss_r: 9.049e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 330, Loss: 7.415e-04, Loss_0: 3.150e-05, Loss_r: 7.100e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.952e-04, Loss_0: 2.439e-05, Loss_r: 5.708e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.915e-04, Loss_0: 1.777e-05, Loss_r: 4.737e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.169e-04, Loss_0: 1.321e-05, Loss_r: 4.037e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 3.624e-04, Loss_0: 1.038e-05, Loss_r: 3.520e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 3.260e-04, Loss_0: 8.415e-06, Loss_r: 3.175e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.022e-03, Loss_0: 1.644e-05, Loss_r: 5.005e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.752e-03, Loss_0: 6.417e-06, Loss_r: 1.746e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 410, Loss: 3.234e-04, Loss_0: 1.624e-06, Loss_r: 3.218e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.090e-04, Loss_0: 5.309e-06, Loss_r: 3.037e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.736e-04, Loss_0: 4.873e-06, Loss_r: 2.687e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.322e-04, Loss_0: 5.368e-06, Loss_r: 2.269e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.152e-04, Loss_0: 4.370e-06, Loss_r: 2.109e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.076e-04, Loss_0: 3.023e-06, Loss_r: 2.046e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.880e-04, Loss_0: 3.007e-06, Loss_r: 1.850e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.757e-04, Loss_0: 2.542e-06, Loss_r: 1.731e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.647e-04, Loss_0: 2.110e-06, Loss_r: 1.626e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.557e-04, Loss_0: 1.844e-06, Loss_r: 1.538e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.473e-04, Loss_0: 1.650e-06, Loss_r: 1.456e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.396e-04, Loss_0: 1.489e-06, Loss_r: 1.381e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.325e-04, Loss_0: 1.352e-06, Loss_r: 1.312e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.260e-04, Loss_0: 1.231e-06, Loss_r: 1.248e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.200e-04, Loss_0: 1.114e-06, Loss_r: 1.189e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.144e-04, Loss_0: 1.018e-06, Loss_r: 1.134e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.093e-04, Loss_0: 9.334e-07, Loss_r: 1.084e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.045e-04, Loss_0: 8.592e-07, Loss_r: 1.037e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.001e-04, Loss_0: 7.921e-07, Loss_r: 9.933e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 9.611e-05, Loss_0: 7.483e-07, Loss_r: 9.536e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.263e-04, Loss_0: 1.171e-06, Loss_r: 1.252e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.960e-02, Loss_0: 3.282e-05, Loss_r: 1.956e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 8.393e-04, Loss_0: 6.105e-09, Loss_r: 8.393e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.837e-04, Loss_0: 2.169e-08, Loss_r: 2.836e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.303e-04, Loss_0: 2.155e-06, Loss_r: 1.282e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.040e-04, Loss_0: 3.669e-06, Loss_r: 2.004e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.608e-04, Loss_0: 1.201e-07, Loss_r: 1.607e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.171e-04, Loss_0: 1.345e-06, Loss_r: 1.158e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 9.712e-05, Loss_0: 6.460e-07, Loss_r: 9.647e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 700, Loss: 8.776e-05, Loss_0: 8.290e-07, Loss_r: 8.693e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 8.224e-05, Loss_0: 4.957e-07, Loss_r: 8.175e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 7.865e-05, Loss_0: 5.883e-07, Loss_r: 7.806e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 730, Loss: 7.610e-05, Loss_0: 5.000e-07, Loss_r: 7.560e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.373e-05, Loss_0: 4.314e-07, Loss_r: 7.330e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 7.150e-05, Loss_0: 4.118e-07, Loss_r: 7.109e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 6.946e-05, Loss_0: 3.845e-07, Loss_r: 6.907e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 6.753e-05, Loss_0: 3.585e-07, Loss_r: 6.717e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 780, Loss: 6.572e-05, Loss_0: 3.292e-07, Loss_r: 6.539e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 6.400e-05, Loss_0: 3.087e-07, Loss_r: 6.369e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 800, Loss: 6.238e-05, Loss_0: 2.887e-07, Loss_r: 6.209e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.084e-05, Loss_0: 2.721e-07, Loss_r: 6.056e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.937e-05, Loss_0: 2.562e-07, Loss_r: 5.911e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 5.797e-05, Loss_0: 2.420e-07, Loss_r: 5.773e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 5.663e-05, Loss_0: 2.285e-07, Loss_r: 5.640e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 5.535e-05, Loss_0: 2.161e-07, Loss_r: 5.514e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 5.413e-05, Loss_0: 2.045e-07, Loss_r: 5.392e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 5.295e-05, Loss_0: 1.939e-07, Loss_r: 5.276e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 5.182e-05, Loss_0: 1.842e-07, Loss_r: 5.163e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 890, Loss: 5.073e-05, Loss_0: 1.749e-07, Loss_r: 5.055e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 900, Loss: 4.967e-05, Loss_0: 1.664e-07, Loss_r: 4.951e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 910, Loss: 4.866e-05, Loss_0: 1.581e-07, Loss_r: 4.850e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 4.767e-05, Loss_0: 1.495e-07, Loss_r: 4.752e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 930, Loss: 4.676e-05, Loss_0: 1.330e-07, Loss_r: 4.663e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 5.817e-05, Loss_0: 1.481e-08, Loss_r: 5.816e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 8.055e-03, Loss_0: 4.247e-05, Loss_r: 8.013e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 960, Loss: 4.217e-04, Loss_0: 1.135e-07, Loss_r: 4.216e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 3.284e-04, Loss_0: 6.013e-06, Loss_r: 3.224e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.048e-04, Loss_0: 1.455e-06, Loss_r: 1.034e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 5.992e-05, Loss_0: 1.506e-06, Loss_r: 5.841e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 6.298e-05, Loss_0: 6.520e-10, Loss_r: 6.298e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 5.787e-05, Loss_0: 4.805e-07, Loss_r: 5.739e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 5.230e-05, Loss_0: 3.024e-07, Loss_r: 5.200e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 4.931e-05, Loss_0: 2.634e-07, Loss_r: 4.904e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 4.792e-05, Loss_0: 1.985e-07, Loss_r: 4.772e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 4.681e-05, Loss_0: 1.342e-07, Loss_r: 4.668e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 4.551e-05, Loss_0: 2.168e-07, Loss_r: 4.529e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 4.431e-05, Loss_0: 1.591e-07, Loss_r: 4.415e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 4.332e-05, Loss_0: 1.380e-07, Loss_r: 4.318e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 4.237e-05, Loss_0: 1.397e-07, Loss_r: 4.223e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.146e-05, Loss_0: 1.305e-07, Loss_r: 4.133e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 4.058e-05, Loss_0: 1.204e-07, Loss_r: 4.046e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.974e-05, Loss_0: 1.142e-07, Loss_r: 3.963e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 3.893e-05, Loss_0: 1.093e-07, Loss_r: 3.882e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.814e-05, Loss_0: 1.046e-07, Loss_r: 3.803e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.737e-05, Loss_0: 9.977e-08, Loss_r: 3.727e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 3.662e-05, Loss_0: 9.525e-08, Loss_r: 3.653e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 3.590e-05, Loss_0: 9.109e-08, Loss_r: 3.581e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 3.519e-05, Loss_0: 8.706e-08, Loss_r: 3.511e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 3.450e-05, Loss_0: 8.339e-08, Loss_r: 3.442e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 3.383e-05, Loss_0: 7.989e-08, Loss_r: 3.375e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 3.318e-05, Loss_0: 7.660e-08, Loss_r: 3.310e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.254e-05, Loss_0: 7.340e-08, Loss_r: 3.247e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.192e-05, Loss_0: 7.053e-08, Loss_r: 3.185e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.131e-05, Loss_0: 6.775e-08, Loss_r: 3.124e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.071e-05, Loss_0: 6.511e-08, Loss_r: 3.064e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.012e-05, Loss_0: 6.255e-08, Loss_r: 3.006e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.955e-05, Loss_0: 6.031e-08, Loss_r: 2.949e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.899e-05, Loss_0: 5.803e-08, Loss_r: 2.893e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.844e-05, Loss_0: 5.580e-08, Loss_r: 2.839e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.791e-05, Loss_0: 5.354e-08, Loss_r: 2.785e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.738e-05, Loss_0: 5.064e-08, Loss_r: 2.733e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.692e-05, Loss_0: 3.913e-08, Loss_r: 2.688e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 4.041e-05, Loss_0: 4.040e-08, Loss_r: 4.037e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 7.079e-03, Loss_0: 9.382e-05, Loss_r: 6.985e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.133e-04, Loss_0: 5.109e-06, Loss_r: 1.082e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.326e-03, Loss_0: 3.073e-05, Loss_r: 1.295e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 5.241e-04, Loss_0: 1.256e-05, Loss_r: 5.115e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.927e-04, Loss_0: 4.389e-06, Loss_r: 1.884e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 8.614e-05, Loss_0: 2.106e-06, Loss_r: 8.403e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 5.267e-05, Loss_0: 2.432e-07, Loss_r: 5.242e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 4.000e-05, Loss_0: 8.006e-07, Loss_r: 3.920e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 3.376e-05, Loss_0: 1.510e-09, Loss_r: 3.376e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.035e-05, Loss_0: 2.554e-07, Loss_r: 3.009e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.878e-05, Loss_0: 6.632e-08, Loss_r: 2.872e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.810e-05, Loss_0: 4.954e-08, Loss_r: 2.805e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.745e-05, Loss_0: 9.135e-08, Loss_r: 2.736e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.678e-05, Loss_0: 6.278e-08, Loss_r: 2.672e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.619e-05, Loss_0: 5.589e-08, Loss_r: 2.613e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.561e-05, Loss_0: 5.993e-08, Loss_r: 2.555e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.505e-05, Loss_0: 5.758e-08, Loss_r: 2.500e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.452e-05, Loss_0: 5.296e-08, Loss_r: 2.446e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.399e-05, Loss_0: 4.939e-08, Loss_r: 2.395e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.349e-05, Loss_0: 4.724e-08, Loss_r: 2.344e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 2.300e-05, Loss_0: 4.561e-08, Loss_r: 2.295e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.252e-05, Loss_0: 4.367e-08, Loss_r: 2.248e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.205e-05, Loss_0: 4.178e-08, Loss_r: 2.201e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.160e-05, Loss_0: 4.014e-08, Loss_r: 2.156e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.116e-05, Loss_0: 3.831e-08, Loss_r: 2.112e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.073e-05, Loss_0: 3.671e-08, Loss_r: 2.069e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.031e-05, Loss_0: 3.526e-08, Loss_r: 2.027e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.990e-05, Loss_0: 3.385e-08, Loss_r: 1.987e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.950e-05, Loss_0: 3.243e-08, Loss_r: 1.947e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.911e-05, Loss_0: 3.126e-08, Loss_r: 1.908e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.873e-05, Loss_0: 3.004e-08, Loss_r: 1.870e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.835e-05, Loss_0: 2.895e-08, Loss_r: 1.832e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.799e-05, Loss_0: 2.776e-08, Loss_r: 1.796e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.763e-05, Loss_0: 2.677e-08, Loss_r: 1.760e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.728e-05, Loss_0: 2.584e-08, Loss_r: 1.725e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.694e-05, Loss_0: 2.473e-08, Loss_r: 1.691e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.660e-05, Loss_0: 2.384e-08, Loss_r: 1.658e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.627e-05, Loss_0: 2.302e-08, Loss_r: 1.625e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.595e-05, Loss_0: 2.227e-08, Loss_r: 1.593e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.564e-05, Loss_0: 2.134e-08, Loss_r: 1.562e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.533e-05, Loss_0: 2.046e-08, Loss_r: 1.531e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.503e-05, Loss_0: 1.800e-08, Loss_r: 1.501e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.512e-05, Loss_0: 2.883e-10, Loss_r: 1.512e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 2.334e-04, Loss_0: 7.789e-06, Loss_r: 2.256e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 5.742e-03, Loss_0: 1.518e-04, Loss_r: 5.591e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 2.128e-03, Loss_0: 8.785e-05, Loss_r: 2.040e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 6.656e-04, Loss_0: 3.916e-05, Loss_r: 6.264e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 3.823e-04, Loss_0: 2.221e-05, Loss_r: 3.601e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.592e-04, Loss_0: 9.798e-06, Loss_r: 1.494e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 5.463e-05, Loss_0: 4.556e-06, Loss_r: 5.007e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 2.670e-05, Loss_0: 8.353e-08, Loss_r: 2.662e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 2.084e-05, Loss_0: 2.843e-07, Loss_r: 2.055e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.933e-05, Loss_0: 9.636e-13, Loss_r: 1.933e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.861e-05, Loss_0: 1.138e-07, Loss_r: 1.850e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.806e-05, Loss_0: 1.321e-08, Loss_r: 1.805e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.755e-05, Loss_0: 7.015e-08, Loss_r: 1.748e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.706e-05, Loss_0: 1.542e-08, Loss_r: 1.705e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.661e-05, Loss_0: 4.419e-08, Loss_r: 1.656e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.619e-05, Loss_0: 2.847e-08, Loss_r: 1.616e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.580e-05, Loss_0: 2.569e-08, Loss_r: 1.577e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.542e-05, Loss_0: 2.954e-08, Loss_r: 1.539e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.506e-05, Loss_0: 2.540e-08, Loss_r: 1.503e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.471e-05, Loss_0: 2.369e-08, Loss_r: 1.468e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.437e-05, Loss_0: 2.323e-08, Loss_r: 1.434e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.404e-05, Loss_0: 2.225e-08, Loss_r: 1.402e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.372e-05, Loss_0: 2.099e-08, Loss_r: 1.370e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.341e-05, Loss_0: 1.989e-08, Loss_r: 1.339e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.312e-05, Loss_0: 1.895e-08, Loss_r: 1.310e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.283e-05, Loss_0: 1.816e-08, Loss_r: 1.281e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.255e-05, Loss_0: 1.731e-08, Loss_r: 1.253e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.227e-05, Loss_0: 1.659e-08, Loss_r: 1.226e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.201e-05, Loss_0: 1.587e-08, Loss_r: 1.199e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.175e-05, Loss_0: 1.520e-08, Loss_r: 1.173e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.150e-05, Loss_0: 1.446e-08, Loss_r: 1.148e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.125e-05, Loss_0: 1.382e-08, Loss_r: 1.124e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.101e-05, Loss_0: 1.337e-08, Loss_r: 1.100e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.078e-05, Loss_0: 1.279e-08, Loss_r: 1.076e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.055e-05, Loss_0: 1.220e-08, Loss_r: 1.054e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.033e-05, Loss_0: 1.168e-08, Loss_r: 1.032e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.011e-05, Loss_0: 1.137e-08, Loss_r: 1.010e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 9.901e-06, Loss_0: 1.092e-08, Loss_r: 9.890e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 9.695e-06, Loss_0: 1.050e-08, Loss_r: 9.684e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 9.494e-06, Loss_0: 1.013e-08, Loss_r: 9.484e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 9.298e-06, Loss_0: 9.677e-09, Loss_r: 9.288e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 9.106e-06, Loss_0: 9.275e-09, Loss_r: 9.097e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 8.919e-06, Loss_0: 9.004e-09, Loss_r: 8.910e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 8.737e-06, Loss_0: 8.595e-09, Loss_r: 8.728e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 8.559e-06, Loss_0: 8.190e-09, Loss_r: 8.551e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 8.385e-06, Loss_0: 7.798e-09, Loss_r: 8.377e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 8.216e-06, Loss_0: 7.507e-09, Loss_r: 8.208e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 8.050e-06, Loss_0: 6.977e-09, Loss_r: 8.043e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 7.891e-06, Loss_0: 4.995e-09, Loss_r: 7.886e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 7.930e-06, Loss_0: 2.217e-09, Loss_r: 7.927e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 4.608e-05, Loss_0: 2.980e-06, Loss_r: 4.310e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 8.018e-06, Loss_0: 1.976e-08, Loss_r: 7.998e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.148e-05, Loss_0: 4.495e-07, Loss_r: 1.103e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 8.793e-06, Loss_0: 1.962e-07, Loss_r: 8.597e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 7.354e-06, Loss_0: 6.249e-09, Loss_r: 7.348e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 7.094e-06, Loss_0: 1.401e-09, Loss_r: 7.093e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 6.933e-06, Loss_0: 3.000e-08, Loss_r: 6.903e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 6.708e-06, Loss_0: 1.853e-09, Loss_r: 6.706e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 6.579e-06, Loss_0: 5.246e-09, Loss_r: 6.574e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 6.464e-06, Loss_0: 5.517e-09, Loss_r: 6.459e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 6.352e-06, Loss_0: 5.116e-09, Loss_r: 6.347e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 6.242e-06, Loss_0: 3.885e-09, Loss_r: 6.238e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 6.134e-06, Loss_0: 5.578e-09, Loss_r: 6.129e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 6.028e-06, Loss_0: 4.976e-09, Loss_r: 6.023e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 5.924e-06, Loss_0: 4.508e-09, Loss_r: 5.920e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 5.823e-06, Loss_0: 4.339e-09, Loss_r: 5.819e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 5.724e-06, Loss_0: 4.708e-09, Loss_r: 5.719e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 5.652e-06, Loss_0: 1.157e-08, Loss_r: 5.640e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 1.023e-05, Loss_0: 4.551e-07, Loss_r: 9.774e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 5.524e-06, Loss_0: 1.934e-08, Loss_r: 5.505e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 5.864e-06, Loss_0: 1.933e-08, Loss_r: 5.845e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 5.482e-06, Loss_0: 4.316e-09, Loss_r: 5.478e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 5.239e-06, Loss_0: 1.203e-08, Loss_r: 5.227e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 5.148e-06, Loss_0: 9.525e-09, Loss_r: 5.138e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 5.064e-06, Loss_0: 5.596e-10, Loss_r: 5.064e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 4.976e-06, Loss_0: 4.238e-09, Loss_r: 4.972e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 4.901e-06, Loss_0: 3.150e-09, Loss_r: 4.898e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 4.829e-06, Loss_0: 2.731e-09, Loss_r: 4.826e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 4.757e-06, Loss_0: 2.981e-09, Loss_r: 4.754e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 4.687e-06, Loss_0: 2.973e-09, Loss_r: 4.684e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 4.618e-06, Loss_0: 2.468e-09, Loss_r: 4.616e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 4.550e-06, Loss_0: 2.602e-09, Loss_r: 4.548e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 4.484e-06, Loss_0: 2.830e-09, Loss_r: 4.481e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 4.419e-06, Loss_0: 2.922e-09, Loss_r: 4.416e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 4.358e-06, Loss_0: 4.549e-09, Loss_r: 4.354e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 4.511e-06, Loss_0: 3.228e-08, Loss_r: 4.479e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 3.365e-05, Loss_0: 2.410e-06, Loss_r: 3.124e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 5.526e-06, Loss_0: 1.357e-07, Loss_r: 5.390e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 5.858e-06, Loss_0: 1.000e-07, Loss_r: 5.758e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 5.789e-06, Loss_0: 9.797e-08, Loss_r: 5.691e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 4.021e-06, Loss_0: 2.606e-09, Loss_r: 4.018e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 4.211e-06, Loss_0: 3.251e-08, Loss_r: 4.178e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 3.928e-06, Loss_0: 3.510e-10, Loss_r: 3.927e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 3.883e-06, Loss_0: 1.259e-10, Loss_r: 3.882e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 3.831e-06, Loss_0: 5.511e-09, Loss_r: 3.826e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 3.774e-06, Loss_0: 6.482e-10, Loss_r: 3.774e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 3.724e-06, Loss_0: 2.777e-09, Loss_r: 3.721e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 3.676e-06, Loss_0: 1.240e-09, Loss_r: 3.674e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 3.629e-06, Loss_0: 2.139e-09, Loss_r: 3.627e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 3.583e-06, Loss_0: 1.633e-09, Loss_r: 3.581e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 3.537e-06, Loss_0: 1.509e-09, Loss_r: 3.536e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 3.493e-06, Loss_0: 1.465e-09, Loss_r: 3.491e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 3.449e-06, Loss_0: 1.296e-09, Loss_r: 3.448e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 3.409e-06, Loss_0: 5.690e-10, Loss_r: 3.408e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 3.538e-06, Loss_0: 5.697e-09, Loss_r: 3.533e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 2.645e-05, Loss_0: 1.608e-06, Loss_r: 2.484e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 4.404e-06, Loss_0: 6.162e-08, Loss_r: 4.343e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 4.528e-06, Loss_0: 1.178e-07, Loss_r: 4.411e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 4.577e-06, Loss_0: 1.237e-07, Loss_r: 4.453e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 3.179e-06, Loss_0: 1.848e-09, Loss_r: 3.177e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 3.333e-06, Loss_0: 6.627e-09, Loss_r: 3.326e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 3.111e-06, Loss_0: 2.637e-09, Loss_r: 3.109e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 3.088e-06, Loss_0: 4.679e-09, Loss_r: 3.084e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 3.049e-06, Loss_0: 6.770e-11, Loss_r: 3.049e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 3.008e-06, Loss_0: 2.311e-09, Loss_r: 3.006e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 2.973e-06, Loss_0: 7.289e-10, Loss_r: 2.972e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 2.939e-06, Loss_0: 1.520e-09, Loss_r: 2.938e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 2.907e-06, Loss_0: 8.497e-10, Loss_r: 2.906e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 2.874e-06, Loss_0: 1.239e-09, Loss_r: 2.873e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 2.842e-06, Loss_0: 1.184e-09, Loss_r: 2.841e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 2.811e-06, Loss_0: 1.111e-09, Loss_r: 2.810e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 2.780e-06, Loss_0: 1.148e-09, Loss_r: 2.779e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 2.750e-06, Loss_0: 1.436e-09, Loss_r: 2.748e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 2.740e-06, Loss_0: 5.025e-09, Loss_r: 2.735e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 4.860e-06, Loss_0: 1.808e-07, Loss_r: 4.679e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 2.821e-06, Loss_0: 1.865e-08, Loss_r: 2.802e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 2.709e-06, Loss_0: 1.755e-09, Loss_r: 2.708e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 2.738e-06, Loss_0: 4.290e-09, Loss_r: 2.734e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 2.588e-06, Loss_0: 1.697e-10, Loss_r: 2.588e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 2.573e-06, Loss_0: 3.874e-09, Loss_r: 2.569e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 2.533e-06, Loss_0: 1.080e-09, Loss_r: 2.532e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 2.510e-06, Loss_0: 2.726e-10, Loss_r: 2.510e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 2.483e-06, Loss_0: 1.210e-09, Loss_r: 2.482e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 2.458e-06, Loss_0: 7.249e-10, Loss_r: 2.458e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 2.434e-06, Loss_0: 7.508e-10, Loss_r: 2.433e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 2.410e-06, Loss_0: 7.947e-10, Loss_r: 2.409e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 2.386e-06, Loss_0: 7.668e-10, Loss_r: 2.385e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 2.362e-06, Loss_0: 7.075e-10, Loss_r: 2.361e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 2.339e-06, Loss_0: 7.685e-10, Loss_r: 2.338e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 2.315e-06, Loss_0: 7.272e-10, Loss_r: 2.315e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 2.292e-06, Loss_0: 6.753e-10, Loss_r: 2.292e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 2.270e-06, Loss_0: 6.077e-10, Loss_r: 2.269e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 2.247e-06, Loss_0: 4.006e-10, Loss_r: 2.247e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 2.254e-06, Loss_0: 3.852e-10, Loss_r: 2.254e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 6.489e-06, Loss_0: 2.728e-07, Loss_r: 6.216e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 2.353e-06, Loss_0: 7.067e-09, Loss_r: 2.346e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 2.445e-06, Loss_0: 2.727e-08, Loss_r: 2.418e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 2.387e-06, Loss_0: 2.392e-08, Loss_r: 2.363e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 2.125e-06, Loss_0: 3.082e-10, Loss_r: 2.125e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 2.140e-06, Loss_0: 6.061e-10, Loss_r: 2.140e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 2.088e-06, Loss_0: 1.192e-09, Loss_r: 2.087e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 2.070e-06, Loss_0: 1.266e-09, Loss_r: 2.069e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 2.051e-06, Loss_0: 1.644e-10, Loss_r: 2.051e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 2.031e-06, Loss_0: 8.935e-10, Loss_r: 2.030e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 2.012e-06, Loss_0: 4.038e-10, Loss_r: 2.012e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 1.994e-06, Loss_0: 6.153e-10, Loss_r: 1.993e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 1.976e-06, Loss_0: 4.377e-10, Loss_r: 1.975e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 1.958e-06, Loss_0: 5.251e-10, Loss_r: 1.957e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 1.940e-06, Loss_0: 5.050e-10, Loss_r: 1.940e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 1.922e-06, Loss_0: 4.381e-10, Loss_r: 1.922e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 1.905e-06, Loss_0: 4.557e-10, Loss_r: 1.904e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 1.887e-06, Loss_0: 3.653e-10, Loss_r: 1.887e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 1.874e-06, Loss_0: 1.892e-11, Loss_r: 1.874e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 2.193e-06, Loss_0: 1.731e-08, Loss_r: 2.176e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.871e-06, Loss_0: 7.347e-10, Loss_r: 1.870e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.828e-06, Loss_0: 1.664e-09, Loss_r: 1.826e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 1.826e-06, Loss_0: 3.194e-09, Loss_r: 1.823e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.794e-06, Loss_0: 1.089e-09, Loss_r: 1.793e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.779e-06, Loss_0: 1.065e-10, Loss_r: 1.778e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.763e-06, Loss_0: 2.426e-10, Loss_r: 1.762e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 1.748e-06, Loss_0: 6.167e-10, Loss_r: 1.747e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 1.733e-06, Loss_0: 3.698e-10, Loss_r: 1.733e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 1.718e-06, Loss_0: 3.509e-10, Loss_r: 1.718e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 1.704e-06, Loss_0: 4.354e-10, Loss_r: 1.703e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 1.690e-06, Loss_0: 3.611e-10, Loss_r: 1.689e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 1.675e-06, Loss_0: 3.954e-10, Loss_r: 1.675e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 1.661e-06, Loss_0: 3.719e-10, Loss_r: 1.661e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 1.647e-06, Loss_0: 3.620e-10, Loss_r: 1.647e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 1.633e-06, Loss_0: 3.743e-10, Loss_r: 1.633e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 1.619e-06, Loss_0: 3.146e-10, Loss_r: 1.619e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 1.605e-06, Loss_0: 2.810e-10, Loss_r: 1.605e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 1.592e-06, Loss_0: 1.612e-10, Loss_r: 1.592e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 1.592e-06, Loss_0: 1.633e-10, Loss_r: 1.592e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 2.765e-06, Loss_0: 7.138e-08, Loss_r: 2.694e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.665e-06, Loss_0: 4.833e-09, Loss_r: 1.660e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.563e-06, Loss_0: 3.215e-09, Loss_r: 1.559e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 1.596e-06, Loss_0: 7.253e-09, Loss_r: 1.588e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.524e-06, Loss_0: 1.593e-09, Loss_r: 1.522e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.510e-06, Loss_0: 4.679e-12, Loss_r: 1.510e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.494e-06, Loss_0: 6.718e-11, Loss_r: 1.494e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.482e-06, Loss_0: 6.799e-10, Loss_r: 1.481e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.469e-06, Loss_0: 2.464e-10, Loss_r: 1.469e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.458e-06, Loss_0: 2.327e-10, Loss_r: 1.457e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.446e-06, Loss_0: 3.419e-10, Loss_r: 1.446e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 1.434e-06, Loss_0: 2.346e-10, Loss_r: 1.434e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 1.423e-06, Loss_0: 2.956e-10, Loss_r: 1.423e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 1.412e-06, Loss_0: 2.695e-10, Loss_r: 1.411e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 1.400e-06, Loss_0: 2.675e-10, Loss_r: 1.400e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 1.389e-06, Loss_0: 2.635e-10, Loss_r: 1.389e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 1.378e-06, Loss_0: 2.385e-10, Loss_r: 1.378e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 1.367e-06, Loss_0: 2.168e-10, Loss_r: 1.367e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 1.356e-06, Loss_0: 1.981e-10, Loss_r: 1.356e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 1.346e-06, Loss_0: 3.372e-11, Loss_r: 1.346e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 1.447e-06, Loss_0: 5.128e-09, Loss_r: 1.441e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.336e-06, Loss_0: 1.520e-10, Loss_r: 1.336e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 1.316e-06, Loss_0: 6.343e-10, Loss_r: 1.315e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 1.310e-06, Loss_0: 1.256e-09, Loss_r: 1.309e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.296e-06, Loss_0: 5.207e-10, Loss_r: 1.295e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.286e-06, Loss_0: 9.865e-11, Loss_r: 1.285e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 1.276e-06, Loss_0: 1.314e-10, Loss_r: 1.276e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 1.266e-06, Loss_0: 3.103e-10, Loss_r: 1.266e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 1.257e-06, Loss_0: 2.192e-10, Loss_r: 1.256e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 1.247e-06, Loss_0: 1.980e-10, Loss_r: 1.247e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 1.238e-06, Loss_0: 2.325e-10, Loss_r: 1.237e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 1.228e-06, Loss_0: 2.023e-10, Loss_r: 1.228e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 1.219e-06, Loss_0: 2.167e-10, Loss_r: 1.219e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 1.210e-06, Loss_0: 1.804e-10, Loss_r: 1.210e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 1.201e-06, Loss_0: 2.148e-10, Loss_r: 1.200e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 1.191e-06, Loss_0: 2.264e-10, Loss_r: 1.191e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 1.182e-06, Loss_0: 2.052e-10, Loss_r: 1.182e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 1.173e-06, Loss_0: 2.077e-10, Loss_r: 1.173e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 1.164e-06, Loss_0: 2.561e-10, Loss_r: 1.164e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 1.157e-06, Loss_0: 5.977e-10, Loss_r: 1.156e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 1.247e-06, Loss_0: 9.148e-09, Loss_r: 1.238e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 1.151e-06, Loss_0: 1.789e-09, Loss_r: 1.149e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 1.131e-06, Loss_0: 5.059e-11, Loss_r: 1.131e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 1.127e-06, Loss_0: 1.925e-11, Loss_r: 1.127e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 1.115e-06, Loss_0: 1.776e-11, Loss_r: 1.115e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 1.106e-06, Loss_0: 2.707e-10, Loss_r: 1.106e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 1.099e-06, Loss_0: 2.909e-10, Loss_r: 1.098e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 1.091e-06, Loss_0: 1.394e-10, Loss_r: 1.090e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 1.083e-06, Loss_0: 1.500e-10, Loss_r: 1.083e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 1.075e-06, Loss_0: 1.915e-10, Loss_r: 1.075e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 1.067e-06, Loss_0: 1.532e-10, Loss_r: 1.067e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 1.060e-06, Loss_0: 1.756e-10, Loss_r: 1.059e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 1.052e-06, Loss_0: 1.436e-10, Loss_r: 1.052e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 1.044e-06, Loss_0: 1.616e-10, Loss_r: 1.044e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 1.037e-06, Loss_0: 1.478e-10, Loss_r: 1.037e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 1.029e-06, Loss_0: 1.606e-10, Loss_r: 1.029e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 1.022e-06, Loss_0: 1.742e-10, Loss_r: 1.021e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 1.014e-06, Loss_0: 1.953e-10, Loss_r: 1.014e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 1.007e-06, Loss_0: 2.353e-10, Loss_r: 1.006e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 1.001e-06, Loss_0: 5.898e-10, Loss_r: 1.001e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 1.101e-06, Loss_0: 9.275e-09, Loss_r: 1.091e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 1.002e-06, Loss_0: 2.044e-09, Loss_r: 9.999e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 9.786e-07, Loss_0: 1.110e-10, Loss_r: 9.785e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 9.758e-07, Loss_0: 1.837e-11, Loss_r: 9.758e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 9.671e-07, Loss_0: 1.031e-12, Loss_r: 9.671e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 9.588e-07, Loss_0: 1.578e-10, Loss_r: 9.586e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 9.526e-07, Loss_0: 2.620e-10, Loss_r: 9.523e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 9.458e-07, Loss_0: 1.245e-10, Loss_r: 9.457e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 9.393e-07, Loss_0: 1.097e-10, Loss_r: 9.392e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 9.329e-07, Loss_0: 1.564e-10, Loss_r: 9.327e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 9.265e-07, Loss_0: 1.286e-10, Loss_r: 9.263e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 9.201e-07, Loss_0: 1.189e-10, Loss_r: 9.200e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 9.137e-07, Loss_0: 1.331e-10, Loss_r: 9.136e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 9.073e-07, Loss_0: 1.291e-10, Loss_r: 9.072e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 9.010e-07, Loss_0: 1.246e-10, Loss_r: 9.009e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 8.948e-07, Loss_0: 1.241e-10, Loss_r: 8.946e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 8.885e-07, Loss_0: 1.335e-10, Loss_r: 8.884e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 8.823e-07, Loss_0: 1.558e-10, Loss_r: 8.822e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 8.761e-07, Loss_0: 1.697e-10, Loss_r: 8.759e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 8.705e-07, Loss_0: 2.915e-10, Loss_r: 8.702e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 8.835e-07, Loss_0: 2.192e-09, Loss_r: 8.813e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 2.359e-06, Loss_0: 1.041e-07, Loss_r: 2.255e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 1.013e-06, Loss_0: 1.275e-08, Loss_r: 9.999e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 8.643e-07, Loss_0: 5.354e-10, Loss_r: 8.638e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 9.189e-07, Loss_0: 3.651e-09, Loss_r: 9.153e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 8.492e-07, Loss_0: 3.654e-10, Loss_r: 8.488e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 8.343e-07, Loss_0: 6.950e-10, Loss_r: 8.336e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 8.274e-07, Loss_0: 5.282e-10, Loss_r: 8.269e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 8.204e-07, Loss_0: 7.715e-12, Loss_r: 8.204e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 8.142e-07, Loss_0: 7.021e-11, Loss_r: 8.141e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 8.090e-07, Loss_0: 1.929e-10, Loss_r: 8.088e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 8.036e-07, Loss_0: 5.185e-11, Loss_r: 8.035e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 7.982e-07, Loss_0: 1.310e-10, Loss_r: 7.980e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 7.929e-07, Loss_0: 8.425e-11, Loss_r: 7.928e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 7.876e-07, Loss_0: 1.192e-10, Loss_r: 7.875e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 7.823e-07, Loss_0: 9.598e-11, Loss_r: 7.822e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 7.771e-07, Loss_0: 9.866e-11, Loss_r: 7.770e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 7.719e-07, Loss_0: 8.985e-11, Loss_r: 7.718e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 7.667e-07, Loss_0: 9.800e-11, Loss_r: 7.666e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 7.615e-07, Loss_0: 9.896e-11, Loss_r: 7.614e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 7.564e-07, Loss_0: 1.238e-10, Loss_r: 7.563e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 7.527e-07, Loss_0: 3.890e-10, Loss_r: 7.523e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 9.049e-07, Loss_0: 1.228e-08, Loss_r: 8.927e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 7.528e-07, Loss_0: 1.315e-09, Loss_r: 7.514e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 7.424e-07, Loss_0: 8.071e-11, Loss_r: 7.424e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 7.418e-07, Loss_0: 2.291e-10, Loss_r: 7.416e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 7.283e-07, Loss_0: 2.270e-11, Loss_r: 7.283e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 7.244e-07, Loss_0: 2.896e-10, Loss_r: 7.241e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 7.189e-07, Loss_0: 1.295e-10, Loss_r: 7.188e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 7.146e-07, Loss_0: 3.686e-11, Loss_r: 7.145e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 7.099e-07, Loss_0: 1.015e-10, Loss_r: 7.098e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 7.055e-07, Loss_0: 8.696e-11, Loss_r: 7.054e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 7.010e-07, Loss_0: 8.497e-11, Loss_r: 7.009e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 6.966e-07, Loss_0: 8.676e-11, Loss_r: 6.965e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 6.922e-07, Loss_0: 8.271e-11, Loss_r: 6.921e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 6.878e-07, Loss_0: 8.417e-11, Loss_r: 6.877e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 6.834e-07, Loss_0: 8.032e-11, Loss_r: 6.833e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 6.790e-07, Loss_0: 8.069e-11, Loss_r: 6.790e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 6.747e-07, Loss_0: 8.887e-11, Loss_r: 6.746e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 6.703e-07, Loss_0: 8.003e-11, Loss_r: 6.702e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 6.660e-07, Loss_0: 7.317e-11, Loss_r: 6.659e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 6.617e-07, Loss_0: 7.197e-11, Loss_r: 6.616e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 6.574e-07, Loss_0: 7.901e-11, Loss_r: 6.574e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 6.532e-07, Loss_0: 7.408e-11, Loss_r: 6.531e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 6.500e-07, Loss_0: 4.892e-14, Loss_r: 6.500e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 1.209e-06, Loss_0: 3.266e-08, Loss_r: 1.177e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 6.499e-07, Loss_0: 1.053e-09, Loss_r: 6.488e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 7.462e-07, Loss_0: 8.419e-09, Loss_r: 7.378e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 6.336e-07, Loss_0: 1.524e-10, Loss_r: 6.334e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 6.439e-07, Loss_0: 4.699e-10, Loss_r: 6.434e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 6.273e-07, Loss_0: 3.240e-10, Loss_r: 6.269e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 6.224e-07, Loss_0: 1.545e-10, Loss_r: 6.222e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 6.190e-07, Loss_0: 5.769e-12, Loss_r: 6.190e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 6.150e-07, Loss_0: 1.641e-10, Loss_r: 6.148e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 6.111e-07, Loss_0: 2.905e-11, Loss_r: 6.110e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 6.074e-07, Loss_0: 9.180e-11, Loss_r: 6.073e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 6.036e-07, Loss_0: 6.008e-11, Loss_r: 6.035e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 6.000e-07, Loss_0: 5.753e-11, Loss_r: 5.999e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 5.963e-07, Loss_0: 6.842e-11, Loss_r: 5.962e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 5.927e-07, Loss_0: 6.068e-11, Loss_r: 5.927e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "Training time: 20.6502\n",
            "[1, 128, 128, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 8.185e-01, Loss_0: 1.126e-03, Loss_r: 8.174e-01, Time: 0.90, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.350e-01, Loss_0: 2.186e-02, Loss_r: 1.132e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.234e-01, Loss_0: 3.298e-02, Loss_r: 9.046e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.193e-01, Loss_0: 5.380e-02, Loss_r: 6.548e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.186e-01, Loss_0: 5.178e-02, Loss_r: 6.678e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.172e-01, Loss_0: 6.078e-02, Loss_r: 5.645e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.164e-01, Loss_0: 5.439e-02, Loss_r: 6.201e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.159e-01, Loss_0: 5.562e-02, Loss_r: 6.033e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.157e-01, Loss_0: 5.276e-02, Loss_r: 6.295e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.155e-01, Loss_0: 5.325e-02, Loss_r: 6.227e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.153e-01, Loss_0: 5.305e-02, Loss_r: 6.224e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.150e-01, Loss_0: 5.319e-02, Loss_r: 6.180e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.146e-01, Loss_0: 5.331e-02, Loss_r: 6.126e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.139e-01, Loss_0: 5.299e-02, Loss_r: 6.096e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.130e-01, Loss_0: 5.272e-02, Loss_r: 6.024e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.113e-01, Loss_0: 5.234e-02, Loss_r: 5.897e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.084e-01, Loss_0: 5.175e-02, Loss_r: 5.661e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.028e-01, Loss_0: 5.069e-02, Loss_r: 5.213e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 180, Loss: 9.266e-02, Loss_0: 4.819e-02, Loss_r: 4.447e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 190, Loss: 7.676e-02, Loss_0: 4.120e-02, Loss_r: 3.557e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.733e-02, Loss_0: 2.540e-02, Loss_r: 3.193e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.234e-02, Loss_0: 1.042e-02, Loss_r: 3.192e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 220, Loss: 3.279e-02, Loss_0: 7.214e-03, Loss_r: 2.557e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 230, Loss: 2.284e-02, Loss_0: 4.392e-03, Loss_r: 1.845e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.618e-02, Loss_0: 3.128e-03, Loss_r: 1.305e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.091e-02, Loss_0: 1.621e-03, Loss_r: 9.290e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 7.768e-03, Loss_0: 8.860e-04, Loss_r: 6.882e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 270, Loss: 6.010e-03, Loss_0: 5.406e-04, Loss_r: 5.469e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 6.073e-03, Loss_0: 3.733e-04, Loss_r: 5.700e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 4.481e-03, Loss_0: 2.678e-04, Loss_r: 4.213e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.016e-03, Loss_0: 1.880e-04, Loss_r: 2.828e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 2.262e-03, Loss_0: 1.297e-04, Loss_r: 2.132e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.861e-03, Loss_0: 9.584e-05, Loss_r: 1.765e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.508e-03, Loss_0: 7.568e-05, Loss_r: 1.432e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.260e-03, Loss_0: 5.519e-05, Loss_r: 1.205e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.083e-03, Loss_0: 4.353e-05, Loss_r: 1.039e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 9.484e-04, Loss_0: 3.323e-05, Loss_r: 9.152e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 8.449e-04, Loss_0: 2.637e-05, Loss_r: 8.186e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 380, Loss: 7.617e-04, Loss_0: 2.124e-05, Loss_r: 7.404e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 6.923e-04, Loss_0: 1.740e-05, Loss_r: 6.750e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 400, Loss: 6.326e-04, Loss_0: 1.441e-05, Loss_r: 6.182e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.798e-04, Loss_0: 1.216e-05, Loss_r: 5.677e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.324e-04, Loss_0: 1.035e-05, Loss_r: 5.220e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 4.892e-04, Loss_0: 8.904e-06, Loss_r: 4.803e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 440, Loss: 4.498e-04, Loss_0: 7.742e-06, Loss_r: 4.421e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.383e-04, Loss_0: 7.681e-06, Loss_r: 4.307e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.020e-03, Loss_0: 1.257e-05, Loss_r: 2.007e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.011e-03, Loss_0: 6.787e-07, Loss_r: 1.010e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 3.392e-04, Loss_0: 5.159e-06, Loss_r: 3.340e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.940e-04, Loss_0: 7.251e-06, Loss_r: 3.868e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.509e-04, Loss_0: 2.977e-06, Loss_r: 3.479e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.739e-04, Loss_0: 3.577e-06, Loss_r: 2.704e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.623e-04, Loss_0: 3.595e-06, Loss_r: 2.587e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.376e-04, Loss_0: 2.963e-06, Loss_r: 2.346e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.200e-04, Loss_0: 2.536e-06, Loss_r: 2.175e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.048e-04, Loss_0: 2.275e-06, Loss_r: 2.025e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.909e-04, Loss_0: 2.073e-06, Loss_r: 1.888e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.781e-04, Loss_0: 1.879e-06, Loss_r: 1.762e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.664e-04, Loss_0: 1.662e-06, Loss_r: 1.647e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.557e-04, Loss_0: 1.485e-06, Loss_r: 1.542e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.459e-04, Loss_0: 1.361e-06, Loss_r: 1.445e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.369e-04, Loss_0: 1.224e-06, Loss_r: 1.357e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.288e-04, Loss_0: 1.101e-06, Loss_r: 1.277e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.216e-04, Loss_0: 9.497e-07, Loss_r: 1.206e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.996e-04, Loss_0: 1.491e-07, Loss_r: 1.994e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 650, Loss: 7.001e-04, Loss_0: 6.421e-07, Loss_r: 6.995e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 660, Loss: 4.860e-04, Loss_0: 6.048e-06, Loss_r: 4.800e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.536e-04, Loss_0: 5.796e-06, Loss_r: 3.478e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.244e-04, Loss_0: 1.322e-07, Loss_r: 2.243e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.068e-04, Loss_0: 5.143e-07, Loss_r: 1.063e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.113e-04, Loss_0: 1.171e-06, Loss_r: 1.101e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 9.465e-05, Loss_0: 7.803e-07, Loss_r: 9.387e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 8.648e-05, Loss_0: 5.274e-07, Loss_r: 8.595e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 8.327e-05, Loss_0: 4.715e-07, Loss_r: 8.280e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.988e-05, Loss_0: 4.422e-07, Loss_r: 7.944e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 750, Loss: 7.672e-05, Loss_0: 4.105e-07, Loss_r: 7.631e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 7.389e-05, Loss_0: 3.946e-07, Loss_r: 7.349e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 7.132e-05, Loss_0: 3.834e-07, Loss_r: 7.094e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 780, Loss: 6.893e-05, Loss_0: 3.552e-07, Loss_r: 6.858e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 6.672e-05, Loss_0: 3.180e-07, Loss_r: 6.640e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 6.466e-05, Loss_0: 2.992e-07, Loss_r: 6.436e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.275e-05, Loss_0: 2.806e-07, Loss_r: 6.247e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 6.096e-05, Loss_0: 2.611e-07, Loss_r: 6.070e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 5.929e-05, Loss_0: 2.433e-07, Loss_r: 5.905e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 840, Loss: 5.773e-05, Loss_0: 2.281e-07, Loss_r: 5.750e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 5.626e-05, Loss_0: 2.113e-07, Loss_r: 5.605e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 860, Loss: 5.515e-05, Loss_0: 1.651e-07, Loss_r: 5.498e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.736e-04, Loss_0: 2.300e-07, Loss_r: 1.734e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 8.175e-04, Loss_0: 4.559e-06, Loss_r: 8.130e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 4.379e-04, Loss_0: 9.514e-06, Loss_r: 4.283e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 900, Loss: 3.378e-04, Loss_0: 2.459e-06, Loss_r: 3.354e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.206e-04, Loss_0: 4.268e-07, Loss_r: 2.202e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 5.366e-05, Loss_0: 2.919e-07, Loss_r: 5.337e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 930, Loss: 7.501e-05, Loss_0: 6.765e-07, Loss_r: 7.434e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 5.195e-05, Loss_0: 1.827e-07, Loss_r: 5.177e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 5.318e-05, Loss_0: 3.829e-08, Loss_r: 5.314e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 960, Loss: 4.908e-05, Loss_0: 1.609e-07, Loss_r: 4.892e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 4.803e-05, Loss_0: 1.704e-07, Loss_r: 4.786e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 4.703e-05, Loss_0: 1.838e-07, Loss_r: 4.685e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 990, Loss: 4.599e-05, Loss_0: 1.513e-07, Loss_r: 4.584e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 4.504e-05, Loss_0: 1.435e-07, Loss_r: 4.489e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 4.415e-05, Loss_0: 1.284e-07, Loss_r: 4.402e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 4.330e-05, Loss_0: 1.216e-07, Loss_r: 4.318e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 4.249e-05, Loss_0: 1.124e-07, Loss_r: 4.238e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 4.171e-05, Loss_0: 1.056e-07, Loss_r: 4.160e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 4.096e-05, Loss_0: 1.006e-07, Loss_r: 4.086e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 4.023e-05, Loss_0: 9.686e-08, Loss_r: 4.013e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 3.953e-05, Loss_0: 9.258e-08, Loss_r: 3.944e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 3.885e-05, Loss_0: 8.794e-08, Loss_r: 3.876e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 3.818e-05, Loss_0: 8.473e-08, Loss_r: 3.810e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 3.754e-05, Loss_0: 8.081e-08, Loss_r: 3.746e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 3.691e-05, Loss_0: 7.789e-08, Loss_r: 3.683e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 3.630e-05, Loss_0: 7.486e-08, Loss_r: 3.622e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 3.570e-05, Loss_0: 7.239e-08, Loss_r: 3.563e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 3.512e-05, Loss_0: 7.325e-08, Loss_r: 3.505e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 3.524e-05, Loss_0: 1.204e-07, Loss_r: 3.512e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.706e-04, Loss_0: 3.618e-06, Loss_r: 2.670e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.639e-04, Loss_0: 3.684e-06, Loss_r: 1.602e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.911e-04, Loss_0: 2.644e-06, Loss_r: 2.884e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 4.816e-04, Loss_0: 6.750e-06, Loss_r: 4.748e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 6.448e-05, Loss_0: 1.195e-06, Loss_r: 6.329e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 8.321e-05, Loss_0: 1.526e-06, Loss_r: 8.169e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 3.809e-05, Loss_0: 5.199e-10, Loss_r: 3.809e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 4.152e-05, Loss_0: 7.492e-09, Loss_r: 4.151e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.348e-05, Loss_0: 5.324e-08, Loss_r: 3.343e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.310e-05, Loss_0: 1.296e-07, Loss_r: 3.297e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.247e-05, Loss_0: 1.176e-07, Loss_r: 3.236e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 3.168e-05, Loss_0: 9.148e-08, Loss_r: 3.159e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 3.105e-05, Loss_0: 7.563e-08, Loss_r: 3.097e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 3.050e-05, Loss_0: 6.767e-08, Loss_r: 3.044e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.999e-05, Loss_0: 6.278e-08, Loss_r: 2.993e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.949e-05, Loss_0: 5.872e-08, Loss_r: 2.943e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.901e-05, Loss_0: 5.476e-08, Loss_r: 2.895e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.854e-05, Loss_0: 5.116e-08, Loss_r: 2.849e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.808e-05, Loss_0: 4.859e-08, Loss_r: 2.803e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.763e-05, Loss_0: 4.707e-08, Loss_r: 2.759e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 2.720e-05, Loss_0: 4.566e-08, Loss_r: 2.715e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 2.677e-05, Loss_0: 4.362e-08, Loss_r: 2.672e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 2.635e-05, Loss_0: 4.212e-08, Loss_r: 2.631e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.594e-05, Loss_0: 4.060e-08, Loss_r: 2.590e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 2.553e-05, Loss_0: 3.921e-08, Loss_r: 2.549e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 2.514e-05, Loss_0: 3.778e-08, Loss_r: 2.510e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 2.475e-05, Loss_0: 3.651e-08, Loss_r: 2.471e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 2.437e-05, Loss_0: 3.546e-08, Loss_r: 2.433e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.399e-05, Loss_0: 3.434e-08, Loss_r: 2.396e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.363e-05, Loss_0: 3.399e-08, Loss_r: 2.359e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.329e-05, Loss_0: 4.062e-08, Loss_r: 2.325e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 3.118e-05, Loss_0: 3.413e-07, Loss_r: 3.084e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 4.621e-03, Loss_0: 9.390e-05, Loss_r: 4.527e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.456e-05, Loss_0: 1.533e-07, Loss_r: 2.441e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.087e-03, Loss_0: 3.613e-05, Loss_r: 1.051e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.554e-04, Loss_0: 3.345e-06, Loss_r: 1.521e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 3.090e-05, Loss_0: 1.891e-08, Loss_r: 3.088e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 6.084e-05, Loss_0: 2.026e-06, Loss_r: 5.881e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 3.609e-05, Loss_0: 1.750e-07, Loss_r: 3.591e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.416e-05, Loss_0: 2.830e-08, Loss_r: 2.413e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.558e-05, Loss_0: 2.500e-07, Loss_r: 2.533e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.317e-05, Loss_0: 3.283e-08, Loss_r: 2.314e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.297e-05, Loss_0: 1.130e-08, Loss_r: 2.296e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.232e-05, Loss_0: 3.632e-08, Loss_r: 2.228e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.194e-05, Loss_0: 5.037e-08, Loss_r: 2.189e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.156e-05, Loss_0: 4.417e-08, Loss_r: 2.151e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 2.118e-05, Loss_0: 3.665e-08, Loss_r: 2.115e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.083e-05, Loss_0: 3.264e-08, Loss_r: 2.080e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.049e-05, Loss_0: 3.079e-08, Loss_r: 2.046e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.016e-05, Loss_0: 2.983e-08, Loss_r: 2.013e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.984e-05, Loss_0: 2.878e-08, Loss_r: 1.981e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.952e-05, Loss_0: 2.785e-08, Loss_r: 1.950e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.922e-05, Loss_0: 2.687e-08, Loss_r: 1.919e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.892e-05, Loss_0: 2.591e-08, Loss_r: 1.890e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.863e-05, Loss_0: 2.500e-08, Loss_r: 1.861e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.835e-05, Loss_0: 2.404e-08, Loss_r: 1.832e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.807e-05, Loss_0: 2.315e-08, Loss_r: 1.805e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.780e-05, Loss_0: 2.234e-08, Loss_r: 1.778e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.753e-05, Loss_0: 2.147e-08, Loss_r: 1.751e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.727e-05, Loss_0: 2.084e-08, Loss_r: 1.725e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.702e-05, Loss_0: 2.020e-08, Loss_r: 1.700e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.677e-05, Loss_0: 1.950e-08, Loss_r: 1.675e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.652e-05, Loss_0: 1.883e-08, Loss_r: 1.650e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.628e-05, Loss_0: 1.815e-08, Loss_r: 1.626e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.604e-05, Loss_0: 1.784e-08, Loss_r: 1.602e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.581e-05, Loss_0: 1.752e-08, Loss_r: 1.579e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.558e-05, Loss_0: 1.689e-08, Loss_r: 1.556e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.535e-05, Loss_0: 1.783e-08, Loss_r: 1.534e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.521e-05, Loss_0: 3.292e-08, Loss_r: 1.517e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.007e-05, Loss_0: 8.465e-07, Loss_r: 2.922e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 4.662e-03, Loss_0: 1.950e-04, Loss_r: 4.467e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.998e-04, Loss_0: 9.073e-06, Loss_r: 1.908e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 4.821e-04, Loss_0: 2.700e-05, Loss_r: 4.550e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.549e-04, Loss_0: 9.538e-06, Loss_r: 1.454e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 8.677e-05, Loss_0: 3.714e-06, Loss_r: 8.306e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 2.708e-05, Loss_0: 3.807e-07, Loss_r: 2.670e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.548e-05, Loss_0: 9.052e-07, Loss_r: 2.457e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.858e-05, Loss_0: 3.118e-07, Loss_r: 1.827e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.612e-05, Loss_0: 4.027e-10, Loss_r: 1.612e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.614e-05, Loss_0: 1.961e-09, Loss_r: 1.614e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.537e-05, Loss_0: 3.746e-09, Loss_r: 1.537e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.499e-05, Loss_0: 1.854e-08, Loss_r: 1.497e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.474e-05, Loss_0: 2.565e-08, Loss_r: 1.471e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.449e-05, Loss_0: 2.431e-08, Loss_r: 1.446e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.424e-05, Loss_0: 2.181e-08, Loss_r: 1.422e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.401e-05, Loss_0: 1.935e-08, Loss_r: 1.399e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.379e-05, Loss_0: 1.761e-08, Loss_r: 1.377e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.357e-05, Loss_0: 1.610e-08, Loss_r: 1.356e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.337e-05, Loss_0: 1.504e-08, Loss_r: 1.335e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.316e-05, Loss_0: 1.437e-08, Loss_r: 1.315e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.297e-05, Loss_0: 1.404e-08, Loss_r: 1.295e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.278e-05, Loss_0: 1.365e-08, Loss_r: 1.276e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 1.259e-05, Loss_0: 1.304e-08, Loss_r: 1.258e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.241e-05, Loss_0: 1.251e-08, Loss_r: 1.239e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.223e-05, Loss_0: 1.203e-08, Loss_r: 1.222e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.206e-05, Loss_0: 1.176e-08, Loss_r: 1.204e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.189e-05, Loss_0: 1.115e-08, Loss_r: 1.188e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.172e-05, Loss_0: 1.096e-08, Loss_r: 1.171e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.156e-05, Loss_0: 1.038e-08, Loss_r: 1.155e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.140e-05, Loss_0: 1.022e-08, Loss_r: 1.139e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.124e-05, Loss_0: 9.939e-09, Loss_r: 1.123e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.109e-05, Loss_0: 9.391e-09, Loss_r: 1.108e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.094e-05, Loss_0: 8.743e-09, Loss_r: 1.093e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 1.079e-05, Loss_0: 6.556e-09, Loss_r: 1.079e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 1.083e-05, Loss_0: 3.828e-10, Loss_r: 1.083e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 4.124e-05, Loss_0: 1.883e-06, Loss_r: 3.936e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.114e-05, Loss_0: 1.919e-08, Loss_r: 1.112e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.303e-05, Loss_0: 2.797e-07, Loss_r: 1.275e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.164e-05, Loss_0: 1.698e-07, Loss_r: 1.148e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.014e-05, Loss_0: 6.123e-12, Loss_r: 1.014e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.008e-05, Loss_0: 6.924e-10, Loss_r: 1.008e-05, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 9.856e-06, Loss_0: 2.412e-08, Loss_r: 9.832e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 9.672e-06, Loss_0: 5.753e-09, Loss_r: 9.666e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 9.559e-06, Loss_0: 5.130e-09, Loss_r: 9.554e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 9.447e-06, Loss_0: 8.624e-09, Loss_r: 9.439e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 9.336e-06, Loss_0: 5.664e-09, Loss_r: 9.330e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 9.227e-06, Loss_0: 6.362e-09, Loss_r: 9.221e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 9.121e-06, Loss_0: 7.040e-09, Loss_r: 9.114e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 9.015e-06, Loss_0: 6.077e-09, Loss_r: 9.009e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 8.912e-06, Loss_0: 5.660e-09, Loss_r: 8.906e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 8.810e-06, Loss_0: 5.095e-09, Loss_r: 8.805e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 8.716e-06, Loss_0: 2.810e-09, Loss_r: 8.713e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 9.111e-06, Loss_0: 1.265e-08, Loss_r: 9.098e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 8.877e-05, Loss_0: 5.232e-06, Loss_r: 8.354e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 1.164e-05, Loss_0: 1.581e-07, Loss_r: 1.149e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 1.328e-05, Loss_0: 4.305e-07, Loss_r: 1.285e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.300e-05, Loss_0: 4.148e-07, Loss_r: 1.259e-05, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 8.190e-06, Loss_0: 5.207e-09, Loss_r: 8.185e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 8.772e-06, Loss_0: 2.077e-08, Loss_r: 8.751e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 8.044e-06, Loss_0: 1.108e-08, Loss_r: 8.033e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 7.990e-06, Loss_0: 1.553e-08, Loss_r: 7.975e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 7.900e-06, Loss_0: 3.905e-10, Loss_r: 7.900e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 7.798e-06, Loss_0: 9.032e-09, Loss_r: 7.789e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 7.713e-06, Loss_0: 2.577e-09, Loss_r: 7.711e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 7.635e-06, Loss_0: 5.996e-09, Loss_r: 7.629e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 7.558e-06, Loss_0: 3.392e-09, Loss_r: 7.554e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 7.482e-06, Loss_0: 4.234e-09, Loss_r: 7.478e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 7.409e-06, Loss_0: 4.637e-09, Loss_r: 7.404e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 7.336e-06, Loss_0: 4.752e-09, Loss_r: 7.331e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 7.266e-06, Loss_0: 5.828e-09, Loss_r: 7.260e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 7.262e-06, Loss_0: 1.722e-08, Loss_r: 7.245e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 1.209e-05, Loss_0: 4.187e-07, Loss_r: 1.167e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 7.574e-06, Loss_0: 6.206e-08, Loss_r: 7.512e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 7.076e-06, Loss_0: 1.541e-10, Loss_r: 7.076e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 7.213e-06, Loss_0: 6.081e-09, Loss_r: 7.207e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 6.906e-06, Loss_0: 1.987e-10, Loss_r: 6.906e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 6.841e-06, Loss_0: 9.945e-09, Loss_r: 6.831e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 6.761e-06, Loss_0: 5.493e-09, Loss_r: 6.756e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 6.704e-06, Loss_0: 1.525e-09, Loss_r: 6.703e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 6.641e-06, Loss_0: 3.848e-09, Loss_r: 6.638e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 6.584e-06, Loss_0: 3.416e-09, Loss_r: 6.580e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 6.527e-06, Loss_0: 2.881e-09, Loss_r: 6.524e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 6.470e-06, Loss_0: 3.271e-09, Loss_r: 6.467e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 6.415e-06, Loss_0: 2.981e-09, Loss_r: 6.412e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 6.360e-06, Loss_0: 2.969e-09, Loss_r: 6.357e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 6.305e-06, Loss_0: 3.036e-09, Loss_r: 6.302e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 6.251e-06, Loss_0: 2.929e-09, Loss_r: 6.248e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 6.198e-06, Loss_0: 2.995e-09, Loss_r: 6.195e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 6.146e-06, Loss_0: 3.479e-09, Loss_r: 6.142e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 6.109e-06, Loss_0: 7.222e-09, Loss_r: 6.101e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 7.373e-06, Loss_0: 1.254e-07, Loss_r: 7.248e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 6.110e-06, Loss_0: 1.964e-08, Loss_r: 6.090e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 5.983e-06, Loss_0: 6.798e-12, Loss_r: 5.983e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 5.982e-06, Loss_0: 5.037e-10, Loss_r: 5.982e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 5.864e-06, Loss_0: 1.082e-09, Loss_r: 5.863e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 5.823e-06, Loss_0: 5.478e-09, Loss_r: 5.818e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 5.771e-06, Loss_0: 2.953e-09, Loss_r: 5.768e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 5.729e-06, Loss_0: 1.518e-09, Loss_r: 5.727e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 5.684e-06, Loss_0: 2.804e-09, Loss_r: 5.681e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 5.641e-06, Loss_0: 2.273e-09, Loss_r: 5.638e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 5.598e-06, Loss_0: 2.227e-09, Loss_r: 5.596e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 5.556e-06, Loss_0: 2.275e-09, Loss_r: 5.553e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 5.514e-06, Loss_0: 2.191e-09, Loss_r: 5.512e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 5.472e-06, Loss_0: 2.112e-09, Loss_r: 5.470e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 5.431e-06, Loss_0: 2.141e-09, Loss_r: 5.429e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 5.390e-06, Loss_0: 2.122e-09, Loss_r: 5.388e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 5.350e-06, Loss_0: 2.107e-09, Loss_r: 5.347e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 5.309e-06, Loss_0: 2.087e-09, Loss_r: 5.307e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 5.270e-06, Loss_0: 2.537e-09, Loss_r: 5.268e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 5.266e-06, Loss_0: 9.000e-09, Loss_r: 5.257e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 1.133e-05, Loss_0: 4.780e-07, Loss_r: 1.085e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 5.320e-06, Loss_0: 2.237e-08, Loss_r: 5.298e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 5.639e-06, Loss_0: 2.093e-08, Loss_r: 5.618e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 5.404e-06, Loss_0: 1.080e-08, Loss_r: 5.394e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 5.065e-06, Loss_0: 5.030e-09, Loss_r: 5.060e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 5.063e-06, Loss_0: 9.529e-09, Loss_r: 5.054e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 4.993e-06, Loss_0: 3.350e-10, Loss_r: 4.993e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 4.952e-06, Loss_0: 1.300e-09, Loss_r: 4.950e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 4.920e-06, Loss_0: 2.730e-09, Loss_r: 4.917e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 4.886e-06, Loss_0: 1.090e-09, Loss_r: 4.885e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 4.852e-06, Loss_0: 2.129e-09, Loss_r: 4.850e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 4.819e-06, Loss_0: 1.378e-09, Loss_r: 4.818e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 4.786e-06, Loss_0: 1.692e-09, Loss_r: 4.785e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 4.754e-06, Loss_0: 1.615e-09, Loss_r: 4.752e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 4.722e-06, Loss_0: 1.470e-09, Loss_r: 4.720e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 4.690e-06, Loss_0: 1.498e-09, Loss_r: 4.688e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 4.658e-06, Loss_0: 1.358e-09, Loss_r: 4.657e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 4.628e-06, Loss_0: 8.231e-10, Loss_r: 4.627e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 4.668e-06, Loss_0: 9.819e-10, Loss_r: 4.667e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 1.310e-05, Loss_0: 5.246e-07, Loss_r: 1.258e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 5.054e-06, Loss_0: 2.217e-08, Loss_r: 5.031e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 4.875e-06, Loss_0: 3.806e-08, Loss_r: 4.837e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 4.993e-06, Loss_0: 4.989e-08, Loss_r: 4.944e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 4.462e-06, Loss_0: 3.536e-09, Loss_r: 4.458e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 4.492e-06, Loss_0: 8.530e-10, Loss_r: 4.491e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 4.401e-06, Loss_0: 1.166e-09, Loss_r: 4.399e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 4.381e-06, Loss_0: 3.582e-09, Loss_r: 4.378e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 4.349e-06, Loss_0: 5.298e-10, Loss_r: 4.349e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 4.320e-06, Loss_0: 1.554e-09, Loss_r: 4.318e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 4.293e-06, Loss_0: 1.255e-09, Loss_r: 4.292e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 4.267e-06, Loss_0: 1.260e-09, Loss_r: 4.265e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 4.240e-06, Loss_0: 1.154e-09, Loss_r: 4.239e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 4.214e-06, Loss_0: 1.319e-09, Loss_r: 4.213e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 4.188e-06, Loss_0: 1.157e-09, Loss_r: 4.187e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 4.162e-06, Loss_0: 1.135e-09, Loss_r: 4.161e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 4.136e-06, Loss_0: 1.122e-09, Loss_r: 4.135e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 4.110e-06, Loss_0: 1.097e-09, Loss_r: 4.109e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 4.085e-06, Loss_0: 7.940e-10, Loss_r: 4.084e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 4.085e-06, Loss_0: 6.719e-11, Loss_r: 4.085e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 7.582e-06, Loss_0: 2.087e-07, Loss_r: 7.373e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 4.168e-06, Loss_0: 4.901e-09, Loss_r: 4.163e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 4.202e-06, Loss_0: 2.330e-08, Loss_r: 4.178e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 4.173e-06, Loss_0: 2.267e-08, Loss_r: 4.150e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 3.944e-06, Loss_0: 1.005e-09, Loss_r: 3.943e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 3.950e-06, Loss_0: 1.460e-10, Loss_r: 3.950e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 3.900e-06, Loss_0: 1.466e-09, Loss_r: 3.898e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 3.879e-06, Loss_0: 1.979e-09, Loss_r: 3.877e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 3.856e-06, Loss_0: 4.786e-10, Loss_r: 3.856e-06, Time: 0.03, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 3.833e-06, Loss_0: 1.327e-09, Loss_r: 3.832e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 3.811e-06, Loss_0: 8.486e-10, Loss_r: 3.810e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 3.789e-06, Loss_0: 1.030e-09, Loss_r: 3.788e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 3.767e-06, Loss_0: 9.285e-10, Loss_r: 3.766e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 3.745e-06, Loss_0: 9.698e-10, Loss_r: 3.744e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 3.724e-06, Loss_0: 1.004e-09, Loss_r: 3.723e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 3.702e-06, Loss_0: 1.005e-09, Loss_r: 3.701e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 3.680e-06, Loss_0: 9.816e-10, Loss_r: 3.679e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 3.659e-06, Loss_0: 1.115e-09, Loss_r: 3.658e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 3.642e-06, Loss_0: 2.204e-09, Loss_r: 3.640e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 3.914e-06, Loss_0: 2.930e-08, Loss_r: 3.885e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 3.631e-06, Loss_0: 6.024e-09, Loss_r: 3.625e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 3.581e-06, Loss_0: 2.260e-10, Loss_r: 3.581e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 3.574e-06, Loss_0: 6.920e-12, Loss_r: 3.574e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 3.543e-06, Loss_0: 2.387e-10, Loss_r: 3.543e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 3.522e-06, Loss_0: 1.275e-09, Loss_r: 3.521e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 3.503e-06, Loss_0: 1.216e-09, Loss_r: 3.502e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 3.484e-06, Loss_0: 6.228e-10, Loss_r: 3.483e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 3.465e-06, Loss_0: 7.553e-10, Loss_r: 3.464e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 3.446e-06, Loss_0: 8.939e-10, Loss_r: 3.445e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 3.427e-06, Loss_0: 7.214e-10, Loss_r: 3.427e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 3.409e-06, Loss_0: 8.239e-10, Loss_r: 3.408e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 3.390e-06, Loss_0: 7.180e-10, Loss_r: 3.390e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 3.372e-06, Loss_0: 7.787e-10, Loss_r: 3.371e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 3.353e-06, Loss_0: 7.328e-10, Loss_r: 3.352e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 3.335e-06, Loss_0: 7.357e-10, Loss_r: 3.334e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 3.316e-06, Loss_0: 7.504e-10, Loss_r: 3.316e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 3.298e-06, Loss_0: 7.776e-10, Loss_r: 3.297e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 3.280e-06, Loss_0: 7.682e-10, Loss_r: 3.279e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 3.262e-06, Loss_0: 9.576e-10, Loss_r: 3.261e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 3.262e-06, Loss_0: 3.847e-09, Loss_r: 3.258e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 5.929e-06, Loss_0: 2.051e-07, Loss_r: 5.724e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 3.324e-06, Loss_0: 1.301e-08, Loss_r: 3.311e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 3.359e-06, Loss_0: 6.428e-09, Loss_r: 3.353e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 3.334e-06, Loss_0: 5.908e-09, Loss_r: 3.328e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 3.161e-06, Loss_0: 6.946e-10, Loss_r: 3.160e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 3.166e-06, Loss_0: 4.013e-09, Loss_r: 3.162e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 3.129e-06, Loss_0: 3.989e-10, Loss_r: 3.128e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 3.114e-06, Loss_0: 2.131e-10, Loss_r: 3.114e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 3.097e-06, Loss_0: 1.099e-09, Loss_r: 3.096e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 3.080e-06, Loss_0: 4.547e-10, Loss_r: 3.080e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 3.064e-06, Loss_0: 6.770e-10, Loss_r: 3.063e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 3.048e-06, Loss_0: 5.735e-10, Loss_r: 3.047e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 3.032e-06, Loss_0: 6.510e-10, Loss_r: 3.031e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 3.016e-06, Loss_0: 5.447e-10, Loss_r: 3.016e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 3.000e-06, Loss_0: 5.996e-10, Loss_r: 3.000e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 2.984e-06, Loss_0: 6.457e-10, Loss_r: 2.984e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 2.968e-06, Loss_0: 6.301e-10, Loss_r: 2.968e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 2.953e-06, Loss_0: 6.248e-10, Loss_r: 2.952e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 2.937e-06, Loss_0: 8.206e-10, Loss_r: 2.936e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 2.943e-06, Loss_0: 3.839e-09, Loss_r: 2.939e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 5.551e-06, Loss_0: 1.977e-07, Loss_r: 5.353e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 3.048e-06, Loss_0: 1.582e-08, Loss_r: 3.032e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 2.992e-06, Loss_0: 4.120e-09, Loss_r: 2.988e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 3.021e-06, Loss_0: 6.314e-09, Loss_r: 3.015e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 2.852e-06, Loss_0: 9.171e-11, Loss_r: 2.852e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 2.854e-06, Loss_0: 3.495e-09, Loss_r: 2.851e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 2.821e-06, Loss_0: 6.358e-10, Loss_r: 2.821e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 2.810e-06, Loss_0: 9.176e-11, Loss_r: 2.810e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 2.794e-06, Loss_0: 8.752e-10, Loss_r: 2.793e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 2.779e-06, Loss_0: 4.565e-10, Loss_r: 2.779e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 2.765e-06, Loss_0: 4.704e-10, Loss_r: 2.765e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 2.751e-06, Loss_0: 5.236e-10, Loss_r: 2.751e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 2.737e-06, Loss_0: 4.933e-10, Loss_r: 2.737e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 2.723e-06, Loss_0: 4.936e-10, Loss_r: 2.723e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 2.710e-06, Loss_0: 4.970e-10, Loss_r: 2.709e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 2.696e-06, Loss_0: 4.756e-10, Loss_r: 2.695e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 2.682e-06, Loss_0: 4.681e-10, Loss_r: 2.681e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 2.668e-06, Loss_0: 4.589e-10, Loss_r: 2.668e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 2.654e-06, Loss_0: 5.216e-10, Loss_r: 2.654e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 2.641e-06, Loss_0: 6.583e-10, Loss_r: 2.640e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 2.652e-06, Loss_0: 3.917e-09, Loss_r: 2.648e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 7.350e-06, Loss_0: 3.404e-07, Loss_r: 7.009e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 2.717e-06, Loss_0: 1.198e-08, Loss_r: 2.705e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 2.989e-06, Loss_0: 2.034e-08, Loss_r: 2.969e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 2.826e-06, Loss_0: 1.170e-08, Loss_r: 2.815e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 2.570e-06, Loss_0: 1.645e-09, Loss_r: 2.568e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 2.588e-06, Loss_0: 4.923e-09, Loss_r: 2.583e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 2.543e-06, Loss_0: 1.936e-11, Loss_r: 2.543e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 2.528e-06, Loss_0: 1.216e-10, Loss_r: 2.528e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 2.517e-06, Loss_0: 1.022e-09, Loss_r: 2.516e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 2.503e-06, Loss_0: 1.717e-10, Loss_r: 2.503e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 2.491e-06, Loss_0: 5.781e-10, Loss_r: 2.490e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 2.478e-06, Loss_0: 3.192e-10, Loss_r: 2.478e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 2.466e-06, Loss_0: 4.774e-10, Loss_r: 2.465e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 2.454e-06, Loss_0: 3.661e-10, Loss_r: 2.453e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 2.441e-06, Loss_0: 3.779e-10, Loss_r: 2.441e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 2.429e-06, Loss_0: 3.966e-10, Loss_r: 2.429e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 2.417e-06, Loss_0: 4.086e-10, Loss_r: 2.417e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 2.405e-06, Loss_0: 3.352e-10, Loss_r: 2.405e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 2.393e-06, Loss_0: 1.757e-10, Loss_r: 2.393e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 2.412e-06, Loss_0: 7.133e-10, Loss_r: 2.412e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 6.362e-06, Loss_0: 2.461e-07, Loss_r: 6.116e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 2.596e-06, Loss_0: 1.132e-08, Loss_r: 2.585e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 2.513e-06, Loss_0: 1.542e-08, Loss_r: 2.497e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 2.576e-06, Loss_0: 2.102e-08, Loss_r: 2.555e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 2.331e-06, Loss_0: 1.530e-09, Loss_r: 2.330e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 2.342e-06, Loss_0: 5.780e-10, Loss_r: 2.342e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 2.304e-06, Loss_0: 1.595e-10, Loss_r: 2.304e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 2.297e-06, Loss_0: 1.279e-09, Loss_r: 2.296e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 2.282e-06, Loss_0: 1.398e-10, Loss_r: 2.282e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 2.271e-06, Loss_0: 3.508e-10, Loss_r: 2.271e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 2.260e-06, Loss_0: 4.037e-10, Loss_r: 2.260e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 2.249e-06, Loss_0: 2.778e-10, Loss_r: 2.249e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 2.238e-06, Loss_0: 3.760e-10, Loss_r: 2.238e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 2.228e-06, Loss_0: 3.346e-10, Loss_r: 2.227e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 2.217e-06, Loss_0: 2.753e-10, Loss_r: 2.217e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 2.206e-06, Loss_0: 3.491e-10, Loss_r: 2.206e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 2.195e-06, Loss_0: 3.307e-10, Loss_r: 2.195e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 2.184e-06, Loss_0: 3.382e-10, Loss_r: 2.184e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 2.174e-06, Loss_0: 3.987e-10, Loss_r: 2.173e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 2.164e-06, Loss_0: 6.301e-10, Loss_r: 2.163e-06, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 2.218e-06, Loss_0: 7.071e-09, Loss_r: 2.211e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 1.252e-05, Loss_0: 7.174e-07, Loss_r: 1.180e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 2.657e-06, Loss_0: 4.172e-08, Loss_r: 2.615e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 2.599e-06, Loss_0: 2.550e-08, Loss_r: 2.574e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 2.742e-06, Loss_0: 3.477e-08, Loss_r: 2.707e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 2.120e-06, Loss_0: 2.438e-10, Loss_r: 2.120e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 2.166e-06, Loss_0: 7.475e-09, Loss_r: 2.158e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 2.087e-06, Loss_0: 9.038e-10, Loss_r: 2.086e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 2.086e-06, Loss_0: 1.119e-10, Loss_r: 2.086e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 2.067e-06, Loss_0: 7.107e-10, Loss_r: 2.066e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 2.056e-06, Loss_0: 3.550e-10, Loss_r: 2.055e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 2.046e-06, Loss_0: 1.677e-10, Loss_r: 2.046e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 2.037e-06, Loss_0: 4.022e-10, Loss_r: 2.036e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 2.027e-06, Loss_0: 2.254e-10, Loss_r: 2.027e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 2.017e-06, Loss_0: 3.013e-10, Loss_r: 2.017e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 2.008e-06, Loss_0: 3.178e-10, Loss_r: 2.007e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 1.998e-06, Loss_0: 2.765e-10, Loss_r: 1.998e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.989e-06, Loss_0: 2.713e-10, Loss_r: 1.988e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.979e-06, Loss_0: 2.695e-10, Loss_r: 1.979e-06, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.969e-06, Loss_0: 2.876e-10, Loss_r: 1.969e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 1.960e-06, Loss_0: 3.468e-10, Loss_r: 1.960e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 1.953e-06, Loss_0: 8.569e-10, Loss_r: 1.952e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 2.320e-06, Loss_0: 3.033e-08, Loss_r: 2.290e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 1.949e-06, Loss_0: 2.441e-09, Loss_r: 1.947e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 1.946e-06, Loss_0: 5.014e-10, Loss_r: 1.946e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 1.937e-06, Loss_0: 4.900e-10, Loss_r: 1.937e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 1.907e-06, Loss_0: 2.237e-10, Loss_r: 1.906e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 1.901e-06, Loss_0: 9.274e-10, Loss_r: 1.900e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 1.890e-06, Loss_0: 2.154e-10, Loss_r: 1.889e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 1.881e-06, Loss_0: 1.374e-10, Loss_r: 1.881e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 1.873e-06, Loss_0: 3.393e-10, Loss_r: 1.872e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 1.864e-06, Loss_0: 2.069e-10, Loss_r: 1.864e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 1.855e-06, Loss_0: 2.485e-10, Loss_r: 1.855e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 1.847e-06, Loss_0: 2.390e-10, Loss_r: 1.847e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 1.838e-06, Loss_0: 2.422e-10, Loss_r: 1.838e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 1.830e-06, Loss_0: 2.242e-10, Loss_r: 1.830e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 1.821e-06, Loss_0: 2.396e-10, Loss_r: 1.821e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 1.813e-06, Loss_0: 2.266e-10, Loss_r: 1.813e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 1.804e-06, Loss_0: 2.117e-10, Loss_r: 1.804e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.796e-06, Loss_0: 2.272e-10, Loss_r: 1.796e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 1.787e-06, Loss_0: 2.228e-10, Loss_r: 1.787e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 1.779e-06, Loss_0: 1.301e-10, Loss_r: 1.779e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 1.789e-06, Loss_0: 4.134e-10, Loss_r: 1.789e-06, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 5.250e-06, Loss_0: 2.152e-07, Loss_r: 5.035e-06, Time: 0.03, Learning Rate: 0.00021\n",
            "Training time: 20.7168\n",
            "[1, 256, 128, 64, 32, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.764e-01, Loss_0: 1.397e-03, Loss_r: 3.750e-01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.751e-01, Loss_0: 1.409e-02, Loss_r: 1.611e-01, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.367e-01, Loss_0: 5.609e-02, Loss_r: 8.063e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.227e-01, Loss_0: 4.970e-02, Loss_r: 7.297e-02, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.171e-01, Loss_0: 6.378e-02, Loss_r: 5.334e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.140e-01, Loss_0: 5.378e-02, Loss_r: 6.021e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.116e-01, Loss_0: 5.177e-02, Loss_r: 5.987e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.075e-01, Loss_0: 5.127e-02, Loss_r: 5.626e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 80, Loss: 9.747e-02, Loss_0: 5.076e-02, Loss_r: 4.671e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 90, Loss: 7.941e-02, Loss_0: 4.615e-02, Loss_r: 3.327e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.587e-02, Loss_0: 2.478e-02, Loss_r: 3.109e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.881e-02, Loss_0: 7.910e-03, Loss_r: 3.090e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.645e-02, Loss_0: 6.396e-03, Loss_r: 2.006e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.634e-02, Loss_0: 3.055e-03, Loss_r: 1.329e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.075e-02, Loss_0: 1.333e-03, Loss_r: 9.418e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 7.294e-03, Loss_0: 7.061e-04, Loss_r: 6.587e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.279e-03, Loss_0: 4.102e-04, Loss_r: 4.869e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 170, Loss: 3.926e-03, Loss_0: 2.650e-04, Loss_r: 3.661e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.892e-03, Loss_0: 1.985e-04, Loss_r: 2.693e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 190, Loss: 2.112e-03, Loss_0: 1.305e-04, Loss_r: 1.981e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.568e-03, Loss_0: 8.523e-05, Loss_r: 1.483e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.201e-03, Loss_0: 6.216e-05, Loss_r: 1.139e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.650e-04, Loss_0: 4.071e-05, Loss_r: 9.243e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.328e-03, Loss_0: 3.674e-05, Loss_r: 1.292e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 7.231e-04, Loss_0: 1.941e-05, Loss_r: 7.037e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.744e-04, Loss_0: 1.512e-05, Loss_r: 5.593e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.283e-04, Loss_0: 1.322e-05, Loss_r: 5.150e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 4.455e-04, Loss_0: 9.543e-06, Loss_r: 4.360e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.987e-04, Loss_0: 7.373e-06, Loss_r: 3.914e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.582e-04, Loss_0: 6.092e-06, Loss_r: 3.521e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.239e-04, Loss_0: 5.190e-06, Loss_r: 3.187e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 2.933e-04, Loss_0: 4.212e-06, Loss_r: 2.890e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.669e-04, Loss_0: 3.546e-06, Loss_r: 2.633e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.441e-04, Loss_0: 2.888e-06, Loss_r: 2.412e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 2.589e-04, Loss_0: 1.588e-06, Loss_r: 2.573e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 2.073e-04, Loss_0: 2.208e-06, Loss_r: 2.051e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 360, Loss: 2.080e-04, Loss_0: 3.068e-06, Loss_r: 2.049e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.368e-04, Loss_0: 3.703e-06, Loss_r: 2.331e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 380, Loss: 2.013e-04, Loss_0: 2.640e-06, Loss_r: 1.987e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.715e-04, Loss_0: 1.973e-06, Loss_r: 1.695e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.532e-04, Loss_0: 1.563e-06, Loss_r: 1.516e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.419e-04, Loss_0: 1.002e-06, Loss_r: 1.409e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.353e-04, Loss_0: 7.915e-07, Loss_r: 1.345e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 1.284e-04, Loss_0: 9.312e-07, Loss_r: 1.275e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.223e-04, Loss_0: 6.755e-07, Loss_r: 1.217e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.171e-04, Loss_0: 5.984e-07, Loss_r: 1.165e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 1.173e-04, Loss_0: 3.178e-07, Loss_r: 1.170e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.138e-03, Loss_0: 5.483e-06, Loss_r: 1.133e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.149e-04, Loss_0: 1.309e-06, Loss_r: 1.136e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.050e-04, Loss_0: 2.345e-07, Loss_r: 1.047e-04, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.143e-04, Loss_0: 5.713e-08, Loss_r: 1.142e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.044e-04, Loss_0: 8.282e-08, Loss_r: 1.043e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 9.380e-05, Loss_0: 2.105e-07, Loss_r: 9.359e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 530, Loss: 8.892e-05, Loss_0: 3.807e-07, Loss_r: 8.854e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 8.721e-05, Loss_0: 4.762e-07, Loss_r: 8.673e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 550, Loss: 8.343e-05, Loss_0: 3.213e-07, Loss_r: 8.311e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 8.101e-05, Loss_0: 2.214e-07, Loss_r: 8.079e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 7.848e-05, Loss_0: 2.899e-07, Loss_r: 7.819e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 7.606e-05, Loss_0: 2.177e-07, Loss_r: 7.584e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 7.381e-05, Loss_0: 1.979e-07, Loss_r: 7.361e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 7.178e-05, Loss_0: 1.576e-07, Loss_r: 7.163e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 610, Loss: 8.771e-05, Loss_0: 1.127e-08, Loss_r: 8.770e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.952e-03, Loss_0: 3.265e-05, Loss_r: 1.920e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 630, Loss: 5.053e-04, Loss_0: 7.886e-06, Loss_r: 4.974e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.502e-04, Loss_0: 3.172e-06, Loss_r: 2.470e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.014e-04, Loss_0: 3.346e-07, Loss_r: 1.010e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 6.475e-05, Loss_0: 1.804e-08, Loss_r: 6.473e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 670, Loss: 6.006e-05, Loss_0: 1.149e-07, Loss_r: 5.995e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 5.837e-05, Loss_0: 1.030e-07, Loss_r: 5.827e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 5.703e-05, Loss_0: 6.712e-08, Loss_r: 5.696e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 5.542e-05, Loss_0: 6.109e-08, Loss_r: 5.536e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 5.344e-05, Loss_0: 1.023e-07, Loss_r: 5.334e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.196e-05, Loss_0: 1.454e-07, Loss_r: 5.181e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.043e-05, Loss_0: 1.054e-07, Loss_r: 5.033e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 4.900e-05, Loss_0: 1.007e-07, Loss_r: 4.890e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 4.760e-05, Loss_0: 9.902e-08, Loss_r: 4.750e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 4.625e-05, Loss_0: 9.763e-08, Loss_r: 4.615e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.492e-05, Loss_0: 8.739e-08, Loss_r: 4.484e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.364e-05, Loss_0: 7.721e-08, Loss_r: 4.356e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 790, Loss: 4.272e-05, Loss_0: 3.201e-08, Loss_r: 4.269e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.305e-04, Loss_0: 2.058e-06, Loss_r: 1.284e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.408e-04, Loss_0: 4.477e-06, Loss_r: 1.363e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.498e-05, Loss_0: 1.021e-06, Loss_r: 5.396e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.240e-04, Loss_0: 1.173e-05, Loss_r: 2.123e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 5.604e-05, Loss_0: 1.635e-06, Loss_r: 5.440e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 4.677e-05, Loss_0: 2.256e-07, Loss_r: 4.655e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 4.525e-05, Loss_0: 2.155e-07, Loss_r: 4.504e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 3.678e-05, Loss_0: 6.366e-10, Loss_r: 3.678e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 880, Loss: 3.443e-05, Loss_0: 4.219e-08, Loss_r: 3.439e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 3.341e-05, Loss_0: 6.520e-08, Loss_r: 3.335e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 3.248e-05, Loss_0: 5.738e-08, Loss_r: 3.242e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 910, Loss: 3.158e-05, Loss_0: 4.551e-08, Loss_r: 3.154e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.071e-05, Loss_0: 3.970e-08, Loss_r: 3.067e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 2.985e-05, Loss_0: 4.249e-08, Loss_r: 2.980e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.901e-05, Loss_0: 4.999e-08, Loss_r: 2.896e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 950, Loss: 2.820e-05, Loss_0: 4.985e-08, Loss_r: 2.815e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 2.741e-05, Loss_0: 4.270e-08, Loss_r: 2.737e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 970, Loss: 2.665e-05, Loss_0: 4.266e-08, Loss_r: 2.660e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.590e-05, Loss_0: 4.022e-08, Loss_r: 2.586e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.518e-05, Loss_0: 3.893e-08, Loss_r: 2.514e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.447e-05, Loss_0: 3.633e-08, Loss_r: 2.444e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.379e-05, Loss_0: 3.529e-08, Loss_r: 2.376e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.313e-05, Loss_0: 3.437e-08, Loss_r: 2.309e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 2.248e-05, Loss_0: 3.798e-08, Loss_r: 2.245e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 2.240e-05, Loss_0: 1.410e-07, Loss_r: 2.226e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 2.942e-04, Loss_0: 2.136e-05, Loss_r: 2.728e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.194e-03, Loss_0: 1.001e-04, Loss_r: 1.093e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 6.203e-04, Loss_0: 5.962e-05, Loss_r: 5.606e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 9.374e-05, Loss_0: 9.986e-06, Loss_r: 8.375e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 4.336e-05, Loss_0: 3.329e-06, Loss_r: 4.003e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 4.771e-05, Loss_0: 2.933e-06, Loss_r: 4.477e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 2.001e-05, Loss_0: 5.016e-08, Loss_r: 1.996e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.303e-05, Loss_0: 7.987e-07, Loss_r: 2.223e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.892e-05, Loss_0: 4.577e-08, Loss_r: 1.887e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.877e-05, Loss_0: 3.700e-09, Loss_r: 1.877e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.802e-05, Loss_0: 1.451e-09, Loss_r: 1.802e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.743e-05, Loss_0: 1.951e-08, Loss_r: 1.741e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.696e-05, Loss_0: 2.973e-08, Loss_r: 1.693e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.651e-05, Loss_0: 3.007e-08, Loss_r: 1.648e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.608e-05, Loss_0: 2.647e-08, Loss_r: 1.605e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.566e-05, Loss_0: 2.335e-08, Loss_r: 1.564e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.525e-05, Loss_0: 2.063e-08, Loss_r: 1.523e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.486e-05, Loss_0: 1.872e-08, Loss_r: 1.484e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.447e-05, Loss_0: 1.780e-08, Loss_r: 1.446e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.410e-05, Loss_0: 1.728e-08, Loss_r: 1.409e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.374e-05, Loss_0: 1.703e-08, Loss_r: 1.373e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.339e-05, Loss_0: 1.596e-08, Loss_r: 1.338e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.306e-05, Loss_0: 1.527e-08, Loss_r: 1.304e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.273e-05, Loss_0: 1.473e-08, Loss_r: 1.271e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 1.241e-05, Loss_0: 1.401e-08, Loss_r: 1.239e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.210e-05, Loss_0: 1.351e-08, Loss_r: 1.209e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.180e-05, Loss_0: 1.298e-08, Loss_r: 1.179e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.151e-05, Loss_0: 1.249e-08, Loss_r: 1.149e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.123e-05, Loss_0: 1.204e-08, Loss_r: 1.121e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.095e-05, Loss_0: 1.136e-08, Loss_r: 1.094e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.069e-05, Loss_0: 1.111e-08, Loss_r: 1.067e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.043e-05, Loss_0: 1.277e-08, Loss_r: 1.042e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.052e-05, Loss_0: 1.045e-07, Loss_r: 1.041e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.325e-04, Loss_0: 4.835e-05, Loss_r: 2.842e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.434e-03, Loss_0: 3.074e-04, Loss_r: 2.126e-03, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 7.955e-05, Loss_0: 1.554e-05, Loss_r: 6.401e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.587e-04, Loss_0: 2.155e-05, Loss_r: 1.372e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 8.942e-05, Loss_0: 2.036e-05, Loss_r: 6.906e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 4.614e-05, Loss_0: 1.002e-05, Loss_r: 3.612e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.419e-05, Loss_0: 1.774e-06, Loss_r: 2.241e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 1.372e-05, Loss_0: 1.031e-06, Loss_r: 1.269e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 1.033e-05, Loss_0: 3.802e-08, Loss_r: 1.029e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 9.994e-06, Loss_0: 4.653e-09, Loss_r: 9.989e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 9.838e-06, Loss_0: 1.342e-07, Loss_r: 9.704e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 9.415e-06, Loss_0: 6.474e-10, Loss_r: 9.415e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 9.175e-06, Loss_0: 1.685e-09, Loss_r: 9.174e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 8.957e-06, Loss_0: 2.142e-08, Loss_r: 8.935e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 8.744e-06, Loss_0: 1.271e-08, Loss_r: 8.731e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 8.545e-06, Loss_0: 6.337e-09, Loss_r: 8.539e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 8.354e-06, Loss_0: 7.020e-09, Loss_r: 8.347e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 8.169e-06, Loss_0: 7.710e-09, Loss_r: 8.162e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 7.992e-06, Loss_0: 8.060e-09, Loss_r: 7.984e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 7.820e-06, Loss_0: 7.601e-09, Loss_r: 7.813e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 7.655e-06, Loss_0: 7.056e-09, Loss_r: 7.648e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 7.494e-06, Loss_0: 6.820e-09, Loss_r: 7.488e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 7.339e-06, Loss_0: 6.361e-09, Loss_r: 7.333e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 7.189e-06, Loss_0: 6.208e-09, Loss_r: 7.183e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 7.043e-06, Loss_0: 5.878e-09, Loss_r: 7.037e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 6.902e-06, Loss_0: 5.584e-09, Loss_r: 6.896e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 6.765e-06, Loss_0: 5.453e-09, Loss_r: 6.759e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 6.632e-06, Loss_0: 5.084e-09, Loss_r: 6.627e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 6.503e-06, Loss_0: 4.916e-09, Loss_r: 6.498e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 6.378e-06, Loss_0: 4.685e-09, Loss_r: 6.373e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 6.256e-06, Loss_0: 4.614e-09, Loss_r: 6.251e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 6.138e-06, Loss_0: 4.400e-09, Loss_r: 6.133e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 6.023e-06, Loss_0: 4.244e-09, Loss_r: 6.019e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 5.912e-06, Loss_0: 4.100e-09, Loss_r: 5.907e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 5.803e-06, Loss_0: 3.829e-09, Loss_r: 5.799e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 5.698e-06, Loss_0: 3.744e-09, Loss_r: 5.694e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 5.596e-06, Loss_0: 3.653e-09, Loss_r: 5.592e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 5.496e-06, Loss_0: 3.441e-09, Loss_r: 5.493e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 5.400e-06, Loss_0: 3.265e-09, Loss_r: 5.396e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 5.306e-06, Loss_0: 3.015e-09, Loss_r: 5.303e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 5.215e-06, Loss_0: 1.927e-09, Loss_r: 5.213e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 5.207e-06, Loss_0: 6.760e-09, Loss_r: 5.200e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 2.618e-05, Loss_0: 4.638e-06, Loss_r: 2.154e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 4.273e-03, Loss_0: 7.704e-04, Loss_r: 3.502e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 8.460e-04, Loss_0: 1.679e-04, Loss_r: 6.781e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.604e-04, Loss_0: 5.424e-05, Loss_r: 2.062e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 6.077e-05, Loss_0: 1.016e-05, Loss_r: 5.061e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.014e-05, Loss_0: 8.521e-06, Loss_r: 2.162e-05, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.412e-05, Loss_0: 1.998e-06, Loss_r: 1.212e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 9.198e-06, Loss_0: 1.133e-06, Loss_r: 8.065e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 5.736e-06, Loss_0: 2.057e-07, Loss_r: 5.531e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 6.052e-06, Loss_0: 2.407e-07, Loss_r: 5.811e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 5.270e-06, Loss_0: 5.423e-08, Loss_r: 5.216e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 5.087e-06, Loss_0: 4.441e-10, Loss_r: 5.086e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 5.008e-06, Loss_0: 3.269e-10, Loss_r: 5.008e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 4.910e-06, Loss_0: 8.717e-15, Loss_r: 4.910e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 4.818e-06, Loss_0: 3.825e-10, Loss_r: 4.817e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 4.733e-06, Loss_0: 1.096e-09, Loss_r: 4.732e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 4.653e-06, Loss_0: 1.646e-09, Loss_r: 4.651e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 4.577e-06, Loss_0: 1.906e-09, Loss_r: 4.575e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 4.503e-06, Loss_0: 2.234e-09, Loss_r: 4.501e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 4.432e-06, Loss_0: 2.318e-09, Loss_r: 4.430e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 4.364e-06, Loss_0: 2.403e-09, Loss_r: 4.361e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 4.297e-06, Loss_0: 2.339e-09, Loss_r: 4.295e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 4.232e-06, Loss_0: 2.183e-09, Loss_r: 4.230e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 4.169e-06, Loss_0: 2.072e-09, Loss_r: 4.167e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 4.108e-06, Loss_0: 2.003e-09, Loss_r: 4.106e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 4.048e-06, Loss_0: 1.829e-09, Loss_r: 4.047e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 3.990e-06, Loss_0: 1.888e-09, Loss_r: 3.988e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 3.933e-06, Loss_0: 1.851e-09, Loss_r: 3.931e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 3.878e-06, Loss_0: 1.700e-09, Loss_r: 3.876e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 3.824e-06, Loss_0: 1.670e-09, Loss_r: 3.822e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 3.771e-06, Loss_0: 1.573e-09, Loss_r: 3.770e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 3.720e-06, Loss_0: 1.605e-09, Loss_r: 3.718e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 3.669e-06, Loss_0: 1.606e-09, Loss_r: 3.668e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 3.620e-06, Loss_0: 1.541e-09, Loss_r: 3.619e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 3.572e-06, Loss_0: 1.450e-09, Loss_r: 3.571e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 3.525e-06, Loss_0: 1.433e-09, Loss_r: 3.524e-06, Time: 0.03, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 3.479e-06, Loss_0: 1.464e-09, Loss_r: 3.477e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 3.437e-06, Loss_0: 4.401e-09, Loss_r: 3.433e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 5.212e-06, Loss_0: 5.506e-07, Loss_r: 4.661e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 3.402e-06, Loss_0: 6.928e-09, Loss_r: 3.395e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 3.672e-06, Loss_0: 7.778e-08, Loss_r: 3.594e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 3.279e-06, Loss_0: 4.433e-09, Loss_r: 3.275e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 3.276e-06, Loss_0: 1.788e-08, Loss_r: 3.258e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 3.217e-06, Loss_0: 9.871e-10, Loss_r: 3.216e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 3.168e-06, Loss_0: 2.678e-09, Loss_r: 3.165e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 3.132e-06, Loss_0: 8.626e-10, Loss_r: 3.131e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 3.097e-06, Loss_0: 1.098e-09, Loss_r: 3.096e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 3.063e-06, Loss_0: 7.039e-10, Loss_r: 3.063e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 3.030e-06, Loss_0: 1.386e-09, Loss_r: 3.029e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 2.997e-06, Loss_0: 8.220e-10, Loss_r: 2.996e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 2.965e-06, Loss_0: 6.541e-10, Loss_r: 2.964e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 2.933e-06, Loss_0: 5.662e-10, Loss_r: 2.933e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 2.903e-06, Loss_0: 1.302e-10, Loss_r: 2.903e-06, Time: 0.03, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 2.916e-06, Loss_0: 6.656e-09, Loss_r: 2.910e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 7.426e-06, Loss_0: 1.165e-06, Loss_r: 6.261e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 3.124e-06, Loss_0: 6.741e-08, Loss_r: 3.057e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 2.969e-06, Loss_0: 6.145e-08, Loss_r: 2.908e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 3.036e-06, Loss_0: 8.938e-08, Loss_r: 2.947e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 2.739e-06, Loss_0: 3.123e-09, Loss_r: 2.736e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 2.746e-06, Loss_0: 5.074e-09, Loss_r: 2.741e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 2.685e-06, Loss_0: 9.081e-10, Loss_r: 2.684e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 2.664e-06, Loss_0: 3.344e-09, Loss_r: 2.660e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 2.637e-06, Loss_0: 1.998e-11, Loss_r: 2.637e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 2.611e-06, Loss_0: 1.310e-09, Loss_r: 2.610e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 2.587e-06, Loss_0: 4.713e-10, Loss_r: 2.586e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 2.563e-06, Loss_0: 8.163e-10, Loss_r: 2.562e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 2.539e-06, Loss_0: 4.661e-10, Loss_r: 2.539e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 2.516e-06, Loss_0: 6.944e-10, Loss_r: 2.515e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 2.493e-06, Loss_0: 6.451e-10, Loss_r: 2.492e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.470e-06, Loss_0: 4.475e-10, Loss_r: 2.470e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 2.448e-06, Loss_0: 1.871e-10, Loss_r: 2.448e-06, Time: 0.03, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 2.433e-06, Loss_0: 4.544e-10, Loss_r: 2.433e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 2.834e-06, Loss_0: 9.779e-08, Loss_r: 2.736e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 2.441e-06, Loss_0: 9.758e-09, Loss_r: 2.431e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 2.367e-06, Loss_0: 2.119e-09, Loss_r: 2.364e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 2.366e-06, Loss_0: 9.220e-09, Loss_r: 2.356e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 2.331e-06, Loss_0: 3.340e-09, Loss_r: 2.328e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 2.308e-06, Loss_0: 4.672e-11, Loss_r: 2.308e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 2.289e-06, Loss_0: 6.067e-11, Loss_r: 2.289e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 2.270e-06, Loss_0: 9.502e-10, Loss_r: 2.269e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 2.252e-06, Loss_0: 5.306e-10, Loss_r: 2.251e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 2.233e-06, Loss_0: 3.193e-10, Loss_r: 2.233e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 2.215e-06, Loss_0: 5.642e-10, Loss_r: 2.215e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.197e-06, Loss_0: 3.883e-10, Loss_r: 2.197e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.180e-06, Loss_0: 4.632e-10, Loss_r: 2.179e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.162e-06, Loss_0: 4.119e-10, Loss_r: 2.162e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.145e-06, Loss_0: 3.794e-10, Loss_r: 2.144e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.127e-06, Loss_0: 3.609e-10, Loss_r: 2.127e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.110e-06, Loss_0: 4.073e-10, Loss_r: 2.110e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.093e-06, Loss_0: 4.390e-10, Loss_r: 2.093e-06, Time: 0.03, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 2.076e-06, Loss_0: 3.552e-10, Loss_r: 2.076e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 2.060e-06, Loss_0: 1.228e-10, Loss_r: 2.060e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 2.084e-06, Loss_0: 7.056e-09, Loss_r: 2.077e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.254e-05, Loss_0: 2.631e-06, Loss_r: 9.906e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 2.072e-06, Loss_0: 1.101e-08, Loss_r: 2.061e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 3.336e-06, Loss_0: 3.649e-07, Loss_r: 2.971e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.351e-06, Loss_0: 1.058e-07, Loss_r: 2.245e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.078e-06, Loss_0: 2.182e-08, Loss_r: 2.056e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.990e-06, Loss_0: 5.702e-09, Loss_r: 1.984e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.974e-06, Loss_0: 1.203e-08, Loss_r: 1.962e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.930e-06, Loss_0: 6.444e-11, Loss_r: 1.930e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.913e-06, Loss_0: 2.017e-10, Loss_r: 1.913e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.900e-06, Loss_0: 7.279e-10, Loss_r: 1.899e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.885e-06, Loss_0: 1.430e-10, Loss_r: 1.885e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.872e-06, Loss_0: 3.343e-10, Loss_r: 1.871e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.858e-06, Loss_0: 4.662e-10, Loss_r: 1.858e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.845e-06, Loss_0: 2.187e-10, Loss_r: 1.845e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.831e-06, Loss_0: 2.853e-10, Loss_r: 1.831e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.818e-06, Loss_0: 3.155e-10, Loss_r: 1.818e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.805e-06, Loss_0: 2.927e-10, Loss_r: 1.805e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.792e-06, Loss_0: 2.621e-10, Loss_r: 1.791e-06, Time: 0.03, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.779e-06, Loss_0: 4.548e-11, Loss_r: 1.779e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.834e-06, Loss_0: 1.311e-08, Loss_r: 1.821e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 2.398e-05, Loss_0: 5.496e-06, Loss_r: 1.849e-05, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 1.769e-06, Loss_0: 4.061e-09, Loss_r: 1.765e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 4.878e-06, Loss_0: 8.147e-07, Loss_r: 4.063e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 2.391e-06, Loss_0: 1.810e-07, Loss_r: 2.210e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 1.984e-06, Loss_0: 5.964e-08, Loss_r: 1.924e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.760e-06, Loss_0: 1.174e-08, Loss_r: 1.749e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.758e-06, Loss_0: 2.219e-08, Loss_r: 1.736e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.683e-06, Loss_0: 5.693e-10, Loss_r: 1.682e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.665e-06, Loss_0: 1.065e-10, Loss_r: 1.665e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.655e-06, Loss_0: 8.100e-10, Loss_r: 1.654e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.644e-06, Loss_0: 5.323e-11, Loss_r: 1.644e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.633e-06, Loss_0: 2.739e-10, Loss_r: 1.632e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.622e-06, Loss_0: 4.038e-10, Loss_r: 1.622e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.611e-06, Loss_0: 1.122e-10, Loss_r: 1.611e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.601e-06, Loss_0: 1.521e-10, Loss_r: 1.601e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.590e-06, Loss_0: 1.694e-10, Loss_r: 1.590e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.580e-06, Loss_0: 7.711e-11, Loss_r: 1.580e-06, Time: 0.03, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.573e-06, Loss_0: 2.498e-10, Loss_r: 1.573e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.795e-06, Loss_0: 5.110e-08, Loss_r: 1.744e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.578e-06, Loss_0: 4.719e-09, Loss_r: 1.573e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.543e-06, Loss_0: 1.270e-09, Loss_r: 1.541e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.543e-06, Loss_0: 4.572e-09, Loss_r: 1.539e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.525e-06, Loss_0: 1.506e-09, Loss_r: 1.523e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.514e-06, Loss_0: 7.002e-12, Loss_r: 1.514e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.505e-06, Loss_0: 1.212e-11, Loss_r: 1.505e-06, Time: 0.03, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.496e-06, Loss_0: 4.450e-10, Loss_r: 1.495e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.487e-06, Loss_0: 2.156e-10, Loss_r: 1.487e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.478e-06, Loss_0: 1.308e-10, Loss_r: 1.478e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.469e-06, Loss_0: 2.458e-10, Loss_r: 1.469e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.460e-06, Loss_0: 1.599e-10, Loss_r: 1.460e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.452e-06, Loss_0: 1.588e-10, Loss_r: 1.452e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.443e-06, Loss_0: 2.020e-10, Loss_r: 1.443e-06, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.434e-06, Loss_0: 2.099e-10, Loss_r: 1.434e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.426e-06, Loss_0: 1.824e-10, Loss_r: 1.426e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.417e-06, Loss_0: 1.998e-10, Loss_r: 1.417e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.409e-06, Loss_0: 2.933e-10, Loss_r: 1.409e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.401e-06, Loss_0: 6.113e-10, Loss_r: 1.401e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.406e-06, Loss_0: 5.119e-09, Loss_r: 1.401e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 2.389e-06, Loss_0: 2.572e-07, Loss_r: 2.132e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 1.487e-06, Loss_0: 3.128e-08, Loss_r: 1.455e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 1.381e-06, Loss_0: 1.617e-09, Loss_r: 1.379e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 1.414e-06, Loss_0: 9.958e-09, Loss_r: 1.404e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 1.363e-06, Loss_0: 1.078e-09, Loss_r: 1.362e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 1.350e-06, Loss_0: 1.589e-09, Loss_r: 1.349e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.342e-06, Loss_0: 1.004e-09, Loss_r: 1.341e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.334e-06, Loss_0: 1.786e-14, Loss_r: 1.334e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.326e-06, Loss_0: 1.383e-10, Loss_r: 1.326e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.319e-06, Loss_0: 2.628e-10, Loss_r: 1.318e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.312e-06, Loss_0: 8.934e-11, Loss_r: 1.312e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.305e-06, Loss_0: 1.925e-10, Loss_r: 1.304e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 1.298e-06, Loss_0: 1.250e-10, Loss_r: 1.297e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.290e-06, Loss_0: 1.607e-10, Loss_r: 1.290e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.284e-06, Loss_0: 1.502e-10, Loss_r: 1.283e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.277e-06, Loss_0: 1.443e-10, Loss_r: 1.277e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 1.270e-06, Loss_0: 1.869e-10, Loss_r: 1.270e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 1.263e-06, Loss_0: 1.844e-10, Loss_r: 1.263e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 1.256e-06, Loss_0: 1.917e-10, Loss_r: 1.256e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 1.250e-06, Loss_0: 3.678e-10, Loss_r: 1.249e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 1.255e-06, Loss_0: 4.468e-09, Loss_r: 1.251e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 3.065e-06, Loss_0: 4.550e-07, Loss_r: 2.610e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.306e-06, Loss_0: 2.143e-08, Loss_r: 1.284e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.339e-06, Loss_0: 2.399e-08, Loss_r: 1.315e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.324e-06, Loss_0: 2.169e-08, Loss_r: 1.302e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.212e-06, Loss_0: 1.875e-10, Loss_r: 1.212e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.221e-06, Loss_0: 5.043e-09, Loss_r: 1.216e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.201e-06, Loss_0: 5.690e-12, Loss_r: 1.201e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.196e-06, Loss_0: 2.930e-11, Loss_r: 1.196e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.190e-06, Loss_0: 5.899e-10, Loss_r: 1.189e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.183e-06, Loss_0: 3.029e-11, Loss_r: 1.183e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.178e-06, Loss_0: 1.951e-10, Loss_r: 1.177e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 1.172e-06, Loss_0: 6.292e-11, Loss_r: 1.172e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.166e-06, Loss_0: 1.947e-10, Loss_r: 1.166e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.161e-06, Loss_0: 9.845e-11, Loss_r: 1.160e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.155e-06, Loss_0: 1.315e-10, Loss_r: 1.155e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.149e-06, Loss_0: 1.380e-10, Loss_r: 1.149e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.144e-06, Loss_0: 1.098e-10, Loss_r: 1.144e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.138e-06, Loss_0: 1.046e-10, Loss_r: 1.138e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.133e-06, Loss_0: 3.351e-11, Loss_r: 1.133e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 1.133e-06, Loss_0: 6.522e-10, Loss_r: 1.132e-06, Time: 0.03, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 1.775e-06, Loss_0: 1.466e-07, Loss_r: 1.628e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 1.155e-06, Loss_0: 7.256e-09, Loss_r: 1.148e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 1.141e-06, Loss_0: 8.632e-09, Loss_r: 1.132e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.146e-06, Loss_0: 1.130e-08, Loss_r: 1.134e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.103e-06, Loss_0: 4.744e-10, Loss_r: 1.102e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 1.102e-06, Loss_0: 5.510e-10, Loss_r: 1.102e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.093e-06, Loss_0: 7.113e-11, Loss_r: 1.093e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.089e-06, Loss_0: 4.865e-10, Loss_r: 1.088e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 1.083e-06, Loss_0: 1.740e-11, Loss_r: 1.083e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.078e-06, Loss_0: 1.306e-10, Loss_r: 1.078e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 1.074e-06, Loss_0: 1.119e-10, Loss_r: 1.074e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 1.069e-06, Loss_0: 7.702e-11, Loss_r: 1.069e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.064e-06, Loss_0: 8.687e-11, Loss_r: 1.064e-06, Time: 0.03, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.060e-06, Loss_0: 9.205e-11, Loss_r: 1.060e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 1.055e-06, Loss_0: 8.837e-11, Loss_r: 1.055e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 1.050e-06, Loss_0: 1.430e-10, Loss_r: 1.050e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 1.046e-06, Loss_0: 1.514e-10, Loss_r: 1.046e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 1.041e-06, Loss_0: 2.321e-10, Loss_r: 1.041e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 1.038e-06, Loss_0: 9.446e-10, Loss_r: 1.038e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 1.102e-06, Loss_0: 1.888e-08, Loss_r: 1.083e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 1.040e-06, Loss_0: 3.927e-09, Loss_r: 1.036e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 1.024e-06, Loss_0: 1.218e-10, Loss_r: 1.024e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 1.022e-06, Loss_0: 1.306e-10, Loss_r: 1.022e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 1.017e-06, Loss_0: 5.488e-11, Loss_r: 1.017e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 1.012e-06, Loss_0: 8.320e-11, Loss_r: 1.012e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 1.008e-06, Loss_0: 2.284e-10, Loss_r: 1.008e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 1.004e-06, Loss_0: 8.892e-11, Loss_r: 1.004e-06, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 1.000e-06, Loss_0: 6.238e-11, Loss_r: 1.000e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 9.962e-07, Loss_0: 1.046e-10, Loss_r: 9.961e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 9.922e-07, Loss_0: 9.731e-11, Loss_r: 9.921e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 9.882e-07, Loss_0: 7.947e-11, Loss_r: 9.882e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 9.844e-07, Loss_0: 7.621e-11, Loss_r: 9.843e-07, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 9.805e-07, Loss_0: 1.020e-10, Loss_r: 9.804e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 9.766e-07, Loss_0: 8.670e-11, Loss_r: 9.766e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 9.728e-07, Loss_0: 6.838e-11, Loss_r: 9.727e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 9.690e-07, Loss_0: 8.602e-11, Loss_r: 9.689e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 9.651e-07, Loss_0: 8.121e-11, Loss_r: 9.650e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 9.613e-07, Loss_0: 7.339e-11, Loss_r: 9.612e-07, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 9.575e-07, Loss_0: 6.713e-11, Loss_r: 9.574e-07, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 9.538e-07, Loss_0: 2.026e-11, Loss_r: 9.538e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 9.561e-07, Loss_0: 8.225e-10, Loss_r: 9.552e-07, Time: 0.03, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 1.790e-06, Loss_0: 1.881e-07, Loss_r: 1.602e-06, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 9.824e-07, Loss_0: 7.571e-09, Loss_r: 9.748e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 9.870e-07, Loss_0: 1.297e-08, Loss_r: 9.740e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 9.861e-07, Loss_0: 1.348e-08, Loss_r: 9.726e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 9.329e-07, Loss_0: 1.759e-10, Loss_r: 9.328e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 9.366e-07, Loss_0: 1.051e-09, Loss_r: 9.355e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 9.263e-07, Loss_0: 1.310e-10, Loss_r: 9.262e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 9.237e-07, Loss_0: 4.500e-10, Loss_r: 9.233e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 9.199e-07, Loss_0: 1.105e-12, Loss_r: 9.199e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 9.164e-07, Loss_0: 1.245e-10, Loss_r: 9.162e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 9.131e-07, Loss_0: 7.369e-11, Loss_r: 9.130e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 9.099e-07, Loss_0: 7.578e-11, Loss_r: 9.098e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 9.066e-07, Loss_0: 5.414e-11, Loss_r: 9.066e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 9.034e-07, Loss_0: 1.133e-10, Loss_r: 9.033e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 9.001e-07, Loss_0: 7.851e-11, Loss_r: 9.000e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 8.971e-07, Loss_0: 5.377e-11, Loss_r: 8.970e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 8.937e-07, Loss_0: 9.128e-11, Loss_r: 8.936e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 8.906e-07, Loss_0: 1.498e-10, Loss_r: 8.904e-07, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 8.881e-07, Loss_0: 4.883e-10, Loss_r: 8.877e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 9.160e-07, Loss_0: 8.842e-09, Loss_r: 9.072e-07, Time: 0.03, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 3.751e-06, Loss_0: 6.764e-07, Loss_r: 3.075e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 1.156e-06, Loss_0: 6.796e-08, Loss_r: 1.088e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 9.192e-07, Loss_0: 8.466e-09, Loss_r: 9.107e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 1.027e-06, Loss_0: 3.244e-08, Loss_r: 9.941e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 8.939e-07, Loss_0: 4.358e-09, Loss_r: 8.895e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 8.761e-07, Loss_0: 2.873e-09, Loss_r: 8.732e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 8.690e-07, Loss_0: 1.704e-09, Loss_r: 8.673e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 8.634e-07, Loss_0: 1.738e-10, Loss_r: 8.632e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 8.588e-07, Loss_0: 2.479e-11, Loss_r: 8.587e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 8.562e-07, Loss_0: 2.621e-10, Loss_r: 8.559e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 8.533e-07, Loss_0: 5.022e-12, Loss_r: 8.533e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 8.505e-07, Loss_0: 1.314e-10, Loss_r: 8.504e-07, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 8.477e-07, Loss_0: 2.716e-11, Loss_r: 8.477e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 8.449e-07, Loss_0: 1.143e-10, Loss_r: 8.448e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 8.421e-07, Loss_0: 6.273e-11, Loss_r: 8.420e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 8.394e-07, Loss_0: 3.549e-11, Loss_r: 8.393e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 8.368e-07, Loss_0: 4.706e-11, Loss_r: 8.367e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 8.340e-07, Loss_0: 8.911e-11, Loss_r: 8.339e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 8.316e-07, Loss_0: 2.211e-10, Loss_r: 8.313e-07, Time: 0.03, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 8.331e-07, Loss_0: 1.602e-09, Loss_r: 8.315e-07, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 1.012e-06, Loss_0: 4.556e-08, Loss_r: 9.662e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 8.537e-07, Loss_0: 8.239e-09, Loss_r: 8.454e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 8.210e-07, Loss_0: 4.330e-11, Loss_r: 8.210e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 8.246e-07, Loss_0: 8.496e-10, Loss_r: 8.237e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 8.196e-07, Loss_0: 3.965e-10, Loss_r: 8.192e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 8.138e-07, Loss_0: 4.699e-11, Loss_r: 8.137e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 8.120e-07, Loss_0: 3.326e-10, Loss_r: 8.116e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 8.090e-07, Loss_0: 5.919e-11, Loss_r: 8.090e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 8.067e-07, Loss_0: 1.690e-11, Loss_r: 8.067e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 8.043e-07, Loss_0: 7.633e-11, Loss_r: 8.042e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 8.019e-07, Loss_0: 5.469e-11, Loss_r: 8.019e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 7.995e-07, Loss_0: 4.327e-11, Loss_r: 7.995e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 7.972e-07, Loss_0: 5.110e-11, Loss_r: 7.972e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 7.948e-07, Loss_0: 4.983e-11, Loss_r: 7.947e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 7.924e-07, Loss_0: 6.070e-11, Loss_r: 7.924e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 7.901e-07, Loss_0: 4.447e-11, Loss_r: 7.900e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 7.878e-07, Loss_0: 4.626e-11, Loss_r: 7.877e-07, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 7.854e-07, Loss_0: 4.996e-11, Loss_r: 7.853e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 7.830e-07, Loss_0: 3.804e-11, Loss_r: 7.830e-07, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 7.807e-07, Loss_0: 2.630e-11, Loss_r: 7.807e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 7.787e-07, Loss_0: 1.435e-12, Loss_r: 7.787e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 7.946e-07, Loss_0: 3.387e-09, Loss_r: 7.912e-07, Time: 0.03, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 3.088e-06, Loss_0: 5.143e-07, Loss_r: 2.574e-06, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 9.212e-07, Loss_0: 3.117e-08, Loss_r: 8.900e-07, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 8.520e-07, Loss_0: 2.047e-08, Loss_r: 8.315e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 9.055e-07, Loss_0: 3.389e-08, Loss_r: 8.716e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 7.721e-07, Loss_0: 2.036e-09, Loss_r: 7.700e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 7.776e-07, Loss_0: 2.469e-09, Loss_r: 7.751e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 7.625e-07, Loss_0: 8.090e-11, Loss_r: 7.624e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 7.619e-07, Loss_0: 9.599e-10, Loss_r: 7.609e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 7.574e-07, Loss_0: 1.528e-13, Loss_r: 7.574e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 7.553e-07, Loss_0: 1.238e-11, Loss_r: 7.553e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 7.533e-07, Loss_0: 1.302e-10, Loss_r: 7.531e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 7.512e-07, Loss_0: 1.251e-11, Loss_r: 7.512e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 7.491e-07, Loss_0: 8.221e-11, Loss_r: 7.490e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 7.470e-07, Loss_0: 3.711e-11, Loss_r: 7.470e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 7.450e-07, Loss_0: 3.233e-11, Loss_r: 7.450e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 7.429e-07, Loss_0: 1.061e-10, Loss_r: 7.428e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 7.410e-07, Loss_0: 5.406e-11, Loss_r: 7.409e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 7.389e-07, Loss_0: 2.072e-11, Loss_r: 7.389e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 7.369e-07, Loss_0: 1.401e-12, Loss_r: 7.369e-07, Time: 0.03, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 7.373e-07, Loss_0: 2.901e-10, Loss_r: 7.370e-07, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 8.700e-07, Loss_0: 2.861e-08, Loss_r: 8.414e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 7.500e-07, Loss_0: 3.414e-09, Loss_r: 7.466e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 7.295e-07, Loss_0: 2.212e-10, Loss_r: 7.292e-07, Time: 0.03, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 7.331e-07, Loss_0: 1.814e-09, Loss_r: 7.313e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 7.275e-07, Loss_0: 7.545e-10, Loss_r: 7.267e-07, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 20.5798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CSol[idx] = predict_CSol[idx].reshape(exact_C.shape)\n",
        "\n",
        "  error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "  print('Error C Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_C) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epBFO1t_qAC2",
        "outputId": "322d5d66-cfc7-4578-b8d1-8d59b922a1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Sol [1, 64, 64, 64, 1] : 3.535698e-03\n",
            "Error C Sol [1, 128, 128, 128, 1] : 2.439325e-03\n",
            "Error C Sol [1, 256, 256, 256, 1] : 1.245334e-03\n",
            "Error C Sol [1, 64, 64, 64, 64, 1] : 1.695138e-03\n",
            "Error C Sol [1, 256, 256, 256, 256, 1] : 2.823933e-03\n",
            "Error C Sol [1, 128, 128, 128, 128, 1] : 6.463824e-04\n",
            "Error C Sol [1, 128, 128, 64, 64, 1] : 1.055088e-03\n",
            "Error C Sol [1, 256, 128, 64, 32, 1] : 6.619894e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [4,5,7]:\n",
        "  plt.plot(t, predict_CSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_C.flatten(), 'c', label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "RluNTT9jqBmk",
        "outputId": "50c5e48f-e3ce-4510-e95e-111882522406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f115c140610>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PINN intermediate with ODE"
      ],
      "metadata": {
        "id": "hzoWWeL3u70i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,1],[1,128,128,128,1],[1,256,256,256,1],[1,64,64,64,64,1],[1,256,256,256,256,1],[1,128,128,128,128,1],[1,128,128,64,64,1],[1,256,128,64,32,1]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = ReactionPINN_ODEIntermediate(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2520e6b6-61c7-4524-c353-876527b61177",
        "id": "QmlVwkU1u70p"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.122e+00, Loss_0: 5.811e-04, Loss_r: 1.122e+00, Time: 3.11, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.158e-01, Loss_0: 1.253e-02, Loss_r: 1.905e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.305e-01, Loss_0: 2.036e-04, Loss_r: 2.285e-01, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.924e-01, Loss_0: 1.062e-03, Loss_r: 1.818e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.944e-01, Loss_0: 3.012e-03, Loss_r: 1.642e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.926e-01, Loss_0: 1.332e-03, Loss_r: 1.792e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.917e-01, Loss_0: 1.344e-03, Loss_r: 1.783e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.907e-01, Loss_0: 1.370e-03, Loss_r: 1.770e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.902e-01, Loss_0: 1.660e-03, Loss_r: 1.736e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.897e-01, Loss_0: 1.391e-03, Loss_r: 1.758e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.892e-01, Loss_0: 1.456e-03, Loss_r: 1.746e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.885e-01, Loss_0: 1.476e-03, Loss_r: 1.738e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.877e-01, Loss_0: 1.448e-03, Loss_r: 1.732e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.866e-01, Loss_0: 1.431e-03, Loss_r: 1.723e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.852e-01, Loss_0: 1.438e-03, Loss_r: 1.708e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.832e-01, Loss_0: 1.410e-03, Loss_r: 1.691e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.805e-01, Loss_0: 1.393e-03, Loss_r: 1.666e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.765e-01, Loss_0: 1.365e-03, Loss_r: 1.628e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.703e-01, Loss_0: 1.320e-03, Loss_r: 1.571e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.607e-01, Loss_0: 1.257e-03, Loss_r: 1.482e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.457e-01, Loss_0: 1.156e-03, Loss_r: 1.341e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.257e-01, Loss_0: 1.041e-03, Loss_r: 1.152e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.764e-02, Loss_0: 7.629e-04, Loss_r: 9.001e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 7.134e-02, Loss_0: 4.607e-04, Loss_r: 6.673e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.512e-02, Loss_0: 2.307e-04, Loss_r: 5.281e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 4.880e-02, Loss_0: 1.291e-04, Loss_r: 4.751e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 4.531e-02, Loss_0: 9.515e-05, Loss_r: 4.435e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 4.139e-02, Loss_0: 8.800e-05, Loss_r: 4.051e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.754e-02, Loss_0: 8.683e-05, Loss_r: 3.668e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.399e-02, Loss_0: 8.190e-05, Loss_r: 3.317e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.052e-02, Loss_0: 7.197e-05, Loss_r: 2.981e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 2.711e-02, Loss_0: 5.992e-05, Loss_r: 2.651e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.383e-02, Loss_0: 4.873e-05, Loss_r: 2.335e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.079e-02, Loss_0: 3.943e-05, Loss_r: 2.039e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.804e-02, Loss_0: 3.174e-05, Loss_r: 1.772e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.565e-02, Loss_0: 2.523e-05, Loss_r: 1.540e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.364e-02, Loss_0: 1.987e-05, Loss_r: 1.344e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.200e-02, Loss_0: 1.567e-05, Loss_r: 1.185e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.071e-02, Loss_0: 1.245e-05, Loss_r: 1.059e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 9.708e-03, Loss_0: 1.001e-05, Loss_r: 9.607e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 8.913e-03, Loss_0: 8.167e-06, Loss_r: 8.831e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 6.958e-03, Loss_0: 4.382e-06, Loss_r: 6.914e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 420, Loss: 6.438e-03, Loss_0: 4.530e-06, Loss_r: 6.393e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.991e-03, Loss_0: 3.557e-06, Loss_r: 5.955e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.589e-03, Loss_0: 2.901e-06, Loss_r: 5.560e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.219e-03, Loss_0: 2.682e-06, Loss_r: 5.192e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.872e-03, Loss_0: 2.306e-06, Loss_r: 4.848e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.544e-03, Loss_0: 1.992e-06, Loss_r: 4.524e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.234e-03, Loss_0: 1.775e-06, Loss_r: 4.217e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.942e-03, Loss_0: 1.580e-06, Loss_r: 3.926e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.670e-03, Loss_0: 9.792e-07, Loss_r: 3.660e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.418e-03, Loss_0: 2.048e-06, Loss_r: 3.397e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 520, Loss: 3.402e-03, Loss_0: 7.226e-06, Loss_r: 3.330e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.977e-03, Loss_0: 2.810e-07, Loss_r: 2.974e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.771e-03, Loss_0: 2.012e-07, Loss_r: 2.769e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.558e-03, Loss_0: 1.035e-06, Loss_r: 2.547e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.376e-03, Loss_0: 1.099e-06, Loss_r: 2.365e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.201e-03, Loss_0: 8.690e-07, Loss_r: 2.192e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.038e-03, Loss_0: 6.598e-07, Loss_r: 2.031e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.886e-03, Loss_0: 4.587e-07, Loss_r: 1.882e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.745e-03, Loss_0: 4.119e-07, Loss_r: 1.741e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.502e-03, Loss_0: 3.451e-07, Loss_r: 1.498e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.380e-03, Loss_0: 2.080e-07, Loss_r: 1.378e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.274e-03, Loss_0: 3.429e-08, Loss_r: 1.274e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.015e-03, Loss_0: 9.055e-06, Loss_r: 1.925e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.071e-03, Loss_0: 3.237e-07, Loss_r: 1.068e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.019e-03, Loss_0: 1.330e-06, Loss_r: 1.005e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 9.463e-04, Loss_0: 1.383e-06, Loss_r: 9.324e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 8.451e-04, Loss_0: 5.666e-07, Loss_r: 8.395e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 7.746e-04, Loss_0: 1.295e-08, Loss_r: 7.745e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 7.113e-04, Loss_0: 1.162e-07, Loss_r: 7.101e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 710, Loss: 6.572e-04, Loss_0: 9.547e-08, Loss_r: 6.562e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 6.088e-04, Loss_0: 1.669e-07, Loss_r: 6.072e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.642e-04, Loss_0: 1.404e-07, Loss_r: 5.628e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.473e-04, Loss_0: 7.932e-07, Loss_r: 5.394e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.701e-03, Loss_0: 2.394e-05, Loss_r: 1.461e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 7.020e-04, Loss_0: 5.679e-06, Loss_r: 6.452e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.751e-04, Loss_0: 1.310e-06, Loss_r: 4.620e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.063e-04, Loss_0: 1.832e-07, Loss_r: 4.045e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 3.848e-04, Loss_0: 9.483e-09, Loss_r: 3.847e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 3.667e-04, Loss_0: 3.825e-08, Loss_r: 3.663e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.272e-04, Loss_0: 1.734e-07, Loss_r: 6.254e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.939e-04, Loss_0: 2.206e-07, Loss_r: 5.917e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 5.620e-04, Loss_0: 2.386e-08, Loss_r: 5.618e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 5.321e-04, Loss_0: 9.882e-08, Loss_r: 5.311e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 5.052e-04, Loss_0: 1.196e-07, Loss_r: 5.040e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 4.811e-04, Loss_0: 1.584e-07, Loss_r: 4.795e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 5.411e-04, Loss_0: 2.487e-06, Loss_r: 5.162e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 8.107e-04, Loss_0: 1.018e-05, Loss_r: 7.089e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 8.109e-04, Loss_0: 1.060e-05, Loss_r: 7.049e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 5.619e-04, Loss_0: 4.695e-06, Loss_r: 5.149e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 910, Loss: 4.487e-04, Loss_0: 2.035e-06, Loss_r: 4.283e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.875e-04, Loss_0: 6.166e-07, Loss_r: 3.814e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 3.593e-04, Loss_0: 2.692e-08, Loss_r: 3.590e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 3.490e-04, Loss_0: 1.119e-08, Loss_r: 3.489e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.332e-04, Loss_0: 3.964e-08, Loss_r: 3.328e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 3.214e-04, Loss_0: 5.496e-08, Loss_r: 3.208e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 3.100e-04, Loss_0: 1.164e-08, Loss_r: 3.099e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 2.991e-04, Loss_0: 2.648e-08, Loss_r: 2.989e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.890e-04, Loss_0: 3.898e-08, Loss_r: 2.886e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.794e-04, Loss_0: 4.970e-08, Loss_r: 2.789e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 3.724e-04, Loss_0: 7.412e-06, Loss_r: 2.983e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.263e-04, Loss_0: 6.256e-07, Loss_r: 1.200e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 3.666e-04, Loss_0: 7.365e-06, Loss_r: 2.930e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.064e-04, Loss_0: 1.517e-07, Loss_r: 1.049e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.327e-04, Loss_0: 9.036e-07, Loss_r: 1.237e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.016e-04, Loss_0: 2.062e-07, Loss_r: 9.958e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 9.296e-05, Loss_0: 6.618e-09, Loss_r: 9.289e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 9.108e-05, Loss_0: 7.983e-09, Loss_r: 9.100e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 8.842e-05, Loss_0: 9.276e-09, Loss_r: 8.833e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 8.613e-05, Loss_0: 4.365e-09, Loss_r: 8.609e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 8.423e-05, Loss_0: 2.759e-09, Loss_r: 8.421e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 8.226e-05, Loss_0: 4.191e-09, Loss_r: 8.222e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 8.348e-05, Loss_0: 8.026e-08, Loss_r: 8.268e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.872e-04, Loss_0: 5.751e-06, Loss_r: 2.296e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.667e-04, Loss_0: 5.330e-06, Loss_r: 2.134e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 4.600e-04, Loss_0: 1.092e-05, Loss_r: 3.508e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 8.357e-05, Loss_0: 2.460e-07, Loss_r: 8.111e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.042e-04, Loss_0: 9.908e-07, Loss_r: 9.433e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 8.252e-05, Loss_0: 3.782e-07, Loss_r: 7.874e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 7.303e-05, Loss_0: 8.992e-08, Loss_r: 7.213e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 1.171e-04, Loss_0: 2.030e-10, Loss_r: 1.171e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 1.145e-04, Loss_0: 3.242e-08, Loss_r: 1.142e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.115e-04, Loss_0: 1.049e-09, Loss_r: 1.115e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.085e-04, Loss_0: 1.259e-08, Loss_r: 1.084e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 1.057e-04, Loss_0: 3.852e-09, Loss_r: 1.057e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 1.031e-04, Loss_0: 2.938e-10, Loss_r: 1.031e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.007e-04, Loss_0: 1.503e-10, Loss_r: 1.007e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 1.012e-04, Loss_0: 6.623e-08, Loss_r: 1.006e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 3.880e-04, Loss_0: 8.539e-06, Loss_r: 3.027e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 1.029e-04, Loss_0: 2.257e-07, Loss_r: 1.006e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 4.912e-04, Loss_0: 1.189e-05, Loss_r: 3.723e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.739e-04, Loss_0: 5.419e-06, Loss_r: 2.197e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.256e-04, Loss_0: 1.028e-06, Loss_r: 1.153e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 8.708e-05, Loss_0: 3.067e-09, Loss_r: 8.705e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 9.227e-05, Loss_0: 2.729e-07, Loss_r: 8.954e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 8.422e-05, Loss_0: 5.726e-08, Loss_r: 8.364e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 8.204e-05, Loss_0: 1.694e-08, Loss_r: 8.187e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 7.911e-05, Loss_0: 2.329e-09, Loss_r: 7.908e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 7.728e-05, Loss_0: 8.261e-09, Loss_r: 7.720e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 7.540e-05, Loss_0: 2.776e-11, Loss_r: 7.540e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 5.688e-05, Loss_0: 3.439e-09, Loss_r: 5.684e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 5.536e-05, Loss_0: 1.081e-10, Loss_r: 5.536e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 5.392e-05, Loss_0: 9.026e-10, Loss_r: 5.391e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 5.254e-05, Loss_0: 1.645e-09, Loss_r: 5.253e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 5.124e-05, Loss_0: 4.471e-09, Loss_r: 5.120e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 5.204e-05, Loss_0: 8.440e-08, Loss_r: 5.120e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 3.714e-04, Loss_0: 1.044e-05, Loss_r: 2.670e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 5.572e-05, Loss_0: 2.127e-07, Loss_r: 5.360e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.945e-04, Loss_0: 4.837e-06, Loss_r: 1.461e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.113e-04, Loss_0: 2.143e-06, Loss_r: 8.991e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.265e-05, Loss_0: 9.047e-07, Loss_r: 6.360e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.021e-05, Loss_0: 5.249e-07, Loss_r: 5.496e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 5.357e-05, Loss_0: 3.363e-07, Loss_r: 5.020e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 4.712e-05, Loss_0: 1.496e-07, Loss_r: 4.562e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 4.249e-05, Loss_0: 1.927e-08, Loss_r: 4.229e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.126e-05, Loss_0: 2.394e-09, Loss_r: 4.123e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 4.027e-05, Loss_0: 2.447e-09, Loss_r: 4.025e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 3.915e-05, Loss_0: 2.530e-09, Loss_r: 3.913e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 3.820e-05, Loss_0: 1.752e-09, Loss_r: 3.819e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 3.729e-05, Loss_0: 2.418e-11, Loss_r: 3.729e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.396e-05, Loss_0: 2.727e-10, Loss_r: 2.396e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 2.328e-05, Loss_0: 2.378e-10, Loss_r: 2.328e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 2.269e-05, Loss_0: 4.543e-10, Loss_r: 2.269e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.215e-05, Loss_0: 3.705e-11, Loss_r: 2.215e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.162e-05, Loss_0: 1.141e-10, Loss_r: 2.162e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.112e-05, Loss_0: 1.422e-10, Loss_r: 2.112e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.065e-05, Loss_0: 8.930e-11, Loss_r: 2.065e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 2.021e-05, Loss_0: 1.544e-10, Loss_r: 2.021e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 2.243e-05, Loss_0: 8.013e-08, Loss_r: 2.162e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 8.396e-04, Loss_0: 2.656e-05, Loss_r: 5.740e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 2.043e-03, Loss_0: 6.680e-05, Loss_r: 1.375e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 7.962e-04, Loss_0: 2.595e-05, Loss_r: 5.368e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.493e-04, Loss_0: 7.702e-06, Loss_r: 1.722e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.331e-05, Loss_0: 4.737e-07, Loss_r: 2.857e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 2.169e-05, Loss_0: 7.486e-08, Loss_r: 2.094e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.499e-05, Loss_0: 1.913e-07, Loss_r: 2.307e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 2.213e-05, Loss_0: 1.113e-07, Loss_r: 2.102e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.965e-05, Loss_0: 4.337e-08, Loss_r: 1.922e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.835e-05, Loss_0: 1.361e-08, Loss_r: 1.821e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.764e-05, Loss_0: 2.780e-09, Loss_r: 1.761e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.859e-05, Loss_0: 7.401e-11, Loss_r: 1.859e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.826e-05, Loss_0: 3.307e-10, Loss_r: 1.826e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.798e-05, Loss_0: 7.888e-10, Loss_r: 1.797e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.769e-05, Loss_0: 3.672e-10, Loss_r: 1.769e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.742e-05, Loss_0: 6.688e-11, Loss_r: 1.742e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.716e-05, Loss_0: 6.621e-11, Loss_r: 1.716e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.691e-05, Loss_0: 1.837e-10, Loss_r: 1.690e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.666e-05, Loss_0: 1.143e-10, Loss_r: 1.666e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.642e-05, Loss_0: 1.182e-10, Loss_r: 1.642e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.619e-05, Loss_0: 1.303e-10, Loss_r: 1.619e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.596e-05, Loss_0: 1.222e-10, Loss_r: 1.596e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.574e-05, Loss_0: 1.154e-10, Loss_r: 1.574e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.552e-05, Loss_0: 1.132e-10, Loss_r: 1.552e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.531e-05, Loss_0: 1.248e-10, Loss_r: 1.531e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.510e-05, Loss_0: 1.340e-10, Loss_r: 1.510e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.490e-05, Loss_0: 2.139e-10, Loss_r: 1.490e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.472e-05, Loss_0: 1.286e-09, Loss_r: 1.471e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.668e-05, Loss_0: 7.913e-08, Loss_r: 1.589e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 5.487e-04, Loss_0: 1.816e-05, Loss_r: 3.672e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 6.427e-04, Loss_0: 2.100e-05, Loss_r: 4.327e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 3.173e-04, Loss_0: 7.265e-06, Loss_r: 2.446e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.423e-04, Loss_0: 8.335e-06, Loss_r: 2.589e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.070e-04, Loss_0: 3.933e-06, Loss_r: 1.676e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.152e-04, Loss_0: 9.416e-07, Loss_r: 1.058e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 8.694e-05, Loss_0: 1.688e-07, Loss_r: 8.525e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 7.855e-05, Loss_0: 3.543e-08, Loss_r: 7.819e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 7.427e-05, Loss_0: 1.436e-08, Loss_r: 7.413e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 7.093e-05, Loss_0: 7.742e-09, Loss_r: 7.085e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 6.790e-05, Loss_0: 2.084e-09, Loss_r: 6.788e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 6.520e-05, Loss_0: 2.661e-10, Loss_r: 6.520e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 6.294e-05, Loss_0: 4.487e-09, Loss_r: 6.290e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 6.089e-05, Loss_0: 4.590e-09, Loss_r: 6.085e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 5.899e-05, Loss_0: 1.546e-09, Loss_r: 5.898e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 5.723e-05, Loss_0: 1.514e-09, Loss_r: 5.722e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 5.560e-05, Loss_0: 2.245e-09, Loss_r: 5.557e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 5.406e-05, Loss_0: 1.465e-09, Loss_r: 5.404e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 5.261e-05, Loss_0: 1.771e-09, Loss_r: 5.259e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 5.124e-05, Loss_0: 1.435e-09, Loss_r: 5.123e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 4.994e-05, Loss_0: 1.467e-09, Loss_r: 4.993e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 4.871e-05, Loss_0: 1.347e-09, Loss_r: 4.869e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 2.764e-05, Loss_0: 3.831e-07, Loss_r: 2.381e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 9.190e-05, Loss_0: 2.790e-06, Loss_r: 6.400e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 3.790e-05, Loss_0: 8.694e-07, Loss_r: 2.920e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.839e-05, Loss_0: 1.817e-07, Loss_r: 1.657e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.295e-05, Loss_0: 6.645e-09, Loss_r: 1.288e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.259e-05, Loss_0: 1.187e-08, Loss_r: 1.247e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.228e-05, Loss_0: 1.676e-08, Loss_r: 1.211e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.145e-05, Loss_0: 1.307e-09, Loss_r: 1.144e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.113e-05, Loss_0: 1.733e-09, Loss_r: 1.112e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.085e-05, Loss_0: 5.730e-10, Loss_r: 1.084e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.063e-05, Loss_0: 1.307e-10, Loss_r: 1.063e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.044e-05, Loss_0: 5.725e-11, Loss_r: 1.044e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.028e-05, Loss_0: 8.281e-11, Loss_r: 1.028e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.014e-05, Loss_0: 4.434e-12, Loss_r: 1.014e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.003e-05, Loss_0: 7.026e-11, Loss_r: 1.003e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 9.926e-06, Loss_0: 2.613e-11, Loss_r: 9.926e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 9.836e-06, Loss_0: 3.539e-11, Loss_r: 9.835e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 9.755e-06, Loss_0: 4.531e-11, Loss_r: 9.755e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 9.682e-06, Loss_0: 3.351e-11, Loss_r: 9.682e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 9.615e-06, Loss_0: 2.762e-11, Loss_r: 9.615e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 1.922e-04, Loss_0: 3.301e-06, Loss_r: 1.592e-04, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.111e-04, Loss_0: 8.480e-07, Loss_r: 1.026e-04, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 8.159e-05, Loss_0: 8.844e-08, Loss_r: 8.070e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 7.340e-05, Loss_0: 2.148e-08, Loss_r: 7.318e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 7.022e-05, Loss_0: 9.121e-08, Loss_r: 6.931e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 6.514e-05, Loss_0: 3.023e-08, Loss_r: 6.483e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 6.167e-05, Loss_0: 7.894e-11, Loss_r: 6.167e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 5.881e-05, Loss_0: 3.374e-10, Loss_r: 5.881e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 5.641e-05, Loss_0: 7.776e-09, Loss_r: 5.633e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 5.428e-05, Loss_0: 1.038e-09, Loss_r: 5.427e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 5.242e-05, Loss_0: 2.831e-09, Loss_r: 5.239e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 5.076e-05, Loss_0: 1.528e-09, Loss_r: 5.075e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 4.927e-05, Loss_0: 2.372e-09, Loss_r: 4.925e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 4.792e-05, Loss_0: 1.373e-09, Loss_r: 4.790e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 4.667e-05, Loss_0: 1.289e-09, Loss_r: 4.666e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 4.553e-05, Loss_0: 1.379e-09, Loss_r: 4.551e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 4.446e-05, Loss_0: 1.202e-09, Loss_r: 4.445e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 4.348e-05, Loss_0: 2.675e-10, Loss_r: 4.348e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 4.386e-05, Loss_0: 3.592e-08, Loss_r: 4.350e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 4.049e-04, Loss_0: 1.357e-05, Loss_r: 2.692e-04, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 5.290e-05, Loss_0: 1.149e-06, Loss_r: 4.141e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 2.128e-05, Loss_0: 5.112e-08, Loss_r: 2.077e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 3.145e-05, Loss_0: 4.687e-07, Loss_r: 2.676e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.594e-05, Loss_0: 2.789e-07, Loss_r: 2.315e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.834e-05, Loss_0: 1.115e-08, Loss_r: 1.823e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.835e-05, Loss_0: 2.317e-08, Loss_r: 1.812e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.748e-05, Loss_0: 4.208e-09, Loss_r: 1.744e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.717e-05, Loss_0: 4.859e-09, Loss_r: 1.712e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.684e-05, Loss_0: 1.605e-10, Loss_r: 1.684e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.666e-05, Loss_0: 1.962e-10, Loss_r: 1.666e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.649e-05, Loss_0: 6.508e-10, Loss_r: 1.649e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.635e-05, Loss_0: 9.399e-13, Loss_r: 1.635e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.622e-05, Loss_0: 2.211e-10, Loss_r: 1.622e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.611e-05, Loss_0: 2.396e-11, Loss_r: 1.611e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.601e-05, Loss_0: 1.410e-10, Loss_r: 1.600e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.591e-05, Loss_0: 1.266e-10, Loss_r: 1.591e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 1.582e-05, Loss_0: 5.571e-11, Loss_r: 1.582e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 1.574e-05, Loss_0: 3.360e-11, Loss_r: 1.574e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 1.566e-05, Loss_0: 1.537e-12, Loss_r: 1.566e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2800, Loss: 1.568e-05, Loss_0: 2.560e-09, Loss_r: 1.565e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2810, Loss: 1.449e-05, Loss_0: 7.204e-08, Loss_r: 1.377e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 3.217e-04, Loss_0: 1.127e-05, Loss_r: 2.090e-04, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 6.230e-05, Loss_0: 1.860e-06, Loss_r: 4.370e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.364e-05, Loss_0: 1.033e-07, Loss_r: 1.261e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.180e-05, Loss_0: 4.396e-08, Loss_r: 1.136e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.440e-05, Loss_0: 1.452e-07, Loss_r: 1.294e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.232e-05, Loss_0: 7.738e-08, Loss_r: 1.155e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.008e-05, Loss_0: 3.094e-09, Loss_r: 1.005e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.003e-05, Loss_0: 6.885e-09, Loss_r: 9.965e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 9.802e-06, Loss_0: 2.436e-09, Loss_r: 9.778e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 9.666e-06, Loss_0: 6.307e-10, Loss_r: 9.660e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 9.559e-06, Loss_0: 4.214e-11, Loss_r: 9.559e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 9.484e-06, Loss_0: 2.917e-10, Loss_r: 9.481e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 9.409e-06, Loss_0: 1.296e-11, Loss_r: 9.409e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 9.342e-06, Loss_0: 3.738e-11, Loss_r: 9.342e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 9.281e-06, Loss_0: 6.332e-12, Loss_r: 9.281e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 9.224e-06, Loss_0: 1.596e-11, Loss_r: 9.224e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 9.170e-06, Loss_0: 7.879e-12, Loss_r: 9.170e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 9.118e-06, Loss_0: 2.676e-11, Loss_r: 9.118e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 9.068e-06, Loss_0: 9.927e-12, Loss_r: 9.068e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 6.273e-06, Loss_0: 6.882e-09, Loss_r: 6.205e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 6.261e-06, Loss_0: 1.278e-08, Loss_r: 6.133e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 8.314e-06, Loss_0: 9.218e-08, Loss_r: 7.392e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 6.713e-05, Loss_0: 2.216e-06, Loss_r: 4.497e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 2.113e-05, Loss_0: 5.624e-07, Loss_r: 1.551e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 7.295e-06, Loss_0: 6.931e-08, Loss_r: 6.602e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 5.290e-06, Loss_0: 1.271e-09, Loss_r: 5.277e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 5.829e-06, Loss_0: 2.421e-08, Loss_r: 5.587e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 5.466e-06, Loss_0: 1.435e-08, Loss_r: 5.323e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 4.989e-06, Loss_0: 2.276e-10, Loss_r: 4.987e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 4.947e-06, Loss_0: 1.604e-09, Loss_r: 4.931e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 4.829e-06, Loss_0: 1.542e-10, Loss_r: 4.827e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 4.758e-06, Loss_0: 2.582e-10, Loss_r: 4.755e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 4.680e-06, Loss_0: 1.241e-12, Loss_r: 4.680e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 4.611e-06, Loss_0: 1.939e-11, Loss_r: 4.611e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 4.545e-06, Loss_0: 1.338e-11, Loss_r: 4.545e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 4.480e-06, Loss_0: 5.460e-12, Loss_r: 4.480e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 4.418e-06, Loss_0: 1.923e-12, Loss_r: 4.418e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 4.358e-06, Loss_0: 2.527e-12, Loss_r: 4.358e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 4.299e-06, Loss_0: 5.975e-15, Loss_r: 4.299e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 4.654e-05, Loss_0: 7.761e-08, Loss_r: 4.576e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 4.039e-05, Loss_0: 1.730e-08, Loss_r: 4.022e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 3.741e-05, Loss_0: 4.377e-09, Loss_r: 3.736e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 3.534e-05, Loss_0: 6.703e-10, Loss_r: 3.533e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 3.358e-05, Loss_0: 8.522e-11, Loss_r: 3.358e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 3.213e-05, Loss_0: 6.151e-11, Loss_r: 3.213e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 3.094e-05, Loss_0: 4.374e-10, Loss_r: 3.093e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 2.994e-05, Loss_0: 9.666e-10, Loss_r: 2.993e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 2.909e-05, Loss_0: 1.107e-09, Loss_r: 2.908e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 2.836e-05, Loss_0: 8.055e-10, Loss_r: 2.835e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 2.771e-05, Loss_0: 5.803e-10, Loss_r: 2.771e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 2.714e-05, Loss_0: 6.152e-10, Loss_r: 2.713e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 2.663e-05, Loss_0: 6.185e-10, Loss_r: 2.662e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 2.616e-05, Loss_0: 5.315e-10, Loss_r: 2.615e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 2.573e-05, Loss_0: 5.425e-10, Loss_r: 2.573e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 2.533e-05, Loss_0: 5.052e-10, Loss_r: 2.533e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 2.496e-05, Loss_0: 4.817e-10, Loss_r: 2.496e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 2.462e-05, Loss_0: 4.479e-10, Loss_r: 2.461e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 2.428e-05, Loss_0: 4.525e-10, Loss_r: 2.428e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 2.397e-05, Loss_0: 3.874e-10, Loss_r: 2.397e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 2.558e-05, Loss_0: 4.662e-07, Loss_r: 2.092e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 6.575e-05, Loss_0: 2.001e-06, Loss_r: 4.574e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.947e-05, Loss_0: 7.057e-07, Loss_r: 2.242e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.595e-05, Loss_0: 2.314e-07, Loss_r: 1.363e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.073e-05, Loss_0: 6.221e-08, Loss_r: 1.011e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 8.857e-06, Loss_0: 7.762e-09, Loss_r: 8.780e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 8.341e-06, Loss_0: 1.423e-10, Loss_r: 8.340e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 8.152e-06, Loss_0: 2.640e-09, Loss_r: 8.126e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 7.898e-06, Loss_0: 1.353e-09, Loss_r: 7.885e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 7.675e-06, Loss_0: 1.811e-13, Loss_r: 7.675e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 7.526e-06, Loss_0: 2.549e-10, Loss_r: 7.524e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 7.389e-06, Loss_0: 1.609e-11, Loss_r: 7.389e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 7.275e-06, Loss_0: 1.319e-11, Loss_r: 7.275e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 7.175e-06, Loss_0: 1.262e-11, Loss_r: 7.174e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 7.086e-06, Loss_0: 8.090e-12, Loss_r: 7.086e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 7.008e-06, Loss_0: 1.181e-12, Loss_r: 7.008e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 6.936e-06, Loss_0: 8.201e-12, Loss_r: 6.936e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 6.871e-06, Loss_0: 2.422e-12, Loss_r: 6.871e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 6.811e-06, Loss_0: 4.864e-12, Loss_r: 6.811e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 6.754e-06, Loss_0: 5.482e-12, Loss_r: 6.754e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 3.131e-05, Loss_0: 6.181e-08, Loss_r: 3.069e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 2.798e-05, Loss_0: 1.627e-08, Loss_r: 2.782e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 2.652e-05, Loss_0: 4.908e-09, Loss_r: 2.647e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 2.555e-05, Loss_0: 9.994e-10, Loss_r: 2.554e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 2.473e-05, Loss_0: 1.433e-10, Loss_r: 2.472e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 2.405e-05, Loss_0: 3.611e-11, Loss_r: 2.405e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 2.349e-05, Loss_0: 3.460e-10, Loss_r: 2.349e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 2.301e-05, Loss_0: 6.878e-10, Loss_r: 2.300e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 2.258e-05, Loss_0: 6.707e-10, Loss_r: 2.257e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 2.220e-05, Loss_0: 4.634e-10, Loss_r: 2.219e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 2.185e-05, Loss_0: 3.399e-10, Loss_r: 2.184e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 2.153e-05, Loss_0: 3.933e-10, Loss_r: 2.152e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 2.123e-05, Loss_0: 3.965e-10, Loss_r: 2.123e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 2.096e-05, Loss_0: 3.586e-10, Loss_r: 2.095e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 2.070e-05, Loss_0: 3.645e-10, Loss_r: 2.069e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 2.045e-05, Loss_0: 3.242e-10, Loss_r: 2.045e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 2.022e-05, Loss_0: 3.566e-10, Loss_r: 2.022e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 1.999e-05, Loss_0: 2.836e-10, Loss_r: 1.999e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 1.978e-05, Loss_0: 3.234e-10, Loss_r: 1.978e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 1.957e-05, Loss_0: 3.337e-10, Loss_r: 1.957e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 1.137e-05, Loss_0: 5.429e-08, Loss_r: 1.083e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 1.086e-05, Loss_0: 6.114e-08, Loss_r: 1.025e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 1.682e-05, Loss_0: 2.970e-07, Loss_r: 1.385e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 1.200e-04, Loss_0: 4.087e-06, Loss_r: 7.917e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 4.829e-05, Loss_0: 1.468e-06, Loss_r: 3.361e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 2.241e-05, Loss_0: 5.297e-07, Loss_r: 1.711e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 1.278e-05, Loss_0: 1.840e-07, Loss_r: 1.094e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 9.152e-06, Loss_0: 5.696e-08, Loss_r: 8.582e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 7.791e-06, Loss_0: 1.208e-08, Loss_r: 7.670e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 7.362e-06, Loss_0: 5.379e-10, Loss_r: 7.357e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 7.263e-06, Loss_0: 5.653e-10, Loss_r: 7.257e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 7.182e-06, Loss_0: 9.637e-10, Loss_r: 7.172e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 7.074e-06, Loss_0: 1.324e-10, Loss_r: 7.072e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 6.993e-06, Loss_0: 6.598e-11, Loss_r: 6.993e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 6.924e-06, Loss_0: 7.872e-11, Loss_r: 6.923e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 6.857e-06, Loss_0: 2.878e-13, Loss_r: 6.857e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 6.797e-06, Loss_0: 2.539e-13, Loss_r: 6.797e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 6.740e-06, Loss_0: 1.403e-11, Loss_r: 6.740e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 6.685e-06, Loss_0: 4.286e-12, Loss_r: 6.685e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 6.634e-06, Loss_0: 5.309e-12, Loss_r: 6.634e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 6.031e-06, Loss_0: 1.547e-10, Loss_r: 6.029e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 5.877e-06, Loss_0: 3.217e-12, Loss_r: 5.877e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 5.758e-06, Loss_0: 6.636e-12, Loss_r: 5.758e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 5.648e-06, Loss_0: 4.990e-11, Loss_r: 5.647e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 5.543e-06, Loss_0: 7.335e-12, Loss_r: 5.543e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 5.448e-06, Loss_0: 2.342e-11, Loss_r: 5.448e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 5.361e-06, Loss_0: 5.044e-11, Loss_r: 5.360e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 5.287e-06, Loss_0: 3.741e-10, Loss_r: 5.283e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 5.433e-06, Loss_0: 8.522e-09, Loss_r: 5.348e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 1.832e-05, Loss_0: 4.755e-07, Loss_r: 1.357e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 7.165e-06, Loss_0: 7.606e-08, Loss_r: 6.404e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 4.999e-06, Loss_0: 1.920e-11, Loss_r: 4.999e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 5.315e-06, Loss_0: 1.327e-08, Loss_r: 5.183e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 5.134e-06, Loss_0: 8.882e-09, Loss_r: 5.045e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 4.830e-06, Loss_0: 9.005e-11, Loss_r: 4.829e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 4.804e-06, Loss_0: 1.140e-09, Loss_r: 4.792e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 4.723e-06, Loss_0: 7.423e-11, Loss_r: 4.722e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 4.675e-06, Loss_0: 1.437e-10, Loss_r: 4.674e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 4.622e-06, Loss_0: 1.193e-11, Loss_r: 4.622e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 4.574e-06, Loss_0: 1.254e-11, Loss_r: 4.574e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 1.988e-05, Loss_0: 8.473e-09, Loss_r: 1.980e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 1.822e-05, Loss_0: 4.427e-09, Loss_r: 1.817e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 1.739e-05, Loss_0: 5.204e-10, Loss_r: 1.739e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 1.680e-05, Loss_0: 1.017e-10, Loss_r: 1.680e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 1.627e-05, Loss_0: 4.430e-11, Loss_r: 1.627e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.583e-05, Loss_0: 1.046e-10, Loss_r: 1.583e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 1.548e-05, Loss_0: 1.449e-10, Loss_r: 1.548e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 1.519e-05, Loss_0: 1.820e-10, Loss_r: 1.519e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 1.494e-05, Loss_0: 1.877e-10, Loss_r: 1.494e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 1.473e-05, Loss_0: 1.816e-10, Loss_r: 1.472e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 1.454e-05, Loss_0: 1.915e-10, Loss_r: 1.454e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 1.438e-05, Loss_0: 1.977e-10, Loss_r: 1.438e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 1.423e-05, Loss_0: 1.845e-10, Loss_r: 1.423e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 1.410e-05, Loss_0: 1.738e-10, Loss_r: 1.410e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 1.398e-05, Loss_0: 1.639e-10, Loss_r: 1.398e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 1.387e-05, Loss_0: 1.623e-10, Loss_r: 1.387e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 1.376e-05, Loss_0: 1.429e-10, Loss_r: 1.376e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 1.366e-05, Loss_0: 1.428e-10, Loss_r: 1.366e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 1.357e-05, Loss_0: 1.545e-10, Loss_r: 1.356e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 1.347e-05, Loss_0: 1.363e-10, Loss_r: 1.347e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 9.508e-06, Loss_0: 1.424e-09, Loss_r: 9.494e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 8.745e-06, Loss_0: 9.479e-10, Loss_r: 8.735e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 8.437e-06, Loss_0: 1.560e-10, Loss_r: 8.436e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 8.195e-06, Loss_0: 4.493e-11, Loss_r: 8.195e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 7.986e-06, Loss_0: 1.144e-10, Loss_r: 7.985e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 7.825e-06, Loss_0: 1.496e-11, Loss_r: 7.825e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 7.697e-06, Loss_0: 7.921e-12, Loss_r: 7.697e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 7.592e-06, Loss_0: 8.910e-12, Loss_r: 7.592e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 7.512e-06, Loss_0: 2.197e-10, Loss_r: 7.510e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 7.665e-06, Loss_0: 7.881e-09, Loss_r: 7.586e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 2.258e-05, Loss_0: 5.420e-07, Loss_r: 1.716e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 9.741e-06, Loss_0: 8.524e-08, Loss_r: 8.889e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 7.271e-06, Loss_0: 4.228e-11, Loss_r: 7.270e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 7.616e-06, Loss_0: 1.500e-08, Loss_r: 7.466e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 7.486e-06, Loss_0: 1.164e-08, Loss_r: 7.369e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 7.159e-06, Loss_0: 4.971e-10, Loss_r: 7.154e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 7.145e-06, Loss_0: 8.438e-10, Loss_r: 7.137e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 7.085e-06, Loss_0: 6.036e-11, Loss_r: 7.084e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 7.053e-06, Loss_0: 3.102e-10, Loss_r: 7.050e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 7.017e-06, Loss_0: 8.585e-12, Loss_r: 7.017e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 1.481e-05, Loss_0: 4.723e-09, Loss_r: 1.476e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 1.352e-05, Loss_0: 4.003e-10, Loss_r: 1.352e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 1.281e-05, Loss_0: 1.158e-09, Loss_r: 1.280e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 1.223e-05, Loss_0: 1.685e-12, Loss_r: 1.223e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 1.171e-05, Loss_0: 5.466e-10, Loss_r: 1.171e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 1.129e-05, Loss_0: 9.015e-11, Loss_r: 1.129e-05, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 1.094e-05, Loss_0: 1.377e-10, Loss_r: 1.094e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 1.064e-05, Loss_0: 2.459e-10, Loss_r: 1.064e-05, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 1.039e-05, Loss_0: 3.040e-10, Loss_r: 1.039e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 1.017e-05, Loss_0: 6.063e-10, Loss_r: 1.016e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 1.008e-05, Loss_0: 5.755e-09, Loss_r: 1.002e-05, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 1.791e-05, Loss_0: 3.099e-07, Loss_r: 1.481e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.070e-05, Loss_0: 4.287e-08, Loss_r: 1.027e-05, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 9.550e-06, Loss_0: 3.799e-10, Loss_r: 9.546e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 9.747e-06, Loss_0: 1.025e-08, Loss_r: 9.644e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 9.413e-06, Loss_0: 3.170e-09, Loss_r: 9.381e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 9.188e-06, Loss_0: 5.295e-10, Loss_r: 9.183e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 9.103e-06, Loss_0: 1.445e-09, Loss_r: 9.089e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 8.987e-06, Loss_0: 1.467e-11, Loss_r: 8.987e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 8.895e-06, Loss_0: 5.168e-12, Loss_r: 8.895e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 1.506e-05, Loss_0: 5.237e-10, Loss_r: 1.506e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 1.493e-05, Loss_0: 4.296e-11, Loss_r: 1.493e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 1.480e-05, Loss_0: 2.527e-10, Loss_r: 1.480e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 1.467e-05, Loss_0: 1.259e-10, Loss_r: 1.467e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 1.455e-05, Loss_0: 2.017e-10, Loss_r: 1.455e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 1.443e-05, Loss_0: 1.806e-10, Loss_r: 1.443e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 1.431e-05, Loss_0: 2.027e-10, Loss_r: 1.431e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 1.419e-05, Loss_0: 1.803e-10, Loss_r: 1.419e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 1.408e-05, Loss_0: 1.581e-10, Loss_r: 1.408e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 1.397e-05, Loss_0: 1.166e-10, Loss_r: 1.396e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 1.386e-05, Loss_0: 1.259e-11, Loss_r: 1.386e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 1.389e-05, Loss_0: 3.426e-09, Loss_r: 1.385e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 3.226e-05, Loss_0: 6.591e-07, Loss_r: 2.567e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 1.544e-05, Loss_0: 6.257e-08, Loss_r: 1.481e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.358e-05, Loss_0: 6.097e-09, Loss_r: 1.352e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 1.419e-05, Loss_0: 3.410e-08, Loss_r: 1.385e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 1.358e-05, Loss_0: 1.357e-08, Loss_r: 1.345e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 1.320e-05, Loss_0: 3.264e-14, Loss_r: 1.320e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 1.316e-05, Loss_0: 1.044e-09, Loss_r: 1.315e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "Training time: 26.3837\n",
            "[1, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 5.511e-01, Loss_0: 1.120e-03, Loss_r: 5.500e-01, Time: 0.51, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.274e-01, Loss_0: 1.456e-05, Loss_r: 2.272e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.027e-01, Loss_0: 2.166e-03, Loss_r: 1.811e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.983e-01, Loss_0: 1.600e-03, Loss_r: 1.823e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.953e-01, Loss_0: 1.542e-03, Loss_r: 1.799e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.948e-01, Loss_0: 1.305e-03, Loss_r: 1.817e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.944e-01, Loss_0: 1.771e-03, Loss_r: 1.767e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.941e-01, Loss_0: 1.352e-03, Loss_r: 1.805e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.937e-01, Loss_0: 1.595e-03, Loss_r: 1.778e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.934e-01, Loss_0: 1.495e-03, Loss_r: 1.785e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.932e-01, Loss_0: 1.495e-03, Loss_r: 1.782e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.928e-01, Loss_0: 1.525e-03, Loss_r: 1.775e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.923e-01, Loss_0: 1.492e-03, Loss_r: 1.774e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.917e-01, Loss_0: 1.499e-03, Loss_r: 1.767e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.908e-01, Loss_0: 1.491e-03, Loss_r: 1.759e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.894e-01, Loss_0: 1.483e-03, Loss_r: 1.746e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.873e-01, Loss_0: 1.467e-03, Loss_r: 1.726e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.837e-01, Loss_0: 1.444e-03, Loss_r: 1.692e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 1.769e-01, Loss_0: 1.400e-03, Loss_r: 1.629e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.630e-01, Loss_0: 1.313e-03, Loss_r: 1.499e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.356e-01, Loss_0: 1.126e-03, Loss_r: 1.244e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 210, Loss: 9.535e-02, Loss_0: 7.697e-04, Loss_r: 8.766e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.693e-02, Loss_0: 2.656e-04, Loss_r: 5.428e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 4.723e-02, Loss_0: 8.751e-05, Loss_r: 4.636e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 3.905e-02, Loss_0: 7.310e-05, Loss_r: 3.832e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 3.217e-02, Loss_0: 7.825e-05, Loss_r: 3.139e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 2.687e-02, Loss_0: 5.378e-05, Loss_r: 2.634e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 2.283e-02, Loss_0: 6.372e-05, Loss_r: 2.219e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.722e-02, Loss_0: 2.616e-05, Loss_r: 1.696e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.352e-02, Loss_0: 1.844e-05, Loss_r: 1.334e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.077e-02, Loss_0: 9.896e-06, Loss_r: 1.067e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 8.717e-03, Loss_0: 7.114e-06, Loss_r: 8.646e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 320, Loss: 7.368e-03, Loss_0: 6.120e-06, Loss_r: 7.307e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 6.629e-03, Loss_0: 2.265e-06, Loss_r: 6.607e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 6.568e-03, Loss_0: 7.764e-08, Loss_r: 6.567e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.894e-03, Loss_0: 4.680e-08, Loss_r: 5.894e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.147e-03, Loss_0: 1.494e-07, Loss_r: 5.145e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 4.541e-03, Loss_0: 3.332e-07, Loss_r: 4.537e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.009e-03, Loss_0: 2.599e-06, Loss_r: 3.983e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 3.620e-03, Loss_0: 6.671e-07, Loss_r: 3.613e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 3.253e-03, Loss_0: 1.850e-06, Loss_r: 3.234e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 410, Loss: 6.775e-03, Loss_0: 3.120e-05, Loss_r: 6.463e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 3.149e-03, Loss_0: 1.392e-06, Loss_r: 3.135e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 3.751e-03, Loss_0: 1.446e-05, Loss_r: 3.606e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 2.852e-03, Loss_0: 6.214e-06, Loss_r: 2.790e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 2.386e-03, Loss_0: 1.250e-06, Loss_r: 2.373e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.152e-03, Loss_0: 8.579e-07, Loss_r: 2.143e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.949e-03, Loss_0: 1.404e-06, Loss_r: 1.935e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.740e-03, Loss_0: 8.559e-07, Loss_r: 1.731e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.555e-03, Loss_0: 2.021e-07, Loss_r: 1.553e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 1.387e-03, Loss_0: 1.499e-07, Loss_r: 1.386e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 1.310e-03, Loss_0: 3.067e-07, Loss_r: 1.307e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 9.554e-03, Loss_0: 1.033e-04, Loss_r: 8.520e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.512e-03, Loss_0: 1.626e-05, Loss_r: 2.349e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.117e-03, Loss_0: 5.410e-06, Loss_r: 1.063e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.155e-03, Loss_0: 7.816e-06, Loss_r: 1.077e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 7.488e-04, Loss_0: 9.735e-07, Loss_r: 7.390e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 6.518e-04, Loss_0: 6.223e-08, Loss_r: 6.511e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.782e-04, Loss_0: 6.629e-08, Loss_r: 5.776e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.141e-04, Loss_0: 2.288e-07, Loss_r: 5.118e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 4.580e-04, Loss_0: 2.925e-07, Loss_r: 4.551e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.298e-04, Loss_0: 1.065e-07, Loss_r: 1.288e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.087e-04, Loss_0: 6.357e-08, Loss_r: 1.081e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 9.512e-05, Loss_0: 1.252e-09, Loss_r: 9.510e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 640, Loss: 8.356e-05, Loss_0: 6.922e-09, Loss_r: 8.349e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 7.590e-05, Loss_0: 6.487e-08, Loss_r: 7.526e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.086e-04, Loss_0: 3.025e-06, Loss_r: 1.783e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 7.635e-03, Loss_0: 1.213e-04, Loss_r: 6.422e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 2.473e-03, Loss_0: 4.155e-05, Loss_r: 2.058e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 6.667e-04, Loss_0: 1.148e-05, Loss_r: 5.519e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 9.029e-05, Loss_0: 8.353e-07, Loss_r: 8.193e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 5.458e-05, Loss_0: 2.599e-08, Loss_r: 5.432e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.478e-05, Loss_0: 1.017e-07, Loss_r: 5.376e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 4.619e-05, Loss_0: 2.737e-08, Loss_r: 4.592e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 4.087e-05, Loss_0: 1.732e-09, Loss_r: 4.085e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 3.911e-05, Loss_0: 3.281e-08, Loss_r: 3.878e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 3.647e-05, Loss_0: 2.353e-08, Loss_r: 3.624e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 3.378e-05, Loss_0: 1.268e-10, Loss_r: 3.378e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 3.199e-05, Loss_0: 1.021e-10, Loss_r: 3.199e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 3.042e-05, Loss_0: 4.114e-09, Loss_r: 3.038e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.902e-05, Loss_0: 2.939e-10, Loss_r: 2.902e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 5.546e-05, Loss_0: 2.615e-09, Loss_r: 5.544e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.326e-05, Loss_0: 4.971e-09, Loss_r: 5.321e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 5.164e-05, Loss_0: 2.524e-08, Loss_r: 5.139e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 8.770e-05, Loss_0: 8.149e-07, Loss_r: 7.955e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 7.680e-03, Loss_0: 1.399e-04, Loss_r: 6.280e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 3.354e-04, Loss_0: 5.373e-06, Loss_r: 2.816e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.572e-04, Loss_0: 1.752e-06, Loss_r: 1.397e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 4.335e-04, Loss_0: 7.235e-06, Loss_r: 3.612e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.804e-04, Loss_0: 2.658e-06, Loss_r: 1.538e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 6.108e-05, Loss_0: 2.629e-07, Loss_r: 5.845e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 4.960e-05, Loss_0: 2.391e-09, Loss_r: 4.957e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 920, Loss: 4.832e-05, Loss_0: 6.431e-10, Loss_r: 4.832e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 4.665e-05, Loss_0: 1.158e-09, Loss_r: 4.663e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 940, Loss: 4.544e-05, Loss_0: 1.084e-08, Loss_r: 4.533e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 4.435e-05, Loss_0: 1.588e-08, Loss_r: 4.419e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 4.294e-05, Loss_0: 7.073e-09, Loss_r: 4.287e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 4.177e-05, Loss_0: 5.656e-10, Loss_r: 4.176e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 4.072e-05, Loss_0: 8.847e-10, Loss_r: 4.072e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 3.975e-05, Loss_0: 2.838e-09, Loss_r: 3.972e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 3.883e-05, Loss_0: 9.864e-10, Loss_r: 3.882e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.063e-04, Loss_0: 1.094e-07, Loss_r: 2.052e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.887e-04, Loss_0: 6.434e-08, Loss_r: 1.881e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.718e-04, Loss_0: 1.225e-08, Loss_r: 1.716e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.631e-04, Loss_0: 4.133e-08, Loss_r: 1.627e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.090e-03, Loss_0: 2.193e-05, Loss_r: 8.704e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 5.375e-04, Loss_0: 7.525e-06, Loss_r: 4.623e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 7.006e-04, Loss_0: 1.484e-05, Loss_r: 5.522e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 9.668e-04, Loss_0: 2.090e-05, Loss_r: 7.578e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.472e-04, Loss_0: 2.850e-06, Loss_r: 2.187e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.357e-04, Loss_0: 9.170e-08, Loss_r: 1.348e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.446e-04, Loss_0: 3.674e-07, Loss_r: 1.409e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.262e-04, Loss_0: 1.542e-07, Loss_r: 1.247e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.111e-04, Loss_0: 4.965e-08, Loss_r: 1.106e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.010e-04, Loss_0: 4.987e-09, Loss_r: 1.010e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 9.391e-05, Loss_0: 6.036e-09, Loss_r: 9.385e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 8.869e-05, Loss_0: 2.491e-08, Loss_r: 8.845e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 8.382e-05, Loss_0: 1.868e-08, Loss_r: 8.364e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 7.945e-05, Loss_0: 4.145e-09, Loss_r: 7.940e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 7.547e-05, Loss_0: 8.363e-09, Loss_r: 7.539e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 7.187e-05, Loss_0: 6.914e-09, Loss_r: 7.180e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 6.935e-05, Loss_0: 1.042e-08, Loss_r: 6.925e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 5.393e-05, Loss_0: 6.349e-08, Loss_r: 5.329e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 4.316e-05, Loss_0: 8.387e-12, Loss_r: 4.316e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.850e-05, Loss_0: 1.393e-08, Loss_r: 3.836e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 4.181e-05, Loss_0: 1.933e-07, Loss_r: 3.987e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.834e-04, Loss_0: 7.717e-06, Loss_r: 3.062e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 5.976e-03, Loss_0: 1.223e-04, Loss_r: 4.753e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.251e-05, Loss_0: 1.813e-08, Loss_r: 2.233e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 3.874e-04, Loss_0: 8.084e-06, Loss_r: 3.065e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.169e-04, Loss_0: 4.390e-06, Loss_r: 1.730e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 3.806e-05, Loss_0: 4.508e-07, Loss_r: 3.355e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.081e-05, Loss_0: 1.117e-07, Loss_r: 1.969e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.439e-05, Loss_0: 2.034e-07, Loss_r: 2.235e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.424e-05, Loss_0: 1.556e-10, Loss_r: 1.424e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.493e-05, Loss_0: 2.696e-08, Loss_r: 1.466e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.344e-05, Loss_0: 4.688e-09, Loss_r: 1.339e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.284e-05, Loss_0: 3.001e-11, Loss_r: 1.284e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.253e-05, Loss_0: 3.502e-10, Loss_r: 1.252e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.224e-05, Loss_0: 2.292e-10, Loss_r: 1.223e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.198e-05, Loss_0: 8.879e-12, Loss_r: 1.198e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 8.499e-05, Loss_0: 1.352e-08, Loss_r: 8.485e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 7.434e-05, Loss_0: 4.934e-08, Loss_r: 7.385e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 7.605e-05, Loss_0: 2.641e-07, Loss_r: 7.341e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.853e-04, Loss_0: 5.579e-06, Loss_r: 2.295e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 8.193e-03, Loss_0: 1.828e-04, Loss_r: 6.365e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 3.713e-04, Loss_0: 6.636e-06, Loss_r: 3.049e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 6.618e-04, Loss_0: 1.382e-05, Loss_r: 5.235e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 3.458e-04, Loss_0: 6.165e-06, Loss_r: 2.841e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.364e-04, Loss_0: 1.595e-06, Loss_r: 1.205e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 6.258e-05, Loss_0: 2.060e-08, Loss_r: 6.238e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 6.711e-05, Loss_0: 2.686e-07, Loss_r: 6.443e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 6.007e-05, Loss_0: 1.066e-07, Loss_r: 5.900e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 5.658e-05, Loss_0: 1.243e-08, Loss_r: 5.645e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 5.396e-05, Loss_0: 1.105e-09, Loss_r: 5.395e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 5.276e-05, Loss_0: 1.263e-08, Loss_r: 5.263e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 5.149e-05, Loss_0: 3.249e-11, Loss_r: 5.149e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 5.033e-05, Loss_0: 4.866e-09, Loss_r: 5.029e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 4.931e-05, Loss_0: 3.032e-09, Loss_r: 4.928e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 4.839e-05, Loss_0: 8.043e-10, Loss_r: 4.839e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 4.755e-05, Loss_0: 2.149e-10, Loss_r: 4.755e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 4.292e-05, Loss_0: 3.148e-07, Loss_r: 3.977e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 6.648e-04, Loss_0: 1.523e-05, Loss_r: 5.125e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.553e-03, Loss_0: 7.817e-05, Loss_r: 2.771e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 7.435e-04, Loss_0: 1.715e-05, Loss_r: 5.720e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.805e-05, Loss_0: 7.582e-10, Loss_r: 1.805e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.632e-05, Loss_0: 1.489e-06, Loss_r: 6.143e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 7.460e-05, Loss_0: 1.397e-06, Loss_r: 6.064e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 3.367e-05, Loss_0: 4.468e-07, Loss_r: 2.920e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.486e-05, Loss_0: 3.950e-09, Loss_r: 1.482e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.660e-05, Loss_0: 5.622e-08, Loss_r: 1.604e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.448e-05, Loss_0: 1.150e-08, Loss_r: 1.437e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.397e-05, Loss_0: 5.952e-09, Loss_r: 1.391e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.349e-05, Loss_0: 4.714e-10, Loss_r: 1.349e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.331e-05, Loss_0: 1.370e-09, Loss_r: 1.330e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.309e-05, Loss_0: 3.233e-10, Loss_r: 1.308e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 1.291e-05, Loss_0: 7.922e-11, Loss_r: 1.291e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 1.275e-05, Loss_0: 6.176e-12, Loss_r: 1.275e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 1.262e-05, Loss_0: 3.323e-11, Loss_r: 1.262e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 1.249e-05, Loss_0: 4.079e-12, Loss_r: 1.249e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 1.237e-05, Loss_0: 1.128e-11, Loss_r: 1.237e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.906e-05, Loss_0: 2.646e-12, Loss_r: 2.906e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.838e-05, Loss_0: 4.333e-10, Loss_r: 2.837e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.989e-05, Loss_0: 4.872e-08, Loss_r: 2.941e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.688e-04, Loss_0: 3.307e-06, Loss_r: 1.357e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 9.933e-03, Loss_0: 2.279e-04, Loss_r: 7.654e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.620e-04, Loss_0: 3.626e-06, Loss_r: 1.258e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 9.372e-05, Loss_0: 1.823e-06, Loss_r: 7.548e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 3.274e-05, Loss_0: 1.455e-08, Loss_r: 3.260e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 4.494e-05, Loss_0: 2.926e-07, Loss_r: 4.201e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.195e-05, Loss_0: 2.251e-07, Loss_r: 3.970e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 3.477e-05, Loss_0: 8.750e-08, Loss_r: 3.390e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 3.099e-05, Loss_0: 7.689e-09, Loss_r: 3.091e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 2.998e-05, Loss_0: 3.252e-09, Loss_r: 2.995e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 2.987e-05, Loss_0: 1.526e-08, Loss_r: 2.972e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 2.937e-05, Loss_0: 1.287e-08, Loss_r: 2.924e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 2.874e-05, Loss_0: 1.745e-09, Loss_r: 2.872e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 2.843e-05, Loss_0: 6.839e-12, Loss_r: 2.843e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 2.809e-05, Loss_0: 3.286e-10, Loss_r: 2.808e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 2.780e-05, Loss_0: 1.301e-09, Loss_r: 2.779e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 2.753e-05, Loss_0: 3.410e-10, Loss_r: 2.752e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 3.174e-05, Loss_0: 6.340e-09, Loss_r: 3.168e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 3.056e-05, Loss_0: 1.837e-12, Loss_r: 3.056e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.980e-05, Loss_0: 1.423e-09, Loss_r: 2.978e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.923e-05, Loss_0: 1.812e-09, Loss_r: 2.921e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.871e-05, Loss_0: 2.770e-10, Loss_r: 2.871e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.829e-05, Loss_0: 3.383e-11, Loss_r: 2.829e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 2.805e-05, Loss_0: 1.776e-09, Loss_r: 2.803e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 3.465e-05, Loss_0: 1.770e-07, Loss_r: 3.288e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 8.840e-04, Loss_0: 2.362e-05, Loss_r: 6.478e-04, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 1.328e-04, Loss_0: 2.785e-06, Loss_r: 1.050e-04, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 2.742e-05, Loss_0: 1.197e-08, Loss_r: 2.730e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 5.205e-05, Loss_0: 7.346e-07, Loss_r: 4.470e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 4.417e-05, Loss_0: 5.144e-07, Loss_r: 3.903e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.691e-05, Loss_0: 1.433e-08, Loss_r: 2.676e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 2.843e-05, Loss_0: 4.698e-08, Loss_r: 2.796e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 2.622e-05, Loss_0: 4.667e-10, Loss_r: 2.622e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 2.629e-05, Loss_0: 1.378e-08, Loss_r: 2.615e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 2.583e-05, Loss_0: 4.309e-10, Loss_r: 2.583e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 2.559e-05, Loss_0: 8.244e-10, Loss_r: 2.558e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 2.542e-05, Loss_0: 4.090e-10, Loss_r: 2.541e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 5.138e-05, Loss_0: 3.536e-09, Loss_r: 5.135e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 4.914e-05, Loss_0: 8.588e-09, Loss_r: 4.905e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 4.766e-05, Loss_0: 4.022e-11, Loss_r: 4.766e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 4.643e-05, Loss_0: 3.443e-10, Loss_r: 4.643e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 4.543e-05, Loss_0: 2.891e-11, Loss_r: 4.543e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 4.572e-05, Loss_0: 2.246e-08, Loss_r: 4.549e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.471e-04, Loss_0: 2.972e-06, Loss_r: 1.174e-04, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2280, Loss: 5.098e-05, Loss_0: 1.952e-07, Loss_r: 4.903e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2290, Loss: 4.657e-05, Loss_0: 1.389e-07, Loss_r: 4.519e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2300, Loss: 4.858e-05, Loss_0: 2.193e-07, Loss_r: 4.639e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2310, Loss: 4.196e-05, Loss_0: 4.025e-09, Loss_r: 4.192e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 4.239e-05, Loss_0: 1.601e-08, Loss_r: 4.223e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 4.119e-05, Loss_0: 4.731e-09, Loss_r: 4.115e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 4.082e-05, Loss_0: 3.876e-09, Loss_r: 4.078e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 4.049e-05, Loss_0: 7.151e-13, Loss_r: 4.049e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 4.014e-05, Loss_0: 2.875e-09, Loss_r: 4.011e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 3.982e-05, Loss_0: 3.163e-10, Loss_r: 3.981e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 3.951e-05, Loss_0: 1.159e-09, Loss_r: 3.950e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 3.922e-05, Loss_0: 1.199e-09, Loss_r: 3.921e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 3.894e-05, Loss_0: 7.671e-10, Loss_r: 3.893e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 4.622e-05, Loss_0: 3.945e-07, Loss_r: 4.228e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2420, Loss: 3.181e-05, Loss_0: 1.270e-07, Loss_r: 3.054e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2430, Loss: 2.561e-05, Loss_0: 3.916e-08, Loss_r: 2.522e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2440, Loss: 2.228e-05, Loss_0: 9.929e-09, Loss_r: 2.218e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2450, Loss: 2.007e-05, Loss_0: 9.883e-10, Loss_r: 2.006e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 1.854e-05, Loss_0: 1.655e-10, Loss_r: 1.854e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 1.741e-05, Loss_0: 6.301e-10, Loss_r: 1.741e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 1.649e-05, Loss_0: 4.076e-10, Loss_r: 1.649e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 1.574e-05, Loss_0: 2.304e-11, Loss_r: 1.574e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 1.514e-05, Loss_0: 8.829e-12, Loss_r: 1.514e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 1.463e-05, Loss_0: 1.069e-13, Loss_r: 1.463e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 1.421e-05, Loss_0: 1.532e-11, Loss_r: 1.421e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 1.384e-05, Loss_0: 1.873e-12, Loss_r: 1.384e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.353e-05, Loss_0: 2.413e-13, Loss_r: 1.353e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 1.325e-05, Loss_0: 1.177e-12, Loss_r: 1.325e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 1.299e-05, Loss_0: 2.036e-13, Loss_r: 1.299e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 1.277e-05, Loss_0: 4.851e-14, Loss_r: 1.277e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 1.256e-05, Loss_0: 6.752e-14, Loss_r: 1.256e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 1.237e-05, Loss_0: 4.066e-14, Loss_r: 1.237e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 1.219e-05, Loss_0: 6.347e-14, Loss_r: 1.219e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 1.066e-05, Loss_0: 3.505e-09, Loss_r: 1.062e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 1.039e-05, Loss_0: 2.327e-10, Loss_r: 1.038e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 1.024e-05, Loss_0: 1.192e-10, Loss_r: 1.024e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.015e-05, Loss_0: 1.077e-09, Loss_r: 1.014e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.124e-05, Loss_0: 3.332e-08, Loss_r: 1.091e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 9.563e-05, Loss_0: 2.258e-06, Loss_r: 7.305e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 2.114e-05, Loss_0: 2.989e-07, Loss_r: 1.815e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 9.836e-06, Loss_0: 5.151e-09, Loss_r: 9.785e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.296e-05, Loss_0: 8.964e-08, Loss_r: 1.206e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.082e-05, Loss_0: 3.596e-08, Loss_r: 1.046e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 9.384e-06, Loss_0: 4.769e-10, Loss_r: 9.379e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 9.511e-06, Loss_0: 5.969e-09, Loss_r: 9.451e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 9.203e-06, Loss_0: 7.000e-11, Loss_r: 9.203e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 9.148e-06, Loss_0: 6.513e-10, Loss_r: 9.142e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 9.059e-06, Loss_0: 2.065e-10, Loss_r: 9.057e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 8.980e-06, Loss_0: 1.471e-11, Loss_r: 8.980e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 8.912e-06, Loss_0: 3.836e-12, Loss_r: 8.912e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 8.847e-06, Loss_0: 5.503e-13, Loss_r: 8.847e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 8.784e-06, Loss_0: 2.396e-12, Loss_r: 8.784e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 8.724e-06, Loss_0: 8.212e-13, Loss_r: 8.724e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.020e-05, Loss_0: 5.236e-10, Loss_r: 1.020e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 1.015e-05, Loss_0: 8.939e-11, Loss_r: 1.015e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 1.012e-05, Loss_0: 8.401e-11, Loss_r: 1.012e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 1.010e-05, Loss_0: 5.703e-11, Loss_r: 1.009e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 1.007e-05, Loss_0: 3.241e-11, Loss_r: 1.007e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 1.005e-05, Loss_0: 2.441e-11, Loss_r: 1.005e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 1.003e-05, Loss_0: 2.410e-11, Loss_r: 1.003e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 1.000e-05, Loss_0: 1.610e-11, Loss_r: 1.000e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 9.983e-06, Loss_0: 1.429e-11, Loss_r: 9.983e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 9.963e-06, Loss_0: 1.396e-11, Loss_r: 9.963e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 9.943e-06, Loss_0: 1.316e-11, Loss_r: 9.943e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 9.924e-06, Loss_0: 1.668e-11, Loss_r: 9.924e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 9.905e-06, Loss_0: 1.475e-11, Loss_r: 9.905e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 9.886e-06, Loss_0: 1.572e-11, Loss_r: 9.886e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 9.868e-06, Loss_0: 1.511e-11, Loss_r: 9.868e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 9.850e-06, Loss_0: 1.542e-11, Loss_r: 9.850e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 9.833e-06, Loss_0: 1.781e-11, Loss_r: 9.832e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 9.815e-06, Loss_0: 1.581e-11, Loss_r: 9.815e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 9.799e-06, Loss_0: 2.127e-11, Loss_r: 9.799e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 9.782e-06, Loss_0: 1.267e-11, Loss_r: 9.782e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 2.572e-05, Loss_0: 4.665e-09, Loss_r: 2.568e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 2.388e-05, Loss_0: 6.872e-10, Loss_r: 2.388e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 2.273e-05, Loss_0: 1.439e-12, Loss_r: 2.273e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 2.184e-05, Loss_0: 1.209e-10, Loss_r: 2.184e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 2.108e-05, Loss_0: 9.858e-11, Loss_r: 2.108e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 2.043e-05, Loss_0: 2.450e-10, Loss_r: 2.043e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.989e-05, Loss_0: 3.430e-10, Loss_r: 1.989e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.944e-05, Loss_0: 4.531e-10, Loss_r: 1.943e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.904e-05, Loss_0: 5.024e-10, Loss_r: 1.904e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.870e-05, Loss_0: 4.202e-10, Loss_r: 1.869e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.840e-05, Loss_0: 3.468e-10, Loss_r: 1.839e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.813e-05, Loss_0: 3.590e-10, Loss_r: 1.812e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.789e-05, Loss_0: 3.400e-10, Loss_r: 1.788e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.767e-05, Loss_0: 3.179e-10, Loss_r: 1.766e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.746e-05, Loss_0: 2.914e-10, Loss_r: 1.746e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.728e-05, Loss_0: 2.902e-10, Loss_r: 1.727e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.710e-05, Loss_0: 2.654e-10, Loss_r: 1.710e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.694e-05, Loss_0: 2.651e-10, Loss_r: 1.694e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.678e-05, Loss_0: 2.639e-10, Loss_r: 1.678e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.664e-05, Loss_0: 2.548e-10, Loss_r: 1.664e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 2.110e-05, Loss_0: 9.773e-08, Loss_r: 2.013e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 1.683e-05, Loss_0: 3.180e-08, Loss_r: 1.651e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 1.470e-05, Loss_0: 7.570e-09, Loss_r: 1.462e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.342e-05, Loss_0: 1.098e-09, Loss_r: 1.341e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.250e-05, Loss_0: 1.208e-14, Loss_r: 1.250e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 1.180e-05, Loss_0: 3.821e-10, Loss_r: 1.180e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 1.123e-05, Loss_0: 3.258e-10, Loss_r: 1.123e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 1.076e-05, Loss_0: 6.956e-11, Loss_r: 1.075e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 1.036e-05, Loss_0: 4.017e-13, Loss_r: 1.036e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 1.003e-05, Loss_0: 2.345e-15, Loss_r: 1.003e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 9.745e-06, Loss_0: 1.026e-11, Loss_r: 9.745e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 9.499e-06, Loss_0: 1.021e-11, Loss_r: 9.499e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 9.282e-06, Loss_0: 2.402e-12, Loss_r: 9.282e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 9.090e-06, Loss_0: 5.044e-12, Loss_r: 9.090e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 8.916e-06, Loss_0: 4.054e-12, Loss_r: 8.916e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 8.758e-06, Loss_0: 3.629e-12, Loss_r: 8.758e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 8.613e-06, Loss_0: 2.216e-12, Loss_r: 8.613e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 8.478e-06, Loss_0: 1.602e-12, Loss_r: 8.478e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 8.352e-06, Loss_0: 3.072e-12, Loss_r: 8.352e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 8.235e-06, Loss_0: 1.512e-12, Loss_r: 8.235e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.102e-05, Loss_0: 2.565e-10, Loss_r: 1.101e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3420, Loss: 1.097e-05, Loss_0: 2.747e-11, Loss_r: 1.097e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.093e-05, Loss_0: 4.823e-11, Loss_r: 1.093e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 1.089e-05, Loss_0: 5.905e-11, Loss_r: 1.089e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 1.086e-05, Loss_0: 2.659e-11, Loss_r: 1.086e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 1.082e-05, Loss_0: 1.745e-11, Loss_r: 1.082e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 1.079e-05, Loss_0: 2.071e-11, Loss_r: 1.079e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 1.076e-05, Loss_0: 1.806e-11, Loss_r: 1.076e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 1.073e-05, Loss_0: 1.455e-11, Loss_r: 1.073e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 1.070e-05, Loss_0: 1.434e-11, Loss_r: 1.070e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 1.068e-05, Loss_0: 1.518e-11, Loss_r: 1.068e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.065e-05, Loss_0: 1.778e-11, Loss_r: 1.065e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.063e-05, Loss_0: 1.420e-11, Loss_r: 1.062e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.060e-05, Loss_0: 1.689e-11, Loss_r: 1.060e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.058e-05, Loss_0: 1.758e-11, Loss_r: 1.058e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.055e-05, Loss_0: 1.696e-11, Loss_r: 1.055e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.053e-05, Loss_0: 1.661e-11, Loss_r: 1.053e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.051e-05, Loss_0: 1.664e-11, Loss_r: 1.051e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.049e-05, Loss_0: 1.955e-11, Loss_r: 1.049e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.046e-05, Loss_0: 1.429e-11, Loss_r: 1.046e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.189e-04, Loss_0: 4.281e-08, Loss_r: 1.185e-04, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 1.029e-04, Loss_0: 1.918e-08, Loss_r: 1.027e-04, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 9.186e-05, Loss_0: 4.762e-10, Loss_r: 9.185e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 8.330e-05, Loss_0: 6.367e-10, Loss_r: 8.330e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 7.625e-05, Loss_0: 1.700e-09, Loss_r: 7.623e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 7.062e-05, Loss_0: 8.554e-09, Loss_r: 7.054e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 6.610e-05, Loss_0: 7.909e-09, Loss_r: 6.602e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 6.237e-05, Loss_0: 4.711e-09, Loss_r: 6.233e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 5.928e-05, Loss_0: 3.505e-09, Loss_r: 5.924e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 5.665e-05, Loss_0: 4.290e-09, Loss_r: 5.661e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 5.440e-05, Loss_0: 3.209e-09, Loss_r: 5.437e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 5.245e-05, Loss_0: 3.209e-09, Loss_r: 5.241e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 5.072e-05, Loss_0: 2.682e-09, Loss_r: 5.069e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 4.918e-05, Loss_0: 2.653e-09, Loss_r: 4.915e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 4.779e-05, Loss_0: 2.444e-09, Loss_r: 4.777e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 4.653e-05, Loss_0: 2.316e-09, Loss_r: 4.651e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 4.537e-05, Loss_0: 2.459e-09, Loss_r: 4.535e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 4.435e-05, Loss_0: 5.814e-09, Loss_r: 4.429e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 4.885e-05, Loss_0: 2.000e-07, Loss_r: 4.685e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 1.541e-03, Loss_0: 4.314e-05, Loss_r: 1.110e-03, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 5.463e-04, Loss_0: 1.235e-05, Loss_r: 4.229e-04, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 2.586e-05, Loss_0: 5.928e-08, Loss_r: 2.527e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 9.208e-05, Loss_0: 1.790e-06, Loss_r: 7.418e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 2.280e-05, Loss_0: 2.992e-08, Loss_r: 2.250e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 2.718e-05, Loss_0: 1.835e-07, Loss_r: 2.534e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 2.090e-05, Loss_0: 4.320e-08, Loss_r: 2.047e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 1.869e-05, Loss_0: 2.466e-09, Loss_r: 1.866e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 1.836e-05, Loss_0: 9.275e-09, Loss_r: 1.827e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 1.762e-05, Loss_0: 2.169e-09, Loss_r: 1.760e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 1.717e-05, Loss_0: 1.277e-11, Loss_r: 1.717e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 1.687e-05, Loss_0: 1.522e-10, Loss_r: 1.687e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 1.663e-05, Loss_0: 1.660e-10, Loss_r: 1.663e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 1.642e-05, Loss_0: 9.629e-11, Loss_r: 1.642e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 1.625e-05, Loss_0: 5.888e-11, Loss_r: 1.625e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 1.610e-05, Loss_0: 4.143e-11, Loss_r: 1.610e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 1.597e-05, Loss_0: 2.932e-11, Loss_r: 1.597e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 1.586e-05, Loss_0: 2.724e-11, Loss_r: 1.586e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 1.576e-05, Loss_0: 2.585e-11, Loss_r: 1.576e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 1.566e-05, Loss_0: 2.118e-11, Loss_r: 1.566e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 1.558e-05, Loss_0: 2.260e-11, Loss_r: 1.558e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 1.568e-05, Loss_0: 2.848e-10, Loss_r: 1.567e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 1.496e-05, Loss_0: 9.577e-10, Loss_r: 1.495e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 1.459e-05, Loss_0: 9.563e-17, Loss_r: 1.459e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 1.434e-05, Loss_0: 1.818e-10, Loss_r: 1.434e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 1.410e-05, Loss_0: 9.374e-12, Loss_r: 1.410e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 1.391e-05, Loss_0: 2.339e-12, Loss_r: 1.391e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 1.374e-05, Loss_0: 2.129e-11, Loss_r: 1.374e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 1.359e-05, Loss_0: 5.352e-12, Loss_r: 1.359e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 1.346e-05, Loss_0: 5.830e-12, Loss_r: 1.346e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 1.334e-05, Loss_0: 1.292e-11, Loss_r: 1.334e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 1.323e-05, Loss_0: 6.251e-12, Loss_r: 1.323e-05, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 1.313e-05, Loss_0: 1.176e-11, Loss_r: 1.313e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 1.303e-05, Loss_0: 9.906e-12, Loss_r: 1.303e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 1.294e-05, Loss_0: 1.079e-11, Loss_r: 1.294e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 1.286e-05, Loss_0: 1.014e-11, Loss_r: 1.286e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 1.278e-05, Loss_0: 1.409e-11, Loss_r: 1.278e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 1.270e-05, Loss_0: 1.273e-11, Loss_r: 1.270e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 1.263e-05, Loss_0: 1.033e-11, Loss_r: 1.263e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 1.256e-05, Loss_0: 5.954e-14, Loss_r: 1.256e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 1.254e-05, Loss_0: 8.049e-10, Loss_r: 1.253e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 1.213e-05, Loss_0: 5.357e-10, Loss_r: 1.212e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 2.145e-05, Loss_0: 2.135e-07, Loss_r: 1.931e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 1.175e-05, Loss_0: 8.949e-10, Loss_r: 1.174e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 1.328e-05, Loss_0: 3.797e-08, Loss_r: 1.290e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 1.151e-05, Loss_0: 1.706e-09, Loss_r: 1.149e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.153e-05, Loss_0: 5.001e-09, Loss_r: 1.148e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 1.120e-05, Loss_0: 4.975e-12, Loss_r: 1.120e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 1.112e-05, Loss_0: 5.382e-10, Loss_r: 1.111e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 1.100e-05, Loss_0: 2.741e-10, Loss_r: 1.100e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 1.090e-05, Loss_0: 2.003e-11, Loss_r: 1.090e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 1.080e-05, Loss_0: 9.471e-12, Loss_r: 1.080e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 1.072e-05, Loss_0: 3.704e-13, Loss_r: 1.072e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 1.063e-05, Loss_0: 6.670e-12, Loss_r: 1.063e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 1.055e-05, Loss_0: 2.193e-14, Loss_r: 1.055e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 1.048e-05, Loss_0: 5.298e-12, Loss_r: 1.048e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 1.040e-05, Loss_0: 3.727e-12, Loss_r: 1.040e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 1.033e-05, Loss_0: 3.387e-12, Loss_r: 1.033e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 1.026e-05, Loss_0: 5.702e-12, Loss_r: 1.026e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 1.020e-05, Loss_0: 1.266e-11, Loss_r: 1.020e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 1.014e-05, Loss_0: 1.613e-10, Loss_r: 1.014e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 2.688e-04, Loss_0: 4.124e-06, Loss_r: 2.275e-04, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 1.147e-04, Loss_0: 1.030e-06, Loss_r: 1.044e-04, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 6.762e-05, Loss_0: 1.281e-07, Loss_r: 6.634e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.502e-05, Loss_0: 6.383e-09, Loss_r: 5.495e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 5.264e-05, Loss_0: 7.197e-08, Loss_r: 5.192e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 4.855e-05, Loss_0: 5.487e-08, Loss_r: 4.801e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 4.453e-05, Loss_0: 2.524e-09, Loss_r: 4.450e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 4.257e-05, Loss_0: 5.027e-10, Loss_r: 4.257e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 4.064e-05, Loss_0: 4.030e-09, Loss_r: 4.060e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 3.920e-05, Loss_0: 2.988e-09, Loss_r: 3.917e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 3.800e-05, Loss_0: 7.230e-10, Loss_r: 3.800e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 3.698e-05, Loss_0: 2.342e-09, Loss_r: 3.695e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 3.609e-05, Loss_0: 1.030e-09, Loss_r: 3.608e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 3.531e-05, Loss_0: 1.316e-09, Loss_r: 3.530e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 3.462e-05, Loss_0: 1.393e-09, Loss_r: 3.461e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 3.400e-05, Loss_0: 1.106e-09, Loss_r: 3.399e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 3.343e-05, Loss_0: 9.875e-10, Loss_r: 3.342e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 3.290e-05, Loss_0: 7.522e-10, Loss_r: 3.290e-05, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 3.245e-05, Loss_0: 4.717e-12, Loss_r: 3.245e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 3.637e-05, Loss_0: 8.914e-08, Loss_r: 3.548e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 4.449e-04, Loss_0: 9.903e-06, Loss_r: 3.459e-04, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 1.733e-04, Loss_0: 3.416e-06, Loss_r: 1.391e-04, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 8.268e-05, Loss_0: 1.357e-06, Loss_r: 6.911e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 3.966e-05, Loss_0: 4.281e-07, Loss_r: 3.538e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 2.446e-05, Loss_0: 1.053e-07, Loss_r: 2.340e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 1.986e-05, Loss_0: 2.461e-08, Loss_r: 1.961e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 1.813e-05, Loss_0: 6.948e-09, Loss_r: 1.806e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 1.712e-05, Loss_0: 3.058e-09, Loss_r: 1.709e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 1.634e-05, Loss_0: 1.715e-09, Loss_r: 1.633e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 1.569e-05, Loss_0: 9.619e-10, Loss_r: 1.568e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 1.512e-05, Loss_0: 4.223e-10, Loss_r: 1.512e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 1.463e-05, Loss_0: 1.148e-10, Loss_r: 1.462e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.419e-05, Loss_0: 6.082e-12, Loss_r: 1.419e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.380e-05, Loss_0: 1.539e-12, Loss_r: 1.380e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.344e-05, Loss_0: 7.446e-14, Loss_r: 1.344e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 1.312e-05, Loss_0: 5.270e-12, Loss_r: 1.312e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 1.281e-05, Loss_0: 6.907e-12, Loss_r: 1.281e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 1.254e-05, Loss_0: 2.770e-12, Loss_r: 1.254e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 1.227e-05, Loss_0: 1.918e-12, Loss_r: 1.227e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 1.203e-05, Loss_0: 2.914e-12, Loss_r: 1.203e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 1.198e-05, Loss_0: 7.861e-10, Loss_r: 1.197e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 1.156e-05, Loss_0: 4.038e-10, Loss_r: 1.156e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 1.131e-05, Loss_0: 2.389e-10, Loss_r: 1.131e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 1.110e-05, Loss_0: 8.883e-11, Loss_r: 1.110e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 1.090e-05, Loss_0: 6.234e-11, Loss_r: 1.090e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 1.073e-05, Loss_0: 5.868e-12, Loss_r: 1.073e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 1.056e-05, Loss_0: 1.200e-13, Loss_r: 1.056e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 1.040e-05, Loss_0: 5.909e-12, Loss_r: 1.040e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 1.025e-05, Loss_0: 4.596e-12, Loss_r: 1.025e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 1.011e-05, Loss_0: 1.834e-12, Loss_r: 1.011e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 9.971e-06, Loss_0: 7.974e-12, Loss_r: 9.970e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 9.843e-06, Loss_0: 1.358e-10, Loss_r: 9.842e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 1.007e-05, Loss_0: 7.817e-09, Loss_r: 9.987e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 5.368e-05, Loss_0: 9.608e-07, Loss_r: 4.407e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.423e-05, Loss_0: 1.040e-07, Loss_r: 1.319e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 9.566e-06, Loss_0: 4.184e-09, Loss_r: 9.524e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 1.104e-05, Loss_0: 3.828e-08, Loss_r: 1.065e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 9.947e-06, Loss_0: 1.670e-08, Loss_r: 9.780e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 9.081e-06, Loss_0: 1.949e-13, Loss_r: 9.081e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "Training time: 23.6439\n",
            "[1, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.376e-01, Loss_0: 1.747e-03, Loss_r: 2.359e-01, Time: 0.54, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.078e-01, Loss_0: 7.826e-04, Loss_r: 1.999e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.101e-01, Loss_0: 1.122e-03, Loss_r: 1.989e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 30, Loss: 2.045e-01, Loss_0: 2.044e-03, Loss_r: 1.841e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.973e-01, Loss_0: 1.386e-03, Loss_r: 1.835e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.964e-01, Loss_0: 1.455e-03, Loss_r: 1.819e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.965e-01, Loss_0: 1.664e-03, Loss_r: 1.799e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.960e-01, Loss_0: 1.548e-03, Loss_r: 1.805e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.958e-01, Loss_0: 1.492e-03, Loss_r: 1.809e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.954e-01, Loss_0: 1.537e-03, Loss_r: 1.801e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.949e-01, Loss_0: 1.545e-03, Loss_r: 1.795e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.941e-01, Loss_0: 1.536e-03, Loss_r: 1.787e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.926e-01, Loss_0: 1.524e-03, Loss_r: 1.773e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.895e-01, Loss_0: 1.501e-03, Loss_r: 1.745e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.824e-01, Loss_0: 1.456e-03, Loss_r: 1.679e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.661e-01, Loss_0: 1.338e-03, Loss_r: 1.527e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.342e-01, Loss_0: 1.056e-03, Loss_r: 1.237e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 8.653e-02, Loss_0: 5.444e-04, Loss_r: 8.108e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 180, Loss: 4.107e-02, Loss_0: 1.448e-04, Loss_r: 3.963e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 8.725e-02, Loss_0: 1.882e-04, Loss_r: 8.537e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 3.631e-02, Loss_0: 3.654e-05, Loss_r: 3.595e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 2.779e-02, Loss_0: 2.053e-05, Loss_r: 2.758e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.831e-02, Loss_0: 3.612e-05, Loss_r: 1.795e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.472e-02, Loss_0: 1.630e-05, Loss_r: 1.456e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.123e-02, Loss_0: 1.525e-05, Loss_r: 1.108e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 250, Loss: 8.410e-03, Loss_0: 1.008e-05, Loss_r: 8.309e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 6.209e-03, Loss_0: 6.204e-06, Loss_r: 6.147e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 4.588e-03, Loss_0: 4.095e-06, Loss_r: 4.547e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.452e-03, Loss_0: 2.556e-06, Loss_r: 3.426e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 2.670e-03, Loss_0: 1.675e-06, Loss_r: 2.653e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 2.151e-03, Loss_0: 1.200e-06, Loss_r: 2.139e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.805e-03, Loss_0: 8.867e-07, Loss_r: 1.796e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.558e-03, Loss_0: 6.464e-07, Loss_r: 1.551e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.368e-03, Loss_0: 4.859e-07, Loss_r: 1.363e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.212e-03, Loss_0: 3.937e-07, Loss_r: 1.208e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.097e-03, Loss_0: 3.773e-07, Loss_r: 1.093e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.576e-02, Loss_0: 7.538e-06, Loss_r: 4.569e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 1.160e-03, Loss_0: 1.224e-05, Loss_r: 1.038e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.153e-03, Loss_0: 4.236e-05, Loss_r: 5.730e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.186e-03, Loss_0: 8.315e-08, Loss_r: 1.185e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.361e-03, Loss_0: 5.694e-06, Loss_r: 1.304e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.927e-03, Loss_0: 2.254e-07, Loss_r: 1.925e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.711e-03, Loss_0: 7.316e-06, Loss_r: 1.637e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 1.438e-03, Loss_0: 2.631e-07, Loss_r: 1.435e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.227e-03, Loss_0: 1.769e-06, Loss_r: 1.209e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.067e-03, Loss_0: 1.802e-06, Loss_r: 1.049e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 9.299e-04, Loss_0: 8.537e-07, Loss_r: 9.213e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 470, Loss: 8.145e-04, Loss_0: 6.920e-07, Loss_r: 8.076e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 7.148e-04, Loss_0: 5.691e-08, Loss_r: 7.142e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 6.286e-04, Loss_0: 1.203e-07, Loss_r: 6.274e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.890e-04, Loss_0: 1.884e-06, Loss_r: 5.702e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.299e-03, Loss_0: 3.012e-05, Loss_r: 2.998e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.035e-03, Loss_0: 2.047e-05, Loss_r: 8.303e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.356e-03, Loss_0: 5.037e-06, Loss_r: 1.305e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.733e-03, Loss_0: 1.776e-05, Loss_r: 1.555e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 7.732e-03, Loss_0: 3.396e-05, Loss_r: 7.392e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.554e-03, Loss_0: 9.962e-06, Loss_r: 1.454e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.773e-04, Loss_0: 4.272e-08, Loss_r: 3.768e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.048e-04, Loss_0: 3.089e-06, Loss_r: 4.739e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 3.534e-04, Loss_0: 1.317e-07, Loss_r: 3.521e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 3.712e-04, Loss_0: 4.957e-07, Loss_r: 3.663e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.179e-04, Loss_0: 1.866e-06, Loss_r: 2.992e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.523e-02, Loss_0: 1.290e-04, Loss_r: 1.394e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.949e-03, Loss_0: 2.385e-05, Loss_r: 1.710e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.924e-04, Loss_0: 2.096e-06, Loss_r: 1.714e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.609e-04, Loss_0: 2.261e-06, Loss_r: 2.382e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 3.027e-04, Loss_0: 1.994e-06, Loss_r: 2.827e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 1.739e-04, Loss_0: 1.383e-06, Loss_r: 1.601e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.133e-04, Loss_0: 3.951e-07, Loss_r: 1.093e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 8.850e-05, Loss_0: 9.598e-08, Loss_r: 8.754e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 7.384e-05, Loss_0: 4.467e-09, Loss_r: 7.380e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 6.962e-05, Loss_0: 3.424e-08, Loss_r: 6.927e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 6.590e-05, Loss_0: 1.595e-08, Loss_r: 6.574e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 6.322e-05, Loss_0: 1.136e-10, Loss_r: 6.322e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 6.070e-05, Loss_0: 1.249e-08, Loss_r: 6.057e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.853e-05, Loss_0: 2.083e-09, Loss_r: 5.851e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 5.682e-05, Loss_0: 3.675e-10, Loss_r: 5.682e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 5.612e-05, Loss_0: 6.435e-09, Loss_r: 5.606e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.489e-04, Loss_0: 1.857e-06, Loss_r: 1.303e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.464e-02, Loss_0: 3.318e-04, Loss_r: 2.132e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.978e-03, Loss_0: 1.263e-07, Loss_r: 1.977e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.741e-03, Loss_0: 7.966e-05, Loss_r: 9.442e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.251e-03, Loss_0: 6.038e-05, Loss_r: 6.476e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 6.018e-04, Loss_0: 1.502e-05, Loss_r: 4.516e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.669e-04, Loss_0: 2.972e-06, Loss_r: 2.371e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.145e-04, Loss_0: 2.156e-07, Loss_r: 2.124e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.925e-04, Loss_0: 1.608e-09, Loss_r: 1.925e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.667e-04, Loss_0: 2.099e-07, Loss_r: 1.646e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.475e-04, Loss_0: 1.056e-08, Loss_r: 1.474e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.357e-04, Loss_0: 5.871e-08, Loss_r: 1.351e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.260e-04, Loss_0: 1.760e-08, Loss_r: 1.258e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.182e-04, Loss_0: 3.469e-09, Loss_r: 1.182e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.118e-04, Loss_0: 3.272e-09, Loss_r: 1.118e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.066e-04, Loss_0: 1.166e-08, Loss_r: 1.065e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 1.023e-04, Loss_0: 1.215e-08, Loss_r: 1.021e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 9.868e-05, Loss_0: 7.708e-09, Loss_r: 9.861e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 960, Loss: 9.568e-05, Loss_0: 1.031e-08, Loss_r: 9.558e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 970, Loss: 9.315e-05, Loss_0: 9.297e-09, Loss_r: 9.306e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 980, Loss: 9.099e-05, Loss_0: 7.345e-09, Loss_r: 9.092e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 990, Loss: 8.913e-05, Loss_0: 6.290e-09, Loss_r: 8.907e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 8.755e-05, Loss_0: 9.457e-10, Loss_r: 8.754e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.110e-03, Loss_0: 4.412e-05, Loss_r: 6.689e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 4.327e-03, Loss_0: 3.377e-07, Loss_r: 4.323e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 5.795e-03, Loss_0: 1.855e-06, Loss_r: 5.776e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.240e-03, Loss_0: 2.728e-05, Loss_r: 9.669e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.007e-03, Loss_0: 7.159e-05, Loss_r: 2.910e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 4.166e-04, Loss_0: 6.268e-06, Loss_r: 3.539e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 3.248e-04, Loss_0: 1.427e-09, Loss_r: 3.248e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 2.677e-04, Loss_0: 3.443e-07, Loss_r: 2.642e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 2.230e-04, Loss_0: 7.508e-09, Loss_r: 2.230e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.975e-04, Loss_0: 1.969e-07, Loss_r: 1.956e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.824e-04, Loss_0: 2.475e-08, Loss_r: 1.821e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.692e-04, Loss_0: 1.184e-07, Loss_r: 1.680e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.588e-04, Loss_0: 2.875e-08, Loss_r: 1.585e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.502e-04, Loss_0: 3.258e-08, Loss_r: 1.499e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 1.432e-04, Loss_0: 3.400e-08, Loss_r: 1.428e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.372e-04, Loss_0: 2.037e-08, Loss_r: 1.370e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.322e-04, Loss_0: 1.627e-08, Loss_r: 1.320e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.279e-04, Loss_0: 1.582e-08, Loss_r: 1.277e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.242e-04, Loss_0: 1.452e-08, Loss_r: 1.240e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.209e-04, Loss_0: 1.274e-08, Loss_r: 1.208e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 9.178e-05, Loss_0: 2.423e-09, Loss_r: 9.176e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 8.951e-05, Loss_0: 1.358e-08, Loss_r: 8.937e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 8.802e-05, Loss_0: 2.453e-09, Loss_r: 8.799e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 8.688e-05, Loss_0: 4.354e-09, Loss_r: 8.683e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 8.584e-05, Loss_0: 2.678e-09, Loss_r: 8.581e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 8.482e-05, Loss_0: 5.424e-09, Loss_r: 8.476e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 8.382e-05, Loss_0: 3.926e-09, Loss_r: 8.378e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 8.285e-05, Loss_0: 3.031e-09, Loss_r: 8.282e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 8.193e-05, Loss_0: 1.128e-09, Loss_r: 8.192e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 8.188e-05, Loss_0: 1.868e-08, Loss_r: 8.169e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.198e-04, Loss_0: 5.848e-06, Loss_r: 1.613e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.159e-02, Loss_0: 4.176e-04, Loss_r: 1.741e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 5.495e-04, Loss_0: 1.752e-05, Loss_r: 3.744e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.713e-03, Loss_0: 5.467e-05, Loss_r: 1.166e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.273e-04, Loss_0: 3.697e-09, Loss_r: 1.272e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 3.097e-04, Loss_0: 6.350e-06, Loss_r: 2.462e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.234e-04, Loss_0: 3.967e-07, Loss_r: 1.194e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.319e-04, Loss_0: 1.776e-07, Loss_r: 1.302e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.028e-04, Loss_0: 1.264e-07, Loss_r: 1.015e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 9.757e-05, Loss_0: 2.496e-07, Loss_r: 9.507e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 3.845e-05, Loss_0: 1.292e-07, Loss_r: 3.716e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 3.356e-05, Loss_0: 6.355e-11, Loss_r: 3.356e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.073e-05, Loss_0: 9.164e-09, Loss_r: 3.064e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.957e-05, Loss_0: 1.092e-08, Loss_r: 2.946e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.874e-05, Loss_0: 6.071e-10, Loss_r: 2.873e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 2.806e-05, Loss_0: 9.566e-11, Loss_r: 2.806e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.749e-05, Loss_0: 5.744e-10, Loss_r: 2.749e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.703e-05, Loss_0: 5.654e-10, Loss_r: 2.703e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.666e-05, Loss_0: 2.334e-10, Loss_r: 2.665e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.634e-05, Loss_0: 1.057e-10, Loss_r: 2.634e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.608e-05, Loss_0: 1.007e-10, Loss_r: 2.608e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.586e-05, Loss_0: 1.450e-10, Loss_r: 2.586e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.567e-05, Loss_0: 1.925e-10, Loss_r: 2.566e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 2.550e-05, Loss_0: 1.964e-10, Loss_r: 2.550e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.535e-05, Loss_0: 2.032e-10, Loss_r: 2.535e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.522e-05, Loss_0: 2.205e-10, Loss_r: 2.522e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.510e-05, Loss_0: 2.226e-10, Loss_r: 2.510e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.499e-05, Loss_0: 2.380e-10, Loss_r: 2.499e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.489e-05, Loss_0: 2.261e-10, Loss_r: 2.489e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.480e-05, Loss_0: 2.508e-10, Loss_r: 2.480e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 1.482e-04, Loss_0: 5.933e-10, Loss_r: 1.482e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.283e-04, Loss_0: 2.458e-08, Loss_r: 1.280e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.170e-04, Loss_0: 7.599e-08, Loss_r: 1.163e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.203e-03, Loss_0: 4.099e-05, Loss_r: 7.933e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.092e-03, Loss_0: 3.414e-05, Loss_r: 7.503e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 3.573e-03, Loss_0: 7.150e-05, Loss_r: 2.858e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.769e-03, Loss_0: 1.036e-04, Loss_r: 7.328e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 5.979e-04, Loss_0: 3.038e-05, Loss_r: 2.941e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 4.838e-04, Loss_0: 2.671e-08, Loss_r: 4.836e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.276e-04, Loss_0: 1.216e-06, Loss_r: 2.154e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 2.181e-04, Loss_0: 1.337e-06, Loss_r: 2.048e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.706e-04, Loss_0: 1.188e-06, Loss_r: 1.587e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 1.390e-04, Loss_0: 6.181e-09, Loss_r: 1.389e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 1.237e-04, Loss_0: 2.596e-08, Loss_r: 1.234e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 1.101e-04, Loss_0: 1.653e-08, Loss_r: 1.099e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 9.988e-05, Loss_0: 2.304e-08, Loss_r: 9.964e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 9.187e-05, Loss_0: 3.017e-08, Loss_r: 9.157e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 8.552e-05, Loss_0: 1.818e-08, Loss_r: 8.534e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 8.045e-05, Loss_0: 1.201e-08, Loss_r: 8.033e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 7.634e-05, Loss_0: 9.454e-09, Loss_r: 7.625e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 7.615e-05, Loss_0: 1.467e-08, Loss_r: 7.601e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 5.811e-05, Loss_0: 1.392e-08, Loss_r: 5.797e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.916e-05, Loss_0: 7.909e-09, Loss_r: 4.908e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 4.369e-05, Loss_0: 2.643e-09, Loss_r: 4.366e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.894e-05, Loss_0: 4.971e-10, Loss_r: 3.893e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 3.481e-05, Loss_0: 6.848e-11, Loss_r: 3.481e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 3.140e-05, Loss_0: 7.317e-13, Loss_r: 3.140e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 2.860e-05, Loss_0: 9.086e-12, Loss_r: 2.860e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 2.630e-05, Loss_0: 1.090e-11, Loss_r: 2.630e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 2.438e-05, Loss_0: 1.350e-12, Loss_r: 2.438e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 2.280e-05, Loss_0: 9.563e-17, Loss_r: 2.280e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.148e-05, Loss_0: 1.093e-15, Loss_r: 2.148e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 2.039e-05, Loss_0: 3.307e-13, Loss_r: 2.039e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.948e-05, Loss_0: 2.887e-12, Loss_r: 1.948e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.872e-05, Loss_0: 5.707e-12, Loss_r: 1.872e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.808e-05, Loss_0: 6.096e-12, Loss_r: 1.808e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.754e-05, Loss_0: 6.776e-12, Loss_r: 1.754e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.709e-05, Loss_0: 9.563e-12, Loss_r: 1.709e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.670e-05, Loss_0: 1.251e-11, Loss_r: 1.670e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.637e-05, Loss_0: 1.293e-11, Loss_r: 1.637e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.802e-05, Loss_0: 5.442e-09, Loss_r: 1.796e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.625e-05, Loss_0: 3.666e-12, Loss_r: 1.625e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.580e-05, Loss_0: 3.015e-11, Loss_r: 1.580e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.568e-05, Loss_0: 1.534e-09, Loss_r: 1.567e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.555e-05, Loss_0: 1.744e-09, Loss_r: 1.553e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.545e-05, Loss_0: 5.739e-09, Loss_r: 1.539e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.637e-05, Loss_0: 1.025e-07, Loss_r: 1.535e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 8.361e-05, Loss_0: 6.147e-06, Loss_r: 2.213e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 3.429e-03, Loss_0: 2.738e-04, Loss_r: 6.915e-04, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2100, Loss: 5.266e-04, Loss_0: 2.987e-06, Loss_r: 4.967e-04, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2110, Loss: 7.964e-05, Loss_0: 1.491e-06, Loss_r: 6.472e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2120, Loss: 1.363e-04, Loss_0: 1.381e-06, Loss_r: 1.225e-04, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2130, Loss: 3.066e-05, Loss_0: 1.360e-06, Loss_r: 1.706e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 2.114e-05, Loss_0: 5.408e-07, Loss_r: 1.573e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 1.996e-05, Loss_0: 1.066e-07, Loss_r: 1.890e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 1.722e-05, Loss_0: 1.493e-08, Loss_r: 1.707e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 1.556e-05, Loss_0: 4.752e-08, Loss_r: 1.508e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 1.494e-05, Loss_0: 9.719e-09, Loss_r: 1.484e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 1.473e-05, Loss_0: 3.178e-10, Loss_r: 1.473e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 1.458e-05, Loss_0: 8.115e-10, Loss_r: 1.458e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 1.888e-05, Loss_0: 1.326e-10, Loss_r: 1.887e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 1.570e-05, Loss_0: 1.053e-12, Loss_r: 1.570e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.487e-05, Loss_0: 6.067e-11, Loss_r: 1.487e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 1.454e-05, Loss_0: 4.413e-11, Loss_r: 1.453e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 1.416e-05, Loss_0: 1.366e-12, Loss_r: 1.416e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 1.383e-05, Loss_0: 3.137e-11, Loss_r: 1.383e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 1.357e-05, Loss_0: 5.149e-11, Loss_r: 1.357e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 1.336e-05, Loss_0: 2.542e-11, Loss_r: 1.336e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 1.317e-05, Loss_0: 1.907e-11, Loss_r: 1.317e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 1.300e-05, Loss_0: 3.077e-11, Loss_r: 1.300e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.286e-05, Loss_0: 3.051e-11, Loss_r: 1.286e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.273e-05, Loss_0: 2.861e-11, Loss_r: 1.273e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.261e-05, Loss_0: 3.290e-11, Loss_r: 1.261e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.251e-05, Loss_0: 3.284e-11, Loss_r: 1.251e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.241e-05, Loss_0: 3.663e-11, Loss_r: 1.241e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 1.232e-05, Loss_0: 3.581e-11, Loss_r: 1.232e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.224e-05, Loss_0: 3.556e-11, Loss_r: 1.224e-05, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 1.216e-05, Loss_0: 3.867e-11, Loss_r: 1.216e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 1.209e-05, Loss_0: 4.083e-11, Loss_r: 1.209e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 1.202e-05, Loss_0: 3.989e-11, Loss_r: 1.202e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 1.856e-04, Loss_0: 1.526e-06, Loss_r: 1.703e-04, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.043e-04, Loss_0: 3.450e-07, Loss_r: 1.009e-04, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 7.772e-05, Loss_0: 6.001e-08, Loss_r: 7.712e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 7.474e-05, Loss_0: 2.429e-07, Loss_r: 7.231e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 6.577e-05, Loss_0: 1.651e-08, Loss_r: 6.561e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 6.096e-05, Loss_0: 8.357e-09, Loss_r: 6.088e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 5.811e-05, Loss_0: 2.843e-09, Loss_r: 5.808e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 5.611e-05, Loss_0: 1.213e-08, Loss_r: 5.599e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 5.460e-05, Loss_0: 1.073e-11, Loss_r: 5.460e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 5.340e-05, Loss_0: 8.818e-12, Loss_r: 5.340e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 5.264e-05, Loss_0: 2.159e-09, Loss_r: 5.261e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 5.900e-05, Loss_0: 2.531e-07, Loss_r: 5.647e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 8.311e-04, Loss_0: 3.088e-05, Loss_r: 5.223e-04, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 1.507e-04, Loss_0: 4.085e-06, Loss_r: 1.098e-04, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 5.074e-05, Loss_0: 1.126e-08, Loss_r: 5.063e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2560, Loss: 7.336e-05, Loss_0: 9.731e-07, Loss_r: 6.363e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2570, Loss: 6.520e-05, Loss_0: 6.566e-07, Loss_r: 5.863e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2580, Loss: 4.936e-05, Loss_0: 1.779e-08, Loss_r: 4.919e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2590, Loss: 5.084e-05, Loss_0: 4.425e-08, Loss_r: 5.040e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2600, Loss: 4.840e-05, Loss_0: 5.043e-10, Loss_r: 4.840e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 3.210e-05, Loss_0: 3.318e-09, Loss_r: 3.207e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 3.075e-05, Loss_0: 3.469e-14, Loss_r: 3.075e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 3.014e-05, Loss_0: 8.320e-10, Loss_r: 3.013e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.976e-05, Loss_0: 4.991e-10, Loss_r: 2.976e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.940e-05, Loss_0: 4.332e-10, Loss_r: 2.940e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.908e-05, Loss_0: 4.638e-10, Loss_r: 2.907e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.879e-05, Loss_0: 4.838e-10, Loss_r: 2.879e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.854e-05, Loss_0: 3.992e-10, Loss_r: 2.853e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.830e-05, Loss_0: 4.839e-10, Loss_r: 2.830e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 2.808e-05, Loss_0: 4.975e-10, Loss_r: 2.808e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 2.788e-05, Loss_0: 4.708e-10, Loss_r: 2.787e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 2.768e-05, Loss_0: 4.509e-10, Loss_r: 2.767e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 2.749e-05, Loss_0: 3.661e-10, Loss_r: 2.749e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 2.733e-05, Loss_0: 1.269e-11, Loss_r: 2.733e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 3.144e-05, Loss_0: 9.708e-08, Loss_r: 3.047e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.707e-05, Loss_0: 8.578e-10, Loss_r: 2.706e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.721e-05, Loss_0: 1.489e-08, Loss_r: 2.706e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 2.687e-05, Loss_0: 9.058e-09, Loss_r: 2.678e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 2.652e-05, Loss_0: 7.416e-12, Loss_r: 2.652e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 2.638e-05, Loss_0: 2.572e-11, Loss_r: 2.638e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 2.966e-05, Loss_0: 4.708e-09, Loss_r: 2.961e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 2.782e-05, Loss_0: 6.328e-11, Loss_r: 2.782e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 2.668e-05, Loss_0: 1.517e-09, Loss_r: 2.667e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 2.585e-05, Loss_0: 5.972e-10, Loss_r: 2.585e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 2.515e-05, Loss_0: 1.662e-09, Loss_r: 2.514e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 2.456e-05, Loss_0: 8.886e-10, Loss_r: 2.455e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 2.407e-05, Loss_0: 5.100e-10, Loss_r: 2.406e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 2.366e-05, Loss_0: 1.748e-10, Loss_r: 2.365e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 2.344e-05, Loss_0: 1.403e-09, Loss_r: 2.343e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 3.413e-05, Loss_0: 2.716e-07, Loss_r: 3.141e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 2.371e-05, Loss_0: 1.909e-08, Loss_r: 2.352e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 2.278e-05, Loss_0: 1.262e-08, Loss_r: 2.266e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2930, Loss: 2.293e-05, Loss_0: 2.416e-08, Loss_r: 2.269e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2940, Loss: 2.209e-05, Loss_0: 2.104e-09, Loss_r: 2.207e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2950, Loss: 2.196e-05, Loss_0: 6.047e-10, Loss_r: 2.195e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2960, Loss: 2.169e-05, Loss_0: 4.116e-10, Loss_r: 2.169e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2970, Loss: 2.153e-05, Loss_0: 1.377e-09, Loss_r: 2.151e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 2.135e-05, Loss_0: 1.036e-10, Loss_r: 2.135e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 2.119e-05, Loss_0: 6.466e-10, Loss_r: 2.118e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 2.103e-05, Loss_0: 3.033e-10, Loss_r: 2.102e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.341e-04, Loss_0: 3.533e-08, Loss_r: 1.337e-04, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 1.190e-04, Loss_0: 3.043e-09, Loss_r: 1.190e-04, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 1.061e-04, Loss_0: 1.568e-08, Loss_r: 1.059e-04, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 9.579e-05, Loss_0: 1.969e-08, Loss_r: 9.559e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 8.750e-05, Loss_0: 1.451e-08, Loss_r: 8.736e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 8.090e-05, Loss_0: 1.285e-08, Loss_r: 8.078e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 7.566e-05, Loss_0: 1.148e-08, Loss_r: 7.555e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 7.145e-05, Loss_0: 8.253e-09, Loss_r: 7.136e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 6.799e-05, Loss_0: 7.777e-09, Loss_r: 6.791e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 6.510e-05, Loss_0: 6.660e-09, Loss_r: 6.503e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 6.264e-05, Loss_0: 6.099e-09, Loss_r: 6.258e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 6.050e-05, Loss_0: 5.129e-09, Loss_r: 6.045e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 5.861e-05, Loss_0: 4.436e-09, Loss_r: 5.856e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 5.691e-05, Loss_0: 3.728e-09, Loss_r: 5.687e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 5.539e-05, Loss_0: 1.467e-09, Loss_r: 5.537e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 5.608e-05, Loss_0: 4.005e-08, Loss_r: 5.568e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 6.568e-04, Loss_0: 1.810e-05, Loss_r: 4.757e-04, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3180, Loss: 7.448e-05, Loss_0: 7.813e-07, Loss_r: 6.667e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3190, Loss: 7.491e-05, Loss_0: 7.017e-07, Loss_r: 6.790e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3200, Loss: 8.679e-05, Loss_0: 1.164e-06, Loss_r: 7.515e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3210, Loss: 7.583e-05, Loss_0: 1.691e-07, Loss_r: 7.414e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 6.266e-05, Loss_0: 2.314e-08, Loss_r: 6.242e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 5.658e-05, Loss_0: 5.058e-08, Loss_r: 5.607e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 5.021e-05, Loss_0: 3.646e-09, Loss_r: 5.017e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 4.626e-05, Loss_0: 2.814e-09, Loss_r: 4.624e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 4.315e-05, Loss_0: 2.361e-09, Loss_r: 4.312e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 4.069e-05, Loss_0: 1.575e-11, Loss_r: 4.069e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 3.884e-05, Loss_0: 8.711e-11, Loss_r: 3.884e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 3.737e-05, Loss_0: 3.501e-11, Loss_r: 3.737e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 3.622e-05, Loss_0: 2.124e-10, Loss_r: 3.621e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 3.529e-05, Loss_0: 1.201e-10, Loss_r: 3.528e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 3.453e-05, Loss_0: 1.173e-10, Loss_r: 3.453e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 3.390e-05, Loss_0: 1.929e-10, Loss_r: 3.390e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 3.338e-05, Loss_0: 1.735e-10, Loss_r: 3.338e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 3.293e-05, Loss_0: 2.073e-10, Loss_r: 3.292e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 3.253e-05, Loss_0: 2.191e-10, Loss_r: 3.253e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 3.218e-05, Loss_0: 2.313e-10, Loss_r: 3.218e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 3.186e-05, Loss_0: 2.480e-10, Loss_r: 3.186e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 3.157e-05, Loss_0: 2.620e-10, Loss_r: 3.157e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 3.130e-05, Loss_0: 2.732e-10, Loss_r: 3.130e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 3.239e-05, Loss_0: 9.660e-09, Loss_r: 3.229e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 2.835e-05, Loss_0: 1.794e-09, Loss_r: 2.833e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.648e-05, Loss_0: 2.339e-09, Loss_r: 2.646e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 2.491e-05, Loss_0: 7.034e-11, Loss_r: 2.491e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 2.358e-05, Loss_0: 9.664e-11, Loss_r: 2.358e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 2.246e-05, Loss_0: 6.439e-10, Loss_r: 2.245e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 2.177e-05, Loss_0: 5.578e-09, Loss_r: 2.172e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 2.637e-05, Loss_0: 1.016e-07, Loss_r: 2.535e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 2.154e-04, Loss_0: 3.451e-06, Loss_r: 1.809e-04, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 6.802e-05, Loss_0: 8.556e-07, Loss_r: 5.946e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 2.575e-05, Loss_0: 1.243e-07, Loss_r: 2.451e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 1.815e-05, Loss_0: 3.802e-10, Loss_r: 1.815e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 1.874e-05, Loss_0: 1.813e-08, Loss_r: 1.856e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 1.850e-05, Loss_0: 2.115e-08, Loss_r: 1.829e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 1.721e-05, Loss_0: 4.869e-09, Loss_r: 1.716e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 1.658e-05, Loss_0: 1.101e-10, Loss_r: 1.658e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 1.631e-05, Loss_0: 1.139e-09, Loss_r: 1.630e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 1.594e-05, Loss_0: 7.186e-11, Loss_r: 1.594e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 1.566e-05, Loss_0: 8.176e-11, Loss_r: 1.566e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 1.537e-05, Loss_0: 2.771e-12, Loss_r: 1.537e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.395e-05, Loss_0: 6.881e-10, Loss_r: 1.394e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 1.332e-05, Loss_0: 6.027e-11, Loss_r: 1.332e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 1.302e-05, Loss_0: 1.177e-10, Loss_r: 1.302e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.281e-05, Loss_0: 1.527e-11, Loss_r: 1.281e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.262e-05, Loss_0: 6.805e-11, Loss_r: 1.262e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.244e-05, Loss_0: 2.037e-11, Loss_r: 1.244e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.228e-05, Loss_0: 6.212e-11, Loss_r: 1.228e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.213e-05, Loss_0: 4.891e-11, Loss_r: 1.213e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.199e-05, Loss_0: 3.953e-11, Loss_r: 1.199e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.187e-05, Loss_0: 4.115e-11, Loss_r: 1.187e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 1.175e-05, Loss_0: 3.037e-11, Loss_r: 1.175e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 1.164e-05, Loss_0: 3.085e-12, Loss_r: 1.164e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 1.163e-05, Loss_0: 1.309e-09, Loss_r: 1.161e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 2.977e-05, Loss_0: 3.230e-07, Loss_r: 2.654e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 1.178e-05, Loss_0: 6.977e-09, Loss_r: 1.171e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 1.281e-05, Loss_0: 3.015e-08, Loss_r: 1.251e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 1.213e-05, Loss_0: 1.899e-08, Loss_r: 1.194e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.111e-05, Loss_0: 2.045e-10, Loss_r: 1.111e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 1.115e-05, Loss_0: 1.954e-09, Loss_r: 1.113e-05, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 1.094e-05, Loss_0: 5.106e-10, Loss_r: 1.094e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.749e-05, Loss_0: 4.405e-10, Loss_r: 1.749e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 1.585e-05, Loss_0: 2.602e-10, Loss_r: 1.585e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 1.471e-05, Loss_0: 5.745e-10, Loss_r: 1.471e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.386e-05, Loss_0: 2.934e-11, Loss_r: 1.386e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.316e-05, Loss_0: 3.915e-10, Loss_r: 1.316e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 1.261e-05, Loss_0: 3.325e-10, Loss_r: 1.261e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 1.218e-05, Loss_0: 2.290e-10, Loss_r: 1.218e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 1.184e-05, Loss_0: 1.430e-10, Loss_r: 1.184e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 1.159e-05, Loss_0: 4.602e-11, Loss_r: 1.159e-05, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 1.307e-05, Loss_0: 2.767e-08, Loss_r: 1.279e-05, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 2.580e-04, Loss_0: 4.637e-06, Loss_r: 2.116e-04, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 3.267e-05, Loss_0: 3.934e-07, Loss_r: 2.874e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 1.369e-05, Loss_0: 6.016e-08, Loss_r: 1.309e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 2.279e-05, Loss_0: 2.380e-07, Loss_r: 2.041e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 1.428e-05, Loss_0: 7.106e-08, Loss_r: 1.357e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 1.088e-05, Loss_0: 2.260e-09, Loss_r: 1.086e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 1.127e-05, Loss_0: 9.757e-09, Loss_r: 1.117e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 1.056e-05, Loss_0: 1.527e-09, Loss_r: 1.054e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 1.049e-05, Loss_0: 1.802e-09, Loss_r: 1.047e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 1.040e-05, Loss_0: 1.768e-10, Loss_r: 1.040e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 1.689e-05, Loss_0: 2.532e-12, Loss_r: 1.689e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 1.535e-05, Loss_0: 5.411e-10, Loss_r: 1.534e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 1.468e-05, Loss_0: 2.390e-14, Loss_r: 1.468e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 1.428e-05, Loss_0: 2.277e-10, Loss_r: 1.428e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 1.392e-05, Loss_0: 2.120e-11, Loss_r: 1.392e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 1.362e-05, Loss_0: 9.663e-11, Loss_r: 1.362e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 1.338e-05, Loss_0: 6.619e-11, Loss_r: 1.338e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 1.319e-05, Loss_0: 5.458e-11, Loss_r: 1.319e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 1.302e-05, Loss_0: 7.479e-11, Loss_r: 1.302e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 1.288e-05, Loss_0: 9.367e-11, Loss_r: 1.288e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 1.276e-05, Loss_0: 1.133e-10, Loss_r: 1.276e-05, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 1.266e-05, Loss_0: 2.292e-10, Loss_r: 1.266e-05, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 1.264e-05, Loss_0: 2.113e-09, Loss_r: 1.262e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 1.865e-05, Loss_0: 1.135e-07, Loss_r: 1.751e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 1.305e-05, Loss_0: 1.352e-08, Loss_r: 1.292e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 1.239e-05, Loss_0: 6.568e-10, Loss_r: 1.239e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 1.257e-05, Loss_0: 4.190e-09, Loss_r: 1.253e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 1.225e-05, Loss_0: 5.114e-10, Loss_r: 1.224e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 1.214e-05, Loss_0: 6.587e-10, Loss_r: 1.213e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 1.207e-05, Loss_0: 5.674e-10, Loss_r: 1.207e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 1.817e-05, Loss_0: 9.704e-10, Loss_r: 1.816e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4220, Loss: 1.567e-05, Loss_0: 4.133e-11, Loss_r: 1.567e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 1.439e-05, Loss_0: 1.573e-11, Loss_r: 1.439e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 1.340e-05, Loss_0: 1.603e-10, Loss_r: 1.340e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 1.247e-05, Loss_0: 2.733e-14, Loss_r: 1.247e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.167e-05, Loss_0: 1.056e-11, Loss_r: 1.167e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 1.100e-05, Loss_0: 6.169e-12, Loss_r: 1.100e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 1.041e-05, Loss_0: 4.907e-12, Loss_r: 1.041e-05, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 9.892e-06, Loss_0: 6.682e-12, Loss_r: 9.892e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 9.431e-06, Loss_0: 7.623e-12, Loss_r: 9.431e-06, Time: 0.07, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 9.017e-06, Loss_0: 4.705e-12, Loss_r: 9.017e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 8.641e-06, Loss_0: 7.421e-12, Loss_r: 8.641e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 8.297e-06, Loss_0: 8.571e-12, Loss_r: 8.297e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 7.981e-06, Loss_0: 5.794e-12, Loss_r: 7.980e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 7.686e-06, Loss_0: 2.884e-12, Loss_r: 7.686e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 7.411e-06, Loss_0: 1.800e-13, Loss_r: 7.411e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 7.156e-06, Loss_0: 2.302e-11, Loss_r: 7.155e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 7.011e-06, Loss_0: 1.360e-09, Loss_r: 6.998e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 1.358e-05, Loss_0: 1.000e-07, Loss_r: 1.258e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 7.390e-06, Loss_0: 1.294e-08, Loss_r: 7.260e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 6.047e-06, Loss_0: 6.225e-10, Loss_r: 6.041e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 6.197e-06, Loss_0: 4.798e-09, Loss_r: 6.149e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 5.823e-06, Loss_0: 1.168e-09, Loss_r: 5.812e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 5.632e-06, Loss_0: 1.386e-10, Loss_r: 5.630e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 5.520e-06, Loss_0: 1.967e-10, Loss_r: 5.518e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 5.394e-06, Loss_0: 3.505e-11, Loss_r: 5.394e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 5.284e-06, Loss_0: 1.749e-11, Loss_r: 5.284e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 5.180e-06, Loss_0: 1.010e-11, Loss_r: 5.179e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 5.078e-06, Loss_0: 4.077e-12, Loss_r: 5.078e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 4.980e-06, Loss_0: 5.551e-17, Loss_r: 4.980e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 4.886e-06, Loss_0: 1.672e-13, Loss_r: 4.886e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 4.795e-06, Loss_0: 6.488e-14, Loss_r: 4.795e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 4.707e-06, Loss_0: 9.909e-14, Loss_r: 4.707e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 4.622e-06, Loss_0: 4.011e-15, Loss_r: 4.622e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 4.540e-06, Loss_0: 1.063e-13, Loss_r: 4.540e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 4.461e-06, Loss_0: 3.082e-14, Loss_r: 4.461e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 4.384e-06, Loss_0: 1.851e-13, Loss_r: 4.384e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 4.310e-06, Loss_0: 1.479e-13, Loss_r: 4.310e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 4.239e-06, Loss_0: 8.010e-13, Loss_r: 4.239e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 4.172e-06, Loss_0: 2.154e-11, Loss_r: 4.171e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 9.256e-05, Loss_0: 6.362e-07, Loss_r: 8.620e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 3.672e-05, Loss_0: 5.782e-08, Loss_r: 3.614e-05, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 3.226e-05, Loss_0: 5.479e-08, Loss_r: 3.171e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 3.050e-05, Loss_0: 7.515e-08, Loss_r: 2.975e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 2.576e-05, Loss_0: 7.499e-11, Loss_r: 2.576e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 2.438e-05, Loss_0: 6.844e-10, Loss_r: 2.438e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 2.322e-05, Loss_0: 7.685e-09, Loss_r: 2.314e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 2.223e-05, Loss_0: 2.579e-12, Loss_r: 2.223e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 2.150e-05, Loss_0: 2.274e-09, Loss_r: 2.147e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 2.092e-05, Loss_0: 1.761e-10, Loss_r: 2.092e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 2.043e-05, Loss_0: 1.030e-09, Loss_r: 2.042e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 2.003e-05, Loss_0: 1.029e-09, Loss_r: 2.002e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.968e-05, Loss_0: 7.550e-10, Loss_r: 1.967e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.936e-05, Loss_0: 7.964e-10, Loss_r: 1.936e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.911e-05, Loss_0: 1.941e-09, Loss_r: 1.909e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 2.085e-05, Loss_0: 4.253e-08, Loss_r: 2.043e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 3.703e-04, Loss_0: 5.866e-06, Loss_r: 3.116e-04, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 9.057e-05, Loss_0: 1.199e-06, Loss_r: 7.858e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 3.152e-05, Loss_0: 2.208e-07, Loss_r: 2.931e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 1.896e-05, Loss_0: 1.980e-08, Loss_r: 1.876e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 7.723e-05, Loss_0: 5.133e-09, Loss_r: 7.718e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 7.156e-05, Loss_0: 1.418e-09, Loss_r: 7.155e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 6.510e-05, Loss_0: 1.223e-09, Loss_r: 6.508e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 6.061e-05, Loss_0: 1.805e-08, Loss_r: 6.043e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 5.699e-05, Loss_0: 6.818e-09, Loss_r: 5.692e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 5.413e-05, Loss_0: 1.476e-09, Loss_r: 5.412e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 5.175e-05, Loss_0: 6.406e-09, Loss_r: 5.168e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 4.973e-05, Loss_0: 2.634e-09, Loss_r: 4.970e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 4.798e-05, Loss_0: 3.754e-09, Loss_r: 4.795e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 4.645e-05, Loss_0: 4.060e-09, Loss_r: 4.641e-05, Time: 0.04, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 4.506e-05, Loss_0: 3.683e-09, Loss_r: 4.503e-05, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 4.381e-05, Loss_0: 4.377e-09, Loss_r: 4.376e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 4.292e-05, Loss_0: 1.550e-08, Loss_r: 4.277e-05, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 7.538e-05, Loss_0: 6.883e-07, Loss_r: 6.850e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 4.263e-05, Loss_0: 5.683e-08, Loss_r: 4.206e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 4.136e-05, Loss_0: 1.402e-08, Loss_r: 4.122e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 4.102e-05, Loss_0: 2.059e-08, Loss_r: 4.081e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 3.820e-05, Loss_0: 1.366e-09, Loss_r: 3.819e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 3.773e-05, Loss_0: 1.336e-08, Loss_r: 3.760e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "Training time: 24.6180\n",
            "[1, 64, 64, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 6.764e-01, Loss_0: 7.874e-04, Loss_r: 6.756e-01, Time: 0.71, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.959e-01, Loss_0: 1.203e-03, Loss_r: 1.839e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.988e-01, Loss_0: 1.867e-04, Loss_r: 1.969e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.931e-01, Loss_0: 2.945e-03, Loss_r: 1.637e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.898e-01, Loss_0: 1.312e-03, Loss_r: 1.767e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.877e-01, Loss_0: 1.291e-03, Loss_r: 1.748e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.858e-01, Loss_0: 1.430e-03, Loss_r: 1.715e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.838e-01, Loss_0: 1.478e-03, Loss_r: 1.691e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.811e-01, Loss_0: 1.331e-03, Loss_r: 1.678e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.766e-01, Loss_0: 1.366e-03, Loss_r: 1.629e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.687e-01, Loss_0: 1.300e-03, Loss_r: 1.557e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.543e-01, Loss_0: 1.221e-03, Loss_r: 1.421e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.285e-01, Loss_0: 1.047e-03, Loss_r: 1.180e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 130, Loss: 9.016e-02, Loss_0: 6.754e-04, Loss_r: 8.341e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.823e-02, Loss_0: 2.604e-04, Loss_r: 5.562e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 4.913e-02, Loss_0: 9.240e-05, Loss_r: 4.821e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 4.156e-02, Loss_0: 7.473e-05, Loss_r: 4.082e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 170, Loss: 3.367e-02, Loss_0: 8.746e-05, Loss_r: 3.280e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.778e-02, Loss_0: 7.448e-05, Loss_r: 2.704e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 2.159e-02, Loss_0: 4.322e-05, Loss_r: 2.116e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 200, Loss: 1.629e-02, Loss_0: 2.518e-05, Loss_r: 1.604e-02, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 210, Loss: 1.278e-02, Loss_0: 1.822e-05, Loss_r: 1.259e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 9.745e-03, Loss_0: 1.127e-05, Loss_r: 9.632e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 9.188e-03, Loss_0: 2.237e-05, Loss_r: 8.965e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 6.408e-03, Loss_0: 4.624e-06, Loss_r: 6.362e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.704e-03, Loss_0: 9.752e-07, Loss_r: 5.694e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 4.803e-03, Loss_0: 2.007e-06, Loss_r: 4.783e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 4.180e-03, Loss_0: 1.793e-06, Loss_r: 4.162e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.631e-03, Loss_0: 9.007e-07, Loss_r: 3.622e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.110e-03, Loss_0: 1.362e-06, Loss_r: 3.096e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 300, Loss: 2.664e-03, Loss_0: 3.527e-07, Loss_r: 2.661e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.159e-03, Loss_0: 8.932e-06, Loss_r: 4.069e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.219e-03, Loss_0: 1.080e-06, Loss_r: 2.208e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.699e-03, Loss_0: 1.310e-07, Loss_r: 1.697e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.333e-03, Loss_0: 1.519e-07, Loss_r: 1.331e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.141e-03, Loss_0: 1.430e-06, Loss_r: 1.127e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 9.160e-04, Loss_0: 1.362e-08, Loss_r: 9.159e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 7.692e-04, Loss_0: 9.936e-07, Loss_r: 7.593e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.135e-03, Loss_0: 1.005e-05, Loss_r: 1.034e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 9.758e-04, Loss_0: 6.343e-06, Loss_r: 9.124e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.143e-04, Loss_0: 2.541e-06, Loss_r: 4.889e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 1.630e-04, Loss_0: 1.808e-06, Loss_r: 1.449e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.040e-04, Loss_0: 5.745e-07, Loss_r: 9.829e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 6.632e-05, Loss_0: 3.613e-07, Loss_r: 6.271e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.885e-05, Loss_0: 2.033e-07, Loss_r: 5.682e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.672e-05, Loss_0: 2.753e-07, Loss_r: 5.396e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.493e-04, Loss_0: 7.818e-06, Loss_r: 3.711e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 470, Loss: 6.931e-05, Loss_0: 7.455e-07, Loss_r: 6.186e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 3.934e-04, Loss_0: 7.039e-06, Loss_r: 3.230e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.833e-05, Loss_0: 3.487e-07, Loss_r: 4.484e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 8.182e-05, Loss_0: 1.145e-06, Loss_r: 7.037e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.185e-05, Loss_0: 1.085e-07, Loss_r: 3.077e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.559e-05, Loss_0: 3.434e-08, Loss_r: 2.524e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.720e-05, Loss_0: 7.341e-08, Loss_r: 2.647e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.226e-05, Loss_0: 9.709e-09, Loss_r: 2.216e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 3.232e-05, Loss_0: 2.554e-07, Loss_r: 2.976e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.065e-03, Loss_0: 2.194e-05, Loss_r: 8.453e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.301e-03, Loss_0: 2.694e-05, Loss_r: 1.031e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.582e-04, Loss_0: 2.866e-06, Loss_r: 1.295e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 2.010e-05, Loss_0: 7.619e-09, Loss_r: 2.003e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 4.226e-05, Loss_0: 5.206e-07, Loss_r: 3.705e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 5.975e-05, Loss_0: 6.140e-07, Loss_r: 5.361e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 3.168e-05, Loss_0: 3.441e-10, Loss_r: 3.168e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 630, Loss: 3.322e-05, Loss_0: 5.561e-08, Loss_r: 3.267e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 3.012e-05, Loss_0: 5.003e-08, Loss_r: 2.962e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.785e-05, Loss_0: 1.059e-08, Loss_r: 2.775e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.599e-05, Loss_0: 6.018e-09, Loss_r: 2.593e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.534e-05, Loss_0: 1.945e-08, Loss_r: 2.514e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 3.098e-05, Loss_0: 1.867e-07, Loss_r: 2.911e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 7.017e-04, Loss_0: 1.585e-05, Loss_r: 5.432e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 700, Loss: 5.948e-04, Loss_0: 1.267e-05, Loss_r: 4.680e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 2.854e-05, Loss_0: 1.614e-07, Loss_r: 2.693e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 6.190e-05, Loss_0: 9.670e-07, Loss_r: 5.223e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 730, Loss: 7.022e-05, Loss_0: 1.176e-06, Loss_r: 5.846e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.372e-05, Loss_0: 8.014e-07, Loss_r: 4.571e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 2.474e-05, Loss_0: 1.167e-07, Loss_r: 2.358e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 2.183e-05, Loss_0: 4.133e-08, Loss_r: 2.141e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.956e-05, Loss_0: 9.315e-09, Loss_r: 1.947e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.903e-05, Loss_0: 2.227e-08, Loss_r: 1.881e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.797e-05, Loss_0: 3.905e-09, Loss_r: 1.793e-05, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.722e-05, Loss_0: 5.535e-09, Loss_r: 1.716e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.162e-05, Loss_0: 4.144e-10, Loss_r: 1.162e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 1.129e-05, Loss_0: 2.928e-09, Loss_r: 1.126e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 830, Loss: 1.133e-05, Loss_0: 1.242e-08, Loss_r: 1.121e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.758e-05, Loss_0: 4.106e-07, Loss_r: 2.347e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.772e-03, Loss_0: 4.313e-05, Loss_r: 1.341e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.758e-03, Loss_0: 4.175e-05, Loss_r: 1.341e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 870, Loss: 2.976e-04, Loss_0: 6.972e-06, Loss_r: 2.279e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 880, Loss: 4.030e-05, Loss_0: 7.479e-07, Loss_r: 3.282e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.075e-05, Loss_0: 8.783e-09, Loss_r: 1.066e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 900, Loss: 2.351e-05, Loss_0: 3.128e-07, Loss_r: 2.038e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 910, Loss: 2.011e-05, Loss_0: 2.400e-07, Loss_r: 1.771e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 920, Loss: 9.616e-06, Loss_0: 1.070e-09, Loss_r: 9.606e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.077e-05, Loss_0: 4.025e-08, Loss_r: 1.037e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 940, Loss: 9.116e-06, Loss_0: 1.028e-09, Loss_r: 9.105e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 950, Loss: 8.829e-06, Loss_0: 1.950e-10, Loss_r: 8.827e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 960, Loss: 8.602e-06, Loss_0: 1.328e-09, Loss_r: 8.589e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 970, Loss: 8.355e-06, Loss_0: 4.137e-12, Loss_r: 8.355e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 980, Loss: 8.145e-06, Loss_0: 2.260e-11, Loss_r: 8.145e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 990, Loss: 7.933e-06, Loss_0: 3.997e-10, Loss_r: 7.929e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 7.757e-06, Loss_0: 1.373e-09, Loss_r: 7.743e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 5.098e-04, Loss_0: 7.741e-06, Loss_r: 4.324e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.505e-03, Loss_0: 3.381e-05, Loss_r: 1.166e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 7.421e-04, Loss_0: 1.607e-05, Loss_r: 5.814e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.164e-04, Loss_0: 3.414e-07, Loss_r: 1.130e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 2.193e-04, Loss_0: 4.097e-06, Loss_r: 1.783e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 8.471e-05, Loss_0: 6.049e-08, Loss_r: 8.410e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 9.158e-05, Loss_0: 3.861e-07, Loss_r: 8.772e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 7.616e-05, Loss_0: 3.572e-07, Loss_r: 7.259e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 6.583e-05, Loss_0: 7.309e-08, Loss_r: 6.510e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 5.768e-05, Loss_0: 3.092e-08, Loss_r: 5.737e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 5.558e-05, Loss_0: 1.232e-07, Loss_r: 5.435e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 7.152e-05, Loss_0: 8.749e-07, Loss_r: 6.277e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 1.289e-03, Loss_0: 4.370e-05, Loss_r: 8.524e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 6.769e-04, Loss_0: 1.798e-05, Loss_r: 4.972e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.759e-04, Loss_0: 7.616e-06, Loss_r: 1.998e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.857e-04, Loss_0: 7.746e-06, Loss_r: 2.082e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 6.411e-05, Loss_0: 7.588e-07, Loss_r: 5.652e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 5.606e-05, Loss_0: 4.593e-07, Loss_r: 5.146e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 4.265e-05, Loss_0: 1.323e-07, Loss_r: 4.132e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 3.972e-05, Loss_0: 1.836e-07, Loss_r: 3.788e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 3.606e-05, Loss_0: 9.089e-08, Loss_r: 3.515e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.266e-05, Loss_0: 3.755e-08, Loss_r: 2.229e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 1.650e-05, Loss_0: 1.469e-08, Loss_r: 1.635e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.240e-05, Loss_0: 5.361e-09, Loss_r: 1.235e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 9.186e-06, Loss_0: 5.484e-10, Loss_r: 9.181e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 7.110e-06, Loss_0: 1.384e-09, Loss_r: 7.096e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 5.628e-06, Loss_0: 3.540e-10, Loss_r: 5.624e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 4.634e-06, Loss_0: 1.171e-09, Loss_r: 4.622e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 4.903e-06, Loss_0: 2.835e-08, Loss_r: 4.619e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 9.105e-05, Loss_0: 2.373e-06, Loss_r: 6.732e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 5.812e-03, Loss_0: 1.561e-04, Loss_r: 4.251e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.245e-05, Loss_0: 5.353e-07, Loss_r: 1.710e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 8.462e-06, Loss_0: 1.780e-07, Loss_r: 6.683e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 4.028e-06, Loss_0: 6.024e-08, Loss_r: 3.426e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 3.133e-06, Loss_0: 3.764e-08, Loss_r: 2.756e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 4.126e-06, Loss_0: 6.582e-08, Loss_r: 3.468e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 5.225e-06, Loss_0: 9.665e-08, Loss_r: 4.259e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 4.591e-06, Loss_0: 8.056e-08, Loss_r: 3.786e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.541e-06, Loss_0: 2.595e-08, Loss_r: 2.281e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.552e-06, Loss_0: 5.579e-11, Loss_r: 1.551e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 5.927e-06, Loss_0: 9.727e-11, Loss_r: 5.926e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 4.589e-06, Loss_0: 3.135e-10, Loss_r: 4.586e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.951e-06, Loss_0: 2.529e-12, Loss_r: 3.951e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 3.544e-06, Loss_0: 8.617e-12, Loss_r: 3.544e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 3.235e-06, Loss_0: 7.006e-11, Loss_r: 3.234e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 3.026e-06, Loss_0: 3.549e-11, Loss_r: 3.025e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 2.889e-06, Loss_0: 4.074e-11, Loss_r: 2.889e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.795e-06, Loss_0: 2.080e-11, Loss_r: 2.795e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 2.727e-06, Loss_0: 2.293e-11, Loss_r: 2.727e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 2.676e-06, Loss_0: 2.404e-11, Loss_r: 2.676e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.636e-06, Loss_0: 2.730e-11, Loss_r: 2.636e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.604e-06, Loss_0: 5.268e-11, Loss_r: 2.603e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.596e-06, Loss_0: 8.533e-10, Loss_r: 2.588e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 6.310e-06, Loss_0: 1.131e-07, Loss_r: 5.179e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 1.382e-03, Loss_0: 4.043e-05, Loss_r: 9.777e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 4.158e-03, Loss_0: 1.166e-04, Loss_r: 2.991e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 1.369e-03, Loss_0: 3.886e-05, Loss_r: 9.800e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.099e-05, Loss_0: 1.845e-07, Loss_r: 9.148e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.826e-04, Loss_0: 5.111e-06, Loss_r: 1.315e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 5.179e-06, Loss_0: 1.688e-08, Loss_r: 5.010e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.062e-05, Loss_0: 5.308e-07, Loss_r: 1.532e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 4.997e-06, Loss_0: 9.773e-08, Loss_r: 4.019e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.856e-06, Loss_0: 6.982e-09, Loss_r: 1.786e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 2.259e-06, Loss_0: 2.190e-08, Loss_r: 2.040e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.586e-06, Loss_0: 7.590e-09, Loss_r: 1.510e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.251e-06, Loss_0: 9.411e-10, Loss_r: 1.241e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.154e-06, Loss_0: 3.459e-11, Loss_r: 1.153e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 1.106e-06, Loss_0: 2.130e-12, Loss_r: 1.106e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.071e-06, Loss_0: 1.739e-12, Loss_r: 1.071e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 1.043e-06, Loss_0: 1.140e-13, Loss_r: 1.043e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 1.020e-06, Loss_0: 2.728e-12, Loss_r: 1.020e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 1.000e-06, Loss_0: 5.120e-12, Loss_r: 1.000e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 9.834e-07, Loss_0: 3.943e-12, Loss_r: 9.833e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 9.680e-07, Loss_0: 7.429e-13, Loss_r: 9.680e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 9.539e-07, Loss_0: 5.413e-15, Loss_r: 9.539e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 9.407e-07, Loss_0: 8.470e-14, Loss_r: 9.407e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 9.283e-07, Loss_0: 1.399e-14, Loss_r: 9.283e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 9.163e-07, Loss_0: 3.133e-13, Loss_r: 9.163e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 9.049e-07, Loss_0: 2.341e-13, Loss_r: 9.049e-07, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 8.939e-07, Loss_0: 1.017e-13, Loss_r: 8.939e-07, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 8.456e-05, Loss_0: 6.625e-08, Loss_r: 8.390e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 6.154e-05, Loss_0: 2.758e-08, Loss_r: 6.126e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.891e-05, Loss_0: 2.962e-08, Loss_r: 4.862e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 4.412e-05, Loss_0: 1.584e-07, Loss_r: 4.254e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 2.911e-04, Loss_0: 8.743e-06, Loss_r: 2.037e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 4.164e-03, Loss_0: 1.281e-04, Loss_r: 2.883e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 6.916e-04, Loss_0: 2.068e-05, Loss_r: 4.849e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 6.333e-05, Loss_0: 1.040e-06, Loss_r: 5.293e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 3.035e-05, Loss_0: 9.914e-08, Loss_r: 2.936e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 4.755e-05, Loss_0: 8.119e-07, Loss_r: 3.943e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.495e-05, Loss_0: 8.171e-07, Loss_r: 3.677e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 2.512e-05, Loss_0: 1.590e-07, Loss_r: 2.353e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 2.067e-05, Loss_0: 2.034e-08, Loss_r: 2.046e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.919e-05, Loss_0: 1.876e-08, Loss_r: 1.901e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.751e-05, Loss_0: 1.664e-08, Loss_r: 1.734e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.615e-05, Loss_0: 3.864e-10, Loss_r: 1.615e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.521e-05, Loss_0: 3.081e-11, Loss_r: 1.521e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.435e-05, Loss_0: 1.417e-09, Loss_r: 1.434e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.358e-05, Loss_0: 3.795e-10, Loss_r: 1.357e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.288e-05, Loss_0: 1.173e-10, Loss_r: 1.288e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.205e-05, Loss_0: 5.935e-08, Loss_r: 2.145e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.721e-05, Loss_0: 2.281e-10, Loss_r: 1.721e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 1.606e-05, Loss_0: 7.082e-09, Loss_r: 1.599e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.706e-05, Loss_0: 7.003e-08, Loss_r: 1.636e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 7.248e-05, Loss_0: 1.962e-06, Loss_r: 5.286e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 2.681e-03, Loss_0: 8.915e-05, Loss_r: 1.789e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.425e-03, Loss_0: 4.601e-05, Loss_r: 9.653e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 2.883e-05, Loss_0: 4.110e-07, Loss_r: 2.472e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 1.886e-04, Loss_0: 5.589e-06, Loss_r: 1.327e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 3.912e-05, Loss_0: 7.640e-07, Loss_r: 3.148e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 2.595e-05, Loss_0: 3.891e-07, Loss_r: 2.205e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.909e-05, Loss_0: 1.670e-07, Loss_r: 1.742e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.696e-05, Loss_0: 7.828e-08, Loss_r: 1.618e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.406e-05, Loss_0: 1.960e-09, Loss_r: 1.404e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.393e-05, Loss_0: 6.117e-09, Loss_r: 1.386e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.370e-05, Loss_0: 1.736e-09, Loss_r: 1.368e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.342e-05, Loss_0: 2.440e-09, Loss_r: 1.340e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.319e-05, Loss_0: 6.603e-11, Loss_r: 1.319e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 1.301e-05, Loss_0: 1.337e-12, Loss_r: 1.301e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 1.282e-05, Loss_0: 5.875e-10, Loss_r: 1.281e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 1.368e-05, Loss_0: 1.890e-07, Loss_r: 1.179e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 4.937e-05, Loss_0: 1.379e-06, Loss_r: 3.558e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 1.916e-05, Loss_0: 4.277e-07, Loss_r: 1.489e-05, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 8.234e-06, Loss_0: 9.370e-08, Loss_r: 7.297e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 5.100e-06, Loss_0: 5.176e-09, Loss_r: 5.049e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 4.838e-06, Loss_0: 3.734e-09, Loss_r: 4.800e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 4.817e-06, Loss_0: 7.994e-09, Loss_r: 4.737e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 4.487e-06, Loss_0: 1.577e-09, Loss_r: 4.471e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 4.333e-06, Loss_0: 2.612e-10, Loss_r: 4.331e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 4.241e-06, Loss_0: 4.119e-10, Loss_r: 4.237e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 4.141e-06, Loss_0: 2.529e-11, Loss_r: 4.141e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 4.057e-06, Loss_0: 1.298e-11, Loss_r: 4.057e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 3.978e-06, Loss_0: 3.663e-11, Loss_r: 3.977e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 3.902e-06, Loss_0: 4.159e-13, Loss_r: 3.902e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 3.830e-06, Loss_0: 5.589e-12, Loss_r: 3.830e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 3.762e-06, Loss_0: 5.896e-13, Loss_r: 3.762e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 3.697e-06, Loss_0: 2.739e-12, Loss_r: 3.697e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 3.635e-06, Loss_0: 1.139e-12, Loss_r: 3.635e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 3.576e-06, Loss_0: 3.181e-12, Loss_r: 3.576e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 3.519e-06, Loss_0: 4.586e-12, Loss_r: 3.519e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 7.663e-06, Loss_0: 1.078e-07, Loss_r: 6.585e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 4.828e-06, Loss_0: 3.589e-08, Loss_r: 4.469e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 3.736e-06, Loss_0: 1.018e-08, Loss_r: 3.634e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 3.281e-06, Loss_0: 1.102e-09, Loss_r: 3.270e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 3.118e-06, Loss_0: 1.105e-10, Loss_r: 3.117e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 3.063e-06, Loss_0: 7.509e-10, Loss_r: 3.056e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 3.010e-06, Loss_0: 3.256e-10, Loss_r: 3.007e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 2.974e-06, Loss_0: 1.538e-12, Loss_r: 2.974e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.950e-06, Loss_0: 4.572e-12, Loss_r: 2.950e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 2.927e-06, Loss_0: 2.549e-11, Loss_r: 2.927e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 2.908e-06, Loss_0: 2.233e-11, Loss_r: 2.907e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 2.889e-06, Loss_0: 3.249e-12, Loss_r: 2.889e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 2.870e-06, Loss_0: 2.007e-11, Loss_r: 2.870e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 2.852e-06, Loss_0: 6.415e-12, Loss_r: 2.852e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 2.834e-06, Loss_0: 1.478e-11, Loss_r: 2.834e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 2.816e-06, Loss_0: 9.859e-12, Loss_r: 2.816e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 2.798e-06, Loss_0: 7.048e-12, Loss_r: 2.798e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 2.781e-06, Loss_0: 9.558e-12, Loss_r: 2.781e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 2.763e-06, Loss_0: 1.226e-11, Loss_r: 2.763e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 2.746e-06, Loss_0: 6.824e-12, Loss_r: 2.746e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 6.011e-05, Loss_0: 6.772e-07, Loss_r: 5.333e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 3.597e-05, Loss_0: 1.178e-07, Loss_r: 3.479e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.818e-05, Loss_0: 9.712e-10, Loss_r: 2.817e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 2.697e-05, Loss_0: 4.744e-08, Loss_r: 2.650e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 2.471e-05, Loss_0: 1.855e-08, Loss_r: 2.452e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 2.324e-05, Loss_0: 4.076e-10, Loss_r: 2.323e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 2.214e-05, Loss_0: 3.368e-11, Loss_r: 2.214e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 2.123e-05, Loss_0: 3.892e-09, Loss_r: 2.119e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 2.039e-05, Loss_0: 1.350e-10, Loss_r: 2.038e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.962e-05, Loss_0: 1.280e-09, Loss_r: 1.961e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.892e-05, Loss_0: 3.933e-10, Loss_r: 1.892e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.828e-05, Loss_0: 9.812e-10, Loss_r: 1.827e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.767e-05, Loss_0: 5.510e-10, Loss_r: 1.767e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.712e-05, Loss_0: 4.345e-10, Loss_r: 1.711e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.660e-05, Loss_0: 3.346e-10, Loss_r: 1.659e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.612e-05, Loss_0: 4.976e-11, Loss_r: 1.612e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 1.593e-05, Loss_0: 6.084e-09, Loss_r: 1.587e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 4.599e-05, Loss_0: 1.089e-06, Loss_r: 3.510e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.662e-05, Loss_0: 5.407e-08, Loss_r: 1.608e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.609e-05, Loss_0: 6.673e-08, Loss_r: 1.542e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.344e-05, Loss_0: 5.353e-08, Loss_r: 1.291e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 9.936e-06, Loss_0: 2.185e-11, Loss_r: 9.935e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 8.674e-06, Loss_0: 8.690e-09, Loss_r: 8.587e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 7.358e-06, Loss_0: 4.300e-11, Loss_r: 7.358e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 6.575e-06, Loss_0: 8.412e-10, Loss_r: 6.566e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 5.968e-06, Loss_0: 1.877e-10, Loss_r: 5.966e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 5.521e-06, Loss_0: 8.462e-12, Loss_r: 5.521e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 5.168e-06, Loss_0: 8.119e-12, Loss_r: 5.168e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 4.871e-06, Loss_0: 3.652e-11, Loss_r: 4.870e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 4.611e-06, Loss_0: 5.726e-13, Loss_r: 4.611e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 4.376e-06, Loss_0: 7.421e-12, Loss_r: 4.376e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 4.160e-06, Loss_0: 1.224e-12, Loss_r: 4.160e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 3.959e-06, Loss_0: 1.067e-12, Loss_r: 3.959e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 3.771e-06, Loss_0: 2.344e-12, Loss_r: 3.771e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 3.592e-06, Loss_0: 1.896e-12, Loss_r: 3.592e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 3.424e-06, Loss_0: 6.363e-13, Loss_r: 3.424e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 3.264e-06, Loss_0: 1.817e-12, Loss_r: 3.264e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 3.116e-06, Loss_0: 1.203e-10, Loss_r: 3.115e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 3.317e-06, Loss_0: 1.143e-08, Loss_r: 3.202e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 5.690e-05, Loss_0: 1.812e-06, Loss_r: 3.878e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.620e-05, Loss_0: 1.383e-07, Loss_r: 1.482e-05, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.007e-05, Loss_0: 6.786e-08, Loss_r: 9.395e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 8.187e-06, Loss_0: 9.286e-08, Loss_r: 7.259e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 3.885e-06, Loss_0: 2.545e-09, Loss_r: 3.860e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 3.251e-06, Loss_0: 1.493e-08, Loss_r: 3.102e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 2.534e-06, Loss_0: 2.629e-09, Loss_r: 2.508e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 2.193e-06, Loss_0: 2.173e-10, Loss_r: 2.191e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 2.052e-06, Loss_0: 2.224e-10, Loss_r: 2.050e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.964e-06, Loss_0: 1.461e-12, Loss_r: 1.964e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.900e-06, Loss_0: 4.622e-11, Loss_r: 1.900e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.851e-06, Loss_0: 7.844e-11, Loss_r: 1.850e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 1.809e-06, Loss_0: 2.432e-13, Loss_r: 1.809e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 1.770e-06, Loss_0: 1.180e-11, Loss_r: 1.769e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 1.734e-06, Loss_0: 2.250e-11, Loss_r: 1.734e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 1.702e-06, Loss_0: 3.837e-11, Loss_r: 1.702e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 1.674e-06, Loss_0: 1.900e-10, Loss_r: 1.672e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 1.786e-06, Loss_0: 5.938e-09, Loss_r: 1.726e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 2.022e-05, Loss_0: 7.051e-07, Loss_r: 1.317e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 2.579e-06, Loss_0: 3.834e-08, Loss_r: 2.195e-06, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 2.494e-06, Loss_0: 3.317e-08, Loss_r: 2.162e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 7.475e-06, Loss_0: 6.930e-08, Loss_r: 6.782e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 3.686e-06, Loss_0: 4.260e-09, Loss_r: 3.643e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 2.923e-06, Loss_0: 6.936e-09, Loss_r: 2.854e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 2.196e-06, Loss_0: 7.846e-10, Loss_r: 2.189e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 1.830e-06, Loss_0: 1.218e-09, Loss_r: 1.818e-06, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 1.600e-06, Loss_0: 1.957e-13, Loss_r: 1.600e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.482e-06, Loss_0: 3.922e-11, Loss_r: 1.482e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.401e-06, Loss_0: 6.860e-11, Loss_r: 1.401e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.343e-06, Loss_0: 1.317e-11, Loss_r: 1.342e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.297e-06, Loss_0: 1.358e-11, Loss_r: 1.297e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 1.258e-06, Loss_0: 1.545e-12, Loss_r: 1.258e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 1.224e-06, Loss_0: 1.238e-12, Loss_r: 1.224e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 1.192e-06, Loss_0: 3.459e-13, Loss_r: 1.192e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 1.163e-06, Loss_0: 1.133e-13, Loss_r: 1.163e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 1.136e-06, Loss_0: 6.068e-14, Loss_r: 1.136e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 1.110e-06, Loss_0: 3.497e-15, Loss_r: 1.110e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3370, Loss: 1.086e-06, Loss_0: 2.314e-13, Loss_r: 1.086e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 1.064e-06, Loss_0: 5.335e-14, Loss_r: 1.064e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 1.043e-06, Loss_0: 3.555e-12, Loss_r: 1.043e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 1.037e-06, Loss_0: 4.767e-10, Loss_r: 1.032e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 1.190e-05, Loss_0: 3.737e-07, Loss_r: 8.167e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.280e-06, Loss_0: 2.556e-11, Loss_r: 1.279e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 2.884e-06, Loss_0: 5.615e-08, Loss_r: 2.322e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 1.413e-06, Loss_0: 7.485e-09, Loss_r: 1.338e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.336e-06, Loss_0: 6.159e-09, Loss_r: 1.274e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 1.140e-06, Loss_0: 4.429e-10, Loss_r: 1.136e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 1.135e-06, Loss_0: 1.174e-09, Loss_r: 1.123e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 1.081e-06, Loss_0: 2.246e-10, Loss_r: 1.078e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 1.051e-06, Loss_0: 2.508e-12, Loss_r: 1.051e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 1.029e-06, Loss_0: 1.690e-12, Loss_r: 1.029e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 1.008e-06, Loss_0: 1.988e-12, Loss_r: 1.008e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 9.880e-07, Loss_0: 1.978e-14, Loss_r: 9.880e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 9.696e-07, Loss_0: 4.788e-13, Loss_r: 9.696e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 9.521e-07, Loss_0: 5.441e-13, Loss_r: 9.521e-07, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 9.356e-07, Loss_0: 5.143e-15, Loss_r: 9.356e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 9.202e-07, Loss_0: 6.816e-13, Loss_r: 9.202e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 9.056e-07, Loss_0: 2.020e-12, Loss_r: 9.056e-07, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 8.924e-07, Loss_0: 2.405e-11, Loss_r: 8.922e-07, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 9.056e-07, Loss_0: 9.189e-10, Loss_r: 8.964e-07, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 3.374e-06, Loss_0: 8.578e-08, Loss_r: 2.516e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 2.655e-06, Loss_0: 7.782e-09, Loss_r: 2.577e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 2.220e-06, Loss_0: 1.654e-09, Loss_r: 2.204e-06, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 2.178e-06, Loss_0: 5.016e-09, Loss_r: 2.127e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 1.942e-06, Loss_0: 1.015e-09, Loss_r: 1.932e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 1.825e-06, Loss_0: 1.684e-10, Loss_r: 1.824e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.754e-06, Loss_0: 1.888e-10, Loss_r: 1.752e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.688e-06, Loss_0: 4.066e-11, Loss_r: 1.687e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.635e-06, Loss_0: 1.799e-11, Loss_r: 1.634e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.587e-06, Loss_0: 7.403e-12, Loss_r: 1.587e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.544e-06, Loss_0: 5.945e-12, Loss_r: 1.544e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 1.503e-06, Loss_0: 2.390e-14, Loss_r: 1.503e-06, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 1.465e-06, Loss_0: 8.467e-13, Loss_r: 1.465e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 1.428e-06, Loss_0: 3.642e-13, Loss_r: 1.428e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 1.394e-06, Loss_0: 4.478e-13, Loss_r: 1.394e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 1.361e-06, Loss_0: 3.721e-15, Loss_r: 1.361e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 1.330e-06, Loss_0: 3.818e-13, Loss_r: 1.330e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 1.301e-06, Loss_0: 5.733e-13, Loss_r: 1.301e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3780, Loss: 1.272e-06, Loss_0: 1.269e-12, Loss_r: 1.272e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3790, Loss: 1.246e-06, Loss_0: 9.659e-12, Loss_r: 1.246e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3800, Loss: 1.225e-06, Loss_0: 1.762e-10, Loss_r: 1.223e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3810, Loss: 1.149e-05, Loss_0: 3.314e-07, Loss_r: 8.172e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 2.876e-06, Loss_0: 4.402e-08, Loss_r: 2.435e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 1.556e-06, Loss_0: 5.377e-10, Loss_r: 1.550e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 1.858e-06, Loss_0: 1.233e-08, Loss_r: 1.735e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 1.565e-06, Loss_0: 4.759e-09, Loss_r: 1.518e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 1.382e-06, Loss_0: 7.963e-11, Loss_r: 1.382e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 1.376e-06, Loss_0: 8.039e-10, Loss_r: 1.368e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 1.324e-06, Loss_0: 1.128e-11, Loss_r: 1.324e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 1.302e-06, Loss_0: 1.022e-10, Loss_r: 1.301e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 1.278e-06, Loss_0: 2.513e-11, Loss_r: 1.278e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 1.255e-06, Loss_0: 1.845e-12, Loss_r: 1.255e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 1.235e-06, Loss_0: 9.819e-13, Loss_r: 1.235e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 1.215e-06, Loss_0: 2.501e-13, Loss_r: 1.215e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 1.195e-06, Loss_0: 3.217e-13, Loss_r: 1.195e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 1.177e-06, Loss_0: 1.014e-12, Loss_r: 1.177e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 1.159e-06, Loss_0: 1.270e-14, Loss_r: 1.159e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 1.141e-06, Loss_0: 8.606e-14, Loss_r: 1.141e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 1.124e-06, Loss_0: 5.909e-14, Loss_r: 1.124e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 1.108e-06, Loss_0: 2.938e-13, Loss_r: 1.108e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 1.092e-06, Loss_0: 2.748e-14, Loss_r: 1.092e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 3.027e-05, Loss_0: 7.323e-07, Loss_r: 2.294e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 1.125e-05, Loss_0: 1.629e-07, Loss_r: 9.619e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 5.748e-06, Loss_0: 8.376e-09, Loss_r: 5.664e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 5.295e-06, Loss_0: 9.229e-09, Loss_r: 5.202e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 5.227e-06, Loss_0: 1.609e-08, Loss_r: 5.066e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 4.712e-06, Loss_0: 2.312e-09, Loss_r: 4.689e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 4.610e-06, Loss_0: 6.068e-10, Loss_r: 4.604e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 4.529e-06, Loss_0: 1.404e-10, Loss_r: 4.528e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 4.476e-06, Loss_0: 4.482e-10, Loss_r: 4.472e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 4.427e-06, Loss_0: 3.843e-11, Loss_r: 4.426e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 4.387e-06, Loss_0: 2.245e-12, Loss_r: 4.387e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 4.348e-06, Loss_0: 8.213e-11, Loss_r: 4.348e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 4.311e-06, Loss_0: 1.430e-11, Loss_r: 4.311e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 4.276e-06, Loss_0: 4.855e-11, Loss_r: 4.275e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 4.242e-06, Loss_0: 2.847e-11, Loss_r: 4.241e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 4.209e-06, Loss_0: 2.772e-11, Loss_r: 4.208e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 4.177e-06, Loss_0: 2.890e-11, Loss_r: 4.176e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 4.146e-06, Loss_0: 3.362e-11, Loss_r: 4.145e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 4.116e-06, Loss_0: 4.159e-11, Loss_r: 4.115e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 4.087e-06, Loss_0: 6.414e-11, Loss_r: 4.086e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 1.786e-06, Loss_0: 1.348e-08, Loss_r: 1.651e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 3.531e-05, Loss_0: 1.167e-06, Loss_r: 2.364e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4230, Loss: 5.553e-06, Loss_0: 1.516e-07, Loss_r: 4.037e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4240, Loss: 1.070e-06, Loss_0: 2.391e-09, Loss_r: 1.047e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4250, Loss: 2.257e-06, Loss_0: 4.633e-08, Loss_r: 1.794e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4260, Loss: 1.504e-06, Loss_0: 2.111e-08, Loss_r: 1.293e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 8.929e-07, Loss_0: 3.683e-11, Loss_r: 8.925e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 9.582e-07, Loss_0: 2.913e-09, Loss_r: 9.291e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 8.436e-07, Loss_0: 2.688e-11, Loss_r: 8.434e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 8.344e-07, Loss_0: 4.667e-10, Loss_r: 8.297e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 8.087e-07, Loss_0: 6.530e-11, Loss_r: 8.080e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 7.888e-07, Loss_0: 1.735e-11, Loss_r: 7.886e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 7.729e-07, Loss_0: 1.183e-11, Loss_r: 7.728e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 7.582e-07, Loss_0: 8.787e-13, Loss_r: 7.582e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 7.444e-07, Loss_0: 5.616e-12, Loss_r: 7.443e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 7.315e-07, Loss_0: 7.942e-12, Loss_r: 7.314e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 7.194e-07, Loss_0: 1.634e-12, Loss_r: 7.194e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 7.080e-07, Loss_0: 2.232e-12, Loss_r: 7.079e-07, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 6.973e-07, Loss_0: 2.179e-12, Loss_r: 6.973e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 6.873e-07, Loss_0: 2.895e-12, Loss_r: 6.873e-07, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 1.213e-05, Loss_0: 2.526e-07, Loss_r: 9.602e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 5.909e-06, Loss_0: 8.030e-08, Loss_r: 5.106e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 3.612e-06, Loss_0: 2.245e-08, Loss_r: 3.387e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4440, Loss: 2.615e-06, Loss_0: 3.302e-09, Loss_r: 2.582e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4450, Loss: 2.200e-06, Loss_0: 2.085e-12, Loss_r: 2.200e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4460, Loss: 2.036e-06, Loss_0: 6.111e-10, Loss_r: 2.030e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4470, Loss: 1.917e-06, Loss_0: 5.246e-10, Loss_r: 1.911e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4480, Loss: 1.821e-06, Loss_0: 4.645e-11, Loss_r: 1.820e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 1.760e-06, Loss_0: 2.637e-11, Loss_r: 1.760e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 1.711e-06, Loss_0: 1.172e-11, Loss_r: 1.711e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 1.669e-06, Loss_0: 4.277e-12, Loss_r: 1.669e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 1.631e-06, Loss_0: 1.600e-12, Loss_r: 1.631e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 1.596e-06, Loss_0: 2.332e-13, Loss_r: 1.596e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 1.563e-06, Loss_0: 2.363e-13, Loss_r: 1.563e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 1.532e-06, Loss_0: 1.280e-14, Loss_r: 1.532e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 1.502e-06, Loss_0: 2.459e-13, Loss_r: 1.502e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 1.473e-06, Loss_0: 6.641e-15, Loss_r: 1.473e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 1.445e-06, Loss_0: 1.187e-14, Loss_r: 1.445e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 1.418e-06, Loss_0: 1.675e-13, Loss_r: 1.418e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 1.392e-06, Loss_0: 3.298e-14, Loss_r: 1.392e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 1.696e-05, Loss_0: 1.483e-07, Loss_r: 1.547e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 1.013e-05, Loss_0: 5.051e-08, Loss_r: 9.622e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4630, Loss: 7.151e-06, Loss_0: 1.829e-08, Loss_r: 6.968e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4640, Loss: 5.502e-06, Loss_0: 4.378e-09, Loss_r: 5.459e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4650, Loss: 4.565e-06, Loss_0: 2.622e-10, Loss_r: 4.562e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4660, Loss: 4.104e-06, Loss_0: 3.312e-10, Loss_r: 4.100e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4670, Loss: 3.840e-06, Loss_0: 7.796e-10, Loss_r: 3.832e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4680, Loss: 3.640e-06, Loss_0: 2.912e-10, Loss_r: 3.637e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 3.490e-06, Loss_0: 1.148e-11, Loss_r: 3.490e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 3.366e-06, Loss_0: 1.751e-11, Loss_r: 3.366e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 3.257e-06, Loss_0: 9.174e-11, Loss_r: 3.256e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 3.156e-06, Loss_0: 4.610e-11, Loss_r: 3.156e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 3.064e-06, Loss_0: 3.570e-11, Loss_r: 3.064e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 2.978e-06, Loss_0: 4.168e-11, Loss_r: 2.978e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 2.898e-06, Loss_0: 3.737e-11, Loss_r: 2.898e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 2.824e-06, Loss_0: 3.438e-11, Loss_r: 2.824e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 2.754e-06, Loss_0: 2.833e-11, Loss_r: 2.754e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 2.689e-06, Loss_0: 3.447e-11, Loss_r: 2.689e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 2.628e-06, Loss_0: 2.895e-11, Loss_r: 2.628e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 2.571e-06, Loss_0: 2.322e-11, Loss_r: 2.571e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 7.997e-06, Loss_0: 1.212e-07, Loss_r: 6.785e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 4.616e-06, Loss_0: 3.961e-08, Loss_r: 4.220e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 3.247e-06, Loss_0: 1.199e-08, Loss_r: 3.127e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 2.580e-06, Loss_0: 2.165e-09, Loss_r: 2.558e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 2.267e-06, Loss_0: 5.592e-11, Loss_r: 2.266e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 2.131e-06, Loss_0: 1.805e-10, Loss_r: 2.130e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 2.050e-06, Loss_0: 2.386e-10, Loss_r: 2.048e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 1.990e-06, Loss_0: 3.652e-11, Loss_r: 1.989e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 1.951e-06, Loss_0: 9.909e-12, Loss_r: 1.951e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 1.921e-06, Loss_0: 1.350e-11, Loss_r: 1.921e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 1.897e-06, Loss_0: 1.012e-14, Loss_r: 1.897e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 1.875e-06, Loss_0: 1.652e-14, Loss_r: 1.875e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 1.855e-06, Loss_0: 1.785e-12, Loss_r: 1.855e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 1.837e-06, Loss_0: 7.033e-13, Loss_r: 1.837e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 1.819e-06, Loss_0: 7.002e-13, Loss_r: 1.819e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 1.803e-06, Loss_0: 8.364e-13, Loss_r: 1.803e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 1.787e-06, Loss_0: 4.885e-13, Loss_r: 1.787e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 1.772e-06, Loss_0: 1.548e-12, Loss_r: 1.772e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 1.758e-06, Loss_0: 6.832e-13, Loss_r: 1.758e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "Training time: 26.6949\n",
            "[1, 256, 256, 256, 256, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.235e-01, Loss_0: 3.795e-03, Loss_r: 2.197e-01, Time: 1.15, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.210e-01, Loss_0: 2.997e-03, Loss_r: 1.911e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.115e-01, Loss_0: 2.343e-03, Loss_r: 1.881e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.988e-01, Loss_0: 1.929e-03, Loss_r: 1.795e-01, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.964e-01, Loss_0: 1.482e-03, Loss_r: 1.816e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.963e-01, Loss_0: 1.477e-03, Loss_r: 1.815e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.955e-01, Loss_0: 1.462e-03, Loss_r: 1.809e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.942e-01, Loss_0: 1.502e-03, Loss_r: 1.792e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.912e-01, Loss_0: 1.512e-03, Loss_r: 1.761e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.815e-01, Loss_0: 1.446e-03, Loss_r: 1.670e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.355e-01, Loss_0: 1.153e-03, Loss_r: 1.240e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 110, Loss: 2.303e-01, Loss_0: 1.374e-05, Loss_r: 2.302e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 120, Loss: 6.586e-02, Loss_0: 3.121e-05, Loss_r: 6.554e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 130, Loss: 4.300e-02, Loss_0: 5.973e-05, Loss_r: 4.240e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 140, Loss: 3.704e-02, Loss_0: 1.548e-04, Loss_r: 3.549e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 2.432e-02, Loss_0: 3.900e-05, Loss_r: 2.393e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 1.725e-02, Loss_0: 4.233e-05, Loss_r: 1.683e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.150e-02, Loss_0: 1.971e-05, Loss_r: 1.130e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 7.538e-03, Loss_0: 1.009e-05, Loss_r: 7.437e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.266e-03, Loss_0: 6.422e-06, Loss_r: 5.202e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 200, Loss: 4.128e-03, Loss_0: 9.658e-07, Loss_r: 4.118e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.433e-03, Loss_0: 6.614e-07, Loss_r: 4.427e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 220, Loss: 8.408e-03, Loss_0: 5.557e-05, Loss_r: 7.852e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 230, Loss: 4.221e-03, Loss_0: 2.507e-06, Loss_r: 4.196e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 2.982e-03, Loss_0: 5.425e-06, Loss_r: 2.928e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 2.762e-03, Loss_0: 1.253e-05, Loss_r: 2.637e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 260, Loss: 2.187e-03, Loss_0: 1.527e-07, Loss_r: 2.186e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 1.807e-03, Loss_0: 3.289e-06, Loss_r: 1.775e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 1.479e-03, Loss_0: 1.495e-06, Loss_r: 1.464e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 1.210e-03, Loss_0: 6.558e-07, Loss_r: 1.204e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.078e-03, Loss_0: 3.020e-06, Loss_r: 1.047e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 3.915e-02, Loss_0: 2.732e-04, Loss_r: 3.642e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 7.601e-03, Loss_0: 1.475e-04, Loss_r: 6.126e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.464e-03, Loss_0: 4.728e-05, Loss_r: 1.991e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 2.025e-03, Loss_0: 8.875e-07, Loss_r: 2.016e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.243e-03, Loss_0: 8.893e-06, Loss_r: 1.154e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.040e-03, Loss_0: 1.249e-05, Loss_r: 9.150e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 8.227e-04, Loss_0: 7.406e-07, Loss_r: 8.153e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 380, Loss: 6.592e-04, Loss_0: 7.461e-07, Loss_r: 6.517e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.496e-04, Loss_0: 8.991e-07, Loss_r: 5.406e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.592e-04, Loss_0: 2.046e-07, Loss_r: 4.571e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 410, Loss: 8.134e-04, Loss_0: 9.662e-08, Loss_r: 8.125e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 420, Loss: 7.211e-04, Loss_0: 5.895e-08, Loss_r: 7.205e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 6.434e-04, Loss_0: 9.963e-08, Loss_r: 6.424e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.776e-04, Loss_0: 1.066e-07, Loss_r: 5.765e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.214e-04, Loss_0: 8.032e-08, Loss_r: 5.206e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.735e-04, Loss_0: 7.585e-08, Loss_r: 4.727e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.326e-04, Loss_0: 8.262e-08, Loss_r: 4.318e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 480, Loss: 3.976e-04, Loss_0: 6.425e-08, Loss_r: 3.970e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 3.675e-04, Loss_0: 5.519e-08, Loss_r: 3.669e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 500, Loss: 3.413e-04, Loss_0: 4.764e-08, Loss_r: 3.408e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 510, Loss: 3.183e-04, Loss_0: 4.321e-08, Loss_r: 3.179e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.979e-04, Loss_0: 3.704e-08, Loss_r: 2.976e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.798e-04, Loss_0: 3.520e-08, Loss_r: 2.794e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.684e-04, Loss_0: 2.552e-07, Loss_r: 2.659e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 6.679e-02, Loss_0: 1.280e-03, Loss_r: 5.399e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.335e-02, Loss_0: 9.143e-04, Loss_r: 4.209e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 3.785e-03, Loss_0: 3.155e-04, Loss_r: 6.295e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.571e-03, Loss_0: 6.183e-05, Loss_r: 9.526e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.126e-03, Loss_0: 2.129e-07, Loss_r: 1.124e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 600, Loss: 7.756e-04, Loss_0: 4.735e-07, Loss_r: 7.709e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 4.760e-04, Loss_0: 2.027e-06, Loss_r: 4.557e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 3.953e-04, Loss_0: 1.225e-10, Loss_r: 3.953e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 630, Loss: 3.326e-04, Loss_0: 7.882e-08, Loss_r: 3.318e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.871e-04, Loss_0: 3.385e-07, Loss_r: 2.837e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 650, Loss: 2.525e-04, Loss_0: 3.428e-08, Loss_r: 2.522e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 660, Loss: 2.257e-04, Loss_0: 1.574e-08, Loss_r: 2.255e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 670, Loss: 2.046e-04, Loss_0: 3.235e-09, Loss_r: 2.046e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.881e-04, Loss_0: 3.496e-08, Loss_r: 1.878e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.749e-04, Loss_0: 2.093e-08, Loss_r: 1.746e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.641e-04, Loss_0: 2.151e-08, Loss_r: 1.638e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.551e-04, Loss_0: 1.314e-08, Loss_r: 1.550e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.476e-04, Loss_0: 1.199e-08, Loss_r: 1.474e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.410e-04, Loss_0: 1.053e-08, Loss_r: 1.409e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.353e-04, Loss_0: 9.080e-09, Loss_r: 1.353e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.303e-04, Loss_0: 8.570e-09, Loss_r: 1.302e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.257e-04, Loss_0: 7.850e-09, Loss_r: 1.256e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.214e-04, Loss_0: 7.110e-09, Loss_r: 1.214e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 780, Loss: 1.175e-04, Loss_0: 6.533e-09, Loss_r: 1.175e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 1.139e-04, Loss_0: 6.068e-09, Loss_r: 1.138e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 1.105e-04, Loss_0: 5.672e-09, Loss_r: 1.104e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.411e-05, Loss_0: 5.786e-09, Loss_r: 6.405e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 820, Loss: 5.570e-05, Loss_0: 2.104e-09, Loss_r: 5.567e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 830, Loss: 5.230e-05, Loss_0: 2.734e-09, Loss_r: 5.228e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 4.942e-05, Loss_0: 1.179e-09, Loss_r: 4.941e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 850, Loss: 4.688e-05, Loss_0: 9.995e-11, Loss_r: 4.688e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 4.495e-05, Loss_0: 4.828e-10, Loss_r: 4.494e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 4.340e-05, Loss_0: 6.015e-10, Loss_r: 4.340e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 880, Loss: 4.212e-05, Loss_0: 3.552e-10, Loss_r: 4.212e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 890, Loss: 4.106e-05, Loss_0: 5.742e-10, Loss_r: 4.105e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 900, Loss: 4.016e-05, Loss_0: 4.971e-10, Loss_r: 4.015e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 910, Loss: 3.939e-05, Loss_0: 5.462e-10, Loss_r: 3.938e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 920, Loss: 3.872e-05, Loss_0: 5.302e-10, Loss_r: 3.871e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 930, Loss: 3.813e-05, Loss_0: 5.548e-10, Loss_r: 3.812e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 3.760e-05, Loss_0: 5.413e-10, Loss_r: 3.759e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 950, Loss: 3.712e-05, Loss_0: 5.568e-10, Loss_r: 3.712e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 3.669e-05, Loss_0: 5.613e-10, Loss_r: 3.668e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 970, Loss: 3.629e-05, Loss_0: 5.475e-10, Loss_r: 3.628e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 3.592e-05, Loss_0: 5.525e-10, Loss_r: 3.591e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 3.557e-05, Loss_0: 5.467e-10, Loss_r: 3.556e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 3.524e-05, Loss_0: 5.357e-10, Loss_r: 3.524e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 7.022e-05, Loss_0: 4.174e-10, Loss_r: 7.022e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 6.801e-05, Loss_0: 8.312e-10, Loss_r: 6.800e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 6.617e-05, Loss_0: 1.769e-09, Loss_r: 6.616e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 6.442e-05, Loss_0: 2.445e-09, Loss_r: 6.440e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 6.284e-05, Loss_0: 1.923e-09, Loss_r: 6.282e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 6.143e-05, Loss_0: 1.997e-09, Loss_r: 6.141e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 6.015e-05, Loss_0: 1.925e-09, Loss_r: 6.013e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 5.897e-05, Loss_0: 2.110e-09, Loss_r: 5.895e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 5.789e-05, Loss_0: 1.886e-09, Loss_r: 5.787e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 5.689e-05, Loss_0: 1.749e-09, Loss_r: 5.687e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 5.596e-05, Loss_0: 1.693e-09, Loss_r: 5.594e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 5.508e-05, Loss_0: 1.680e-09, Loss_r: 5.507e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 5.426e-05, Loss_0: 1.588e-09, Loss_r: 5.425e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 5.349e-05, Loss_0: 1.844e-09, Loss_r: 5.347e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 5.287e-05, Loss_0: 9.506e-09, Loss_r: 5.277e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.247e-04, Loss_0: 2.612e-06, Loss_r: 9.862e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.079e-02, Loss_0: 6.353e-04, Loss_r: 4.436e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 3.531e-03, Loss_0: 3.246e-04, Loss_r: 2.850e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 3.134e-04, Loss_0: 9.794e-06, Loss_r: 2.155e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 3.826e-04, Loss_0: 2.228e-05, Loss_r: 1.598e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 9.066e-05, Loss_0: 5.918e-06, Loss_r: 3.148e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 2.676e-05, Loss_0: 1.539e-07, Loss_r: 2.522e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.635e-05, Loss_0: 5.413e-07, Loss_r: 3.094e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 2.621e-05, Loss_0: 2.902e-07, Loss_r: 2.330e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.299e-05, Loss_0: 5.324e-08, Loss_r: 2.246e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.266e-05, Loss_0: 2.634e-08, Loss_r: 2.240e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.152e-05, Loss_0: 2.609e-09, Loss_r: 2.150e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.113e-05, Loss_0: 3.644e-09, Loss_r: 2.109e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.063e-05, Loss_0: 2.066e-10, Loss_r: 2.063e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.024e-05, Loss_0: 2.284e-09, Loss_r: 2.022e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 1.986e-05, Loss_0: 4.666e-12, Loss_r: 1.986e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 1.951e-05, Loss_0: 2.276e-10, Loss_r: 1.950e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 1.918e-05, Loss_0: 4.103e-10, Loss_r: 1.917e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 1.886e-05, Loss_0: 1.542e-10, Loss_r: 1.886e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 1.857e-05, Loss_0: 2.436e-10, Loss_r: 1.857e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.830e-05, Loss_0: 2.454e-10, Loss_r: 1.830e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.804e-05, Loss_0: 1.844e-10, Loss_r: 1.804e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.780e-05, Loss_0: 2.123e-10, Loss_r: 1.780e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.757e-05, Loss_0: 1.957e-10, Loss_r: 1.757e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.736e-05, Loss_0: 1.985e-10, Loss_r: 1.736e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 3.408e-05, Loss_0: 1.293e-09, Loss_r: 3.406e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 3.369e-05, Loss_0: 7.231e-10, Loss_r: 3.369e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 3.337e-05, Loss_0: 5.778e-10, Loss_r: 3.337e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 3.304e-05, Loss_0: 5.629e-10, Loss_r: 3.304e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 3.274e-05, Loss_0: 4.248e-10, Loss_r: 3.274e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 3.245e-05, Loss_0: 4.957e-10, Loss_r: 3.245e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 3.218e-05, Loss_0: 4.796e-10, Loss_r: 3.218e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 3.192e-05, Loss_0: 4.641e-10, Loss_r: 3.192e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 3.168e-05, Loss_0: 4.691e-10, Loss_r: 3.167e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 3.144e-05, Loss_0: 4.529e-10, Loss_r: 3.144e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 3.122e-05, Loss_0: 4.505e-10, Loss_r: 3.121e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 3.101e-05, Loss_0: 4.338e-10, Loss_r: 3.100e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 3.080e-05, Loss_0: 4.270e-10, Loss_r: 3.080e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 3.061e-05, Loss_0: 4.135e-10, Loss_r: 3.060e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 3.042e-05, Loss_0: 4.103e-10, Loss_r: 3.041e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 3.024e-05, Loss_0: 4.006e-10, Loss_r: 3.023e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 3.006e-05, Loss_0: 3.865e-10, Loss_r: 3.006e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.990e-05, Loss_0: 3.961e-10, Loss_r: 2.989e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.974e-05, Loss_0: 3.710e-10, Loss_r: 2.973e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.958e-05, Loss_0: 3.792e-10, Loss_r: 2.958e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.293e-05, Loss_0: 2.382e-09, Loss_r: 3.291e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.273e-05, Loss_0: 3.593e-09, Loss_r: 3.269e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.354e-05, Loss_0: 4.848e-08, Loss_r: 3.305e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 8.205e-05, Loss_0: 2.409e-06, Loss_r: 5.796e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 3.591e-03, Loss_0: 1.855e-04, Loss_r: 1.736e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 7.008e-04, Loss_0: 6.223e-05, Loss_r: 7.858e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 1.863e-04, Loss_0: 1.379e-05, Loss_r: 4.839e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 7.061e-05, Loss_0: 1.050e-06, Loss_r: 6.011e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 5.134e-05, Loss_0: 4.348e-07, Loss_r: 4.699e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 4.159e-05, Loss_0: 5.713e-07, Loss_r: 3.587e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 3.872e-05, Loss_0: 5.276e-07, Loss_r: 3.344e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 3.583e-05, Loss_0: 2.982e-07, Loss_r: 3.285e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 3.317e-05, Loss_0: 6.921e-08, Loss_r: 3.248e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 3.231e-05, Loss_0: 6.747e-13, Loss_r: 3.231e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 3.212e-05, Loss_0: 4.618e-09, Loss_r: 3.208e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 3.173e-05, Loss_0: 1.501e-09, Loss_r: 3.171e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 3.150e-05, Loss_0: 3.570e-09, Loss_r: 3.146e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 3.126e-05, Loss_0: 2.930e-12, Loss_r: 3.126e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 3.104e-05, Loss_0: 1.090e-09, Loss_r: 3.103e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 3.084e-05, Loss_0: 1.709e-10, Loss_r: 3.084e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 1.535e-04, Loss_0: 3.428e-07, Loss_r: 1.501e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.498e-04, Loss_0: 1.230e-06, Loss_r: 1.375e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 4.006e-03, Loss_0: 3.189e-04, Loss_r: 8.173e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 2.156e-03, Loss_0: 7.627e-06, Loss_r: 2.080e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 3.481e-04, Loss_0: 1.278e-06, Loss_r: 3.353e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 3.348e-04, Loss_0: 9.017e-06, Loss_r: 2.446e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.707e-04, Loss_0: 8.666e-09, Loss_r: 1.706e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.195e-04, Loss_0: 3.784e-08, Loss_r: 1.191e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.217e-04, Loss_0: 6.981e-08, Loss_r: 1.210e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.109e-04, Loss_0: 4.779e-07, Loss_r: 1.062e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.037e-04, Loss_0: 3.006e-08, Loss_r: 1.034e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.014e-04, Loss_0: 3.158e-08, Loss_r: 1.010e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 9.840e-05, Loss_0: 3.714e-08, Loss_r: 9.803e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 9.605e-05, Loss_0: 1.587e-10, Loss_r: 9.605e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 9.405e-05, Loss_0: 1.130e-08, Loss_r: 9.393e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 9.222e-05, Loss_0: 3.060e-10, Loss_r: 9.222e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 9.051e-05, Loss_0: 3.835e-09, Loss_r: 9.047e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 8.891e-05, Loss_0: 5.535e-09, Loss_r: 8.886e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 8.749e-05, Loss_0: 1.978e-08, Loss_r: 8.729e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 9.361e-05, Loss_0: 7.797e-07, Loss_r: 8.582e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 2.005e-03, Loss_0: 1.734e-04, Loss_r: 2.705e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 2.566e-03, Loss_0: 1.837e-04, Loss_r: 7.289e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 5.898e-04, Loss_0: 4.312e-05, Loss_r: 1.586e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 2.294e-04, Loss_0: 2.263e-07, Loss_r: 2.272e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 1.789e-04, Loss_0: 8.754e-06, Loss_r: 9.138e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.070e-04, Loss_0: 1.964e-06, Loss_r: 8.734e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 8.625e-05, Loss_0: 3.640e-07, Loss_r: 8.261e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 8.181e-05, Loss_0: 3.777e-07, Loss_r: 7.803e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 7.528e-05, Loss_0: 1.727e-08, Loss_r: 7.511e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 7.324e-05, Loss_0: 4.984e-08, Loss_r: 7.274e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 7.149e-05, Loss_0: 4.245e-08, Loss_r: 7.106e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 6.985e-05, Loss_0: 8.666e-09, Loss_r: 6.977e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 6.857e-05, Loss_0: 1.103e-09, Loss_r: 6.856e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 7.428e-05, Loss_0: 1.454e-08, Loss_r: 7.413e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.016e-03, Loss_0: 3.846e-06, Loss_r: 9.776e-04, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 1.610e-04, Loss_0: 7.439e-07, Loss_r: 1.536e-04, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 1.129e-04, Loss_0: 3.213e-07, Loss_r: 1.096e-04, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 7.715e-05, Loss_0: 8.120e-09, Loss_r: 7.707e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 6.704e-05, Loss_0: 2.697e-08, Loss_r: 6.677e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 6.444e-05, Loss_0: 7.268e-09, Loss_r: 6.437e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 5.613e-05, Loss_0: 2.674e-08, Loss_r: 5.586e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 4.920e-05, Loss_0: 1.148e-10, Loss_r: 4.920e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 4.204e-05, Loss_0: 2.164e-10, Loss_r: 4.204e-05, Time: 0.09, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 3.690e-05, Loss_0: 1.850e-09, Loss_r: 3.688e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 3.294e-05, Loss_0: 6.540e-10, Loss_r: 3.293e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 3.005e-05, Loss_0: 9.083e-11, Loss_r: 3.005e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 2.787e-05, Loss_0: 3.966e-11, Loss_r: 2.787e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 2.620e-05, Loss_0: 3.706e-12, Loss_r: 2.620e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 2.488e-05, Loss_0: 3.328e-11, Loss_r: 2.488e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 2.381e-05, Loss_0: 1.835e-11, Loss_r: 2.381e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 2.293e-05, Loss_0: 1.556e-11, Loss_r: 2.293e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 2.220e-05, Loss_0: 2.444e-11, Loss_r: 2.220e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 2.157e-05, Loss_0: 3.385e-11, Loss_r: 2.157e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 2.104e-05, Loss_0: 4.152e-11, Loss_r: 2.104e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 2.057e-05, Loss_0: 4.927e-11, Loss_r: 2.057e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 2.016e-05, Loss_0: 5.591e-11, Loss_r: 2.016e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.980e-05, Loss_0: 6.075e-11, Loss_r: 1.980e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 1.948e-05, Loss_0: 6.183e-11, Loss_r: 1.948e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 1.919e-05, Loss_0: 6.714e-11, Loss_r: 1.919e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 1.893e-05, Loss_0: 7.298e-11, Loss_r: 1.893e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 4.050e-05, Loss_0: 9.110e-09, Loss_r: 4.041e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 3.929e-05, Loss_0: 1.497e-09, Loss_r: 3.928e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 3.743e-05, Loss_0: 4.093e-10, Loss_r: 3.742e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 3.649e-05, Loss_0: 2.703e-09, Loss_r: 3.646e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 3.578e-05, Loss_0: 1.110e-09, Loss_r: 3.577e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 3.532e-05, Loss_0: 4.226e-10, Loss_r: 3.531e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 3.496e-05, Loss_0: 3.940e-10, Loss_r: 3.496e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 3.467e-05, Loss_0: 4.719e-10, Loss_r: 3.467e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 3.442e-05, Loss_0: 5.140e-10, Loss_r: 3.441e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 3.418e-05, Loss_0: 5.250e-10, Loss_r: 3.417e-05, Time: 0.10, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 3.395e-05, Loss_0: 5.176e-10, Loss_r: 3.395e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 3.373e-05, Loss_0: 4.907e-10, Loss_r: 3.372e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 3.351e-05, Loss_0: 4.605e-10, Loss_r: 3.351e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 3.329e-05, Loss_0: 4.595e-10, Loss_r: 3.329e-05, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 3.308e-05, Loss_0: 4.670e-10, Loss_r: 3.307e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 3.287e-05, Loss_0: 4.682e-10, Loss_r: 3.286e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 3.265e-05, Loss_0: 4.492e-10, Loss_r: 3.265e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 3.244e-05, Loss_0: 4.242e-10, Loss_r: 3.244e-05, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 3.223e-05, Loss_0: 4.301e-10, Loss_r: 3.223e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 3.202e-05, Loss_0: 4.196e-10, Loss_r: 3.202e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 1.839e-05, Loss_0: 1.868e-08, Loss_r: 1.820e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 8.283e-05, Loss_0: 4.612e-08, Loss_r: 8.237e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 2.826e-05, Loss_0: 1.261e-08, Loss_r: 2.813e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 1.554e-05, Loss_0: 1.390e-09, Loss_r: 1.552e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 1.496e-05, Loss_0: 6.626e-10, Loss_r: 1.496e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 1.528e-05, Loss_0: 2.551e-09, Loss_r: 1.525e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 1.425e-05, Loss_0: 9.749e-10, Loss_r: 1.424e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 1.377e-05, Loss_0: 5.778e-12, Loss_r: 1.377e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 1.362e-05, Loss_0: 2.350e-11, Loss_r: 1.362e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 1.343e-05, Loss_0: 1.791e-10, Loss_r: 1.343e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.328e-05, Loss_0: 3.843e-11, Loss_r: 1.328e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2720, Loss: 1.316e-05, Loss_0: 3.516e-11, Loss_r: 1.315e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2730, Loss: 1.304e-05, Loss_0: 4.817e-11, Loss_r: 1.304e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2740, Loss: 1.294e-05, Loss_0: 6.160e-11, Loss_r: 1.294e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2750, Loss: 1.284e-05, Loss_0: 3.798e-11, Loss_r: 1.284e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2760, Loss: 1.275e-05, Loss_0: 4.283e-11, Loss_r: 1.275e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2770, Loss: 1.267e-05, Loss_0: 3.458e-11, Loss_r: 1.267e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2780, Loss: 1.262e-05, Loss_0: 6.309e-12, Loss_r: 1.262e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2790, Loss: 1.615e-05, Loss_0: 8.421e-09, Loss_r: 1.606e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.259e-05, Loss_0: 1.581e-10, Loss_r: 1.259e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 1.547e-05, Loss_0: 3.385e-09, Loss_r: 1.543e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.536e-05, Loss_0: 1.481e-09, Loss_r: 1.534e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.480e-05, Loss_0: 3.044e-11, Loss_r: 1.480e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.465e-05, Loss_0: 2.999e-11, Loss_r: 1.465e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.447e-05, Loss_0: 5.916e-11, Loss_r: 1.447e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 1.435e-05, Loss_0: 2.339e-10, Loss_r: 1.435e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 1.423e-05, Loss_0: 6.488e-11, Loss_r: 1.423e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 1.413e-05, Loss_0: 8.855e-11, Loss_r: 1.413e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 1.403e-05, Loss_0: 7.901e-11, Loss_r: 1.403e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 1.393e-05, Loss_0: 9.508e-11, Loss_r: 1.393e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 1.384e-05, Loss_0: 7.962e-11, Loss_r: 1.384e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 1.375e-05, Loss_0: 8.632e-11, Loss_r: 1.375e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 1.366e-05, Loss_0: 8.284e-11, Loss_r: 1.366e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 1.357e-05, Loss_0: 7.871e-11, Loss_r: 1.357e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 1.348e-05, Loss_0: 5.176e-11, Loss_r: 1.348e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 1.354e-05, Loss_0: 1.864e-10, Loss_r: 1.353e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 5.853e-05, Loss_0: 1.586e-07, Loss_r: 5.694e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2980, Loss: 1.338e-05, Loss_0: 3.635e-11, Loss_r: 1.338e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2990, Loss: 1.793e-05, Loss_0: 2.342e-08, Loss_r: 1.770e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3000, Loss: 1.553e-05, Loss_0: 1.220e-08, Loss_r: 1.540e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3010, Loss: 1.781e-05, Loss_0: 4.154e-09, Loss_r: 1.777e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3020, Loss: 1.746e-05, Loss_0: 9.037e-10, Loss_r: 1.745e-05, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 1.684e-05, Loss_0: 2.714e-09, Loss_r: 1.681e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 1.645e-05, Loss_0: 1.319e-10, Loss_r: 1.645e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 1.617e-05, Loss_0: 8.979e-10, Loss_r: 1.616e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 1.598e-05, Loss_0: 8.829e-11, Loss_r: 1.598e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 1.584e-05, Loss_0: 1.992e-10, Loss_r: 1.583e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 1.573e-05, Loss_0: 3.111e-10, Loss_r: 1.572e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 1.562e-05, Loss_0: 3.262e-10, Loss_r: 1.562e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 1.559e-05, Loss_0: 7.920e-10, Loss_r: 1.558e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 1.825e-05, Loss_0: 1.273e-08, Loss_r: 1.812e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.579e-05, Loss_0: 2.693e-09, Loss_r: 1.576e-05, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.526e-05, Loss_0: 8.247e-11, Loss_r: 1.526e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.528e-05, Loss_0: 8.104e-11, Loss_r: 1.528e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.514e-05, Loss_0: 3.010e-12, Loss_r: 1.514e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.502e-05, Loss_0: 1.998e-10, Loss_r: 1.501e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.494e-05, Loss_0: 2.595e-10, Loss_r: 1.494e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.485e-05, Loss_0: 7.558e-11, Loss_r: 1.485e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.477e-05, Loss_0: 1.099e-10, Loss_r: 1.477e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.470e-05, Loss_0: 1.317e-10, Loss_r: 1.469e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 1.269e-05, Loss_0: 6.424e-09, Loss_r: 1.263e-05, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 1.214e-05, Loss_0: 1.741e-09, Loss_r: 1.212e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 1.094e-05, Loss_0: 3.622e-10, Loss_r: 1.094e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 1.037e-05, Loss_0: 7.752e-11, Loss_r: 1.037e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 9.845e-06, Loss_0: 7.183e-15, Loss_r: 9.845e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 9.481e-06, Loss_0: 1.003e-15, Loss_r: 9.481e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 9.180e-06, Loss_0: 3.501e-12, Loss_r: 9.180e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 8.936e-06, Loss_0: 7.548e-12, Loss_r: 8.936e-06, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 8.790e-06, Loss_0: 2.717e-10, Loss_r: 8.787e-06, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.660e-05, Loss_0: 4.107e-08, Loss_r: 1.619e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 8.619e-06, Loss_0: 8.993e-10, Loss_r: 8.610e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 9.064e-06, Loss_0: 4.626e-09, Loss_r: 9.018e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 8.485e-06, Loss_0: 2.037e-09, Loss_r: 8.465e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 8.144e-06, Loss_0: 3.378e-10, Loss_r: 8.140e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 7.955e-06, Loss_0: 7.801e-12, Loss_r: 7.954e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 7.875e-06, Loss_0: 2.062e-10, Loss_r: 7.873e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 7.779e-06, Loss_0: 1.063e-11, Loss_r: 7.779e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 7.692e-06, Loss_0: 7.052e-11, Loss_r: 7.692e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 7.615e-06, Loss_0: 6.004e-12, Loss_r: 7.615e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 7.547e-06, Loss_0: 4.062e-12, Loss_r: 7.547e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 6.518e-06, Loss_0: 2.917e-11, Loss_r: 6.518e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 6.482e-06, Loss_0: 3.389e-11, Loss_r: 6.482e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 6.448e-06, Loss_0: 3.340e-11, Loss_r: 6.448e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 6.433e-06, Loss_0: 1.818e-10, Loss_r: 6.431e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 1.091e-05, Loss_0: 2.225e-08, Loss_r: 1.068e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 6.362e-06, Loss_0: 7.858e-11, Loss_r: 6.361e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 7.014e-06, Loss_0: 2.640e-09, Loss_r: 6.988e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 6.394e-06, Loss_0: 2.486e-10, Loss_r: 6.392e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 6.365e-06, Loss_0: 6.185e-10, Loss_r: 6.359e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 6.252e-06, Loss_0: 4.430e-11, Loss_r: 6.251e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 6.236e-06, Loss_0: 7.848e-12, Loss_r: 6.236e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 6.204e-06, Loss_0: 1.002e-10, Loss_r: 6.203e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 6.173e-06, Loss_0: 2.254e-12, Loss_r: 6.173e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 6.146e-06, Loss_0: 4.242e-11, Loss_r: 6.145e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 6.119e-06, Loss_0: 1.182e-11, Loss_r: 6.119e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 6.092e-06, Loss_0: 2.592e-11, Loss_r: 6.092e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 6.066e-06, Loss_0: 2.839e-11, Loss_r: 6.066e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 6.040e-06, Loss_0: 2.382e-11, Loss_r: 6.040e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 6.014e-06, Loss_0: 2.564e-11, Loss_r: 6.014e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 5.989e-06, Loss_0: 4.540e-11, Loss_r: 5.989e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 1.445e-05, Loss_0: 2.110e-08, Loss_r: 1.424e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3620, Loss: 6.937e-06, Loss_0: 2.282e-09, Loss_r: 6.914e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3630, Loss: 7.437e-06, Loss_0: 5.763e-09, Loss_r: 7.380e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 6.548e-06, Loss_0: 1.709e-10, Loss_r: 6.547e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 6.281e-06, Loss_0: 4.695e-10, Loss_r: 6.277e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 6.121e-06, Loss_0: 2.074e-10, Loss_r: 6.119e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 6.070e-06, Loss_0: 1.047e-11, Loss_r: 6.070e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 6.004e-06, Loss_0: 1.012e-10, Loss_r: 6.003e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 5.957e-06, Loss_0: 2.223e-11, Loss_r: 5.956e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 5.919e-06, Loss_0: 3.563e-11, Loss_r: 5.919e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 5.884e-06, Loss_0: 2.484e-11, Loss_r: 5.884e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 5.852e-06, Loss_0: 3.961e-11, Loss_r: 5.852e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 5.821e-06, Loss_0: 2.142e-11, Loss_r: 5.821e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 5.791e-06, Loss_0: 2.891e-11, Loss_r: 5.791e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 5.763e-06, Loss_0: 2.895e-11, Loss_r: 5.763e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 5.735e-06, Loss_0: 3.021e-11, Loss_r: 5.735e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 5.708e-06, Loss_0: 4.075e-11, Loss_r: 5.708e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 5.690e-06, Loss_0: 1.166e-10, Loss_r: 5.688e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 6.189e-06, Loss_0: 2.884e-09, Loss_r: 6.160e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3800, Loss: 5.694e-06, Loss_0: 4.607e-10, Loss_r: 5.690e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3810, Loss: 6.680e-06, Loss_0: 8.993e-12, Loss_r: 6.680e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 6.653e-06, Loss_0: 2.392e-11, Loss_r: 6.652e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 6.577e-06, Loss_0: 3.652e-12, Loss_r: 6.577e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 6.528e-06, Loss_0: 3.225e-11, Loss_r: 6.527e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 6.490e-06, Loss_0: 5.590e-11, Loss_r: 6.490e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 6.456e-06, Loss_0: 1.133e-11, Loss_r: 6.456e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 6.425e-06, Loss_0: 1.694e-11, Loss_r: 6.425e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 6.396e-06, Loss_0: 2.668e-11, Loss_r: 6.396e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 6.368e-06, Loss_0: 1.888e-11, Loss_r: 6.368e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 6.341e-06, Loss_0: 2.279e-11, Loss_r: 6.341e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 6.315e-06, Loss_0: 1.986e-11, Loss_r: 6.315e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 6.289e-06, Loss_0: 2.446e-11, Loss_r: 6.289e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 6.263e-06, Loss_0: 2.231e-11, Loss_r: 6.263e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 6.238e-06, Loss_0: 2.281e-11, Loss_r: 6.237e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 6.212e-06, Loss_0: 2.461e-11, Loss_r: 6.212e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 6.187e-06, Loss_0: 2.693e-11, Loss_r: 6.187e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 6.163e-06, Loss_0: 5.160e-11, Loss_r: 6.163e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 6.253e-06, Loss_0: 8.471e-10, Loss_r: 6.245e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 3.085e-05, Loss_0: 1.309e-07, Loss_r: 2.954e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4000, Loss: 6.734e-06, Loss_0: 3.917e-09, Loss_r: 6.694e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4010, Loss: 6.028e-05, Loss_0: 4.052e-11, Loss_r: 6.028e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 5.205e-05, Loss_0: 1.755e-10, Loss_r: 5.205e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 4.539e-05, Loss_0: 5.069e-09, Loss_r: 4.534e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 4.023e-05, Loss_0: 4.465e-09, Loss_r: 4.018e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 3.674e-05, Loss_0: 7.054e-10, Loss_r: 3.673e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 3.425e-05, Loss_0: 1.225e-09, Loss_r: 3.423e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 3.229e-05, Loss_0: 1.124e-09, Loss_r: 3.228e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 3.074e-05, Loss_0: 1.598e-09, Loss_r: 3.073e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 2.947e-05, Loss_0: 7.562e-10, Loss_r: 2.946e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 2.840e-05, Loss_0: 8.292e-10, Loss_r: 2.839e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 2.750e-05, Loss_0: 6.653e-10, Loss_r: 2.749e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 2.672e-05, Loss_0: 6.846e-10, Loss_r: 2.671e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 2.604e-05, Loss_0: 6.326e-10, Loss_r: 2.603e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 2.545e-05, Loss_0: 5.775e-10, Loss_r: 2.544e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 2.492e-05, Loss_0: 5.433e-10, Loss_r: 2.491e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 2.444e-05, Loss_0: 4.987e-10, Loss_r: 2.444e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 2.401e-05, Loss_0: 4.826e-10, Loss_r: 2.400e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 2.361e-05, Loss_0: 4.835e-10, Loss_r: 2.360e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 2.324e-05, Loss_0: 5.516e-10, Loss_r: 2.323e-05, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 2.311e-05, Loss_0: 2.502e-09, Loss_r: 2.309e-05, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 2.974e-05, Loss_0: 3.821e-12, Loss_r: 2.974e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 2.014e-05, Loss_0: 9.176e-09, Loss_r: 2.005e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 1.613e-05, Loss_0: 9.143e-11, Loss_r: 1.613e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 1.431e-05, Loss_0: 6.445e-09, Loss_r: 1.424e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 1.212e-05, Loss_0: 9.792e-10, Loss_r: 1.211e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 1.079e-05, Loss_0: 7.294e-10, Loss_r: 1.078e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 9.658e-06, Loss_0: 5.369e-12, Loss_r: 9.658e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 8.780e-06, Loss_0: 3.486e-11, Loss_r: 8.780e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 8.045e-06, Loss_0: 1.557e-14, Loss_r: 8.045e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 7.424e-06, Loss_0: 4.287e-13, Loss_r: 7.424e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 6.894e-06, Loss_0: 1.183e-11, Loss_r: 6.894e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 6.445e-06, Loss_0: 7.554e-11, Loss_r: 6.444e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 6.560e-06, Loss_0: 4.179e-09, Loss_r: 6.518e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 1.190e-04, Loss_0: 9.126e-07, Loss_r: 1.099e-04, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4350, Loss: 1.614e-05, Loss_0: 9.333e-08, Loss_r: 1.521e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4360, Loss: 5.992e-06, Loss_0: 5.638e-09, Loss_r: 5.935e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4370, Loss: 5.401e-06, Loss_0: 1.637e-09, Loss_r: 5.385e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4380, Loss: 5.848e-06, Loss_0: 8.660e-09, Loss_r: 5.761e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4390, Loss: 5.684e-06, Loss_0: 9.463e-09, Loss_r: 5.589e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4400, Loss: 4.875e-06, Loss_0: 1.692e-09, Loss_r: 4.858e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4410, Loss: 2.898e-05, Loss_0: 4.107e-10, Loss_r: 2.898e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4420, Loss: 2.685e-05, Loss_0: 7.010e-09, Loss_r: 2.678e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4430, Loss: 2.375e-05, Loss_0: 1.414e-10, Loss_r: 2.375e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 2.227e-05, Loss_0: 6.949e-10, Loss_r: 2.227e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 2.116e-05, Loss_0: 9.624e-10, Loss_r: 2.115e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 2.045e-05, Loss_0: 6.032e-10, Loss_r: 2.044e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 1.990e-05, Loss_0: 3.581e-10, Loss_r: 1.990e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 1.946e-05, Loss_0: 3.313e-10, Loss_r: 1.946e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 1.909e-05, Loss_0: 3.932e-10, Loss_r: 1.908e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 1.876e-05, Loss_0: 6.348e-10, Loss_r: 1.876e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 1.862e-05, Loss_0: 2.899e-09, Loss_r: 1.859e-05, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 2.737e-05, Loss_0: 8.447e-08, Loss_r: 2.652e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4530, Loss: 1.944e-05, Loss_0: 1.607e-08, Loss_r: 1.928e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4540, Loss: 1.778e-05, Loss_0: 4.353e-10, Loss_r: 1.777e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4550, Loss: 1.781e-05, Loss_0: 8.607e-10, Loss_r: 1.780e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4560, Loss: 1.758e-05, Loss_0: 5.851e-10, Loss_r: 1.758e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4570, Loss: 1.723e-05, Loss_0: 6.586e-11, Loss_r: 1.723e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4580, Loss: 1.708e-05, Loss_0: 7.932e-10, Loss_r: 1.707e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4590, Loss: 1.690e-05, Loss_0: 3.384e-10, Loss_r: 1.690e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4600, Loss: 1.675e-05, Loss_0: 8.276e-11, Loss_r: 1.674e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4610, Loss: 1.290e-05, Loss_0: 8.174e-10, Loss_r: 1.289e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4620, Loss: 1.272e-05, Loss_0: 1.155e-11, Loss_r: 1.272e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 1.253e-05, Loss_0: 8.830e-13, Loss_r: 1.253e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 1.238e-05, Loss_0: 7.843e-11, Loss_r: 1.238e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 1.225e-05, Loss_0: 1.339e-10, Loss_r: 1.225e-05, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 1.213e-05, Loss_0: 8.857e-11, Loss_r: 1.213e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 1.202e-05, Loss_0: 7.292e-11, Loss_r: 1.202e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 1.191e-05, Loss_0: 8.168e-11, Loss_r: 1.191e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 1.181e-05, Loss_0: 1.315e-10, Loss_r: 1.181e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 1.180e-05, Loss_0: 1.424e-09, Loss_r: 1.179e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 3.554e-05, Loss_0: 2.314e-07, Loss_r: 3.322e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4720, Loss: 1.169e-05, Loss_0: 1.949e-09, Loss_r: 1.167e-05, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4730, Loss: 1.414e-05, Loss_0: 2.373e-08, Loss_r: 1.390e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4740, Loss: 1.251e-05, Loss_0: 9.612e-09, Loss_r: 1.241e-05, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4750, Loss: 1.143e-05, Loss_0: 1.798e-09, Loss_r: 1.141e-05, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4760, Loss: 1.138e-05, Loss_0: 2.202e-09, Loss_r: 1.136e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4770, Loss: 1.122e-05, Loss_0: 2.789e-10, Loss_r: 1.122e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4780, Loss: 1.108e-05, Loss_0: 2.535e-10, Loss_r: 1.108e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4790, Loss: 1.099e-05, Loss_0: 7.317e-11, Loss_r: 1.099e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4800, Loss: 1.091e-05, Loss_0: 8.815e-11, Loss_r: 1.091e-05, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4810, Loss: 5.860e-06, Loss_0: 1.803e-09, Loss_r: 5.841e-06, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 5.541e-06, Loss_0: 5.823e-12, Loss_r: 5.541e-06, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 4.989e-06, Loss_0: 8.107e-10, Loss_r: 4.981e-06, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 4.648e-06, Loss_0: 2.296e-12, Loss_r: 4.648e-06, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 4.361e-06, Loss_0: 7.381e-11, Loss_r: 4.360e-06, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 4.140e-06, Loss_0: 6.579e-11, Loss_r: 4.139e-06, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 4.203e-06, Loss_0: 2.516e-09, Loss_r: 4.178e-06, Time: 0.08, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 5.414e-05, Loss_0: 5.421e-07, Loss_r: 4.871e-05, Time: 0.06, Learning Rate: 0.00019\n",
            "It: 4890, Loss: 6.609e-06, Loss_0: 3.378e-08, Loss_r: 6.271e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4900, Loss: 4.405e-06, Loss_0: 9.558e-09, Loss_r: 4.310e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4910, Loss: 5.926e-06, Loss_0: 2.945e-08, Loss_r: 5.631e-06, Time: 0.06, Learning Rate: 0.00019\n",
            "It: 4920, Loss: 4.284e-06, Loss_0: 1.001e-08, Loss_r: 4.184e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4930, Loss: 3.417e-06, Loss_0: 2.490e-10, Loss_r: 3.415e-06, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4940, Loss: 3.473e-06, Loss_0: 1.893e-09, Loss_r: 3.455e-06, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4950, Loss: 3.277e-06, Loss_0: 1.232e-10, Loss_r: 3.275e-06, Time: 0.04, Learning Rate: 0.00019\n",
            "It: 4960, Loss: 3.213e-06, Loss_0: 3.806e-11, Loss_r: 3.212e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4970, Loss: 3.165e-06, Loss_0: 1.009e-10, Loss_r: 3.164e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4980, Loss: 3.117e-06, Loss_0: 2.109e-11, Loss_r: 3.117e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "It: 4990, Loss: 3.076e-06, Loss_0: 2.237e-11, Loss_r: 3.075e-06, Time: 0.05, Learning Rate: 0.00019\n",
            "Training time: 28.3993\n",
            "[1, 128, 128, 128, 128, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 3.355e-01, Loss_0: 1.546e-03, Loss_r: 3.339e-01, Time: 0.77, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.640e-01, Loss_0: 2.869e-05, Loss_r: 2.637e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.975e-01, Loss_0: 2.424e-03, Loss_r: 1.733e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.939e-01, Loss_0: 1.673e-03, Loss_r: 1.772e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.956e-01, Loss_0: 1.094e-03, Loss_r: 1.847e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.941e-01, Loss_0: 1.845e-03, Loss_r: 1.756e-01, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.926e-01, Loss_0: 1.428e-03, Loss_r: 1.784e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.921e-01, Loss_0: 1.452e-03, Loss_r: 1.775e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.908e-01, Loss_0: 1.472e-03, Loss_r: 1.761e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 90, Loss: 1.890e-01, Loss_0: 1.496e-03, Loss_r: 1.741e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 100, Loss: 1.857e-01, Loss_0: 1.454e-03, Loss_r: 1.712e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.785e-01, Loss_0: 1.406e-03, Loss_r: 1.644e-01, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.579e-01, Loss_0: 1.282e-03, Loss_r: 1.450e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 9.704e-02, Loss_0: 8.581e-04, Loss_r: 8.846e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 140, Loss: 6.242e-02, Loss_0: 3.487e-05, Loss_r: 6.207e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 150, Loss: 4.932e-02, Loss_0: 9.043e-07, Loss_r: 4.931e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 2.375e-02, Loss_0: 8.411e-05, Loss_r: 2.291e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.465e-02, Loss_0: 3.301e-05, Loss_r: 1.432e-02, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 9.676e-03, Loss_0: 2.852e-06, Loss_r: 9.648e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.796e-03, Loss_0: 1.217e-06, Loss_r: 5.784e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 200, Loss: 3.628e-03, Loss_0: 1.584e-06, Loss_r: 3.612e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 3.119e-03, Loss_0: 3.412e-06, Loss_r: 3.085e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 220, Loss: 2.481e-03, Loss_0: 3.146e-07, Loss_r: 2.478e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 2.008e-03, Loss_0: 6.387e-07, Loss_r: 2.001e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.634e-03, Loss_0: 7.092e-07, Loss_r: 1.627e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.314e-03, Loss_0: 3.985e-07, Loss_r: 1.310e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 1.055e-03, Loss_0: 1.195e-07, Loss_r: 1.054e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 270, Loss: 8.432e-04, Loss_0: 1.907e-07, Loss_r: 8.413e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 280, Loss: 6.828e-04, Loss_0: 6.582e-07, Loss_r: 6.762e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 3.205e-02, Loss_0: 2.497e-04, Loss_r: 2.955e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 1.544e-03, Loss_0: 2.032e-05, Loss_r: 1.341e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 3.383e-03, Loss_0: 2.183e-05, Loss_r: 3.164e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 9.333e-04, Loss_0: 1.335e-06, Loss_r: 9.200e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.511e-04, Loss_0: 1.153e-07, Loss_r: 4.500e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.626e-04, Loss_0: 1.761e-06, Loss_r: 4.450e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 350, Loss: 3.432e-04, Loss_0: 9.821e-07, Loss_r: 3.334e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 2.664e-04, Loss_0: 4.250e-07, Loss_r: 2.621e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.185e-04, Loss_0: 1.912e-08, Loss_r: 2.183e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.919e-04, Loss_0: 1.595e-10, Loss_r: 1.919e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 1.728e-04, Loss_0: 5.869e-08, Loss_r: 1.722e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 400, Loss: 1.572e-04, Loss_0: 5.116e-08, Loss_r: 1.567e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 410, Loss: 2.474e-04, Loss_0: 1.019e-07, Loss_r: 2.464e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 420, Loss: 2.241e-04, Loss_0: 5.099e-08, Loss_r: 2.236e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.039e-04, Loss_0: 2.910e-08, Loss_r: 2.036e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.868e-04, Loss_0: 2.302e-08, Loss_r: 1.866e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 1.724e-04, Loss_0: 2.274e-08, Loss_r: 1.722e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 1.602e-04, Loss_0: 1.746e-08, Loss_r: 1.600e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 1.497e-04, Loss_0: 1.241e-08, Loss_r: 1.495e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.450e-04, Loss_0: 7.167e-08, Loss_r: 1.443e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 1.221e-02, Loss_0: 3.367e-04, Loss_r: 8.842e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 500, Loss: 6.005e-03, Loss_0: 7.853e-05, Loss_r: 5.220e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.372e-03, Loss_0: 2.918e-05, Loss_r: 2.080e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 1.658e-03, Loss_0: 3.294e-05, Loss_r: 1.328e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 3.898e-04, Loss_0: 3.228e-06, Loss_r: 3.575e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.763e-04, Loss_0: 1.526e-07, Loss_r: 1.748e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.775e-04, Loss_0: 2.281e-08, Loss_r: 1.772e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.603e-04, Loss_0: 4.373e-07, Loss_r: 1.559e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 570, Loss: 1.436e-04, Loss_0: 3.863e-11, Loss_r: 1.436e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 1.330e-04, Loss_0: 2.130e-08, Loss_r: 1.328e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.259e-04, Loss_0: 6.272e-08, Loss_r: 1.253e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.188e-04, Loss_0: 7.226e-10, Loss_r: 1.188e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 1.284e-04, Loss_0: 4.410e-09, Loss_r: 1.283e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.218e-04, Loss_0: 1.926e-08, Loss_r: 1.216e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 630, Loss: 1.160e-04, Loss_0: 1.510e-08, Loss_r: 1.158e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 1.107e-04, Loss_0: 1.075e-08, Loss_r: 1.106e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 650, Loss: 1.058e-04, Loss_0: 8.894e-09, Loss_r: 1.057e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.013e-04, Loss_0: 7.704e-09, Loss_r: 1.013e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 9.717e-05, Loss_0: 6.810e-09, Loss_r: 9.710e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 9.328e-05, Loss_0: 5.898e-09, Loss_r: 9.322e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 8.964e-05, Loss_0: 4.915e-09, Loss_r: 8.959e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 700, Loss: 8.624e-05, Loss_0: 4.909e-09, Loss_r: 8.619e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 8.306e-05, Loss_0: 4.480e-09, Loss_r: 8.301e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 8.008e-05, Loss_0: 4.292e-09, Loss_r: 8.003e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 7.728e-05, Loss_0: 3.889e-09, Loss_r: 7.724e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 740, Loss: 7.466e-05, Loss_0: 3.673e-09, Loss_r: 7.463e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 7.221e-05, Loss_0: 3.188e-09, Loss_r: 7.218e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 6.998e-05, Loss_0: 1.183e-10, Loss_r: 6.998e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.464e-04, Loss_0: 2.319e-06, Loss_r: 1.232e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.265e-02, Loss_0: 7.296e-04, Loss_r: 3.535e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.424e-03, Loss_0: 8.010e-05, Loss_r: 1.623e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 2.964e-03, Loss_0: 2.719e-05, Loss_r: 2.692e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 6.854e-04, Loss_0: 8.633e-06, Loss_r: 5.990e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.093e-04, Loss_0: 2.807e-07, Loss_r: 2.065e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.539e-04, Loss_0: 3.246e-06, Loss_r: 2.215e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 840, Loss: 1.354e-04, Loss_0: 9.343e-07, Loss_r: 1.261e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 850, Loss: 1.064e-04, Loss_0: 3.250e-08, Loss_r: 1.061e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.004e-04, Loss_0: 3.521e-09, Loss_r: 1.004e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 9.527e-05, Loss_0: 6.489e-08, Loss_r: 9.462e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 9.087e-05, Loss_0: 1.696e-09, Loss_r: 9.085e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 890, Loss: 8.732e-05, Loss_0: 1.202e-08, Loss_r: 8.720e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 900, Loss: 8.436e-05, Loss_0: 9.387e-09, Loss_r: 8.426e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 910, Loss: 8.172e-05, Loss_0: 2.199e-09, Loss_r: 8.170e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 920, Loss: 7.929e-05, Loss_0: 7.355e-09, Loss_r: 7.922e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 930, Loss: 7.705e-05, Loss_0: 4.105e-09, Loss_r: 7.700e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 940, Loss: 7.496e-05, Loss_0: 2.776e-09, Loss_r: 7.494e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 950, Loss: 7.301e-05, Loss_0: 3.293e-09, Loss_r: 7.298e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 960, Loss: 7.118e-05, Loss_0: 3.445e-09, Loss_r: 7.115e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 970, Loss: 6.946e-05, Loss_0: 3.216e-09, Loss_r: 6.943e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 980, Loss: 6.784e-05, Loss_0: 2.944e-09, Loss_r: 6.781e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 990, Loss: 6.631e-05, Loss_0: 2.735e-09, Loss_r: 6.628e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 6.486e-05, Loss_0: 2.599e-09, Loss_r: 6.484e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 3.180e-05, Loss_0: 1.567e-10, Loss_r: 3.180e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 2.316e-05, Loss_0: 3.565e-10, Loss_r: 2.316e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.833e-05, Loss_0: 1.730e-10, Loss_r: 1.832e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.483e-05, Loss_0: 6.798e-10, Loss_r: 1.483e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.225e-05, Loss_0: 4.328e-10, Loss_r: 1.225e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.054e-05, Loss_0: 4.424e-11, Loss_r: 1.054e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 9.415e-06, Loss_0: 2.868e-17, Loss_r: 9.415e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 8.641e-06, Loss_0: 6.767e-12, Loss_r: 8.641e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 8.097e-06, Loss_0: 1.490e-11, Loss_r: 8.097e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 7.705e-06, Loss_0: 3.010e-12, Loss_r: 7.705e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 7.410e-06, Loss_0: 1.298e-12, Loss_r: 7.410e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 7.177e-06, Loss_0: 1.568e-12, Loss_r: 7.177e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 6.985e-06, Loss_0: 5.832e-15, Loss_r: 6.985e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 6.822e-06, Loss_0: 4.391e-13, Loss_r: 6.822e-06, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 6.678e-06, Loss_0: 1.592e-14, Loss_r: 6.678e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 6.549e-06, Loss_0: 3.586e-13, Loss_r: 6.549e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 6.430e-06, Loss_0: 1.378e-12, Loss_r: 6.430e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 6.320e-06, Loss_0: 9.765e-12, Loss_r: 6.319e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 6.229e-06, Loss_0: 3.777e-10, Loss_r: 6.226e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 8.414e-06, Loss_0: 5.861e-08, Loss_r: 7.827e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 7.529e-04, Loss_0: 1.855e-05, Loss_r: 5.674e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 9.209e-04, Loss_0: 1.455e-05, Loss_r: 7.754e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 6.359e-03, Loss_0: 1.406e-04, Loss_r: 4.953e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 1.988e-03, Loss_0: 4.082e-05, Loss_r: 1.580e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 7.842e-04, Loss_0: 1.281e-05, Loss_r: 6.561e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.799e-04, Loss_0: 6.896e-06, Loss_r: 2.110e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 1.273e-04, Loss_0: 1.052e-06, Loss_r: 1.168e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 7.975e-05, Loss_0: 9.410e-07, Loss_r: 7.034e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 6.083e-05, Loss_0: 1.798e-07, Loss_r: 5.904e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 5.037e-05, Loss_0: 1.641e-07, Loss_r: 4.873e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 4.426e-05, Loss_0: 8.526e-09, Loss_r: 4.418e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 4.144e-05, Loss_0: 2.886e-10, Loss_r: 4.144e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 3.989e-05, Loss_0: 9.600e-09, Loss_r: 3.979e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 3.834e-05, Loss_0: 1.154e-11, Loss_r: 3.834e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 3.712e-05, Loss_0: 2.089e-10, Loss_r: 3.712e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 3.604e-05, Loss_0: 1.946e-09, Loss_r: 3.602e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 3.507e-05, Loss_0: 1.720e-09, Loss_r: 3.505e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 3.417e-05, Loss_0: 9.770e-10, Loss_r: 3.416e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 3.335e-05, Loss_0: 7.392e-10, Loss_r: 3.334e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 3.258e-05, Loss_0: 7.104e-10, Loss_r: 3.258e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.536e-04, Loss_0: 1.685e-08, Loss_r: 1.535e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 1.312e-04, Loss_0: 4.985e-09, Loss_r: 1.311e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 1.171e-04, Loss_0: 2.189e-08, Loss_r: 1.169e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 1.067e-04, Loss_0: 2.138e-08, Loss_r: 1.065e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 9.882e-05, Loss_0: 5.729e-09, Loss_r: 9.876e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 9.303e-05, Loss_0: 6.098e-09, Loss_r: 9.297e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 8.836e-05, Loss_0: 7.245e-09, Loss_r: 8.829e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 8.438e-05, Loss_0: 5.484e-09, Loss_r: 8.432e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 8.088e-05, Loss_0: 3.887e-09, Loss_r: 8.084e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 7.775e-05, Loss_0: 3.191e-09, Loss_r: 7.771e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 7.495e-05, Loss_0: 3.555e-10, Loss_r: 7.495e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 7.684e-05, Loss_0: 1.603e-07, Loss_r: 7.524e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.639e-03, Loss_0: 7.180e-05, Loss_r: 9.206e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.521e-03, Loss_0: 2.607e-05, Loss_r: 1.260e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.963e-03, Loss_0: 8.193e-05, Loss_r: 2.144e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.954e-04, Loss_0: 9.481e-06, Loss_r: 2.006e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 5.369e-04, Loss_0: 7.839e-06, Loss_r: 4.585e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 1.028e-04, Loss_0: 2.932e-08, Loss_r: 1.025e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.500e-04, Loss_0: 1.485e-06, Loss_r: 1.352e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 9.165e-05, Loss_0: 1.002e-07, Loss_r: 9.064e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 3.947e-05, Loss_0: 4.797e-08, Loss_r: 3.899e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 3.637e-05, Loss_0: 5.267e-08, Loss_r: 3.584e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 3.272e-05, Loss_0: 9.919e-09, Loss_r: 3.262e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 3.064e-05, Loss_0: 1.542e-10, Loss_r: 3.063e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 2.958e-05, Loss_0: 6.365e-10, Loss_r: 2.958e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.899e-05, Loss_0: 1.094e-09, Loss_r: 2.898e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 2.860e-05, Loss_0: 8.206e-10, Loss_r: 2.859e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 2.830e-05, Loss_0: 5.652e-10, Loss_r: 2.829e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 2.806e-05, Loss_0: 4.260e-10, Loss_r: 2.805e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.784e-05, Loss_0: 3.312e-10, Loss_r: 2.784e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 2.764e-05, Loss_0: 2.825e-10, Loss_r: 2.763e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 2.744e-05, Loss_0: 3.077e-10, Loss_r: 2.743e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 2.724e-05, Loss_0: 3.587e-10, Loss_r: 2.724e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 2.705e-05, Loss_0: 3.734e-10, Loss_r: 2.704e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 2.685e-05, Loss_0: 3.540e-10, Loss_r: 2.685e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 2.666e-05, Loss_0: 3.383e-10, Loss_r: 2.666e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 2.648e-05, Loss_0: 3.465e-10, Loss_r: 2.647e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 2.629e-05, Loss_0: 3.276e-10, Loss_r: 2.628e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 2.610e-05, Loss_0: 3.272e-10, Loss_r: 2.610e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 2.592e-05, Loss_0: 3.175e-10, Loss_r: 2.591e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.268e-05, Loss_0: 1.765e-08, Loss_r: 2.251e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 1.829e-05, Loss_0: 2.235e-08, Loss_r: 1.807e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 1.559e-05, Loss_0: 2.955e-10, Loss_r: 1.558e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.402e-05, Loss_0: 4.919e-09, Loss_r: 1.397e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.284e-05, Loss_0: 5.118e-09, Loss_r: 1.279e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.278e-05, Loss_0: 2.362e-08, Loss_r: 1.255e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 2.851e-05, Loss_0: 4.346e-07, Loss_r: 2.416e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 8.094e-04, Loss_0: 1.994e-05, Loss_r: 6.101e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 6.863e-03, Loss_0: 1.547e-04, Loss_r: 5.316e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 7.801e-04, Loss_0: 1.772e-05, Loss_r: 6.029e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.973e-05, Loss_0: 9.090e-07, Loss_r: 4.064e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.045e-05, Loss_0: 7.173e-09, Loss_r: 1.037e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.294e-05, Loss_0: 7.877e-08, Loss_r: 1.216e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.836e-05, Loss_0: 2.122e-07, Loss_r: 1.624e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.783e-05, Loss_0: 2.196e-07, Loss_r: 1.564e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.191e-05, Loss_0: 8.479e-08, Loss_r: 1.106e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 8.196e-06, Loss_0: 1.540e-09, Loss_r: 8.181e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 8.347e-06, Loss_0: 1.004e-08, Loss_r: 8.247e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 7.980e-06, Loss_0: 2.676e-09, Loss_r: 7.954e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 7.883e-06, Loss_0: 9.116e-10, Loss_r: 7.874e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 6.298e-06, Loss_0: 3.984e-10, Loss_r: 6.294e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 6.062e-06, Loss_0: 1.706e-10, Loss_r: 6.061e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 5.970e-06, Loss_0: 3.199e-11, Loss_r: 5.969e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 5.887e-06, Loss_0: 1.349e-10, Loss_r: 5.886e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 5.833e-06, Loss_0: 4.294e-14, Loss_r: 5.833e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 5.794e-06, Loss_0: 3.477e-11, Loss_r: 5.794e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 5.761e-06, Loss_0: 2.626e-11, Loss_r: 5.761e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 5.732e-06, Loss_0: 9.760e-12, Loss_r: 5.732e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 5.704e-06, Loss_0: 9.494e-12, Loss_r: 5.704e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 5.677e-06, Loss_0: 2.929e-12, Loss_r: 5.677e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 5.654e-06, Loss_0: 2.871e-11, Loss_r: 5.654e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 5.844e-06, Loss_0: 5.120e-09, Loss_r: 5.793e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 3.853e-05, Loss_0: 8.502e-07, Loss_r: 3.003e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2140, Loss: 6.647e-06, Loss_0: 2.656e-08, Loss_r: 6.382e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2150, Loss: 8.125e-06, Loss_0: 6.914e-08, Loss_r: 7.433e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2160, Loss: 7.276e-06, Loss_0: 4.728e-08, Loss_r: 6.803e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2170, Loss: 5.565e-06, Loss_0: 1.180e-09, Loss_r: 5.553e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2180, Loss: 5.721e-06, Loss_0: 5.527e-09, Loss_r: 5.666e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2190, Loss: 5.513e-06, Loss_0: 1.612e-09, Loss_r: 5.497e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2200, Loss: 5.441e-06, Loss_0: 8.482e-11, Loss_r: 5.441e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2210, Loss: 6.687e-06, Loss_0: 5.514e-10, Loss_r: 6.682e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2220, Loss: 6.549e-06, Loss_0: 5.737e-10, Loss_r: 6.543e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2230, Loss: 6.475e-06, Loss_0: 8.328e-11, Loss_r: 6.475e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2240, Loss: 6.411e-06, Loss_0: 1.239e-10, Loss_r: 6.409e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2250, Loss: 6.364e-06, Loss_0: 8.563e-12, Loss_r: 6.364e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2260, Loss: 6.329e-06, Loss_0: 1.839e-13, Loss_r: 6.329e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2270, Loss: 6.296e-06, Loss_0: 2.656e-11, Loss_r: 6.296e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2280, Loss: 6.266e-06, Loss_0: 5.342e-11, Loss_r: 6.266e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2290, Loss: 6.240e-06, Loss_0: 1.808e-10, Loss_r: 6.239e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2300, Loss: 6.297e-06, Loss_0: 2.719e-09, Loss_r: 6.270e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.283e-05, Loss_0: 1.746e-07, Loss_r: 1.109e-05, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2320, Loss: 6.810e-06, Loss_0: 1.801e-08, Loss_r: 6.630e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2330, Loss: 6.253e-06, Loss_0: 2.651e-09, Loss_r: 6.226e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2340, Loss: 6.483e-06, Loss_0: 8.813e-09, Loss_r: 6.394e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2350, Loss: 6.116e-06, Loss_0: 6.231e-10, Loss_r: 6.110e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2360, Loss: 6.095e-06, Loss_0: 1.208e-09, Loss_r: 6.083e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2370, Loss: 6.039e-06, Loss_0: 1.994e-10, Loss_r: 6.037e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2380, Loss: 6.019e-06, Loss_0: 8.579e-11, Loss_r: 6.019e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2390, Loss: 5.989e-06, Loss_0: 8.492e-11, Loss_r: 5.989e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2400, Loss: 5.965e-06, Loss_0: 1.847e-11, Loss_r: 5.965e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2410, Loss: 2.259e-04, Loss_0: 1.272e-07, Loss_r: 2.247e-04, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 1.813e-04, Loss_0: 1.090e-06, Loss_r: 1.704e-04, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 9.486e-04, Loss_0: 3.301e-05, Loss_r: 6.184e-04, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2440, Loss: 1.479e-04, Loss_0: 1.738e-06, Loss_r: 1.305e-04, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2450, Loss: 1.650e-04, Loss_0: 2.205e-06, Loss_r: 1.429e-04, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2460, Loss: 1.347e-04, Loss_0: 1.384e-06, Loss_r: 1.209e-04, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2470, Loss: 9.823e-05, Loss_0: 5.179e-07, Loss_r: 9.305e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2480, Loss: 8.353e-05, Loss_0: 2.400e-08, Loss_r: 8.329e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2490, Loss: 8.013e-05, Loss_0: 1.606e-08, Loss_r: 7.997e-05, Time: 0.07, Learning Rate: 0.00073\n",
            "It: 2500, Loss: 7.575e-05, Loss_0: 2.931e-08, Loss_r: 7.546e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2510, Loss: 7.222e-05, Loss_0: 1.145e-08, Loss_r: 7.210e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2520, Loss: 6.956e-05, Loss_0: 4.179e-09, Loss_r: 6.952e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2530, Loss: 6.686e-05, Loss_0: 4.186e-09, Loss_r: 6.682e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2540, Loss: 6.691e-05, Loss_0: 8.475e-08, Loss_r: 6.606e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2550, Loss: 2.025e-04, Loss_0: 5.713e-06, Loss_r: 1.454e-04, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2560, Loss: 7.757e-05, Loss_0: 6.429e-07, Loss_r: 7.114e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2570, Loss: 5.996e-05, Loss_0: 8.438e-08, Loss_r: 5.912e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2580, Loss: 6.481e-05, Loss_0: 3.852e-07, Loss_r: 6.096e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2590, Loss: 5.639e-05, Loss_0: 4.791e-08, Loss_r: 5.591e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2600, Loss: 5.533e-05, Loss_0: 2.325e-08, Loss_r: 5.510e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2610, Loss: 4.384e-05, Loss_0: 4.334e-08, Loss_r: 4.340e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2620, Loss: 3.308e-05, Loss_0: 6.987e-09, Loss_r: 3.301e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2630, Loss: 2.763e-05, Loss_0: 2.414e-09, Loss_r: 2.761e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2640, Loss: 2.327e-05, Loss_0: 3.167e-09, Loss_r: 2.324e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2650, Loss: 2.025e-05, Loss_0: 3.356e-10, Loss_r: 2.025e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2660, Loss: 1.827e-05, Loss_0: 4.667e-13, Loss_r: 1.827e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2670, Loss: 1.682e-05, Loss_0: 7.027e-11, Loss_r: 1.682e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2680, Loss: 1.572e-05, Loss_0: 3.136e-11, Loss_r: 1.572e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2690, Loss: 1.484e-05, Loss_0: 1.906e-11, Loss_r: 1.484e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2700, Loss: 1.410e-05, Loss_0: 4.099e-12, Loss_r: 1.410e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2710, Loss: 1.345e-05, Loss_0: 9.940e-13, Loss_r: 1.345e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 1.287e-05, Loss_0: 1.851e-13, Loss_r: 1.287e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 1.233e-05, Loss_0: 1.199e-12, Loss_r: 1.233e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 1.184e-05, Loss_0: 6.610e-13, Loss_r: 1.184e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 1.138e-05, Loss_0: 7.238e-13, Loss_r: 1.138e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 1.096e-05, Loss_0: 2.297e-12, Loss_r: 1.096e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 1.056e-05, Loss_0: 2.129e-11, Loss_r: 1.056e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 1.020e-05, Loss_0: 6.000e-10, Loss_r: 1.019e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 1.112e-05, Loss_0: 4.162e-08, Loss_r: 1.071e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 1.985e-04, Loss_0: 6.013e-06, Loss_r: 1.384e-04, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2810, Loss: 9.753e-05, Loss_0: 5.620e-07, Loss_r: 9.191e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2820, Loss: 8.381e-05, Loss_0: 5.583e-07, Loss_r: 7.822e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2830, Loss: 5.805e-05, Loss_0: 6.614e-08, Loss_r: 5.739e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2840, Loss: 5.212e-05, Loss_0: 2.288e-07, Loss_r: 4.983e-05, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 2850, Loss: 4.511e-05, Loss_0: 5.088e-08, Loss_r: 4.460e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2860, Loss: 4.156e-05, Loss_0: 5.160e-08, Loss_r: 4.104e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2870, Loss: 3.918e-05, Loss_0: 1.221e-09, Loss_r: 3.917e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2880, Loss: 3.789e-05, Loss_0: 2.003e-09, Loss_r: 3.787e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2890, Loss: 3.670e-05, Loss_0: 3.898e-11, Loss_r: 3.669e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 2900, Loss: 3.598e-05, Loss_0: 5.974e-09, Loss_r: 3.592e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2910, Loss: 4.875e-05, Loss_0: 5.334e-07, Loss_r: 4.341e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 2920, Loss: 1.694e-03, Loss_0: 6.638e-05, Loss_r: 1.030e-03, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2930, Loss: 5.598e-04, Loss_0: 1.828e-05, Loss_r: 3.770e-04, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 2940, Loss: 2.418e-04, Loss_0: 7.011e-06, Loss_r: 1.717e-04, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 2950, Loss: 1.122e-04, Loss_0: 2.787e-06, Loss_r: 8.433e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2960, Loss: 6.045e-05, Loss_0: 9.444e-07, Loss_r: 5.100e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2970, Loss: 4.040e-05, Loss_0: 2.414e-07, Loss_r: 3.798e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 2980, Loss: 3.309e-05, Loss_0: 3.014e-08, Loss_r: 3.279e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 2990, Loss: 3.141e-05, Loss_0: 2.117e-09, Loss_r: 3.139e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3000, Loss: 3.126e-05, Loss_0: 2.115e-08, Loss_r: 3.105e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3010, Loss: 2.064e-05, Loss_0: 7.613e-11, Loss_r: 2.064e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3020, Loss: 1.748e-05, Loss_0: 5.890e-10, Loss_r: 1.748e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3030, Loss: 1.556e-05, Loss_0: 4.681e-10, Loss_r: 1.555e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3040, Loss: 1.414e-05, Loss_0: 2.019e-13, Loss_r: 1.414e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3050, Loss: 1.326e-05, Loss_0: 1.692e-10, Loss_r: 1.326e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3060, Loss: 1.271e-05, Loss_0: 4.397e-12, Loss_r: 1.271e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3070, Loss: 1.233e-05, Loss_0: 6.717e-15, Loss_r: 1.233e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3080, Loss: 1.203e-05, Loss_0: 4.150e-11, Loss_r: 1.203e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3090, Loss: 1.179e-05, Loss_0: 4.580e-12, Loss_r: 1.179e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3100, Loss: 1.158e-05, Loss_0: 2.416e-11, Loss_r: 1.158e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3110, Loss: 1.139e-05, Loss_0: 1.682e-11, Loss_r: 1.139e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3120, Loss: 1.121e-05, Loss_0: 2.224e-11, Loss_r: 1.121e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3130, Loss: 1.105e-05, Loss_0: 2.032e-11, Loss_r: 1.105e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3140, Loss: 1.089e-05, Loss_0: 2.630e-11, Loss_r: 1.089e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3150, Loss: 1.075e-05, Loss_0: 2.184e-11, Loss_r: 1.075e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3160, Loss: 1.061e-05, Loss_0: 2.230e-11, Loss_r: 1.061e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3170, Loss: 1.048e-05, Loss_0: 2.408e-11, Loss_r: 1.048e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3180, Loss: 1.036e-05, Loss_0: 3.120e-11, Loss_r: 1.036e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3190, Loss: 1.024e-05, Loss_0: 5.130e-11, Loss_r: 1.024e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3200, Loss: 1.014e-05, Loss_0: 3.814e-10, Loss_r: 1.013e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3210, Loss: 8.688e-05, Loss_0: 2.209e-06, Loss_r: 6.479e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3220, Loss: 1.597e-05, Loss_0: 9.380e-08, Loss_r: 1.504e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3230, Loss: 1.469e-05, Loss_0: 9.012e-08, Loss_r: 1.379e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3240, Loss: 1.497e-05, Loss_0: 1.249e-07, Loss_r: 1.372e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3250, Loss: 1.035e-05, Loss_0: 1.005e-08, Loss_r: 1.025e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3260, Loss: 9.848e-06, Loss_0: 8.789e-09, Loss_r: 9.760e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3270, Loss: 9.263e-06, Loss_0: 3.219e-09, Loss_r: 9.231e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3280, Loss: 8.867e-06, Loss_0: 1.197e-09, Loss_r: 8.855e-06, Time: 0.09, Learning Rate: 0.00048\n",
            "It: 3290, Loss: 8.535e-06, Loss_0: 1.005e-10, Loss_r: 8.534e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3300, Loss: 8.273e-06, Loss_0: 3.168e-10, Loss_r: 8.270e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3310, Loss: 8.018e-06, Loss_0: 4.872e-11, Loss_r: 8.018e-06, Time: 0.09, Learning Rate: 0.00048\n",
            "It: 3320, Loss: 7.785e-06, Loss_0: 1.017e-11, Loss_r: 7.785e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3330, Loss: 7.569e-06, Loss_0: 6.694e-13, Loss_r: 7.569e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3340, Loss: 7.367e-06, Loss_0: 2.461e-12, Loss_r: 7.367e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3350, Loss: 7.177e-06, Loss_0: 2.994e-13, Loss_r: 7.177e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3360, Loss: 6.999e-06, Loss_0: 6.013e-12, Loss_r: 6.999e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 6.832e-06, Loss_0: 1.344e-12, Loss_r: 6.832e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 6.675e-06, Loss_0: 1.825e-12, Loss_r: 6.675e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 6.528e-06, Loss_0: 2.479e-12, Loss_r: 6.528e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 6.389e-06, Loss_0: 4.333e-12, Loss_r: 6.389e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 6.939e-06, Loss_0: 3.526e-08, Loss_r: 6.586e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 3.934e-05, Loss_0: 9.451e-07, Loss_r: 2.989e-05, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3430, Loss: 1.294e-05, Loss_0: 2.141e-07, Loss_r: 1.080e-05, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3440, Loss: 5.610e-06, Loss_0: 1.350e-08, Loss_r: 5.475e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3450, Loss: 5.201e-06, Loss_0: 6.421e-09, Loss_r: 5.137e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3460, Loss: 5.394e-06, Loss_0: 1.578e-08, Loss_r: 5.236e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3470, Loss: 4.834e-06, Loss_0: 3.925e-09, Loss_r: 4.794e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3480, Loss: 4.573e-06, Loss_0: 2.260e-10, Loss_r: 4.571e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3490, Loss: 4.477e-06, Loss_0: 8.434e-10, Loss_r: 4.468e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3500, Loss: 4.335e-06, Loss_0: 7.772e-12, Loss_r: 4.335e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3510, Loss: 4.234e-06, Loss_0: 8.998e-11, Loss_r: 4.233e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3520, Loss: 4.133e-06, Loss_0: 2.825e-11, Loss_r: 4.133e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3530, Loss: 4.041e-06, Loss_0: 8.389e-14, Loss_r: 4.041e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3540, Loss: 3.954e-06, Loss_0: 1.095e-12, Loss_r: 3.954e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3550, Loss: 3.873e-06, Loss_0: 2.827e-12, Loss_r: 3.873e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3560, Loss: 3.798e-06, Loss_0: 7.120e-14, Loss_r: 3.798e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3570, Loss: 3.726e-06, Loss_0: 9.893e-13, Loss_r: 3.726e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3580, Loss: 3.660e-06, Loss_0: 3.597e-13, Loss_r: 3.660e-06, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3590, Loss: 3.597e-06, Loss_0: 8.441e-13, Loss_r: 3.597e-06, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3600, Loss: 3.538e-06, Loss_0: 6.149e-13, Loss_r: 3.538e-06, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3610, Loss: 2.768e-06, Loss_0: 2.298e-09, Loss_r: 2.745e-06, Time: 0.08, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 2.964e-06, Loss_0: 7.828e-09, Loss_r: 2.885e-06, Time: 0.10, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 6.696e-06, Loss_0: 1.102e-07, Loss_r: 5.594e-06, Time: 0.08, Learning Rate: 0.00039\n",
            "It: 3640, Loss: 3.664e-06, Loss_0: 2.851e-08, Loss_r: 3.379e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3650, Loss: 2.690e-06, Loss_0: 3.194e-09, Loss_r: 2.658e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3660, Loss: 2.544e-06, Loss_0: 1.623e-10, Loss_r: 2.543e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3670, Loss: 2.567e-06, Loss_0: 1.465e-09, Loss_r: 2.553e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3680, Loss: 2.515e-06, Loss_0: 6.615e-10, Loss_r: 2.508e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3690, Loss: 2.470e-06, Loss_0: 8.716e-14, Loss_r: 2.470e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3700, Loss: 2.452e-06, Loss_0: 7.594e-11, Loss_r: 2.452e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3710, Loss: 2.429e-06, Loss_0: 1.469e-13, Loss_r: 2.429e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3720, Loss: 2.410e-06, Loss_0: 2.000e-11, Loss_r: 2.410e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3730, Loss: 2.391e-06, Loss_0: 1.541e-13, Loss_r: 2.391e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3740, Loss: 2.374e-06, Loss_0: 3.870e-13, Loss_r: 2.374e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3750, Loss: 2.357e-06, Loss_0: 2.602e-12, Loss_r: 2.357e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3760, Loss: 2.342e-06, Loss_0: 3.592e-13, Loss_r: 2.342e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3770, Loss: 2.327e-06, Loss_0: 2.190e-12, Loss_r: 2.327e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 2.312e-06, Loss_0: 1.011e-12, Loss_r: 2.312e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 2.298e-06, Loss_0: 6.490e-13, Loss_r: 2.298e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 2.285e-06, Loss_0: 3.503e-13, Loss_r: 2.285e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.206e-05, Loss_0: 4.366e-08, Loss_r: 1.162e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3820, Loss: 9.470e-06, Loss_0: 1.597e-08, Loss_r: 9.310e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3830, Loss: 8.511e-06, Loss_0: 5.561e-09, Loss_r: 8.455e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3840, Loss: 7.915e-06, Loss_0: 1.066e-09, Loss_r: 7.905e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3850, Loss: 7.631e-06, Loss_0: 4.729e-12, Loss_r: 7.631e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3860, Loss: 7.515e-06, Loss_0: 2.240e-10, Loss_r: 7.512e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3870, Loss: 7.441e-06, Loss_0: 3.369e-10, Loss_r: 7.438e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3880, Loss: 7.383e-06, Loss_0: 1.247e-10, Loss_r: 7.382e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3890, Loss: 7.340e-06, Loss_0: 1.501e-11, Loss_r: 7.340e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3900, Loss: 7.301e-06, Loss_0: 2.621e-11, Loss_r: 7.301e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3910, Loss: 7.265e-06, Loss_0: 6.487e-11, Loss_r: 7.264e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3920, Loss: 7.231e-06, Loss_0: 4.299e-11, Loss_r: 7.230e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 3930, Loss: 7.198e-06, Loss_0: 3.738e-11, Loss_r: 7.198e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3940, Loss: 7.167e-06, Loss_0: 4.919e-11, Loss_r: 7.166e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3950, Loss: 7.136e-06, Loss_0: 4.128e-11, Loss_r: 7.136e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 3960, Loss: 7.107e-06, Loss_0: 4.115e-11, Loss_r: 7.107e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3970, Loss: 7.079e-06, Loss_0: 3.662e-11, Loss_r: 7.078e-06, Time: 0.07, Learning Rate: 0.00035\n",
            "It: 3980, Loss: 7.051e-06, Loss_0: 4.025e-11, Loss_r: 7.051e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 3990, Loss: 7.025e-06, Loss_0: 4.065e-11, Loss_r: 7.024e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4000, Loss: 6.999e-06, Loss_0: 4.143e-11, Loss_r: 6.998e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4010, Loss: 9.701e-06, Loss_0: 1.206e-07, Loss_r: 8.496e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4020, Loss: 5.504e-06, Loss_0: 3.896e-08, Loss_r: 5.114e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4030, Loss: 3.887e-06, Loss_0: 1.170e-08, Loss_r: 3.770e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4040, Loss: 3.040e-06, Loss_0: 2.378e-09, Loss_r: 3.016e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4050, Loss: 2.642e-06, Loss_0: 9.855e-11, Loss_r: 2.641e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4060, Loss: 2.459e-06, Loss_0: 1.214e-10, Loss_r: 2.458e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4070, Loss: 2.334e-06, Loss_0: 2.607e-10, Loss_r: 2.332e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4080, Loss: 2.234e-06, Loss_0: 7.941e-11, Loss_r: 2.233e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4090, Loss: 2.157e-06, Loss_0: 2.040e-15, Loss_r: 2.157e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4100, Loss: 2.093e-06, Loss_0: 9.340e-12, Loss_r: 2.093e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4110, Loss: 2.036e-06, Loss_0: 2.469e-13, Loss_r: 2.036e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4120, Loss: 1.984e-06, Loss_0: 3.098e-12, Loss_r: 1.984e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4130, Loss: 1.936e-06, Loss_0: 9.291e-15, Loss_r: 1.936e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4140, Loss: 1.890e-06, Loss_0: 3.592e-14, Loss_r: 1.890e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4150, Loss: 1.847e-06, Loss_0: 4.724e-13, Loss_r: 1.847e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4160, Loss: 1.807e-06, Loss_0: 1.208e-14, Loss_r: 1.807e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4170, Loss: 1.768e-06, Loss_0: 2.525e-13, Loss_r: 1.768e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4180, Loss: 1.732e-06, Loss_0: 1.198e-14, Loss_r: 1.732e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4190, Loss: 1.698e-06, Loss_0: 1.071e-12, Loss_r: 1.698e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4200, Loss: 1.665e-06, Loss_0: 1.494e-15, Loss_r: 1.665e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4210, Loss: 2.855e-05, Loss_0: 1.568e-08, Loss_r: 2.839e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4220, Loss: 2.318e-05, Loss_0: 5.817e-09, Loss_r: 2.312e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4230, Loss: 2.059e-05, Loss_0: 2.619e-09, Loss_r: 2.057e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4240, Loss: 1.871e-05, Loss_0: 4.557e-10, Loss_r: 1.870e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4250, Loss: 1.754e-05, Loss_0: 5.545e-12, Loss_r: 1.754e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4260, Loss: 1.683e-05, Loss_0: 2.604e-10, Loss_r: 1.683e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4270, Loss: 1.631e-05, Loss_0: 5.761e-10, Loss_r: 1.630e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4280, Loss: 1.588e-05, Loss_0: 5.342e-10, Loss_r: 1.587e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4290, Loss: 1.550e-05, Loss_0: 2.746e-10, Loss_r: 1.550e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4300, Loss: 1.517e-05, Loss_0: 2.074e-10, Loss_r: 1.516e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4310, Loss: 1.485e-05, Loss_0: 2.888e-10, Loss_r: 1.485e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4320, Loss: 1.456e-05, Loss_0: 2.466e-10, Loss_r: 1.456e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4330, Loss: 1.429e-05, Loss_0: 2.300e-10, Loss_r: 1.428e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4340, Loss: 1.403e-05, Loss_0: 2.333e-10, Loss_r: 1.403e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4350, Loss: 1.379e-05, Loss_0: 2.044e-10, Loss_r: 1.379e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4360, Loss: 1.356e-05, Loss_0: 2.216e-10, Loss_r: 1.356e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4370, Loss: 1.335e-05, Loss_0: 2.088e-10, Loss_r: 1.335e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4380, Loss: 1.315e-05, Loss_0: 1.984e-10, Loss_r: 1.314e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4390, Loss: 1.296e-05, Loss_0: 1.680e-10, Loss_r: 1.296e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4400, Loss: 1.278e-05, Loss_0: 1.609e-10, Loss_r: 1.278e-05, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4410, Loss: 1.389e-05, Loss_0: 2.099e-12, Loss_r: 1.389e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4420, Loss: 1.442e-05, Loss_0: 2.019e-08, Loss_r: 1.422e-05, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4430, Loss: 1.380e-04, Loss_0: 3.811e-06, Loss_r: 9.992e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4440, Loss: 2.418e-05, Loss_0: 3.181e-07, Loss_r: 2.100e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4450, Loss: 1.439e-05, Loss_0: 4.627e-08, Loss_r: 1.392e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4460, Loss: 1.894e-05, Loss_0: 1.955e-07, Loss_r: 1.698e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4470, Loss: 1.458e-05, Loss_0: 6.153e-08, Loss_r: 1.396e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4480, Loss: 1.272e-05, Loss_0: 1.556e-09, Loss_r: 1.271e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4490, Loss: 1.283e-05, Loss_0: 7.948e-09, Loss_r: 1.275e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4500, Loss: 1.238e-05, Loss_0: 1.425e-09, Loss_r: 1.237e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4510, Loss: 1.226e-05, Loss_0: 1.532e-09, Loss_r: 1.224e-05, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4520, Loss: 1.213e-05, Loss_0: 1.313e-10, Loss_r: 1.213e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4530, Loss: 1.200e-05, Loss_0: 5.582e-10, Loss_r: 1.199e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4540, Loss: 1.188e-05, Loss_0: 4.846e-11, Loss_r: 1.188e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4550, Loss: 1.178e-05, Loss_0: 2.318e-10, Loss_r: 1.178e-05, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4560, Loss: 1.168e-05, Loss_0: 6.870e-11, Loss_r: 1.167e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4570, Loss: 1.158e-05, Loss_0: 1.836e-10, Loss_r: 1.157e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4580, Loss: 1.148e-05, Loss_0: 1.380e-10, Loss_r: 1.148e-05, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4590, Loss: 1.139e-05, Loss_0: 1.066e-10, Loss_r: 1.139e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4600, Loss: 1.130e-05, Loss_0: 1.116e-10, Loss_r: 1.130e-05, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4610, Loss: 8.927e-06, Loss_0: 1.096e-08, Loss_r: 8.817e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4620, Loss: 2.675e-05, Loss_0: 5.677e-07, Loss_r: 2.107e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4630, Loss: 1.106e-05, Loss_0: 1.065e-07, Loss_r: 9.996e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4640, Loss: 7.219e-06, Loss_0: 2.622e-09, Loss_r: 7.193e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4650, Loss: 7.219e-06, Loss_0: 7.462e-09, Loss_r: 7.145e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4660, Loss: 7.185e-06, Loss_0: 9.786e-09, Loss_r: 7.087e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4670, Loss: 6.783e-06, Loss_0: 1.221e-09, Loss_r: 6.771e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4680, Loss: 6.676e-06, Loss_0: 5.613e-10, Loss_r: 6.670e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4690, Loss: 6.610e-06, Loss_0: 5.401e-10, Loss_r: 6.604e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4700, Loss: 6.538e-06, Loss_0: 1.324e-11, Loss_r: 6.538e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4710, Loss: 6.483e-06, Loss_0: 6.747e-12, Loss_r: 6.482e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4720, Loss: 6.431e-06, Loss_0: 7.286e-11, Loss_r: 6.430e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4730, Loss: 6.380e-06, Loss_0: 1.853e-12, Loss_r: 6.380e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4740, Loss: 6.334e-06, Loss_0: 1.398e-11, Loss_r: 6.334e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4750, Loss: 6.289e-06, Loss_0: 1.443e-11, Loss_r: 6.289e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4760, Loss: 6.247e-06, Loss_0: 9.708e-12, Loss_r: 6.247e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4770, Loss: 6.207e-06, Loss_0: 1.301e-11, Loss_r: 6.207e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4780, Loss: 6.169e-06, Loss_0: 1.546e-11, Loss_r: 6.168e-06, Time: 0.05, Learning Rate: 0.00023\n",
            "It: 4790, Loss: 6.132e-06, Loss_0: 1.123e-11, Loss_r: 6.132e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4800, Loss: 6.097e-06, Loss_0: 1.046e-11, Loss_r: 6.097e-06, Time: 0.06, Learning Rate: 0.00023\n",
            "It: 4810, Loss: 1.608e-05, Loss_0: 3.473e-09, Loss_r: 1.604e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4820, Loss: 1.502e-05, Loss_0: 1.568e-09, Loss_r: 1.501e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4830, Loss: 1.436e-05, Loss_0: 3.962e-10, Loss_r: 1.436e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4840, Loss: 1.388e-05, Loss_0: 9.100e-12, Loss_r: 1.388e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4850, Loss: 1.356e-05, Loss_0: 3.433e-10, Loss_r: 1.356e-05, Time: 0.07, Learning Rate: 0.00021\n",
            "It: 4860, Loss: 1.333e-05, Loss_0: 4.889e-10, Loss_r: 1.333e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4870, Loss: 1.313e-05, Loss_0: 3.227e-10, Loss_r: 1.313e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4880, Loss: 1.296e-05, Loss_0: 1.686e-10, Loss_r: 1.296e-05, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4890, Loss: 1.280e-05, Loss_0: 1.505e-10, Loss_r: 1.280e-05, Time: 0.06, Learning Rate: 0.00021\n",
            "It: 4900, Loss: 1.265e-05, Loss_0: 1.843e-10, Loss_r: 1.265e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4910, Loss: 1.251e-05, Loss_0: 1.759e-10, Loss_r: 1.251e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4920, Loss: 1.237e-05, Loss_0: 1.584e-10, Loss_r: 1.237e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4930, Loss: 1.224e-05, Loss_0: 1.631e-10, Loss_r: 1.224e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4940, Loss: 1.212e-05, Loss_0: 1.554e-10, Loss_r: 1.211e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4950, Loss: 1.199e-05, Loss_0: 1.581e-10, Loss_r: 1.199e-05, Time: 0.04, Learning Rate: 0.00021\n",
            "It: 4960, Loss: 1.188e-05, Loss_0: 1.551e-10, Loss_r: 1.188e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4970, Loss: 1.177e-05, Loss_0: 1.556e-10, Loss_r: 1.176e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4980, Loss: 1.166e-05, Loss_0: 1.445e-10, Loss_r: 1.166e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "It: 4990, Loss: 1.155e-05, Loss_0: 1.516e-10, Loss_r: 1.155e-05, Time: 0.05, Learning Rate: 0.00021\n",
            "Training time: 27.4967\n",
            "[1, 128, 128, 64, 64, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 5.595e-01, Loss_0: 9.679e-04, Loss_r: 5.586e-01, Time: 1.45, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.094e-01, Loss_0: 8.767e-05, Loss_r: 2.086e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.950e-01, Loss_0: 1.560e-03, Loss_r: 1.794e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.911e-01, Loss_0: 1.651e-03, Loss_r: 1.746e-01, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.851e-01, Loss_0: 1.493e-03, Loss_r: 1.701e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 50, Loss: 1.780e-01, Loss_0: 1.095e-03, Loss_r: 1.670e-01, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 60, Loss: 1.629e-01, Loss_0: 1.486e-03, Loss_r: 1.480e-01, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 70, Loss: 1.323e-01, Loss_0: 1.038e-03, Loss_r: 1.219e-01, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 80, Loss: 8.192e-02, Loss_0: 6.430e-04, Loss_r: 7.549e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 90, Loss: 4.132e-02, Loss_0: 1.358e-04, Loss_r: 3.996e-02, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 100, Loss: 2.794e-02, Loss_0: 3.983e-05, Loss_r: 2.754e-02, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 110, Loss: 1.725e-02, Loss_0: 2.884e-05, Loss_r: 1.696e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 120, Loss: 1.127e-02, Loss_0: 2.149e-05, Loss_r: 1.106e-02, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 130, Loss: 7.521e-03, Loss_0: 8.034e-06, Loss_r: 7.441e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.522e-03, Loss_0: 3.980e-06, Loss_r: 5.482e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 150, Loss: 4.246e-03, Loss_0: 2.398e-06, Loss_r: 4.222e-03, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 160, Loss: 3.409e-03, Loss_0: 1.613e-06, Loss_r: 3.393e-03, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 170, Loss: 2.724e-03, Loss_0: 1.184e-06, Loss_r: 2.712e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 180, Loss: 2.124e-03, Loss_0: 7.629e-07, Loss_r: 2.116e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.658e-03, Loss_0: 2.809e-07, Loss_r: 1.656e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 200, Loss: 2.024e-03, Loss_0: 7.025e-06, Loss_r: 1.954e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 210, Loss: 2.448e-03, Loss_0: 6.018e-08, Loss_r: 2.447e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 1.975e-03, Loss_0: 4.639e-09, Loss_r: 1.974e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 1.550e-03, Loss_0: 3.828e-07, Loss_r: 1.546e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 240, Loss: 1.277e-03, Loss_0: 7.347e-08, Loss_r: 1.276e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 1.049e-03, Loss_0: 2.621e-06, Loss_r: 1.022e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 260, Loss: 2.507e-03, Loss_0: 5.431e-05, Loss_r: 1.963e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 270, Loss: 8.233e-04, Loss_0: 5.010e-06, Loss_r: 7.732e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 280, Loss: 6.992e-04, Loss_0: 1.130e-06, Loss_r: 6.879e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 290, Loss: 6.711e-04, Loss_0: 1.828e-06, Loss_r: 6.528e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.886e-04, Loss_0: 3.988e-06, Loss_r: 5.487e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.710e-04, Loss_0: 5.439e-06, Loss_r: 5.166e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 320, Loss: 1.948e-03, Loss_0: 6.262e-05, Loss_r: 1.322e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 4.850e-04, Loss_0: 1.840e-06, Loss_r: 4.666e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.438e-04, Loss_0: 4.910e-06, Loss_r: 4.947e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.770e-04, Loss_0: 5.549e-06, Loss_r: 4.215e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.032e-04, Loss_0: 1.688e-06, Loss_r: 3.864e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.470e-04, Loss_0: 8.032e-06, Loss_r: 4.667e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 380, Loss: 1.090e-03, Loss_0: 2.854e-05, Loss_r: 8.043e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.483e-04, Loss_0: 1.097e-05, Loss_r: 4.386e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.272e-04, Loss_0: 4.701e-06, Loss_r: 3.801e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 410, Loss: 4.599e-04, Loss_0: 5.942e-06, Loss_r: 4.005e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.537e-04, Loss_0: 1.022e-05, Loss_r: 4.515e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 9.465e-04, Loss_0: 2.731e-05, Loss_r: 6.734e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.714e-04, Loss_0: 2.415e-06, Loss_r: 3.473e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.541e-04, Loss_0: 9.582e-06, Loss_r: 4.583e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 3.184e-04, Loss_0: 1.356e-06, Loss_r: 3.049e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.676e-04, Loss_0: 7.488e-06, Loss_r: 3.927e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 1.415e-03, Loss_0: 4.366e-05, Loss_r: 9.789e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 9.067e-04, Loss_0: 2.477e-05, Loss_r: 6.590e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.731e-04, Loss_0: 7.925e-07, Loss_r: 2.652e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.984e-04, Loss_0: 2.926e-06, Loss_r: 2.692e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.472e-04, Loss_0: 9.112e-07, Loss_r: 2.381e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 2.426e-04, Loss_0: 5.647e-07, Loss_r: 2.370e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 2.251e-04, Loss_0: 5.304e-07, Loss_r: 2.198e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 550, Loss: 2.113e-04, Loss_0: 1.313e-07, Loss_r: 2.100e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 560, Loss: 2.041e-04, Loss_0: 6.121e-08, Loss_r: 2.034e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 2.537e-04, Loss_0: 2.908e-06, Loss_r: 2.246e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 580, Loss: 6.241e-03, Loss_0: 2.537e-04, Loss_r: 3.704e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 590, Loss: 7.036e-04, Loss_0: 2.222e-05, Loss_r: 4.814e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 600, Loss: 8.039e-04, Loss_0: 2.450e-05, Loss_r: 5.590e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 610, Loss: 8.321e-05, Loss_0: 1.588e-07, Loss_r: 8.162e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.541e-04, Loss_0: 3.327e-06, Loss_r: 1.208e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 630, Loss: 6.643e-05, Loss_0: 3.153e-08, Loss_r: 6.612e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 7.242e-05, Loss_0: 4.027e-07, Loss_r: 6.839e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 650, Loss: 5.928e-05, Loss_0: 3.844e-09, Loss_r: 5.925e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 660, Loss: 5.812e-05, Loss_0: 3.551e-08, Loss_r: 5.777e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 5.647e-05, Loss_0: 2.215e-08, Loss_r: 5.625e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 680, Loss: 5.507e-05, Loss_0: 5.657e-09, Loss_r: 5.501e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 690, Loss: 5.416e-05, Loss_0: 1.503e-09, Loss_r: 5.415e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 700, Loss: 5.345e-05, Loss_0: 8.127e-10, Loss_r: 5.344e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 710, Loss: 5.281e-05, Loss_0: 7.430e-10, Loss_r: 5.281e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 720, Loss: 5.223e-05, Loss_0: 6.543e-10, Loss_r: 5.222e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 730, Loss: 5.167e-05, Loss_0: 4.175e-10, Loss_r: 5.167e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 740, Loss: 5.114e-05, Loss_0: 1.951e-10, Loss_r: 5.114e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 750, Loss: 5.062e-05, Loss_0: 1.856e-10, Loss_r: 5.062e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 760, Loss: 5.011e-05, Loss_0: 3.027e-10, Loss_r: 5.011e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 770, Loss: 4.962e-05, Loss_0: 2.550e-10, Loss_r: 4.961e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 780, Loss: 4.913e-05, Loss_0: 2.774e-10, Loss_r: 4.912e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 4.865e-05, Loss_0: 2.585e-10, Loss_r: 4.864e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 800, Loss: 4.817e-05, Loss_0: 2.663e-10, Loss_r: 4.817e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 810, Loss: 1.216e-04, Loss_0: 1.494e-07, Loss_r: 1.201e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 820, Loss: 2.044e-04, Loss_0: 3.492e-06, Loss_r: 1.695e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 830, Loss: 4.178e-03, Loss_0: 1.656e-04, Loss_r: 2.522e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 840, Loss: 4.475e-04, Loss_0: 1.465e-05, Loss_r: 3.010e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 850, Loss: 5.348e-04, Loss_0: 1.849e-05, Loss_r: 3.499e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 860, Loss: 2.486e-04, Loss_0: 6.229e-06, Loss_r: 1.863e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.125e-04, Loss_0: 1.842e-07, Loss_r: 1.107e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.252e-04, Loss_0: 6.783e-07, Loss_r: 1.184e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.060e-04, Loss_0: 2.161e-08, Loss_r: 1.058e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 900, Loss: 1.062e-04, Loss_0: 1.966e-07, Loss_r: 1.042e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.025e-04, Loss_0: 4.181e-08, Loss_r: 1.021e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 920, Loss: 9.974e-05, Loss_0: 4.904e-08, Loss_r: 9.925e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 930, Loss: 9.742e-05, Loss_0: 5.910e-09, Loss_r: 9.736e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 940, Loss: 9.601e-05, Loss_0: 3.224e-09, Loss_r: 9.598e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.004e-04, Loss_0: 2.369e-07, Loss_r: 9.804e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 960, Loss: 8.298e-04, Loss_0: 3.363e-05, Loss_r: 4.935e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 970, Loss: 4.332e-04, Loss_0: 1.512e-05, Loss_r: 2.819e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.253e-04, Loss_0: 1.337e-06, Loss_r: 1.120e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 990, Loss: 2.171e-04, Loss_0: 5.611e-06, Loss_r: 1.610e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 2.021e-04, Loss_0: 5.065e-06, Loss_r: 1.515e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 2.217e-04, Loss_0: 2.483e-06, Loss_r: 1.969e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 1.768e-04, Loss_0: 9.585e-07, Loss_r: 1.672e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 1.511e-04, Loss_0: 1.122e-07, Loss_r: 1.500e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 1.424e-04, Loss_0: 4.175e-08, Loss_r: 1.420e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 1.331e-04, Loss_0: 1.637e-09, Loss_r: 1.331e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 1.275e-04, Loss_0: 3.616e-08, Loss_r: 1.271e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 1.226e-04, Loss_0: 1.596e-09, Loss_r: 1.226e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 1.186e-04, Loss_0: 6.425e-11, Loss_r: 1.186e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 1.148e-04, Loss_0: 1.845e-09, Loss_r: 1.148e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 1.118e-04, Loss_0: 3.169e-09, Loss_r: 1.118e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 1.615e-04, Loss_0: 2.694e-06, Loss_r: 1.346e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 1.090e-02, Loss_0: 5.295e-04, Loss_r: 5.603e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.999e-03, Loss_0: 1.376e-04, Loss_r: 1.623e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 1.669e-04, Loss_0: 2.112e-06, Loss_r: 1.457e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 4.494e-04, Loss_0: 1.626e-05, Loss_r: 2.867e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 1.223e-04, Loss_0: 1.840e-07, Loss_r: 1.204e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 1.541e-04, Loss_0: 1.872e-06, Loss_r: 1.353e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 1.088e-04, Loss_0: 1.658e-10, Loss_r: 1.088e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 1.085e-04, Loss_0: 2.439e-07, Loss_r: 1.060e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.037e-04, Loss_0: 1.511e-07, Loss_r: 1.022e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 7.310e-05, Loss_0: 7.486e-08, Loss_r: 7.235e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 5.510e-05, Loss_0: 1.884e-08, Loss_r: 5.491e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 4.586e-05, Loss_0: 4.402e-09, Loss_r: 4.581e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.927e-05, Loss_0: 5.456e-10, Loss_r: 3.927e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 3.487e-05, Loss_0: 9.364e-11, Loss_r: 3.487e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 3.217e-05, Loss_0: 1.459e-11, Loss_r: 3.217e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 3.035e-05, Loss_0: 2.618e-11, Loss_r: 3.035e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.900e-05, Loss_0: 3.257e-11, Loss_r: 2.900e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.793e-05, Loss_0: 3.931e-11, Loss_r: 2.793e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.703e-05, Loss_0: 2.909e-11, Loss_r: 2.703e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.624e-05, Loss_0: 1.709e-11, Loss_r: 2.624e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.551e-05, Loss_0: 1.248e-11, Loss_r: 2.551e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.485e-05, Loss_0: 1.711e-11, Loss_r: 2.485e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.423e-05, Loss_0: 1.934e-11, Loss_r: 2.423e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.365e-05, Loss_0: 2.213e-11, Loss_r: 2.365e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 2.310e-05, Loss_0: 2.200e-11, Loss_r: 2.310e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 2.259e-05, Loss_0: 2.742e-11, Loss_r: 2.259e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 2.211e-05, Loss_0: 2.664e-11, Loss_r: 2.211e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 2.165e-05, Loss_0: 3.054e-11, Loss_r: 2.165e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 2.122e-05, Loss_0: 3.266e-11, Loss_r: 2.122e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 2.675e-05, Loss_0: 1.112e-08, Loss_r: 2.664e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 2.598e-05, Loss_0: 4.165e-09, Loss_r: 2.594e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 2.552e-05, Loss_0: 4.782e-09, Loss_r: 2.547e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 2.524e-05, Loss_0: 1.014e-08, Loss_r: 2.514e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 2.937e-05, Loss_0: 1.982e-07, Loss_r: 2.739e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 3.166e-04, Loss_0: 1.266e-05, Loss_r: 1.899e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 3.300e-03, Loss_0: 1.431e-04, Loss_r: 1.869e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 7.724e-04, Loss_0: 3.311e-05, Loss_r: 4.413e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.582e-04, Loss_0: 5.803e-06, Loss_r: 1.002e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 5.744e-05, Loss_0: 1.392e-06, Loss_r: 4.352e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 2.975e-05, Loss_0: 1.889e-07, Loss_r: 2.786e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 2.477e-05, Loss_0: 5.192e-09, Loss_r: 2.472e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 2.667e-05, Loss_0: 1.224e-07, Loss_r: 2.545e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 2.620e-05, Loss_0: 1.169e-07, Loss_r: 2.503e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 2.373e-05, Loss_0: 1.175e-08, Loss_r: 2.362e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 2.346e-05, Loss_0: 6.982e-09, Loss_r: 2.339e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 2.303e-05, Loss_0: 5.715e-10, Loss_r: 2.302e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 2.280e-05, Loss_0: 3.437e-09, Loss_r: 2.276e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 2.252e-05, Loss_0: 4.312e-11, Loss_r: 2.252e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 2.227e-05, Loss_0: 3.141e-10, Loss_r: 2.227e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 6.743e-05, Loss_0: 3.195e-09, Loss_r: 6.740e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 6.143e-05, Loss_0: 5.519e-08, Loss_r: 6.088e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 5.677e-05, Loss_0: 6.062e-09, Loss_r: 5.671e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 5.358e-05, Loss_0: 7.135e-09, Loss_r: 5.351e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 5.486e-05, Loss_0: 2.188e-07, Loss_r: 5.267e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 1.448e-03, Loss_0: 7.224e-05, Loss_r: 7.260e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 5.477e-03, Loss_0: 2.457e-04, Loss_r: 3.020e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 4.333e-04, Loss_0: 1.652e-05, Loss_r: 2.681e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 6.357e-04, Loss_0: 2.865e-05, Loss_r: 3.491e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 2.198e-04, Loss_0: 7.217e-06, Loss_r: 1.476e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 6.723e-05, Loss_0: 1.595e-07, Loss_r: 6.564e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 7.546e-05, Loss_0: 8.513e-07, Loss_r: 6.695e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 6.440e-05, Loss_0: 3.193e-07, Loss_r: 6.120e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 5.538e-05, Loss_0: 2.026e-08, Loss_r: 5.518e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 5.351e-05, Loss_0: 6.324e-08, Loss_r: 5.288e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 5.117e-05, Loss_0: 2.744e-08, Loss_r: 5.089e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 4.923e-05, Loss_0: 1.517e-09, Loss_r: 4.922e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 4.775e-05, Loss_0: 8.044e-13, Loss_r: 4.775e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 4.633e-05, Loss_0: 5.827e-12, Loss_r: 4.633e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 4.499e-05, Loss_0: 1.529e-10, Loss_r: 4.499e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 2.625e-05, Loss_0: 1.386e-10, Loss_r: 2.625e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.248e-05, Loss_0: 5.656e-12, Loss_r: 2.248e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.045e-05, Loss_0: 5.320e-12, Loss_r: 2.045e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.903e-05, Loss_0: 4.250e-13, Loss_r: 1.903e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.821e-05, Loss_0: 1.168e-13, Loss_r: 1.821e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.772e-05, Loss_0: 3.900e-12, Loss_r: 1.772e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.736e-05, Loss_0: 2.400e-11, Loss_r: 1.736e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 1.706e-05, Loss_0: 4.894e-11, Loss_r: 1.706e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.680e-05, Loss_0: 6.102e-11, Loss_r: 1.680e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 1.655e-05, Loss_0: 5.391e-11, Loss_r: 1.655e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 1.631e-05, Loss_0: 4.860e-11, Loss_r: 1.631e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.608e-05, Loss_0: 5.227e-11, Loss_r: 1.608e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 1.585e-05, Loss_0: 5.742e-11, Loss_r: 1.585e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.564e-05, Loss_0: 5.383e-11, Loss_r: 1.564e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.542e-05, Loss_0: 5.502e-11, Loss_r: 1.542e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.522e-05, Loss_0: 5.003e-11, Loss_r: 1.522e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 1.502e-05, Loss_0: 5.422e-11, Loss_r: 1.502e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 1.482e-05, Loss_0: 4.599e-11, Loss_r: 1.482e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 1.463e-05, Loss_0: 5.226e-11, Loss_r: 1.463e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 1.445e-05, Loss_0: 5.564e-11, Loss_r: 1.445e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 1.036e-05, Loss_0: 6.011e-09, Loss_r: 1.030e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 1.577e-05, Loss_0: 2.905e-07, Loss_r: 1.287e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 6.537e-04, Loss_0: 3.276e-05, Loss_r: 3.261e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.200e-04, Loss_0: 4.805e-06, Loss_r: 7.199e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 2.544e-04, Loss_0: 1.144e-05, Loss_r: 1.400e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 1.678e-05, Loss_0: 1.994e-08, Loss_r: 1.658e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 6.800e-05, Loss_0: 2.755e-06, Loss_r: 4.046e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 5.675e-05, Loss_0: 2.264e-06, Loss_r: 3.411e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 2.948e-05, Loss_0: 9.102e-07, Loss_r: 2.037e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 1.731e-05, Loss_0: 3.127e-07, Loss_r: 1.419e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 1.324e-05, Loss_0: 1.165e-07, Loss_r: 1.208e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 1.171e-05, Loss_0: 4.611e-08, Loss_r: 1.125e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 1.098e-05, Loss_0: 1.413e-08, Loss_r: 1.084e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 1.063e-05, Loss_0: 1.422e-09, Loss_r: 1.062e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 1.050e-05, Loss_0: 2.079e-10, Loss_r: 1.050e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 1.039e-05, Loss_0: 4.168e-10, Loss_r: 1.038e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 1.025e-05, Loss_0: 2.413e-11, Loss_r: 1.025e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 1.014e-05, Loss_0: 2.812e-10, Loss_r: 1.014e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 1.002e-05, Loss_0: 2.733e-11, Loss_r: 1.002e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 9.906e-06, Loss_0: 1.973e-11, Loss_r: 9.906e-06, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 1.268e-05, Loss_0: 1.143e-11, Loss_r: 1.268e-05, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 1.246e-05, Loss_0: 6.535e-10, Loss_r: 1.245e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 1.229e-05, Loss_0: 1.813e-11, Loss_r: 1.229e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 1.212e-05, Loss_0: 2.039e-10, Loss_r: 1.212e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 1.196e-05, Loss_0: 1.385e-10, Loss_r: 1.196e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 1.180e-05, Loss_0: 2.966e-11, Loss_r: 1.180e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 1.165e-05, Loss_0: 1.247e-11, Loss_r: 1.165e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 1.150e-05, Loss_0: 4.560e-11, Loss_r: 1.150e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 1.150e-05, Loss_0: 6.424e-09, Loss_r: 1.143e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 2.664e-05, Loss_0: 7.755e-07, Loss_r: 1.888e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2310, Loss: 1.203e-05, Loss_0: 4.548e-08, Loss_r: 1.157e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2320, Loss: 1.168e-05, Loss_0: 4.035e-08, Loss_r: 1.127e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2330, Loss: 1.174e-05, Loss_0: 5.025e-08, Loss_r: 1.123e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2340, Loss: 1.070e-05, Loss_0: 7.008e-11, Loss_r: 1.070e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2350, Loss: 1.070e-05, Loss_0: 5.322e-09, Loss_r: 1.065e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2360, Loss: 1.046e-05, Loss_0: 7.947e-10, Loss_r: 1.045e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2370, Loss: 1.034e-05, Loss_0: 4.521e-10, Loss_r: 1.033e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2380, Loss: 1.022e-05, Loss_0: 8.988e-11, Loss_r: 1.022e-05, Time: 0.08, Learning Rate: 0.00090\n",
            "It: 2390, Loss: 1.010e-05, Loss_0: 3.314e-10, Loss_r: 1.010e-05, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2400, Loss: 9.982e-06, Loss_0: 6.655e-14, Loss_r: 9.982e-06, Time: 0.07, Learning Rate: 0.00090\n",
            "It: 2410, Loss: 3.976e-05, Loss_0: 2.906e-09, Loss_r: 3.973e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2420, Loss: 3.501e-05, Loss_0: 2.730e-11, Loss_r: 3.501e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2430, Loss: 3.205e-05, Loss_0: 7.040e-12, Loss_r: 3.205e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2440, Loss: 2.984e-05, Loss_0: 4.351e-11, Loss_r: 2.984e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2450, Loss: 2.832e-05, Loss_0: 1.018e-10, Loss_r: 2.832e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2460, Loss: 2.712e-05, Loss_0: 3.560e-10, Loss_r: 2.712e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2470, Loss: 2.608e-05, Loss_0: 6.961e-10, Loss_r: 2.607e-05, Time: 0.09, Learning Rate: 0.00081\n",
            "It: 2480, Loss: 2.514e-05, Loss_0: 6.221e-10, Loss_r: 2.513e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2490, Loss: 2.427e-05, Loss_0: 3.814e-10, Loss_r: 2.427e-05, Time: 0.08, Learning Rate: 0.00081\n",
            "It: 2500, Loss: 2.347e-05, Loss_0: 4.034e-10, Loss_r: 2.347e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2510, Loss: 2.272e-05, Loss_0: 4.046e-10, Loss_r: 2.272e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2520, Loss: 2.203e-05, Loss_0: 3.604e-10, Loss_r: 2.203e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2530, Loss: 2.138e-05, Loss_0: 3.261e-10, Loss_r: 2.138e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2540, Loss: 2.077e-05, Loss_0: 3.229e-10, Loss_r: 2.077e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2550, Loss: 2.020e-05, Loss_0: 2.888e-10, Loss_r: 2.020e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2560, Loss: 1.966e-05, Loss_0: 2.417e-10, Loss_r: 1.966e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2570, Loss: 1.915e-05, Loss_0: 1.871e-10, Loss_r: 1.915e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2580, Loss: 1.868e-05, Loss_0: 2.097e-11, Loss_r: 1.868e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 1.838e-05, Loss_0: 6.424e-09, Loss_r: 1.831e-05, Time: 0.07, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 4.810e-05, Loss_0: 1.688e-06, Loss_r: 3.122e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2610, Loss: 6.032e-05, Loss_0: 1.549e-08, Loss_r: 6.017e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2620, Loss: 5.982e-05, Loss_0: 5.608e-07, Loss_r: 5.421e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2630, Loss: 4.712e-05, Loss_0: 1.356e-08, Loss_r: 4.698e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2640, Loss: 4.285e-05, Loss_0: 2.204e-10, Loss_r: 4.285e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2650, Loss: 4.006e-05, Loss_0: 6.811e-09, Loss_r: 3.999e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2660, Loss: 3.783e-05, Loss_0: 6.094e-09, Loss_r: 3.777e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2670, Loss: 3.596e-05, Loss_0: 3.337e-09, Loss_r: 3.593e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2680, Loss: 3.414e-05, Loss_0: 1.623e-09, Loss_r: 3.413e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2690, Loss: 3.284e-05, Loss_0: 1.545e-08, Loss_r: 3.269e-05, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2700, Loss: 4.536e-05, Loss_0: 8.340e-07, Loss_r: 3.702e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2710, Loss: 1.438e-03, Loss_0: 8.466e-05, Loss_r: 5.918e-04, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2720, Loss: 5.574e-04, Loss_0: 3.069e-05, Loss_r: 2.505e-04, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2730, Loss: 2.097e-04, Loss_0: 1.034e-05, Loss_r: 1.063e-04, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2740, Loss: 6.838e-05, Loss_0: 2.254e-06, Loss_r: 4.584e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2750, Loss: 3.437e-05, Loss_0: 3.714e-07, Loss_r: 3.066e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2760, Loss: 2.806e-05, Loss_0: 7.805e-08, Loss_r: 2.728e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2770, Loss: 2.638e-05, Loss_0: 3.729e-08, Loss_r: 2.600e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2780, Loss: 2.537e-05, Loss_0: 2.921e-08, Loss_r: 2.508e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2790, Loss: 2.435e-05, Loss_0: 1.752e-08, Loss_r: 2.418e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2800, Loss: 2.332e-05, Loss_0: 3.345e-09, Loss_r: 2.329e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2810, Loss: 2.274e-05, Loss_0: 7.122e-09, Loss_r: 2.267e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2820, Loss: 1.708e-05, Loss_0: 2.491e-09, Loss_r: 1.705e-05, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2830, Loss: 1.412e-05, Loss_0: 6.040e-11, Loss_r: 1.412e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2840, Loss: 1.190e-05, Loss_0: 4.641e-10, Loss_r: 1.190e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2850, Loss: 1.063e-05, Loss_0: 3.206e-11, Loss_r: 1.063e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2860, Loss: 9.784e-06, Loss_0: 9.974e-11, Loss_r: 9.783e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2870, Loss: 9.120e-06, Loss_0: 2.316e-11, Loss_r: 9.119e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2880, Loss: 8.570e-06, Loss_0: 1.084e-11, Loss_r: 8.570e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2890, Loss: 8.081e-06, Loss_0: 1.530e-13, Loss_r: 8.081e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2900, Loss: 7.634e-06, Loss_0: 6.894e-13, Loss_r: 7.634e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2910, Loss: 7.222e-06, Loss_0: 3.898e-14, Loss_r: 7.222e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2920, Loss: 6.841e-06, Loss_0: 2.199e-13, Loss_r: 6.841e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2930, Loss: 6.488e-06, Loss_0: 4.568e-14, Loss_r: 6.488e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2940, Loss: 6.161e-06, Loss_0: 1.323e-14, Loss_r: 6.161e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2950, Loss: 5.858e-06, Loss_0: 4.255e-14, Loss_r: 5.858e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2960, Loss: 5.577e-06, Loss_0: 1.017e-13, Loss_r: 5.577e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 5.317e-06, Loss_0: 2.087e-12, Loss_r: 5.317e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 5.076e-06, Loss_0: 9.755e-13, Loss_r: 5.076e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 4.852e-06, Loss_0: 5.580e-12, Loss_r: 4.852e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 4.648e-06, Loss_0: 1.310e-10, Loss_r: 4.646e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 4.401e-06, Loss_0: 4.317e-09, Loss_r: 4.357e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 1.377e-05, Loss_0: 4.831e-07, Loss_r: 8.937e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3030, Loss: 4.556e-06, Loss_0: 3.292e-08, Loss_r: 4.226e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3040, Loss: 4.060e-06, Loss_0: 1.679e-08, Loss_r: 3.892e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3050, Loss: 4.131e-06, Loss_0: 2.840e-08, Loss_r: 3.847e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3060, Loss: 3.426e-06, Loss_0: 1.121e-09, Loss_r: 3.415e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3070, Loss: 3.323e-06, Loss_0: 3.071e-09, Loss_r: 3.292e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3080, Loss: 3.132e-06, Loss_0: 1.617e-10, Loss_r: 3.130e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3090, Loss: 3.016e-06, Loss_0: 4.831e-10, Loss_r: 3.011e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3100, Loss: 2.892e-06, Loss_0: 4.569e-11, Loss_r: 2.892e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3110, Loss: 2.785e-06, Loss_0: 2.890e-12, Loss_r: 2.785e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3120, Loss: 2.687e-06, Loss_0: 1.216e-11, Loss_r: 2.687e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3130, Loss: 2.595e-06, Loss_0: 6.160e-12, Loss_r: 2.595e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3140, Loss: 2.509e-06, Loss_0: 4.483e-12, Loss_r: 2.509e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3150, Loss: 2.430e-06, Loss_0: 2.309e-13, Loss_r: 2.430e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3160, Loss: 2.356e-06, Loss_0: 9.792e-14, Loss_r: 2.356e-06, Time: 0.06, Learning Rate: 0.00059\n",
            "It: 3170, Loss: 2.287e-06, Loss_0: 6.971e-13, Loss_r: 2.287e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3180, Loss: 2.224e-06, Loss_0: 6.215e-13, Loss_r: 2.224e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3190, Loss: 2.165e-06, Loss_0: 1.196e-11, Loss_r: 2.164e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3200, Loss: 2.112e-06, Loss_0: 1.206e-10, Loss_r: 2.110e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3210, Loss: 2.760e-04, Loss_0: 1.107e-05, Loss_r: 1.653e-04, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3220, Loss: 6.739e-05, Loss_0: 1.506e-06, Loss_r: 5.234e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3230, Loss: 3.230e-05, Loss_0: 2.071e-07, Loss_r: 3.023e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3240, Loss: 3.857e-05, Loss_0: 8.309e-07, Loss_r: 3.026e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3250, Loss: 2.194e-05, Loss_0: 9.364e-10, Loss_r: 2.193e-05, Time: 0.06, Learning Rate: 0.00053\n",
            "It: 3260, Loss: 2.216e-05, Loss_0: 8.089e-08, Loss_r: 2.135e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3270, Loss: 1.979e-05, Loss_0: 4.503e-08, Loss_r: 1.934e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3280, Loss: 1.819e-05, Loss_0: 3.933e-09, Loss_r: 1.815e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3290, Loss: 1.715e-05, Loss_0: 4.877e-09, Loss_r: 1.710e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3300, Loss: 1.629e-05, Loss_0: 6.747e-10, Loss_r: 1.629e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3310, Loss: 1.552e-05, Loss_0: 2.540e-09, Loss_r: 1.549e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3320, Loss: 1.482e-05, Loss_0: 6.330e-10, Loss_r: 1.482e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3330, Loss: 1.422e-05, Loss_0: 1.260e-11, Loss_r: 1.422e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3340, Loss: 1.370e-05, Loss_0: 4.541e-10, Loss_r: 1.369e-05, Time: 0.04, Learning Rate: 0.00053\n",
            "It: 3350, Loss: 1.380e-05, Loss_0: 2.896e-08, Loss_r: 1.351e-05, Time: 0.05, Learning Rate: 0.00053\n",
            "It: 3360, Loss: 5.830e-05, Loss_0: 2.468e-06, Loss_r: 3.363e-05, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3370, Loss: 1.752e-05, Loss_0: 2.684e-07, Loss_r: 1.483e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3380, Loss: 1.249e-05, Loss_0: 2.884e-08, Loss_r: 1.220e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3390, Loss: 1.409e-05, Loss_0: 1.398e-07, Loss_r: 1.269e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3400, Loss: 1.185e-05, Loss_0: 2.647e-08, Loss_r: 1.159e-05, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3410, Loss: 1.855e-05, Loss_0: 4.186e-08, Loss_r: 1.813e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3420, Loss: 1.176e-05, Loss_0: 1.556e-08, Loss_r: 1.161e-05, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3430, Loss: 8.645e-06, Loss_0: 3.517e-09, Loss_r: 8.610e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3440, Loss: 6.676e-06, Loss_0: 3.084e-09, Loss_r: 6.645e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3450, Loss: 5.830e-06, Loss_0: 5.891e-10, Loss_r: 5.824e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3460, Loss: 5.336e-06, Loss_0: 3.438e-10, Loss_r: 5.333e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3470, Loss: 4.962e-06, Loss_0: 1.206e-10, Loss_r: 4.960e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3480, Loss: 4.651e-06, Loss_0: 2.153e-11, Loss_r: 4.651e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3490, Loss: 4.368e-06, Loss_0: 1.423e-13, Loss_r: 4.368e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3500, Loss: 4.108e-06, Loss_0: 8.415e-15, Loss_r: 4.108e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3510, Loss: 3.870e-06, Loss_0: 2.143e-12, Loss_r: 3.870e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3520, Loss: 3.651e-06, Loss_0: 1.286e-12, Loss_r: 3.651e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3530, Loss: 3.450e-06, Loss_0: 2.260e-13, Loss_r: 3.450e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3540, Loss: 3.265e-06, Loss_0: 2.205e-12, Loss_r: 3.265e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3550, Loss: 3.095e-06, Loss_0: 1.636e-12, Loss_r: 3.095e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3560, Loss: 2.939e-06, Loss_0: 5.649e-13, Loss_r: 2.939e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3570, Loss: 2.795e-06, Loss_0: 3.004e-13, Loss_r: 2.795e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3580, Loss: 2.664e-06, Loss_0: 6.840e-13, Loss_r: 2.664e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3590, Loss: 2.544e-06, Loss_0: 8.141e-11, Loss_r: 2.544e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3600, Loss: 2.583e-06, Loss_0: 7.268e-09, Loss_r: 2.510e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3610, Loss: 2.047e-04, Loss_0: 8.417e-06, Loss_r: 1.205e-04, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3620, Loss: 2.962e-05, Loss_0: 1.353e-07, Loss_r: 2.827e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3630, Loss: 4.096e-05, Loss_0: 1.014e-06, Loss_r: 3.082e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3640, Loss: 2.821e-05, Loss_0: 4.461e-07, Loss_r: 2.375e-05, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3650, Loss: 2.082e-05, Loss_0: 7.536e-08, Loss_r: 2.007e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3660, Loss: 1.862e-05, Loss_0: 1.641e-08, Loss_r: 1.845e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3670, Loss: 1.796e-05, Loss_0: 4.037e-08, Loss_r: 1.756e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3680, Loss: 1.682e-05, Loss_0: 7.209e-09, Loss_r: 1.675e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3690, Loss: 1.600e-05, Loss_0: 6.175e-09, Loss_r: 1.594e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3700, Loss: 1.534e-05, Loss_0: 5.616e-10, Loss_r: 1.533e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3710, Loss: 1.475e-05, Loss_0: 2.111e-09, Loss_r: 1.473e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3720, Loss: 1.421e-05, Loss_0: 2.495e-10, Loss_r: 1.421e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3730, Loss: 1.373e-05, Loss_0: 2.053e-13, Loss_r: 1.373e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3740, Loss: 1.329e-05, Loss_0: 7.950e-11, Loss_r: 1.329e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3750, Loss: 1.299e-05, Loss_0: 4.473e-09, Loss_r: 1.295e-05, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3760, Loss: 1.791e-05, Loss_0: 2.718e-07, Loss_r: 1.519e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3770, Loss: 5.113e-04, Loss_0: 2.612e-05, Loss_r: 2.501e-04, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3780, Loss: 1.923e-04, Loss_0: 9.374e-06, Loss_r: 9.851e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3790, Loss: 8.146e-05, Loss_0: 3.604e-06, Loss_r: 4.542e-05, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3800, Loss: 3.519e-05, Loss_0: 1.213e-06, Loss_r: 2.307e-05, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3810, Loss: 1.284e-05, Loss_0: 3.922e-07, Loss_r: 8.914e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3820, Loss: 7.810e-06, Loss_0: 1.397e-07, Loss_r: 6.413e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3830, Loss: 6.055e-06, Loss_0: 5.382e-08, Loss_r: 5.516e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3840, Loss: 5.296e-06, Loss_0: 1.971e-08, Loss_r: 5.099e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3850, Loss: 4.931e-06, Loss_0: 4.733e-09, Loss_r: 4.884e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3860, Loss: 4.774e-06, Loss_0: 1.636e-10, Loss_r: 4.773e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3870, Loss: 4.715e-06, Loss_0: 3.930e-10, Loss_r: 4.712e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3880, Loss: 4.661e-06, Loss_0: 5.352e-10, Loss_r: 4.656e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3890, Loss: 4.599e-06, Loss_0: 6.270e-11, Loss_r: 4.599e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3900, Loss: 4.546e-06, Loss_0: 2.798e-12, Loss_r: 4.546e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3910, Loss: 4.492e-06, Loss_0: 9.804e-12, Loss_r: 4.492e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 3920, Loss: 4.440e-06, Loss_0: 3.806e-11, Loss_r: 4.440e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3930, Loss: 4.389e-06, Loss_0: 1.133e-11, Loss_r: 4.389e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3940, Loss: 4.339e-06, Loss_0: 1.436e-11, Loss_r: 4.339e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3950, Loss: 4.290e-06, Loss_0: 2.204e-11, Loss_r: 4.290e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3960, Loss: 4.242e-06, Loss_0: 1.084e-11, Loss_r: 4.241e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3970, Loss: 4.194e-06, Loss_0: 1.950e-11, Loss_r: 4.194e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 3980, Loss: 4.148e-06, Loss_0: 1.314e-11, Loss_r: 4.148e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 3990, Loss: 4.103e-06, Loss_0: 1.916e-11, Loss_r: 4.102e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 4.058e-06, Loss_0: 8.405e-12, Loss_r: 4.058e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 1.151e-05, Loss_0: 2.167e-07, Loss_r: 9.339e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4020, Loss: 7.127e-06, Loss_0: 7.527e-08, Loss_r: 6.375e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4030, Loss: 5.411e-06, Loss_0: 2.759e-08, Loss_r: 5.135e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4040, Loss: 4.448e-06, Loss_0: 9.374e-09, Loss_r: 4.355e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4050, Loss: 4.041e-06, Loss_0: 2.804e-09, Loss_r: 4.013e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4060, Loss: 3.808e-06, Loss_0: 5.240e-10, Loss_r: 3.802e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4070, Loss: 3.642e-06, Loss_0: 1.025e-11, Loss_r: 3.642e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4080, Loss: 3.509e-06, Loss_0: 3.999e-11, Loss_r: 3.508e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4090, Loss: 3.384e-06, Loss_0: 5.481e-11, Loss_r: 3.383e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4100, Loss: 3.266e-06, Loss_0: 4.749e-12, Loss_r: 3.266e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4110, Loss: 3.156e-06, Loss_0: 3.004e-12, Loss_r: 3.156e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4120, Loss: 3.053e-06, Loss_0: 1.129e-12, Loss_r: 3.053e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4130, Loss: 2.956e-06, Loss_0: 1.302e-12, Loss_r: 2.956e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4140, Loss: 2.865e-06, Loss_0: 7.295e-16, Loss_r: 2.865e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4150, Loss: 2.780e-06, Loss_0: 7.703e-14, Loss_r: 2.780e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4160, Loss: 2.700e-06, Loss_0: 1.589e-13, Loss_r: 2.700e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4170, Loss: 2.624e-06, Loss_0: 1.395e-13, Loss_r: 2.624e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4180, Loss: 2.554e-06, Loss_0: 8.674e-15, Loss_r: 2.554e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4190, Loss: 2.487e-06, Loss_0: 2.276e-14, Loss_r: 2.487e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4200, Loss: 2.424e-06, Loss_0: 1.430e-13, Loss_r: 2.424e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4210, Loss: 3.098e-06, Loss_0: 3.326e-09, Loss_r: 3.065e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 2.958e-06, Loss_0: 7.093e-10, Loss_r: 2.951e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 2.877e-06, Loss_0: 1.477e-09, Loss_r: 2.862e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 2.804e-06, Loss_0: 2.137e-09, Loss_r: 2.783e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 3.039e-06, Loss_0: 1.652e-08, Loss_r: 2.874e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 1.227e-05, Loss_0: 4.519e-07, Loss_r: 7.751e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4270, Loss: 4.671e-06, Loss_0: 9.919e-08, Loss_r: 3.679e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4280, Loss: 2.597e-06, Loss_0: 5.034e-09, Loss_r: 2.547e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4290, Loss: 2.522e-06, Loss_0: 4.147e-09, Loss_r: 2.481e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4300, Loss: 2.554e-06, Loss_0: 8.071e-09, Loss_r: 2.473e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4310, Loss: 2.364e-06, Loss_0: 1.499e-09, Loss_r: 2.349e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4320, Loss: 2.291e-06, Loss_0: 2.616e-10, Loss_r: 2.288e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4330, Loss: 2.249e-06, Loss_0: 3.644e-10, Loss_r: 2.245e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4340, Loss: 2.200e-06, Loss_0: 3.477e-11, Loss_r: 2.199e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4350, Loss: 2.160e-06, Loss_0: 2.734e-11, Loss_r: 2.160e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4360, Loss: 2.122e-06, Loss_0: 1.622e-11, Loss_r: 2.122e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4370, Loss: 2.086e-06, Loss_0: 2.065e-12, Loss_r: 2.086e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4380, Loss: 2.052e-06, Loss_0: 5.691e-15, Loss_r: 2.052e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4390, Loss: 2.019e-06, Loss_0: 5.975e-15, Loss_r: 2.019e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 1.988e-06, Loss_0: 2.908e-13, Loss_r: 1.988e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 2.200e-06, Loss_0: 3.009e-11, Loss_r: 2.200e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 2.002e-06, Loss_0: 3.639e-10, Loss_r: 1.999e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 1.906e-06, Loss_0: 1.322e-10, Loss_r: 1.905e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 1.847e-06, Loss_0: 6.904e-11, Loss_r: 1.846e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 1.827e-06, Loss_0: 3.111e-11, Loss_r: 1.826e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 1.820e-06, Loss_0: 2.076e-10, Loss_r: 1.818e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 1.921e-06, Loss_0: 5.228e-09, Loss_r: 1.868e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 8.751e-06, Loss_0: 3.280e-07, Loss_r: 5.471e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4490, Loss: 2.747e-06, Loss_0: 4.472e-08, Loss_r: 2.299e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4500, Loss: 1.804e-06, Loss_0: 7.153e-10, Loss_r: 1.797e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4510, Loss: 2.062e-06, Loss_0: 1.335e-08, Loss_r: 1.929e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4520, Loss: 1.887e-06, Loss_0: 5.260e-09, Loss_r: 1.835e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4530, Loss: 1.774e-06, Loss_0: 7.413e-11, Loss_r: 1.774e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4540, Loss: 1.786e-06, Loss_0: 8.533e-10, Loss_r: 1.777e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4550, Loss: 1.761e-06, Loss_0: 1.866e-11, Loss_r: 1.761e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4560, Loss: 1.757e-06, Loss_0: 1.071e-10, Loss_r: 1.756e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4570, Loss: 1.750e-06, Loss_0: 2.727e-11, Loss_r: 1.750e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4580, Loss: 1.744e-06, Loss_0: 4.729e-12, Loss_r: 1.744e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4590, Loss: 1.738e-06, Loss_0: 2.165e-12, Loss_r: 1.738e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4600, Loss: 1.733e-06, Loss_0: 1.611e-13, Loss_r: 1.733e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4610, Loss: 1.949e-06, Loss_0: 1.231e-11, Loss_r: 1.949e-06, Time: 0.08, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 1.938e-06, Loss_0: 5.627e-12, Loss_r: 1.938e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 1.932e-06, Loss_0: 1.051e-10, Loss_r: 1.931e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 1.924e-06, Loss_0: 1.799e-11, Loss_r: 1.923e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 1.918e-06, Loss_0: 1.751e-11, Loss_r: 1.917e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 1.911e-06, Loss_0: 1.409e-11, Loss_r: 1.911e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 1.908e-06, Loss_0: 1.247e-10, Loss_r: 1.907e-06, Time: 0.07, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 1.977e-06, Loss_0: 3.578e-09, Loss_r: 1.941e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4690, Loss: 1.911e-06, Loss_0: 7.630e-10, Loss_r: 1.903e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4700, Loss: 1.889e-06, Loss_0: 2.295e-11, Loss_r: 1.888e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4710, Loss: 1.883e-06, Loss_0: 6.276e-11, Loss_r: 1.883e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4720, Loss: 1.879e-06, Loss_0: 8.920e-11, Loss_r: 1.878e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4730, Loss: 1.872e-06, Loss_0: 1.555e-11, Loss_r: 1.872e-06, Time: 0.08, Learning Rate: 0.00025\n",
            "It: 4740, Loss: 1.866e-06, Loss_0: 8.708e-13, Loss_r: 1.866e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4750, Loss: 1.861e-06, Loss_0: 1.479e-13, Loss_r: 1.861e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4760, Loss: 1.856e-06, Loss_0: 4.407e-12, Loss_r: 1.856e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4770, Loss: 1.850e-06, Loss_0: 1.477e-12, Loss_r: 1.850e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4780, Loss: 1.845e-06, Loss_0: 2.203e-13, Loss_r: 1.845e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4790, Loss: 1.840e-06, Loss_0: 3.072e-12, Loss_r: 1.840e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4800, Loss: 1.834e-06, Loss_0: 6.311e-13, Loss_r: 1.834e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4810, Loss: 1.252e-05, Loss_0: 1.977e-09, Loss_r: 1.250e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4820, Loss: 1.045e-05, Loss_0: 2.688e-09, Loss_r: 1.043e-05, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4830, Loss: 9.467e-06, Loss_0: 1.205e-09, Loss_r: 9.455e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4840, Loss: 8.768e-06, Loss_0: 6.715e-10, Loss_r: 8.761e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4850, Loss: 8.397e-06, Loss_0: 3.943e-10, Loss_r: 8.393e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4860, Loss: 8.124e-06, Loss_0: 2.529e-10, Loss_r: 8.122e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4870, Loss: 7.881e-06, Loss_0: 1.748e-10, Loss_r: 7.879e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4880, Loss: 7.663e-06, Loss_0: 1.229e-10, Loss_r: 7.662e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4890, Loss: 7.462e-06, Loss_0: 9.940e-11, Loss_r: 7.461e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4900, Loss: 7.278e-06, Loss_0: 7.674e-11, Loss_r: 7.277e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "It: 4910, Loss: 7.108e-06, Loss_0: 6.968e-11, Loss_r: 7.107e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4920, Loss: 6.952e-06, Loss_0: 7.113e-11, Loss_r: 6.951e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4930, Loss: 6.808e-06, Loss_0: 7.592e-11, Loss_r: 6.807e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4940, Loss: 6.674e-06, Loss_0: 6.938e-11, Loss_r: 6.674e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4950, Loss: 6.550e-06, Loss_0: 7.016e-11, Loss_r: 6.550e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4960, Loss: 6.435e-06, Loss_0: 6.601e-11, Loss_r: 6.435e-06, Time: 0.09, Learning Rate: 0.00023\n",
            "It: 4970, Loss: 6.328e-06, Loss_0: 5.665e-11, Loss_r: 6.327e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4980, Loss: 6.227e-06, Loss_0: 6.312e-11, Loss_r: 6.227e-06, Time: 0.07, Learning Rate: 0.00023\n",
            "It: 4990, Loss: 6.133e-06, Loss_0: 4.668e-11, Loss_r: 6.132e-06, Time: 0.08, Learning Rate: 0.00023\n",
            "Training time: 28.6118\n",
            "[1, 256, 128, 64, 32, 1]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.565e-01, Loss_0: 1.635e-03, Loss_r: 2.549e-01, Time: 0.89, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.448e-01, Loss_0: 3.229e-06, Loss_r: 2.447e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.921e-01, Loss_0: 2.781e-03, Loss_r: 1.643e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.704e-01, Loss_0: 1.102e-03, Loss_r: 1.594e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 40, Loss: 1.373e-01, Loss_0: 1.026e-03, Loss_r: 1.270e-01, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 50, Loss: 7.614e-02, Loss_0: 4.725e-04, Loss_r: 7.142e-02, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 60, Loss: 3.612e-02, Loss_0: 1.614e-04, Loss_r: 3.450e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 70, Loss: 2.138e-02, Loss_0: 5.009e-05, Loss_r: 2.088e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 80, Loss: 1.186e-02, Loss_0: 1.372e-05, Loss_r: 1.173e-02, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 90, Loss: 7.221e-03, Loss_0: 8.479e-06, Loss_r: 7.136e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 100, Loss: 4.493e-03, Loss_0: 3.103e-06, Loss_r: 4.462e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 110, Loss: 3.057e-03, Loss_0: 2.723e-06, Loss_r: 3.029e-03, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 120, Loss: 2.186e-03, Loss_0: 6.901e-07, Loss_r: 2.179e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 130, Loss: 1.662e-03, Loss_0: 5.190e-07, Loss_r: 1.656e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 140, Loss: 1.294e-03, Loss_0: 5.876e-07, Loss_r: 1.288e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.034e-03, Loss_0: 2.630e-07, Loss_r: 1.032e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 160, Loss: 8.534e-04, Loss_0: 2.280e-07, Loss_r: 8.511e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 170, Loss: 7.302e-04, Loss_0: 1.688e-07, Loss_r: 7.286e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 180, Loss: 6.472e-04, Loss_0: 5.538e-08, Loss_r: 6.467e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.233e-03, Loss_0: 5.790e-05, Loss_r: 6.543e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 200, Loss: 8.126e-04, Loss_0: 2.069e-05, Loss_r: 6.057e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 210, Loss: 4.335e-04, Loss_0: 7.563e-07, Loss_r: 4.259e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 220, Loss: 4.414e-04, Loss_0: 3.891e-06, Loss_r: 4.025e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 230, Loss: 4.021e-04, Loss_0: 1.960e-06, Loss_r: 3.825e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 240, Loss: 3.700e-04, Loss_0: 4.502e-07, Loss_r: 3.655e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 250, Loss: 3.537e-04, Loss_0: 1.498e-07, Loss_r: 3.522e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 260, Loss: 3.359e-04, Loss_0: 3.870e-07, Loss_r: 3.320e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 270, Loss: 3.943e-04, Loss_0: 7.728e-06, Loss_r: 3.170e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 280, Loss: 3.113e-04, Loss_0: 2.541e-07, Loss_r: 3.088e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 290, Loss: 4.077e-04, Loss_0: 1.056e-05, Loss_r: 3.021e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.814e-04, Loss_0: 6.990e-06, Loss_r: 3.115e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 310, Loss: 2.709e-04, Loss_0: 3.489e-07, Loss_r: 2.675e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 320, Loss: 2.706e-04, Loss_0: 7.472e-07, Loss_r: 2.632e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.472e-04, Loss_0: 1.907e-07, Loss_r: 2.453e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 340, Loss: 2.996e-04, Loss_0: 5.409e-06, Loss_r: 2.455e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 350, Loss: 1.201e-03, Loss_0: 6.853e-05, Loss_r: 5.157e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 360, Loss: 3.271e-04, Loss_0: 7.982e-06, Loss_r: 2.472e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 370, Loss: 2.603e-04, Loss_0: 2.822e-06, Loss_r: 2.320e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 380, Loss: 2.410e-04, Loss_0: 2.280e-06, Loss_r: 2.183e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.228e-04, Loss_0: 2.313e-06, Loss_r: 1.997e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 400, Loss: 2.018e-04, Loss_0: 8.243e-07, Loss_r: 1.935e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 410, Loss: 4.494e-04, Loss_0: 8.878e-06, Loss_r: 3.606e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 420, Loss: 1.375e-03, Loss_0: 7.037e-05, Loss_r: 6.713e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 430, Loss: 2.982e-04, Loss_0: 1.690e-06, Loss_r: 2.813e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 440, Loss: 3.588e-04, Loss_0: 7.448e-06, Loss_r: 2.843e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 450, Loss: 3.232e-04, Loss_0: 4.279e-06, Loss_r: 2.804e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 460, Loss: 2.425e-04, Loss_0: 2.593e-07, Loss_r: 2.399e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.321e-04, Loss_0: 1.614e-05, Loss_r: 2.707e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 480, Loss: 7.403e-04, Loss_0: 3.690e-05, Loss_r: 3.713e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.268e-04, Loss_0: 1.604e-05, Loss_r: 2.665e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 500, Loss: 2.922e-04, Loss_0: 5.251e-06, Loss_r: 2.397e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 510, Loss: 2.078e-04, Loss_0: 3.124e-07, Loss_r: 2.047e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 520, Loss: 2.035e-04, Loss_0: 9.608e-07, Loss_r: 1.939e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 530, Loss: 1.873e-04, Loss_0: 2.359e-07, Loss_r: 1.850e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 540, Loss: 1.936e-04, Loss_0: 8.806e-07, Loss_r: 1.848e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.349e-03, Loss_0: 8.266e-05, Loss_r: 5.222e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 560, Loss: 1.491e-03, Loss_0: 9.144e-05, Loss_r: 5.769e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.313e-04, Loss_0: 2.575e-05, Loss_r: 2.738e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 580, Loss: 2.598e-04, Loss_0: 7.113e-06, Loss_r: 1.886e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 590, Loss: 1.652e-04, Loss_0: 3.996e-07, Loss_r: 1.612e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.700e-04, Loss_0: 8.057e-07, Loss_r: 1.620e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 610, Loss: 2.364e-04, Loss_0: 3.246e-10, Loss_r: 2.364e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 620, Loss: 2.212e-04, Loss_0: 1.347e-07, Loss_r: 2.199e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 630, Loss: 2.124e-04, Loss_0: 5.784e-07, Loss_r: 2.066e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 640, Loss: 2.912e-04, Loss_0: 7.971e-06, Loss_r: 2.115e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 650, Loss: 3.645e-03, Loss_0: 2.478e-04, Loss_r: 1.167e-03, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 660, Loss: 1.319e-03, Loss_0: 8.356e-05, Loss_r: 4.833e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 670, Loss: 3.503e-04, Loss_0: 1.186e-05, Loss_r: 2.318e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 680, Loss: 1.857e-04, Loss_0: 3.674e-08, Loss_r: 1.853e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 690, Loss: 1.810e-04, Loss_0: 1.273e-07, Loss_r: 1.797e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 700, Loss: 1.707e-04, Loss_0: 8.267e-09, Loss_r: 1.706e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 710, Loss: 1.676e-04, Loss_0: 3.791e-07, Loss_r: 1.638e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 720, Loss: 1.608e-04, Loss_0: 2.976e-07, Loss_r: 1.578e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 730, Loss: 1.535e-04, Loss_0: 1.892e-08, Loss_r: 1.533e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 740, Loss: 1.479e-04, Loss_0: 2.809e-08, Loss_r: 1.476e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 750, Loss: 1.434e-04, Loss_0: 5.164e-10, Loss_r: 1.434e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 760, Loss: 1.391e-04, Loss_0: 3.155e-08, Loss_r: 1.388e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 770, Loss: 1.365e-04, Loss_0: 1.761e-07, Loss_r: 1.347e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 780, Loss: 2.762e-04, Loss_0: 1.159e-05, Loss_r: 1.603e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 790, Loss: 2.468e-03, Loss_0: 1.643e-04, Loss_r: 8.251e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 800, Loss: 7.767e-04, Loss_0: 4.675e-05, Loss_r: 3.092e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 810, Loss: 2.871e-04, Loss_0: 5.956e-09, Loss_r: 2.870e-04, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 820, Loss: 3.148e-04, Loss_0: 3.486e-06, Loss_r: 2.800e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 830, Loss: 2.509e-04, Loss_0: 1.246e-06, Loss_r: 2.385e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 840, Loss: 2.080e-04, Loss_0: 1.362e-07, Loss_r: 2.067e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 850, Loss: 2.015e-04, Loss_0: 1.184e-06, Loss_r: 1.897e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 860, Loss: 1.809e-04, Loss_0: 1.603e-07, Loss_r: 1.793e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 870, Loss: 1.693e-04, Loss_0: 3.355e-07, Loss_r: 1.659e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 880, Loss: 1.576e-04, Loss_0: 7.349e-09, Loss_r: 1.575e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 890, Loss: 1.529e-04, Loss_0: 1.737e-07, Loss_r: 1.512e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 900, Loss: 8.014e-04, Loss_0: 5.194e-05, Loss_r: 2.820e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 910, Loss: 1.911e-03, Loss_0: 1.299e-04, Loss_r: 6.128e-04, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 920, Loss: 1.338e-03, Loss_0: 9.017e-05, Loss_r: 4.361e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 930, Loss: 1.659e-04, Loss_0: 1.347e-06, Loss_r: 1.525e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 940, Loss: 2.706e-04, Loss_0: 8.470e-06, Loss_r: 1.859e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 950, Loss: 1.693e-04, Loss_0: 2.132e-06, Loss_r: 1.480e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 960, Loss: 1.294e-04, Loss_0: 2.696e-08, Loss_r: 1.292e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 970, Loss: 1.223e-04, Loss_0: 2.473e-08, Loss_r: 1.220e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 980, Loss: 1.168e-04, Loss_0: 6.822e-09, Loss_r: 1.167e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 990, Loss: 1.122e-04, Loss_0: 3.405e-09, Loss_r: 1.122e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1000, Loss: 1.081e-04, Loss_0: 1.043e-08, Loss_r: 1.080e-04, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1010, Loss: 1.226e-04, Loss_0: 7.672e-08, Loss_r: 1.219e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1020, Loss: 8.715e-05, Loss_0: 2.119e-08, Loss_r: 8.694e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1030, Loss: 6.696e-05, Loss_0: 9.904e-10, Loss_r: 6.695e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1040, Loss: 5.409e-05, Loss_0: 1.160e-10, Loss_r: 5.409e-05, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1050, Loss: 4.708e-05, Loss_0: 3.593e-10, Loss_r: 4.707e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1060, Loss: 4.289e-05, Loss_0: 8.275e-11, Loss_r: 4.289e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1070, Loss: 3.979e-05, Loss_0: 9.722e-12, Loss_r: 3.979e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1080, Loss: 3.726e-05, Loss_0: 6.043e-11, Loss_r: 3.726e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1090, Loss: 3.502e-05, Loss_0: 1.322e-11, Loss_r: 3.502e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1100, Loss: 3.298e-05, Loss_0: 9.256e-13, Loss_r: 3.298e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1110, Loss: 3.111e-05, Loss_0: 3.547e-13, Loss_r: 3.111e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1120, Loss: 2.938e-05, Loss_0: 2.386e-12, Loss_r: 2.938e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1130, Loss: 2.779e-05, Loss_0: 3.536e-13, Loss_r: 2.779e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1140, Loss: 2.632e-05, Loss_0: 3.149e-13, Loss_r: 2.632e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1150, Loss: 2.497e-05, Loss_0: 4.214e-13, Loss_r: 2.497e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1160, Loss: 2.373e-05, Loss_0: 3.131e-16, Loss_r: 2.373e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1170, Loss: 2.258e-05, Loss_0: 7.688e-13, Loss_r: 2.258e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1180, Loss: 2.152e-05, Loss_0: 2.457e-12, Loss_r: 2.152e-05, Time: 0.09, Learning Rate: 0.00100\n",
            "It: 1190, Loss: 2.055e-05, Loss_0: 7.487e-12, Loss_r: 2.055e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1200, Loss: 1.965e-05, Loss_0: 2.798e-11, Loss_r: 1.965e-05, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1210, Loss: 3.233e-05, Loss_0: 4.871e-07, Loss_r: 2.745e-05, Time: 0.08, Learning Rate: 0.00100\n",
            "It: 1220, Loss: 8.257e-04, Loss_0: 5.005e-05, Loss_r: 3.252e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1230, Loss: 3.226e-04, Loss_0: 1.926e-05, Loss_r: 1.299e-04, Time: 0.07, Learning Rate: 0.00100\n",
            "It: 1240, Loss: 3.152e-05, Loss_0: 1.787e-07, Loss_r: 2.974e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1250, Loss: 2.751e-05, Loss_0: 6.515e-08, Loss_r: 2.686e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1260, Loss: 2.773e-05, Loss_0: 2.186e-07, Loss_r: 2.554e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1270, Loss: 2.409e-05, Loss_0: 4.704e-08, Loss_r: 2.362e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1280, Loss: 2.268e-05, Loss_0: 1.227e-08, Loss_r: 2.255e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1290, Loss: 2.337e-05, Loss_0: 1.064e-07, Loss_r: 2.231e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1300, Loss: 2.306e-05, Loss_0: 1.116e-07, Loss_r: 2.195e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1310, Loss: 2.150e-05, Loss_0: 1.989e-08, Loss_r: 2.130e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1320, Loss: 2.102e-05, Loss_0: 3.544e-09, Loss_r: 2.099e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1330, Loss: 2.071e-05, Loss_0: 2.153e-09, Loss_r: 2.069e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1340, Loss: 2.040e-05, Loss_0: 2.557e-09, Loss_r: 2.038e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1350, Loss: 2.010e-05, Loss_0: 1.357e-11, Loss_r: 2.010e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1360, Loss: 1.983e-05, Loss_0: 5.490e-11, Loss_r: 1.983e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1370, Loss: 1.957e-05, Loss_0: 1.732e-10, Loss_r: 1.957e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1380, Loss: 1.931e-05, Loss_0: 1.343e-10, Loss_r: 1.931e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1390, Loss: 1.905e-05, Loss_0: 3.680e-11, Loss_r: 1.905e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1400, Loss: 1.881e-05, Loss_0: 1.202e-10, Loss_r: 1.881e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1410, Loss: 1.982e-05, Loss_0: 3.183e-07, Loss_r: 1.664e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1420, Loss: 4.395e-05, Loss_0: 1.961e-06, Loss_r: 2.434e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1430, Loss: 4.665e-04, Loss_0: 3.062e-05, Loss_r: 1.603e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1440, Loss: 6.125e-04, Loss_0: 4.057e-05, Loss_r: 2.069e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1450, Loss: 3.409e-05, Loss_0: 1.246e-06, Loss_r: 2.163e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1460, Loss: 5.720e-05, Loss_0: 2.858e-06, Loss_r: 2.862e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1470, Loss: 5.641e-05, Loss_0: 2.937e-06, Loss_r: 2.704e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1480, Loss: 2.944e-05, Loss_0: 1.051e-06, Loss_r: 1.892e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1490, Loss: 1.952e-05, Loss_0: 4.181e-07, Loss_r: 1.533e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1500, Loss: 1.619e-05, Loss_0: 1.850e-07, Loss_r: 1.434e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1510, Loss: 1.362e-05, Loss_0: 3.072e-08, Loss_r: 1.331e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1520, Loss: 1.368e-05, Loss_0: 4.606e-08, Loss_r: 1.322e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1530, Loss: 1.314e-05, Loss_0: 1.751e-08, Loss_r: 1.297e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1540, Loss: 1.416e-05, Loss_0: 9.975e-08, Loss_r: 1.316e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1550, Loss: 6.847e-05, Loss_0: 3.869e-06, Loss_r: 2.978e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1560, Loss: 3.448e-03, Loss_0: 2.382e-04, Loss_r: 1.066e-03, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1570, Loss: 6.969e-04, Loss_0: 4.552e-05, Loss_r: 2.416e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1580, Loss: 3.466e-04, Loss_0: 2.198e-05, Loss_r: 1.268e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1590, Loss: 1.348e-04, Loss_0: 8.293e-06, Loss_r: 5.188e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1600, Loss: 6.345e-05, Loss_0: 3.372e-06, Loss_r: 2.973e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1610, Loss: 2.039e-04, Loss_0: 7.417e-07, Loss_r: 1.965e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1620, Loss: 1.487e-04, Loss_0: 1.023e-06, Loss_r: 1.384e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1630, Loss: 1.207e-04, Loss_0: 6.424e-07, Loss_r: 1.143e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1640, Loss: 1.149e-04, Loss_0: 1.485e-06, Loss_r: 1.000e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1650, Loss: 1.555e-03, Loss_0: 1.189e-04, Loss_r: 3.667e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1660, Loss: 2.244e-03, Loss_0: 1.609e-04, Loss_r: 6.350e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1670, Loss: 8.739e-04, Loss_0: 5.971e-05, Loss_r: 2.768e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1680, Loss: 3.410e-04, Loss_0: 2.058e-05, Loss_r: 1.352e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1690, Loss: 1.366e-04, Loss_0: 4.666e-06, Loss_r: 8.997e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1700, Loss: 9.310e-05, Loss_0: 1.611e-06, Loss_r: 7.699e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1710, Loss: 7.958e-05, Loss_0: 1.030e-06, Loss_r: 6.927e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1720, Loss: 6.826e-05, Loss_0: 5.065e-07, Loss_r: 6.320e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1730, Loss: 5.876e-05, Loss_0: 2.279e-08, Loss_r: 5.853e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1740, Loss: 5.563e-05, Loss_0: 3.986e-08, Loss_r: 5.523e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1750, Loss: 5.152e-05, Loss_0: 4.854e-09, Loss_r: 5.147e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1760, Loss: 4.852e-05, Loss_0: 9.242e-09, Loss_r: 4.843e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1770, Loss: 4.580e-05, Loss_0: 1.381e-11, Loss_r: 4.580e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1780, Loss: 4.333e-05, Loss_0: 3.119e-09, Loss_r: 4.330e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1790, Loss: 4.111e-05, Loss_0: 3.064e-09, Loss_r: 4.108e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1800, Loss: 3.909e-05, Loss_0: 4.101e-10, Loss_r: 3.908e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1810, Loss: 4.699e-05, Loss_0: 1.264e-06, Loss_r: 3.435e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1820, Loss: 2.535e-05, Loss_0: 1.399e-07, Loss_r: 2.396e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1830, Loss: 2.001e-05, Loss_0: 1.080e-07, Loss_r: 1.893e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1840, Loss: 1.596e-05, Loss_0: 2.885e-08, Loss_r: 1.567e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1850, Loss: 1.438e-05, Loss_0: 2.598e-09, Loss_r: 1.435e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1860, Loss: 1.378e-05, Loss_0: 2.491e-08, Loss_r: 1.353e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1870, Loss: 1.507e-05, Loss_0: 1.698e-07, Loss_r: 1.337e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1880, Loss: 5.900e-05, Loss_0: 3.491e-06, Loss_r: 2.408e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1890, Loss: 1.728e-03, Loss_0: 1.276e-04, Loss_r: 4.523e-04, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1900, Loss: 7.868e-04, Loss_0: 5.460e-05, Loss_r: 2.408e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1910, Loss: 4.159e-05, Loss_0: 2.000e-06, Loss_r: 2.160e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1920, Loss: 1.254e-04, Loss_0: 8.291e-06, Loss_r: 4.252e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 1930, Loss: 3.148e-05, Loss_0: 1.489e-06, Loss_r: 1.659e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1940, Loss: 1.533e-05, Loss_0: 3.390e-07, Loss_r: 1.194e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1950, Loss: 1.445e-05, Loss_0: 3.110e-07, Loss_r: 1.134e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 1960, Loss: 1.095e-05, Loss_0: 8.881e-08, Loss_r: 1.006e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1970, Loss: 9.601e-06, Loss_0: 3.023e-09, Loss_r: 9.570e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1980, Loss: 9.581e-06, Loss_0: 1.264e-08, Loss_r: 9.455e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 1990, Loss: 9.336e-06, Loss_0: 9.008e-09, Loss_r: 9.246e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2000, Loss: 9.115e-06, Loss_0: 2.525e-09, Loss_r: 9.090e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2010, Loss: 6.764e-05, Loss_0: 3.081e-07, Loss_r: 6.456e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2020, Loss: 5.928e-05, Loss_0: 7.633e-07, Loss_r: 5.165e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2030, Loss: 2.856e-03, Loss_0: 2.174e-04, Loss_r: 6.820e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2040, Loss: 1.888e-03, Loss_0: 1.284e-04, Loss_r: 6.033e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2050, Loss: 8.539e-04, Loss_0: 5.585e-05, Loss_r: 2.953e-04, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2060, Loss: 7.189e-05, Loss_0: 2.221e-07, Loss_r: 6.967e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2070, Loss: 1.444e-04, Loss_0: 6.928e-06, Loss_r: 7.509e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2080, Loss: 7.954e-05, Loss_0: 2.667e-06, Loss_r: 5.287e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2090, Loss: 4.631e-05, Loss_0: 3.714e-07, Loss_r: 4.260e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2100, Loss: 4.203e-05, Loss_0: 3.485e-07, Loss_r: 3.854e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2110, Loss: 3.813e-05, Loss_0: 2.037e-07, Loss_r: 3.609e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2120, Loss: 3.421e-05, Loss_0: 1.174e-08, Loss_r: 3.410e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2130, Loss: 3.244e-05, Loss_0: 1.079e-09, Loss_r: 3.243e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2140, Loss: 3.088e-05, Loss_0: 2.135e-09, Loss_r: 3.086e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2150, Loss: 2.943e-05, Loss_0: 6.306e-10, Loss_r: 2.942e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2160, Loss: 2.812e-05, Loss_0: 2.097e-11, Loss_r: 2.812e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2170, Loss: 2.695e-05, Loss_0: 9.472e-11, Loss_r: 2.695e-05, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2180, Loss: 2.588e-05, Loss_0: 3.827e-10, Loss_r: 2.587e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2190, Loss: 2.489e-05, Loss_0: 6.673e-10, Loss_r: 2.489e-05, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2200, Loss: 2.399e-05, Loss_0: 7.233e-10, Loss_r: 2.398e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2210, Loss: 1.918e-05, Loss_0: 7.573e-09, Loss_r: 1.910e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2220, Loss: 1.428e-05, Loss_0: 7.436e-09, Loss_r: 1.420e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2230, Loss: 1.140e-05, Loss_0: 1.980e-09, Loss_r: 1.138e-05, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2240, Loss: 9.900e-06, Loss_0: 1.307e-10, Loss_r: 9.899e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2250, Loss: 9.199e-06, Loss_0: 5.130e-10, Loss_r: 9.194e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2260, Loss: 8.624e-06, Loss_0: 1.079e-11, Loss_r: 8.624e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2270, Loss: 8.163e-06, Loss_0: 3.899e-11, Loss_r: 8.162e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2280, Loss: 7.747e-06, Loss_0: 2.100e-13, Loss_r: 7.747e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2290, Loss: 7.386e-06, Loss_0: 1.497e-11, Loss_r: 7.386e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2300, Loss: 7.071e-06, Loss_0: 6.276e-14, Loss_r: 7.071e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2310, Loss: 6.797e-06, Loss_0: 3.051e-12, Loss_r: 6.797e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2320, Loss: 6.556e-06, Loss_0: 3.424e-12, Loss_r: 6.556e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2330, Loss: 6.345e-06, Loss_0: 1.844e-12, Loss_r: 6.345e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2340, Loss: 6.159e-06, Loss_0: 3.688e-12, Loss_r: 6.159e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2350, Loss: 5.995e-06, Loss_0: 5.371e-12, Loss_r: 5.995e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2360, Loss: 5.850e-06, Loss_0: 1.305e-12, Loss_r: 5.850e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2370, Loss: 5.721e-06, Loss_0: 3.776e-12, Loss_r: 5.721e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2380, Loss: 5.606e-06, Loss_0: 2.282e-12, Loss_r: 5.606e-06, Time: 0.04, Learning Rate: 0.00100\n",
            "It: 2390, Loss: 5.503e-06, Loss_0: 1.548e-13, Loss_r: 5.503e-06, Time: 0.05, Learning Rate: 0.00100\n",
            "It: 2400, Loss: 5.412e-06, Loss_0: 2.327e-11, Loss_r: 5.412e-06, Time: 0.06, Learning Rate: 0.00100\n",
            "It: 2410, Loss: 2.149e-05, Loss_0: 8.701e-07, Loss_r: 1.279e-05, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2420, Loss: 9.423e-06, Loss_0: 5.227e-08, Loss_r: 8.901e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2430, Loss: 8.781e-06, Loss_0: 6.182e-08, Loss_r: 8.163e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2440, Loss: 8.305e-06, Loss_0: 5.221e-08, Loss_r: 7.783e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2450, Loss: 7.461e-06, Loss_0: 2.538e-09, Loss_r: 7.435e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2460, Loss: 7.241e-06, Loss_0: 3.129e-09, Loss_r: 7.210e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2470, Loss: 7.018e-06, Loss_0: 3.781e-09, Loss_r: 6.980e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2480, Loss: 6.797e-06, Loss_0: 2.333e-10, Loss_r: 6.795e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2490, Loss: 6.617e-06, Loss_0: 3.061e-10, Loss_r: 6.614e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2500, Loss: 6.456e-06, Loss_0: 1.259e-13, Loss_r: 6.456e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2510, Loss: 6.307e-06, Loss_0: 2.283e-10, Loss_r: 6.305e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2520, Loss: 6.167e-06, Loss_0: 6.502e-12, Loss_r: 6.167e-06, Time: 0.04, Learning Rate: 0.00090\n",
            "It: 2530, Loss: 6.038e-06, Loss_0: 1.756e-11, Loss_r: 6.037e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2540, Loss: 5.916e-06, Loss_0: 4.684e-11, Loss_r: 5.915e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2550, Loss: 5.801e-06, Loss_0: 9.689e-11, Loss_r: 5.800e-06, Time: 0.06, Learning Rate: 0.00090\n",
            "It: 2560, Loss: 5.699e-06, Loss_0: 7.597e-10, Loss_r: 5.691e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2570, Loss: 6.090e-06, Loss_0: 4.068e-08, Loss_r: 5.683e-06, Time: 0.05, Learning Rate: 0.00090\n",
            "It: 2580, Loss: 1.012e-04, Loss_0: 7.381e-06, Loss_r: 2.738e-05, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2590, Loss: 7.963e-06, Loss_0: 1.985e-07, Loss_r: 5.978e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2600, Loss: 1.285e-05, Loss_0: 5.639e-07, Loss_r: 7.215e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2610, Loss: 1.526e-05, Loss_0: 4.676e-07, Loss_r: 1.058e-05, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2620, Loss: 7.472e-06, Loss_0: 2.157e-08, Loss_r: 7.257e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2630, Loss: 6.620e-06, Loss_0: 2.826e-08, Loss_r: 6.338e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2640, Loss: 5.843e-06, Loss_0: 1.687e-08, Loss_r: 5.674e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2650, Loss: 5.316e-06, Loss_0: 1.626e-09, Loss_r: 5.300e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2660, Loss: 5.055e-06, Loss_0: 2.285e-09, Loss_r: 5.032e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2670, Loss: 4.813e-06, Loss_0: 6.485e-10, Loss_r: 4.806e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2680, Loss: 4.598e-06, Loss_0: 1.008e-11, Loss_r: 4.598e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2690, Loss: 4.415e-06, Loss_0: 1.091e-10, Loss_r: 4.414e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2700, Loss: 4.248e-06, Loss_0: 4.634e-11, Loss_r: 4.248e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2710, Loss: 4.097e-06, Loss_0: 1.904e-11, Loss_r: 4.097e-06, Time: 0.04, Learning Rate: 0.00081\n",
            "It: 2720, Loss: 3.961e-06, Loss_0: 4.152e-12, Loss_r: 3.961e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2730, Loss: 3.838e-06, Loss_0: 4.422e-12, Loss_r: 3.838e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2740, Loss: 3.727e-06, Loss_0: 1.104e-12, Loss_r: 3.727e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2750, Loss: 3.626e-06, Loss_0: 2.138e-14, Loss_r: 3.626e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2760, Loss: 3.534e-06, Loss_0: 1.729e-12, Loss_r: 3.534e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2770, Loss: 3.450e-06, Loss_0: 2.605e-13, Loss_r: 3.450e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2780, Loss: 3.374e-06, Loss_0: 2.283e-13, Loss_r: 3.374e-06, Time: 0.05, Learning Rate: 0.00081\n",
            "It: 2790, Loss: 3.304e-06, Loss_0: 9.156e-12, Loss_r: 3.304e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2800, Loss: 3.246e-06, Loss_0: 3.584e-10, Loss_r: 3.242e-06, Time: 0.06, Learning Rate: 0.00081\n",
            "It: 2810, Loss: 7.524e-04, Loss_0: 5.279e-05, Loss_r: 2.245e-04, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2820, Loss: 3.045e-05, Loss_0: 2.067e-07, Loss_r: 2.839e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2830, Loss: 1.087e-04, Loss_0: 7.011e-06, Loss_r: 3.857e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2840, Loss: 5.317e-05, Loss_0: 2.872e-06, Loss_r: 2.445e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2850, Loss: 2.011e-05, Loss_0: 3.543e-07, Loss_r: 1.657e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2860, Loss: 1.691e-05, Loss_0: 2.319e-07, Loss_r: 1.460e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2870, Loss: 1.527e-05, Loss_0: 2.352e-07, Loss_r: 1.292e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2880, Loss: 1.218e-05, Loss_0: 5.043e-08, Loss_r: 1.168e-05, Time: 0.06, Learning Rate: 0.00073\n",
            "It: 2890, Loss: 1.081e-05, Loss_0: 2.151e-08, Loss_r: 1.060e-05, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2900, Loss: 1.002e-05, Loss_0: 7.693e-09, Loss_r: 9.941e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2910, Loss: 9.375e-06, Loss_0: 7.781e-09, Loss_r: 9.297e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2920, Loss: 8.816e-06, Loss_0: 1.806e-10, Loss_r: 8.814e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2930, Loss: 8.428e-06, Loss_0: 9.718e-10, Loss_r: 8.418e-06, Time: 0.05, Learning Rate: 0.00073\n",
            "It: 2940, Loss: 8.153e-06, Loss_0: 5.896e-09, Loss_r: 8.094e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2950, Loss: 9.505e-06, Loss_0: 1.322e-07, Loss_r: 8.184e-06, Time: 0.04, Learning Rate: 0.00073\n",
            "It: 2960, Loss: 1.093e-04, Loss_0: 8.060e-06, Loss_r: 2.868e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 2970, Loss: 2.294e-05, Loss_0: 1.224e-06, Loss_r: 1.070e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 2980, Loss: 7.165e-06, Loss_0: 9.301e-10, Loss_r: 7.156e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 2990, Loss: 1.033e-05, Loss_0: 2.757e-07, Loss_r: 7.575e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3000, Loss: 8.679e-06, Loss_0: 1.526e-07, Loss_r: 7.153e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3010, Loss: 1.571e-05, Loss_0: 1.302e-09, Loss_r: 1.570e-05, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3020, Loss: 1.209e-05, Loss_0: 2.463e-08, Loss_r: 1.185e-05, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3030, Loss: 8.965e-06, Loss_0: 1.302e-08, Loss_r: 8.835e-06, Time: 0.06, Learning Rate: 0.00066\n",
            "It: 3040, Loss: 7.602e-06, Loss_0: 8.109e-11, Loss_r: 7.601e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3050, Loss: 6.989e-06, Loss_0: 1.485e-09, Loss_r: 6.975e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3060, Loss: 6.496e-06, Loss_0: 2.641e-10, Loss_r: 6.494e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3070, Loss: 6.106e-06, Loss_0: 1.645e-10, Loss_r: 6.104e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3080, Loss: 5.743e-06, Loss_0: 2.716e-11, Loss_r: 5.743e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3090, Loss: 5.428e-06, Loss_0: 3.087e-11, Loss_r: 5.428e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3100, Loss: 5.147e-06, Loss_0: 4.426e-12, Loss_r: 5.147e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3110, Loss: 4.898e-06, Loss_0: 1.131e-12, Loss_r: 4.898e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3120, Loss: 4.677e-06, Loss_0: 2.633e-13, Loss_r: 4.677e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3130, Loss: 4.479e-06, Loss_0: 1.926e-14, Loss_r: 4.479e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3140, Loss: 4.303e-06, Loss_0: 3.825e-14, Loss_r: 4.303e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3150, Loss: 4.145e-06, Loss_0: 1.208e-14, Loss_r: 4.145e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3160, Loss: 4.004e-06, Loss_0: 1.409e-13, Loss_r: 4.004e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3170, Loss: 3.877e-06, Loss_0: 1.939e-12, Loss_r: 3.877e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3180, Loss: 3.763e-06, Loss_0: 5.181e-12, Loss_r: 3.763e-06, Time: 0.05, Learning Rate: 0.00066\n",
            "It: 3190, Loss: 3.661e-06, Loss_0: 5.738e-12, Loss_r: 3.661e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3200, Loss: 3.569e-06, Loss_0: 3.092e-11, Loss_r: 3.569e-06, Time: 0.04, Learning Rate: 0.00066\n",
            "It: 3210, Loss: 1.202e-04, Loss_0: 7.375e-06, Loss_r: 4.641e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3220, Loss: 2.079e-05, Loss_0: 2.825e-07, Loss_r: 1.797e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3230, Loss: 2.422e-05, Loss_0: 8.200e-07, Loss_r: 1.602e-05, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3240, Loss: 1.660e-05, Loss_0: 2.999e-07, Loss_r: 1.361e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3250, Loss: 1.356e-05, Loss_0: 9.661e-08, Loss_r: 1.259e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3260, Loss: 1.157e-05, Loss_0: 2.451e-09, Loss_r: 1.154e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3270, Loss: 1.119e-05, Loss_0: 2.329e-08, Loss_r: 1.096e-05, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3280, Loss: 1.056e-05, Loss_0: 7.743e-09, Loss_r: 1.049e-05, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3290, Loss: 1.005e-05, Loss_0: 5.789e-09, Loss_r: 9.992e-06, Time: 0.04, Learning Rate: 0.00059\n",
            "It: 3300, Loss: 9.637e-06, Loss_0: 6.947e-10, Loss_r: 9.630e-06, Time: 0.05, Learning Rate: 0.00059\n",
            "It: 3310, Loss: 9.283e-06, Loss_0: 5.260e-10, Loss_r: 9.278e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3320, Loss: 8.986e-06, Loss_0: 7.215e-10, Loss_r: 8.979e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3330, Loss: 8.716e-06, Loss_0: 1.287e-10, Loss_r: 8.715e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3340, Loss: 8.476e-06, Loss_0: 4.317e-12, Loss_r: 8.476e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3350, Loss: 8.273e-06, Loss_0: 6.433e-10, Loss_r: 8.267e-06, Time: 0.08, Learning Rate: 0.00059\n",
            "It: 3360, Loss: 8.892e-06, Loss_0: 6.063e-08, Loss_r: 8.286e-06, Time: 0.07, Learning Rate: 0.00059\n",
            "It: 3370, Loss: 1.170e-04, Loss_0: 8.482e-06, Loss_r: 3.221e-05, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3380, Loss: 1.710e-05, Loss_0: 7.164e-07, Loss_r: 9.940e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3390, Loss: 9.273e-06, Loss_0: 1.356e-07, Loss_r: 7.918e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3400, Loss: 1.331e-05, Loss_0: 4.670e-07, Loss_r: 8.639e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3410, Loss: 7.246e-06, Loss_0: 7.795e-08, Loss_r: 6.466e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3420, Loss: 5.394e-06, Loss_0: 8.030e-09, Loss_r: 5.314e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3430, Loss: 4.893e-06, Loss_0: 2.332e-08, Loss_r: 4.660e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3440, Loss: 4.225e-06, Loss_0: 4.418e-10, Loss_r: 4.221e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3450, Loss: 4.011e-06, Loss_0: 2.563e-09, Loss_r: 3.985e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3460, Loss: 3.771e-06, Loss_0: 5.765e-12, Loss_r: 3.771e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3470, Loss: 3.601e-06, Loss_0: 3.878e-10, Loss_r: 3.598e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3480, Loss: 3.442e-06, Loss_0: 5.673e-11, Loss_r: 3.442e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3490, Loss: 3.305e-06, Loss_0: 1.299e-12, Loss_r: 3.305e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3500, Loss: 3.186e-06, Loss_0: 9.167e-12, Loss_r: 3.186e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3510, Loss: 3.080e-06, Loss_0: 1.432e-12, Loss_r: 3.080e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3520, Loss: 2.987e-06, Loss_0: 1.153e-12, Loss_r: 2.987e-06, Time: 0.08, Learning Rate: 0.00053\n",
            "It: 3530, Loss: 2.905e-06, Loss_0: 2.717e-14, Loss_r: 2.905e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3540, Loss: 2.832e-06, Loss_0: 1.552e-13, Loss_r: 2.832e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3550, Loss: 2.766e-06, Loss_0: 1.616e-12, Loss_r: 2.766e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3560, Loss: 2.708e-06, Loss_0: 5.317e-12, Loss_r: 2.708e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3570, Loss: 2.655e-06, Loss_0: 3.515e-12, Loss_r: 2.655e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3580, Loss: 2.608e-06, Loss_0: 1.548e-11, Loss_r: 2.608e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3590, Loss: 2.568e-06, Loss_0: 2.074e-10, Loss_r: 2.566e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3600, Loss: 2.645e-06, Loss_0: 9.053e-09, Loss_r: 2.555e-06, Time: 0.07, Learning Rate: 0.00053\n",
            "It: 3610, Loss: 1.084e-05, Loss_0: 5.345e-07, Loss_r: 5.498e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3620, Loss: 4.372e-06, Loss_0: 5.447e-08, Loss_r: 3.827e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3630, Loss: 3.647e-06, Loss_0: 6.059e-09, Loss_r: 3.586e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3640, Loss: 3.872e-06, Loss_0: 2.726e-08, Loss_r: 3.599e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3650, Loss: 3.542e-06, Loss_0: 5.668e-09, Loss_r: 3.485e-06, Time: 0.09, Learning Rate: 0.00048\n",
            "It: 3660, Loss: 3.442e-06, Loss_0: 1.036e-09, Loss_r: 3.431e-06, Time: 0.08, Learning Rate: 0.00048\n",
            "It: 3670, Loss: 3.404e-06, Loss_0: 1.073e-09, Loss_r: 3.394e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3680, Loss: 3.357e-06, Loss_0: 2.442e-10, Loss_r: 3.355e-06, Time: 0.07, Learning Rate: 0.00048\n",
            "It: 3690, Loss: 3.322e-06, Loss_0: 5.635e-11, Loss_r: 3.321e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3700, Loss: 3.291e-06, Loss_0: 5.262e-11, Loss_r: 3.291e-06, Time: 0.06, Learning Rate: 0.00048\n",
            "It: 3710, Loss: 3.261e-06, Loss_0: 2.564e-11, Loss_r: 3.261e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3720, Loss: 3.234e-06, Loss_0: 6.193e-13, Loss_r: 3.234e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3730, Loss: 3.207e-06, Loss_0: 4.403e-12, Loss_r: 3.207e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3740, Loss: 3.182e-06, Loss_0: 2.808e-13, Loss_r: 3.182e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3750, Loss: 3.158e-06, Loss_0: 2.820e-12, Loss_r: 3.158e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3760, Loss: 3.135e-06, Loss_0: 2.372e-13, Loss_r: 3.135e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3770, Loss: 3.113e-06, Loss_0: 1.702e-12, Loss_r: 3.113e-06, Time: 0.04, Learning Rate: 0.00048\n",
            "It: 3780, Loss: 3.091e-06, Loss_0: 1.050e-16, Loss_r: 3.091e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3790, Loss: 3.070e-06, Loss_0: 3.494e-12, Loss_r: 3.070e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3800, Loss: 3.052e-06, Loss_0: 9.123e-11, Loss_r: 3.051e-06, Time: 0.05, Learning Rate: 0.00048\n",
            "It: 3810, Loss: 1.269e-04, Loss_0: 8.136e-06, Loss_r: 4.552e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3820, Loss: 2.086e-05, Loss_0: 3.793e-07, Loss_r: 1.707e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3830, Loss: 2.130e-05, Loss_0: 6.381e-07, Loss_r: 1.492e-05, Time: 0.06, Learning Rate: 0.00043\n",
            "It: 3840, Loss: 1.852e-05, Loss_0: 4.909e-07, Loss_r: 1.361e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3850, Loss: 1.203e-05, Loss_0: 1.075e-08, Loss_r: 1.192e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3860, Loss: 1.200e-05, Loss_0: 5.256e-08, Loss_r: 1.148e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3870, Loss: 1.098e-05, Loss_0: 2.222e-08, Loss_r: 1.076e-05, Time: 0.07, Learning Rate: 0.00043\n",
            "It: 3880, Loss: 1.031e-05, Loss_0: 2.046e-11, Loss_r: 1.031e-05, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3890, Loss: 9.924e-06, Loss_0: 5.107e-11, Loss_r: 9.923e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3900, Loss: 9.588e-06, Loss_0: 8.533e-10, Loss_r: 9.579e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3910, Loss: 9.289e-06, Loss_0: 2.868e-11, Loss_r: 9.288e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3920, Loss: 9.025e-06, Loss_0: 9.740e-12, Loss_r: 9.025e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3930, Loss: 8.790e-06, Loss_0: 4.830e-10, Loss_r: 8.785e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3940, Loss: 8.575e-06, Loss_0: 2.463e-10, Loss_r: 8.572e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3950, Loss: 8.380e-06, Loss_0: 1.155e-10, Loss_r: 8.379e-06, Time: 0.04, Learning Rate: 0.00043\n",
            "It: 3960, Loss: 8.201e-06, Loss_0: 1.144e-10, Loss_r: 8.200e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3970, Loss: 8.036e-06, Loss_0: 2.528e-10, Loss_r: 8.034e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3980, Loss: 7.935e-06, Loss_0: 5.377e-09, Loss_r: 7.881e-06, Time: 0.05, Learning Rate: 0.00043\n",
            "It: 3990, Loss: 1.917e-05, Loss_0: 9.177e-07, Loss_r: 9.991e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4000, Loss: 7.754e-06, Loss_0: 1.251e-08, Loss_r: 7.629e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4010, Loss: 8.608e-06, Loss_0: 9.754e-08, Loss_r: 7.633e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4020, Loss: 7.041e-06, Loss_0: 6.279e-08, Loss_r: 6.413e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4030, Loss: 5.394e-06, Loss_0: 1.074e-09, Loss_r: 5.383e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4040, Loss: 4.963e-06, Loss_0: 6.261e-09, Loss_r: 4.900e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4050, Loss: 4.556e-06, Loss_0: 9.677e-10, Loss_r: 4.547e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4060, Loss: 4.276e-06, Loss_0: 8.066e-10, Loss_r: 4.268e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4070, Loss: 4.031e-06, Loss_0: 9.038e-11, Loss_r: 4.030e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4080, Loss: 3.818e-06, Loss_0: 1.295e-10, Loss_r: 3.817e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4090, Loss: 3.625e-06, Loss_0: 3.307e-11, Loss_r: 3.625e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4100, Loss: 3.453e-06, Loss_0: 2.748e-12, Loss_r: 3.453e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4110, Loss: 3.298e-06, Loss_0: 4.622e-13, Loss_r: 3.298e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4120, Loss: 3.159e-06, Loss_0: 7.001e-12, Loss_r: 3.159e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4130, Loss: 3.033e-06, Loss_0: 5.875e-13, Loss_r: 3.033e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4140, Loss: 2.919e-06, Loss_0: 1.788e-13, Loss_r: 2.919e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4150, Loss: 2.815e-06, Loss_0: 1.166e-11, Loss_r: 2.815e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4160, Loss: 2.721e-06, Loss_0: 8.518e-13, Loss_r: 2.721e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4170, Loss: 2.634e-06, Loss_0: 1.749e-13, Loss_r: 2.634e-06, Time: 0.05, Learning Rate: 0.00039\n",
            "It: 4180, Loss: 2.555e-06, Loss_0: 8.661e-14, Loss_r: 2.555e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4190, Loss: 2.482e-06, Loss_0: 1.661e-12, Loss_r: 2.482e-06, Time: 0.04, Learning Rate: 0.00039\n",
            "It: 4200, Loss: 2.416e-06, Loss_0: 8.102e-11, Loss_r: 2.415e-06, Time: 0.06, Learning Rate: 0.00039\n",
            "It: 4210, Loss: 6.147e-05, Loss_0: 3.793e-06, Loss_r: 2.354e-05, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4220, Loss: 1.175e-05, Loss_0: 2.256e-07, Loss_r: 9.491e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4230, Loss: 9.933e-06, Loss_0: 2.507e-07, Loss_r: 7.426e-06, Time: 0.08, Learning Rate: 0.00035\n",
            "It: 4240, Loss: 8.975e-06, Loss_0: 2.338e-07, Loss_r: 6.637e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4250, Loss: 5.731e-06, Loss_0: 2.547e-09, Loss_r: 5.706e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4260, Loss: 5.676e-06, Loss_0: 2.632e-08, Loss_r: 5.413e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4270, Loss: 5.088e-06, Loss_0: 8.025e-09, Loss_r: 5.008e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4280, Loss: 4.739e-06, Loss_0: 6.014e-11, Loss_r: 4.739e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4290, Loss: 4.530e-06, Loss_0: 2.464e-10, Loss_r: 4.528e-06, Time: 0.06, Learning Rate: 0.00035\n",
            "It: 4300, Loss: 4.347e-06, Loss_0: 6.428e-10, Loss_r: 4.341e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4310, Loss: 4.192e-06, Loss_0: 1.704e-11, Loss_r: 4.192e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4320, Loss: 4.062e-06, Loss_0: 7.143e-11, Loss_r: 4.061e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4330, Loss: 3.952e-06, Loss_0: 9.714e-11, Loss_r: 3.951e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4340, Loss: 3.856e-06, Loss_0: 1.720e-12, Loss_r: 3.856e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4350, Loss: 3.774e-06, Loss_0: 3.231e-14, Loss_r: 3.774e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4360, Loss: 3.701e-06, Loss_0: 8.508e-12, Loss_r: 3.701e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4370, Loss: 3.650e-06, Loss_0: 8.030e-10, Loss_r: 3.642e-06, Time: 0.04, Learning Rate: 0.00035\n",
            "It: 4380, Loss: 4.221e-06, Loss_0: 4.890e-08, Loss_r: 3.732e-06, Time: 0.05, Learning Rate: 0.00035\n",
            "It: 4390, Loss: 7.075e-05, Loss_0: 5.293e-06, Loss_r: 1.781e-05, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4400, Loss: 1.361e-05, Loss_0: 7.922e-07, Loss_r: 5.685e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4410, Loss: 6.262e-06, Loss_0: 2.312e-08, Loss_r: 6.031e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4420, Loss: 5.036e-06, Loss_0: 2.155e-08, Loss_r: 4.821e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4430, Loss: 4.615e-06, Loss_0: 4.954e-08, Loss_r: 4.120e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4440, Loss: 4.016e-06, Loss_0: 3.170e-08, Loss_r: 3.699e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4450, Loss: 3.464e-06, Loss_0: 5.309e-09, Loss_r: 3.411e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4460, Loss: 3.217e-06, Loss_0: 2.364e-10, Loss_r: 3.215e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4470, Loss: 3.083e-06, Loss_0: 1.500e-09, Loss_r: 3.068e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4480, Loss: 2.931e-06, Loss_0: 1.078e-10, Loss_r: 2.930e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4490, Loss: 2.816e-06, Loss_0: 1.586e-10, Loss_r: 2.814e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4500, Loss: 2.712e-06, Loss_0: 1.972e-12, Loss_r: 2.712e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4510, Loss: 2.623e-06, Loss_0: 3.071e-11, Loss_r: 2.623e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4520, Loss: 2.544e-06, Loss_0: 2.675e-12, Loss_r: 2.544e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4530, Loss: 2.475e-06, Loss_0: 2.247e-13, Loss_r: 2.475e-06, Time: 0.06, Learning Rate: 0.00031\n",
            "It: 4540, Loss: 2.413e-06, Loss_0: 1.255e-12, Loss_r: 2.413e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4550, Loss: 2.358e-06, Loss_0: 1.155e-12, Loss_r: 2.358e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4560, Loss: 2.309e-06, Loss_0: 1.093e-13, Loss_r: 2.309e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4570, Loss: 2.265e-06, Loss_0: 6.886e-13, Loss_r: 2.265e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4580, Loss: 2.226e-06, Loss_0: 7.045e-14, Loss_r: 2.226e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4590, Loss: 2.190e-06, Loss_0: 3.264e-14, Loss_r: 2.190e-06, Time: 0.05, Learning Rate: 0.00031\n",
            "It: 4600, Loss: 2.157e-06, Loss_0: 1.291e-12, Loss_r: 2.157e-06, Time: 0.04, Learning Rate: 0.00031\n",
            "It: 4610, Loss: 3.934e-06, Loss_0: 9.473e-08, Loss_r: 2.987e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4620, Loss: 3.009e-06, Loss_0: 3.367e-08, Loss_r: 2.673e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4630, Loss: 2.585e-06, Loss_0: 1.092e-08, Loss_r: 2.476e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4640, Loss: 2.398e-06, Loss_0: 3.089e-09, Loss_r: 2.367e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4650, Loss: 2.306e-06, Loss_0: 4.720e-10, Loss_r: 2.302e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4660, Loss: 2.247e-06, Loss_0: 1.502e-12, Loss_r: 2.247e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4670, Loss: 2.202e-06, Loss_0: 7.192e-11, Loss_r: 2.201e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4680, Loss: 2.159e-06, Loss_0: 7.537e-11, Loss_r: 2.158e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4690, Loss: 2.117e-06, Loss_0: 3.702e-12, Loss_r: 2.117e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4700, Loss: 2.079e-06, Loss_0: 4.855e-12, Loss_r: 2.079e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4710, Loss: 2.043e-06, Loss_0: 1.031e-12, Loss_r: 2.043e-06, Time: 0.04, Learning Rate: 0.00028\n",
            "It: 4720, Loss: 2.010e-06, Loss_0: 2.209e-12, Loss_r: 2.010e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4730, Loss: 1.978e-06, Loss_0: 1.656e-13, Loss_r: 1.978e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4740, Loss: 1.948e-06, Loss_0: 3.608e-13, Loss_r: 1.948e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4750, Loss: 1.919e-06, Loss_0: 2.749e-13, Loss_r: 1.919e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4760, Loss: 1.891e-06, Loss_0: 3.987e-13, Loss_r: 1.891e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4770, Loss: 1.865e-06, Loss_0: 5.593e-13, Loss_r: 1.865e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4780, Loss: 1.840e-06, Loss_0: 1.108e-12, Loss_r: 1.840e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4790, Loss: 1.816e-06, Loss_0: 1.693e-12, Loss_r: 1.816e-06, Time: 0.05, Learning Rate: 0.00028\n",
            "It: 4800, Loss: 1.792e-06, Loss_0: 6.512e-14, Loss_r: 1.792e-06, Time: 0.06, Learning Rate: 0.00028\n",
            "It: 4810, Loss: 8.563e-06, Loss_0: 2.182e-07, Loss_r: 6.382e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4820, Loss: 4.888e-06, Loss_0: 8.103e-08, Loss_r: 4.078e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4830, Loss: 3.066e-06, Loss_0: 2.336e-08, Loss_r: 2.832e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4840, Loss: 2.350e-06, Loss_0: 3.292e-09, Loss_r: 2.317e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4850, Loss: 2.141e-06, Loss_0: 1.026e-10, Loss_r: 2.140e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4860, Loss: 2.011e-06, Loss_0: 1.171e-09, Loss_r: 1.999e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4870, Loss: 1.899e-06, Loss_0: 4.529e-10, Loss_r: 1.895e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4880, Loss: 1.803e-06, Loss_0: 2.327e-12, Loss_r: 1.803e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4890, Loss: 1.729e-06, Loss_0: 3.073e-11, Loss_r: 1.729e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4900, Loss: 1.668e-06, Loss_0: 1.698e-11, Loss_r: 1.667e-06, Time: 0.07, Learning Rate: 0.00025\n",
            "It: 4910, Loss: 1.617e-06, Loss_0: 1.310e-11, Loss_r: 1.617e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4920, Loss: 1.575e-06, Loss_0: 3.940e-13, Loss_r: 1.575e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4930, Loss: 1.540e-06, Loss_0: 7.001e-12, Loss_r: 1.540e-06, Time: 0.06, Learning Rate: 0.00025\n",
            "It: 4940, Loss: 1.511e-06, Loss_0: 2.245e-12, Loss_r: 1.511e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4950, Loss: 1.486e-06, Loss_0: 1.936e-12, Loss_r: 1.486e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4960, Loss: 1.465e-06, Loss_0: 4.673e-12, Loss_r: 1.465e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4970, Loss: 1.448e-06, Loss_0: 3.865e-12, Loss_r: 1.448e-06, Time: 0.04, Learning Rate: 0.00025\n",
            "It: 4980, Loss: 1.433e-06, Loss_0: 2.114e-12, Loss_r: 1.433e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "It: 4990, Loss: 1.419e-06, Loss_0: 1.527e-12, Loss_r: 1.419e-06, Time: 0.05, Learning Rate: 0.00025\n",
            "Training time: 27.2270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CSol[idx] = predict_CSol[idx].reshape(exact_C.shape)\n",
        "\n",
        "  error_C = np.linalg.norm(exact_C.flatten()[:,None]-predict_CSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_C.flatten()[:,None],2)\n",
        "  print('Error C Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_C) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfbc784-6fad-4afa-e576-732d399e375d",
        "id": "JxZmLeXqu70q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error C Sol [1, 64, 64, 64, 1] : 2.877967e-03\n",
            "Error C Sol [1, 128, 128, 128, 1] : 3.427431e-03\n",
            "Error C Sol [1, 256, 256, 256, 1] : 4.532454e-03\n",
            "Error C Sol [1, 64, 64, 64, 64, 1] : 8.684873e-04\n",
            "Error C Sol [1, 256, 256, 256, 256, 1] : 2.849317e-03\n",
            "Error C Sol [1, 128, 128, 128, 128, 1] : 1.955925e-03\n",
            "Error C Sol [1, 128, 128, 64, 64, 1] : 1.674987e-03\n",
            "Error C Sol [1, 256, 128, 64, 32, 1] : 1.806937e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [4,5,6,7]:\n",
        "  plt.plot(t, predict_CSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_C.flatten(), 'c', label = 'C expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "8ef8b342-8e44-43ac-b36e-85266e08fdd8",
        "id": "O8FCRBwNu70q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7febc91c0b50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adsorption exercise"
      ],
      "metadata": {
        "id": "klKUjrxh1Hvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This part aim to solve a simple adsorption exercise. A gas of concentration Cg of a molecule goes trhough a packed-bed containing particles which will adsorb this molecule. The concentration of molecule inside the particles is Cs. The ODE governing this system are:\n",
        "dCs / dt = (kg * as / (1 - eps)) * (Cg - Cs/Ke)\n",
        "dCg / dt = - (kg * as / eps) * (Cg - Cs/Ke) + Q/V (Cgin - Cg)\n",
        "\n",
        "with the mass transfert coefficient kg = 0.01 m/s\n",
        "the bed porosity eps = 0.5\n",
        "the constant of the reaction Ke = 10\n",
        "the flow rate Q = 0.01 m3/s\n",
        "the volume of the reactor V = 1 m3\n",
        "the diameter of the particle dp = 0.005 m\n",
        "the surface area of the adsorption as = 6 * (1-eps) / dp\n",
        "\n",
        "with Cg = 0 and Cs = 0 at t = 0."
      ],
      "metadata": {
        "id": "Bycm9NnB1K3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# architecture of the feedforward network with 1 input being time\n",
        "#and 1 outputs being C\n",
        "layers = [1, 128, 128, 128, 128, 2]\n",
        "\n",
        "#number of collocation points for the ODE\n",
        "Nf = 60000\n",
        "\n",
        "#upper and lower boundary of the experiment (here time)\n",
        "lb = np.array([0])\n",
        "ub = np.array([6000])"
      ],
      "metadata": {
        "id": "zG5UNBmPVAZp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get data from matlab workspace\n",
        "data = scipy.io.loadmat(\"../data/ODE_adsorption_data.mat\") #load the simulation data from matlab\n",
        "\n",
        "t = data['t'].flatten()[:,None] # time from simulation\n",
        "exact_Cg = data['Cg'] #Cg from simulation, function of x\n",
        "exact_Cs = data['Cs'] #Cs from simulation, function of x"
      ],
      "metadata": {
        "id": "ZB_UHhK0VAZv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PINN model"
      ],
      "metadata": {
        "id": "MhaO3YO-VY75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdsorptionPINN_ODE:\n",
        "  '''PINN model tailored to answer this exercise'''\n",
        "  def __init__(self, Nf, layers, ub, lb, keep_best_only = True):\n",
        "    '''object constructor (initialize object at creation). Takes the folowing parameters:\n",
        "    Nf = the number of collocation points to enforce the PDEs\n",
        "    layers = an array containing the number of hidden layers and neurons per layer\n",
        "    ub = array of shape (1,) containing the time of the end of the experiment (s)\n",
        "    lb = array of shape (1,) containing the time of the beginning of the experiment (s) (typically 0)\n",
        "    keep_best_only = True to only keep the model with the best loss, False will update the model no matter the loss'''\n",
        "\n",
        "    '''Initialize the constants'''\n",
        "    self.Cg0 = 0 #Concentration in the reactor at t = 0 (mol/L)\n",
        "    self.Cs0 = 0 #Concentration in the particles at t = 0 (mol/L)\n",
        "    self.Cin = 1 #Input concentration (mol/l)\n",
        "    self.Ke = 10 #reaction constant (mol/l.s)\n",
        "    self.kg = 0.0001 #mass transfer coefficient of the gas phase to the particles (m/s)\n",
        "    self. epsb = 0.5 #Bed porosity (no units)\n",
        "    self.dp = 0.005 #diameter of the particles (m)\n",
        "    self.As = 6*(1-self.epsb)/self.dp\n",
        "    self.V = 1 #Volume of the reactor (m3)\n",
        "    self.Q = 0.01 #Flow rate (m3/s)\n",
        "\n",
        "    self.keep_best_only = keep_best_only\n",
        "\n",
        "    '''initialize the collocation points'''\n",
        "          #boundaries\n",
        "    self.ub = ub\n",
        "    self.lb = lb\n",
        "        #initial conditions\n",
        "    self.t0 = np.array([[0]]) #at t=0 (only one point)\n",
        "        #residues from the ODE\n",
        "    self.Nf = Nf\n",
        "    self.Initialize_random_collocation_points()\n",
        "\n",
        "    '''Initialize the self.adaptative coefficient, which will be used to make sure\n",
        "    the initial conditions and the boundaries are enforced properly.'''\n",
        "    self.beta = 0.9\n",
        "    self.lambda_0_value = np.array(1.0)\n",
        "\n",
        "    '''initializing feedforward NN'''\n",
        "    self.layers = layers\n",
        "    self.weights, self.biases = self.Initialize_NN(layers)\n",
        "\n",
        "    '''creating tensorflow placeholder'''\n",
        "    #placeholders for initial conditions\n",
        "    self.t0_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t0.shape[1]])\n",
        "    #place holder for residues\n",
        "    self.t_f_tf = tf.compat.v1.placeholder(tf.float32, shape=[None, self.t_f.shape[1]])\n",
        "    #placeholder for the self-adaptative loss coefficients\n",
        "    self.lambda_0_tf = tf.compat.v1.placeholder(tf.float32, shape = self.lambda_0_value.shape)\n",
        "\n",
        "    '''Creating tensorflow Graphs (operations happening on each epoch during training)'''\n",
        "    #initial conditions graph\n",
        "    self.Cg0_pred,self.Cs0_pred = self.Net_initial(self.t0_tf)\n",
        "    #residues graph\n",
        "    self.rg_pred,self.rp_pred= self.Net_residue(self.t_f_tf)\n",
        "\n",
        "    '''Loss graph'''\n",
        "    #Loss on initial condition\n",
        "    self.loss_0 = self.lambda_0_tf *\\\n",
        "                        (tf.reduce_mean(input_tensor=tf.square(self.Cg0_pred - self.Cg0)) +\\\n",
        "                         tf.reduce_mean(input_tensor=tf.square(self.Cs0_pred - self.Cs0)))\n",
        "    #Residues loss from the ODE\n",
        "    self.loss_r = tf.reduce_mean(input_tensor=tf.square(self.rg_pred)) +\\\n",
        "                  tf.reduce_mean(input_tensor=tf.square(self.rp_pred))\n",
        "\n",
        "    #Global loss function\n",
        "    self.loss = self. loss_r + self.loss_0\n",
        "\n",
        "    '''Computing the self adaptative loss coefficient'''\n",
        "    #graph to get the gradients for each losses\n",
        "    self.grad_0, self.grad_r = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.grad_0.append(tf.gradients(self.loss_0, self.weights[weights_idx])[0])\n",
        "      self.grad_r.append(tf.gradients(self.loss_r, self.weights[weights_idx])[0])\n",
        "\n",
        "    #Getting the mean of these gradients for initial condition, and the max for residues\n",
        "    self.mean_grad_0_list, self.max_grad_r_list = [], []\n",
        "    for weights_idx in range(len(layers)-1):\n",
        "      self.mean_grad_0_list.append(tf.reduce_mean(tf.abs(self.grad_0[weights_idx])))\n",
        "      self.max_grad_r_list.append(tf.reduce_max(tf.abs(self.grad_r[weights_idx])))\n",
        "\n",
        "    self.mean_grad_0 = tf.reduce_mean(tf.stack(self.mean_grad_0_list))\n",
        "    self.max_grad_r = tf.reduce_max(tf.stack(self.max_grad_r_list))\n",
        "\n",
        "    #computing the loss coefficients\n",
        "    self.lambda_0_graph = self.max_grad_r / self.mean_grad_0\n",
        "\n",
        "    '''Logs to store some training parameters for ulterior usage'''\n",
        "    #log for the losses\n",
        "    self.loss_0_log = []\n",
        "    self.loss_r_log = []\n",
        "    self.loss_log = []\n",
        "\n",
        "    #log for the coefficient\n",
        "    self.lambda_0_log = []\n",
        "\n",
        "    #log for the weights and biases\n",
        "    self.weights_log = []\n",
        "    self.biases_log = []\n",
        "\n",
        "    '''Setting the optimizers for the training'''\n",
        "    #Setting an adaptative learning rate (with the method Reduce_Learning_Rate_On_Plateau)\n",
        "    self.learning_rate = np.array([0.001])\n",
        "    self.learning_rate_tf = tf.compat.v1.placeholder(tf.float32, shape=[self.learning_rate.shape[0]])\n",
        "    self.tolerance = 0.9999\n",
        "    self.decay_rate = 0.9\n",
        "    self.patience = 5\n",
        "    self.reduce_LR_cooldown = 5\n",
        "    self.count_cooldown = 50\n",
        "\n",
        "    #The optimizer used during the training is the adam optimizer\n",
        "    self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer(learning_rate = self.learning_rate_tf[0])\n",
        "    #self.optimizer_Adam = tf.compat.v1.train.AdamOptimizer()\n",
        "    self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n",
        "\n",
        "    # tf session\n",
        "    self.sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True,\n",
        "                                                  log_device_placement=True))\n",
        "\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    self.sess.run(init)\n",
        "\n",
        "  def Save_Best_Weights_Biases_losses(self, loss, loss_0, loss_r):\n",
        "    '''Check if the current loss is better than the previous recorded loss.\n",
        "    Then save the best losses, weights and biases between the current model and\n",
        "    the previous one (and reverse to the previous model if it was better).'''\n",
        "    if(len(self.loss_log) < 1 or loss < self.loss_log[-1] or not self.keep_best_only):\n",
        "      #if it is the first epoch of training or if the loss is better than\n",
        "      #the previous one, keep the current model.\n",
        "      self.weights_log.append(self.weights)\n",
        "      self.biases_log.append(self.biases)\n",
        "      self.loss_log.append(loss)\n",
        "      self.loss_0_log.append(loss_0/self.lambda_0_value)\n",
        "      self.loss_r_log.append(loss_r)\n",
        "\n",
        "      return None\n",
        "    else:\n",
        "      #if the loss is worse than the previous one, return to the previous model.\n",
        "      self.weights = self.weights_log[-1]\n",
        "      self.biases = self.biases_log[-1]\n",
        "      self.weights_log.append(self.weights)\n",
        "      self.biases_log.append(self.biases)\n",
        "      self.loss_log.append(self.loss_log[-1])\n",
        "      self.loss_0_log.append(self.loss_0_log[-1])\n",
        "      self.loss_r_log.append(self.loss_r_log[-1])\n",
        "\n",
        "      return None\n",
        "\n",
        "  def Reduce_Learning_Rate_On_Plateau(self, learning_rate):\n",
        "    '''Reduce the learning rate when the loss reach a plateau\n",
        "    It compare the current loss to the \"self.patence\" last recorded losses\n",
        "      (here the losses are recorded every 10 epochs in \"self.loss_log\").\n",
        "    It checks if the current loss is significantly smaller than these\n",
        "      previous losses (at least by a factor of self.tolerance).\n",
        "    If it is not, this method reduces the learning rate by a factor self.decay_rate\n",
        "      and wait for \"self.reduce_LR_cooldown\" before resuming its action.\n",
        "    It is recommanded to set a fairly large cooldown at the beginning since\n",
        "      this method is not necessary when everything goes well at the beginning\n",
        "      and it can slow the training.\n",
        "    For a similar reason, it is recommanded not to run this method every epoch\n",
        "      but every 10 or 20 epochs for example.\n",
        "    '''\n",
        "    if(len(self.loss_log) < self.patience):\n",
        "      #Wait so that enough losses are recorded\n",
        "      return learning_rate\n",
        "    elif(self.count_cooldown > 0):\n",
        "      self.count_cooldown -= 1\n",
        "      return learning_rate\n",
        "    else:\n",
        "      for idx in range(self.patience):\n",
        "        if(self.loss_log[-1] < self.tolerance * self.loss_log[-(idx+2)]):\n",
        "          return learning_rate\n",
        "      self.count_cooldown = self.reduce_LR_cooldown\n",
        "      return learning_rate * self.decay_rate\n",
        "\n",
        "  def Get_latin_hypercubes_samples(self, lower_bounds, upper_bounds, num_samples, seed = None):\n",
        "    '''Return a 'num_samples' number of random points between a lower_bounds and\n",
        "    an upper_bounds (arrays containing a number of ints / floats equal to the\n",
        "    number of dimension. E.G. to generate points in 3 dimensions, upper and\n",
        "    lower bounds must be of shape(3,)). Uses latin_hyper_cubes which generate\n",
        "    quasi-random points with a pseudo-uniform distribution to garantee low discrepancy '''\n",
        "    sampler = qmc.LatinHypercube(d=len(upper_bounds), seed = seed)\n",
        "    samples = sampler.random(num_samples)\n",
        "    samples = qmc.scale(samples, lower_bounds, upper_bounds)\n",
        "    return samples\n",
        "\n",
        "  def Initialize_random_collocation_points(self):\n",
        "        #ODE collocation points\n",
        "    #self.t_f = self.Get_latin_hypercubes_samples(self.lb,self.ub, self.Nf)\n",
        "    self.t_f = np.linspace(self.lb,self.ub,self.Nf)\n",
        "    return\n",
        "\n",
        "  def Xavier_init(self, in_dim,out_dim):\n",
        "    '''Initialize a weight or bias matrix with xavier initializer (= gloriot uniform)'''\n",
        "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "    return tf.Variable(\n",
        "        tf.random.truncated_normal(\n",
        "                [in_dim, out_dim],\n",
        "                stddev = xavier_stddev,\n",
        "                dtype = tf.float32),\n",
        "      dtype = tf.float32)\n",
        "\n",
        "  def Initialize_NN(self, layers):\n",
        "    '''return initial weights and biases for a feed forward neural network\n",
        "    with a given number of layers and neurons per layer'''\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for i in range(num_layers -1):\n",
        "      #create a set of defaults weights and biases between each layer\n",
        "      in_dim = layers[i]\n",
        "      out_dim = layers[i+1]\n",
        "            #initialize the weights using Xavier initialization to reduce problems such as vanishing or exploding gradients\n",
        "      W = self.Xavier_init(in_dim, out_dim)\n",
        "            #initialize biases at 0\n",
        "      b = tf.Variable(tf.zeros([1,layers[i+1]], dtype = tf.float32), dtype = tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  def Neural_net(self, input, weights, biases):\n",
        "    '''Compute the feedforward neural network operations'''\n",
        "    num_layers = len(self.layers)\n",
        "\n",
        "    H = (input - self.lb)/(self.ub - self.lb) #input normalization\n",
        "\n",
        "    for l in range(0, num_layers-2): #compute each hidden layer\n",
        "      W = weights[l]\n",
        "      b = biases[l]\n",
        "      H = tf.tanh(tf.add(tf.matmul(H,W),b)) #weighted sum + activation function (tanh)\n",
        "\n",
        "    #compute the output layer\n",
        "    W = weights[-1]\n",
        "    b = biases[-1]\n",
        "    output = tf.add(tf.matmul(H,W),b)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def Net_initial(self, t):\n",
        "    '''Calculate C at a given t using the neural network'''\n",
        "    CgCs = self.Neural_net(t,self.weights,self.biases)\n",
        "\n",
        "    Cg = CgCs[:,0:1]\n",
        "    Cs = CgCs[:,1:2]\n",
        "\n",
        "    return Cg,Cs\n",
        "\n",
        "  def Net_residue(self, t):\n",
        "    '''Calculate C and dC/dt using the neural network and\n",
        "    return the PDEs in the canonic form, so it should be equal to 0'''\n",
        "\n",
        "    Cg,Cs = self.Net_initial(t)\n",
        "    Cg_t = tf.gradients(ys=Cg, xs=t)[0]\n",
        "    Cs_t = tf.gradients(ys=Cs, xs=t)[0]\n",
        "\n",
        "    #Residue from the gas phase equation\n",
        "    rg = Cg_t + (Cg - (Cs / self.Ke)) * (self.kg * self.As) / (self.epsb) - (self.Q / self.V) * (self.Cin - Cg)\n",
        "    #residue from the particulate phase equation\n",
        "    rp = Cs_t - (Cg - (Cs / self.Ke)) * (self.kg * self.As)/(1-self.epsb)\n",
        "\n",
        "    return rg,rp\n",
        "\n",
        "  def Train(self, nIter):\n",
        "    '''train the network for a given number of iteration'''\n",
        "\n",
        "    start_time = time.time()\n",
        "    for it in range(nIter):\n",
        "\n",
        "      #assign each placeholder to its corresponding data\n",
        "      tf_dict = {self.t0_tf: self.t0, self.t_f_tf: self.t_f,\n",
        "                 self.learning_rate_tf: self.learning_rate,\n",
        "                 self.lambda_0_tf: self.lambda_0_value}\n",
        "\n",
        "      #train the model using the Adam optimizer\n",
        "      self.sess.run(self.train_op_Adam, tf_dict)\n",
        "\n",
        "      # Every 10 steps\n",
        "      if it % 10 == 0:\n",
        "\n",
        "        #Compute the losses\n",
        "        loss_value, loss_0, loss_r = self.sess.run([self.loss, self.loss_0, self.loss_r], tf_dict)\n",
        "\n",
        "        #Reduce the learning rate for next time if there is a need to\n",
        "        self.learning_rate = self.Reduce_Learning_Rate_On_Plateau(self.learning_rate)\n",
        "\n",
        "        #adapt the loss weights\n",
        "        lambda_0_temp = self.sess.run(self.lambda_0_graph,tf_dict)\n",
        "        self.lambda_0_value = min(10.0, lambda_0_temp *(1 - self.beta) + self.beta * self.lambda_0_value)\n",
        "        self.lambda_0_log.append(lambda_0_temp)\n",
        "\n",
        "        #record the best loss, weights and biases\n",
        "        self.Save_Best_Weights_Biases_losses(loss_value,loss_0, loss_r)\n",
        "\n",
        "        #if it % 200 == 0:\n",
        "          # Generate new random collocation points for the next 200 epochs\n",
        "          #self.Initialize_random_collocation_points()\n",
        "\n",
        "                #Print the loss\n",
        "        elapsed = time.time() - start_time\n",
        "        print(\"It: %d, Loss: %.3e, Loss_0: %.3e, Loss_r: %.3e, lambda_0 : %.3e, Time: %.2f, Learning Rate: %.5f\" \\\n",
        "                % (it, loss_value, loss_0, loss_r, lambda_0_temp, elapsed, self.learning_rate))\n",
        "        start_time = time.time()\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  def Predict(self, t):\n",
        "    '''Use to predict C for a given array t of shape (n,1) containing\n",
        "    in this order the space and the time coordinates of the points to predict'''\n",
        "    if(not (len(t.shape) == 2 and t.shape[1] == 1) and not len(t.shape) == 1):\n",
        "      print(\"Error: unexpected shape of t, should be (n, ) or (n,1)\")\n",
        "      return None\n",
        "\n",
        "    tf_dict = {self.t0_tf: t}\n",
        "\n",
        "    Cg,Cs = np.array(self.sess.run([self.Cg0_pred,self.Cs0_pred], tf_dict))\n",
        "\n",
        "    return Cg,Cs"
      ],
      "metadata": {
        "id": "I-q28isaVcCL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the PINN models"
      ],
      "metadata": {
        "id": "IrQmUWYXap5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the PINN with the ODE"
      ],
      "metadata": {
        "id": "aeimxCgbap5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DifferentLayers = [[1,64,64,64,2],[1,128,128,128,2],[1,256,256,256,2],[1,64,64,64,64,2],[1,256,256,256,256,2],[1,128,128,128,128,2],[1,128,128,64,64,2],[1,256,128,64,32,2]]\n",
        "predict_CSol = []\n",
        "for layers_idx in range(len(DifferentLayers)):\n",
        "  print(DifferentLayers[layers_idx])\n",
        "  # PINN model without the differential equation\n",
        "  modelSol = AdsorptionPINN_ODE(Nf, DifferentLayers[layers_idx], ub, lb)\n",
        "\n",
        "  #Training the model\n",
        "  start_time = time.time()\n",
        "  modelSol.Train(5000)\n",
        "  elapsed = time.time() - start_time\n",
        "  print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "  predict_CSol.append(modelSol.Predict(t))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcff1a8-e56d-4e65-c49b-1c27f1892783",
        "id": "OSELMWGsap5_"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 64, 64, 64, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 4.068e-04, Loss_0: 3.586e-04, Loss_r: 4.822e-05, lambda_0 : 1.402e+00, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.440e-04, Loss_0: 8.707e-05, Loss_r: 5.692e-05, lambda_0 : 3.882e+01, Time: 0.19, Learning Rate: 0.00100\n",
            "It: 20, Loss: 8.293e-05, Loss_0: 2.353e-05, Loss_r: 5.940e-05, lambda_0 : 4.558e+01, Time: 0.18, Learning Rate: 0.00100\n",
            "It: 30, Loss: 8.700e-05, Loss_0: 2.843e-05, Loss_r: 5.856e-05, lambda_0 : 8.115e+00, Time: 0.17, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.683e-05, Loss_0: 1.679e-05, Loss_r: 6.005e-05, lambda_0 : 5.714e+01, Time: 0.18, Learning Rate: 0.00100\n",
            "It: 50, Loss: 5.864e-05, Loss_0: 1.064e-06, Loss_r: 5.757e-05, lambda_0 : 1.174e+02, Time: 0.17, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.993e-05, Loss_0: 3.385e-06, Loss_r: 5.655e-05, lambda_0 : 3.599e+00, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.704e-05, Loss_0: 2.720e-07, Loss_r: 5.677e-05, lambda_0 : 5.230e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 80, Loss: 8.349e-05, Loss_0: 2.706e-05, Loss_r: 5.643e-05, lambda_0 : 1.638e+00, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 90, Loss: 6.555e-05, Loss_0: 9.165e-06, Loss_r: 5.639e-05, lambda_0 : 5.028e+00, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.724e-05, Loss_0: 7.646e-07, Loss_r: 5.648e-05, lambda_0 : 1.098e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.689e-05, Loss_0: 5.457e-07, Loss_r: 5.635e-05, lambda_0 : 1.207e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.607e-05, Loss_0: 2.571e-08, Loss_r: 5.605e-05, lambda_0 : 5.223e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.589e-05, Loss_0: 9.508e-08, Loss_r: 5.580e-05, lambda_0 : 3.086e+01, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.548e-05, Loss_0: 1.627e-08, Loss_r: 5.547e-05, lambda_0 : 6.312e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.511e-05, Loss_0: 7.868e-09, Loss_r: 5.511e-05, lambda_0 : 1.328e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.472e-05, Loss_0: 2.735e-08, Loss_r: 5.469e-05, lambda_0 : 5.408e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.425e-05, Loss_0: 1.084e-08, Loss_r: 5.424e-05, lambda_0 : 9.639e+01, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.379e-05, Loss_0: 9.284e-08, Loss_r: 5.369e-05, lambda_0 : 3.347e+01, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 190, Loss: 1.231e-04, Loss_0: 6.883e-05, Loss_r: 5.431e-05, lambda_0 : 1.927e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.870e-05, Loss_0: 3.685e-06, Loss_r: 5.501e-05, lambda_0 : 1.720e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.803e-05, Loss_0: 2.483e-06, Loss_r: 5.555e-05, lambda_0 : 1.658e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.708e-05, Loss_0: 1.695e-06, Loss_r: 5.538e-05, lambda_0 : 9.579e+00, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.571e-05, Loss_0: 7.937e-07, Loss_r: 5.492e-05, lambda_0 : 1.509e+01, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.456e-05, Loss_0: 2.994e-07, Loss_r: 5.426e-05, lambda_0 : 6.005e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.343e-05, Loss_0: 1.353e-08, Loss_r: 5.342e-05, lambda_0 : 2.380e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.244e-05, Loss_0: 6.143e-08, Loss_r: 5.238e-05, lambda_0 : 8.683e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.117e-05, Loss_0: 1.230e-08, Loss_r: 5.116e-05, lambda_0 : 3.281e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 280, Loss: 4.973e-05, Loss_0: 1.477e-08, Loss_r: 4.972e-05, lambda_0 : 2.954e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 290, Loss: 4.819e-05, Loss_0: 5.046e-09, Loss_r: 4.818e-05, lambda_0 : 6.588e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 300, Loss: 4.670e-05, Loss_0: 5.927e-09, Loss_r: 4.669e-05, lambda_0 : 4.017e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 310, Loss: 4.541e-05, Loss_0: 3.084e-09, Loss_r: 4.541e-05, lambda_0 : 3.311e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 320, Loss: 4.435e-05, Loss_0: 6.749e-09, Loss_r: 4.435e-05, lambda_0 : 2.071e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 330, Loss: 1.862e-04, Loss_0: 1.408e-04, Loss_r: 4.540e-05, lambda_0 : 1.641e+00, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 340, Loss: 9.235e-05, Loss_0: 4.016e-05, Loss_r: 5.220e-05, lambda_0 : 7.476e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 350, Loss: 6.186e-05, Loss_0: 9.454e-06, Loss_r: 5.241e-05, lambda_0 : 3.149e+01, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.707e-05, Loss_0: 1.908e-06, Loss_r: 5.516e-05, lambda_0 : 5.085e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.313e-05, Loss_0: 7.474e-07, Loss_r: 5.238e-05, lambda_0 : 2.636e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.223e-05, Loss_0: 3.176e-07, Loss_r: 5.191e-05, lambda_0 : 4.030e+02, Time: 0.17, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.160e-05, Loss_0: 2.259e-07, Loss_r: 5.137e-05, lambda_0 : 3.671e+01, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.099e-05, Loss_0: 4.499e-08, Loss_r: 5.094e-05, lambda_0 : 2.026e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.061e-05, Loss_0: 3.260e-08, Loss_r: 5.058e-05, lambda_0 : 2.449e+02, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.023e-05, Loss_0: 5.581e-09, Loss_r: 5.023e-05, lambda_0 : 2.091e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 430, Loss: 4.988e-05, Loss_0: 1.982e-09, Loss_r: 4.988e-05, lambda_0 : 3.870e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 440, Loss: 4.951e-05, Loss_0: 1.047e-09, Loss_r: 4.951e-05, lambda_0 : 4.613e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.915e-05, Loss_0: 2.927e-09, Loss_r: 4.914e-05, lambda_0 : 2.789e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.877e-05, Loss_0: 3.082e-09, Loss_r: 4.877e-05, lambda_0 : 3.145e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.838e-05, Loss_0: 2.047e-09, Loss_r: 4.838e-05, lambda_0 : 3.997e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.798e-05, Loss_0: 1.582e-09, Loss_r: 4.798e-05, lambda_0 : 4.400e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.756e-05, Loss_0: 1.625e-09, Loss_r: 4.756e-05, lambda_0 : 4.478e+02, Time: 0.15, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.712e-05, Loss_0: 1.750e-09, Loss_r: 4.712e-05, lambda_0 : 4.492e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.666e-05, Loss_0: 1.734e-09, Loss_r: 4.665e-05, lambda_0 : 4.646e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.616e-05, Loss_0: 1.708e-09, Loss_r: 4.616e-05, lambda_0 : 4.758e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.564e-05, Loss_0: 1.657e-09, Loss_r: 4.563e-05, lambda_0 : 4.856e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.507e-05, Loss_0: 1.621e-09, Loss_r: 4.507e-05, lambda_0 : 4.973e+02, Time: 0.16, Learning Rate: 0.00100\n",
            "It: 550, Loss: 4.445e-05, Loss_0: 1.586e-09, Loss_r: 4.445e-05, lambda_0 : 5.057e+02, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 560, Loss: 4.384e-05, Loss_0: 1.557e-09, Loss_r: 4.384e-05, lambda_0 : 5.126e+02, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 570, Loss: 4.318e-05, Loss_0: 1.536e-09, Loss_r: 4.318e-05, lambda_0 : 5.160e+02, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 580, Loss: 4.244e-05, Loss_0: 1.522e-09, Loss_r: 4.244e-05, lambda_0 : 5.161e+02, Time: 0.15, Learning Rate: 0.00090\n",
            "It: 590, Loss: 4.163e-05, Loss_0: 1.513e-09, Loss_r: 4.162e-05, lambda_0 : 5.122e+02, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 600, Loss: 4.072e-05, Loss_0: 1.480e-09, Loss_r: 4.072e-05, lambda_0 : 4.806e+02, Time: 0.15, Learning Rate: 0.00090\n",
            "It: 610, Loss: 3.303e-04, Loss_0: 2.903e-04, Loss_r: 4.004e-05, lambda_0 : 4.357e+00, Time: 0.15, Learning Rate: 0.00090\n",
            "It: 620, Loss: 1.566e-04, Loss_0: 1.087e-04, Loss_r: 4.787e-05, lambda_0 : 2.204e+01, Time: 0.15, Learning Rate: 0.00090\n",
            "It: 630, Loss: 8.115e-05, Loss_0: 2.981e-05, Loss_r: 5.134e-05, lambda_0 : 3.680e+01, Time: 0.15, Learning Rate: 0.00090\n",
            "It: 640, Loss: 5.294e-05, Loss_0: 9.575e-07, Loss_r: 5.198e-05, lambda_0 : 3.216e+01, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 650, Loss: 5.398e-05, Loss_0: 2.237e-06, Loss_r: 5.174e-05, lambda_0 : 6.192e+01, Time: 0.16, Learning Rate: 0.00090\n",
            "It: 660, Loss: 5.232e-05, Loss_0: 1.028e-06, Loss_r: 5.130e-05, lambda_0 : 4.222e+01, Time: 0.15, Learning Rate: 0.00081\n",
            "It: 670, Loss: 5.109e-05, Loss_0: 1.264e-07, Loss_r: 5.096e-05, lambda_0 : 5.413e+01, Time: 0.16, Learning Rate: 0.00081\n",
            "It: 680, Loss: 5.073e-05, Loss_0: 1.024e-08, Loss_r: 5.072e-05, lambda_0 : 1.894e+02, Time: 0.15, Learning Rate: 0.00081\n",
            "It: 690, Loss: 5.052e-05, Loss_0: 2.711e-08, Loss_r: 5.049e-05, lambda_0 : 1.659e+02, Time: 0.17, Learning Rate: 0.00081\n",
            "It: 700, Loss: 5.027e-05, Loss_0: 1.732e-08, Loss_r: 5.025e-05, lambda_0 : 2.003e+02, Time: 0.18, Learning Rate: 0.00081\n",
            "It: 710, Loss: 5.001e-05, Loss_0: 4.388e-09, Loss_r: 5.001e-05, lambda_0 : 3.052e+02, Time: 0.18, Learning Rate: 0.00081\n",
            "It: 720, Loss: 4.976e-05, Loss_0: 2.766e-09, Loss_r: 4.976e-05, lambda_0 : 4.384e+02, Time: 0.17, Learning Rate: 0.00073\n",
            "It: 730, Loss: 4.953e-05, Loss_0: 2.183e-09, Loss_r: 4.953e-05, lambda_0 : 4.438e+02, Time: 0.20, Learning Rate: 0.00073\n",
            "It: 740, Loss: 4.929e-05, Loss_0: 1.952e-09, Loss_r: 4.929e-05, lambda_0 : 4.821e+02, Time: 0.27, Learning Rate: 0.00073\n",
            "It: 750, Loss: 4.905e-05, Loss_0: 1.886e-09, Loss_r: 4.904e-05, lambda_0 : 5.579e+02, Time: 0.19, Learning Rate: 0.00073\n",
            "It: 760, Loss: 4.879e-05, Loss_0: 1.700e-09, Loss_r: 4.879e-05, lambda_0 : 6.038e+02, Time: 0.18, Learning Rate: 0.00073\n",
            "It: 770, Loss: 4.852e-05, Loss_0: 1.616e-09, Loss_r: 4.852e-05, lambda_0 : 6.088e+02, Time: 0.16, Learning Rate: 0.00073\n",
            "It: 780, Loss: 4.824e-05, Loss_0: 1.643e-09, Loss_r: 4.824e-05, lambda_0 : 6.360e+02, Time: 0.19, Learning Rate: 0.00066\n",
            "It: 790, Loss: 4.798e-05, Loss_0: 1.651e-09, Loss_r: 4.798e-05, lambda_0 : 6.282e+02, Time: 0.24, Learning Rate: 0.00066\n",
            "It: 800, Loss: 4.770e-05, Loss_0: 1.607e-09, Loss_r: 4.770e-05, lambda_0 : 6.373e+02, Time: 0.20, Learning Rate: 0.00066\n",
            "It: 810, Loss: 4.741e-05, Loss_0: 1.588e-09, Loss_r: 4.740e-05, lambda_0 : 6.420e+02, Time: 0.17, Learning Rate: 0.00066\n",
            "It: 820, Loss: 4.710e-05, Loss_0: 1.572e-09, Loss_r: 4.709e-05, lambda_0 : 6.407e+02, Time: 0.16, Learning Rate: 0.00066\n",
            "It: 830, Loss: 4.677e-05, Loss_0: 1.549e-09, Loss_r: 4.677e-05, lambda_0 : 6.458e+02, Time: 0.15, Learning Rate: 0.00066\n",
            "It: 840, Loss: 4.642e-05, Loss_0: 1.529e-09, Loss_r: 4.642e-05, lambda_0 : 6.449e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 850, Loss: 4.609e-05, Loss_0: 1.511e-09, Loss_r: 4.608e-05, lambda_0 : 6.428e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 860, Loss: 4.573e-05, Loss_0: 1.490e-09, Loss_r: 4.573e-05, lambda_0 : 6.426e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 870, Loss: 4.536e-05, Loss_0: 1.470e-09, Loss_r: 4.536e-05, lambda_0 : 6.412e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 880, Loss: 4.496e-05, Loss_0: 1.448e-09, Loss_r: 4.496e-05, lambda_0 : 6.390e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 890, Loss: 4.454e-05, Loss_0: 1.424e-09, Loss_r: 4.454e-05, lambda_0 : 6.370e+02, Time: 0.16, Learning Rate: 0.00059\n",
            "It: 900, Loss: 4.410e-05, Loss_0: 1.399e-09, Loss_r: 4.410e-05, lambda_0 : 6.345e+02, Time: 0.16, Learning Rate: 0.00053\n",
            "It: 910, Loss: 4.368e-05, Loss_0: 1.372e-09, Loss_r: 4.368e-05, lambda_0 : 6.326e+02, Time: 0.16, Learning Rate: 0.00053\n",
            "It: 920, Loss: 4.325e-05, Loss_0: 1.343e-09, Loss_r: 4.325e-05, lambda_0 : 6.311e+02, Time: 0.15, Learning Rate: 0.00053\n",
            "It: 930, Loss: 4.279e-05, Loss_0: 1.310e-09, Loss_r: 4.279e-05, lambda_0 : 6.299e+02, Time: 0.15, Learning Rate: 0.00053\n",
            "It: 940, Loss: 4.232e-05, Loss_0: 1.274e-09, Loss_r: 4.232e-05, lambda_0 : 6.291e+02, Time: 0.16, Learning Rate: 0.00053\n",
            "It: 950, Loss: 4.183e-05, Loss_0: 1.233e-09, Loss_r: 4.183e-05, lambda_0 : 6.293e+02, Time: 0.15, Learning Rate: 0.00053\n",
            "It: 960, Loss: 4.133e-05, Loss_0: 1.190e-09, Loss_r: 4.132e-05, lambda_0 : 6.309e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 970, Loss: 4.086e-05, Loss_0: 1.147e-09, Loss_r: 4.086e-05, lambda_0 : 6.346e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 980, Loss: 4.039e-05, Loss_0: 1.101e-09, Loss_r: 4.039e-05, lambda_0 : 6.403e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 990, Loss: 3.992e-05, Loss_0: 1.053e-09, Loss_r: 3.991e-05, lambda_0 : 6.479e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1000, Loss: 3.943e-05, Loss_0: 1.005e-09, Loss_r: 3.943e-05, lambda_0 : 6.572e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1010, Loss: 3.895e-05, Loss_0: 9.557e-10, Loss_r: 3.895e-05, lambda_0 : 6.685e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1020, Loss: 3.846e-05, Loss_0: 9.065e-10, Loss_r: 3.846e-05, lambda_0 : 6.820e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1030, Loss: 3.797e-05, Loss_0: 8.577e-10, Loss_r: 3.797e-05, lambda_0 : 6.978e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1040, Loss: 3.748e-05, Loss_0: 8.102e-10, Loss_r: 3.748e-05, lambda_0 : 7.154e+02, Time: 0.15, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 3.699e-05, Loss_0: 7.642e-10, Loss_r: 3.699e-05, lambda_0 : 7.349e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1060, Loss: 3.649e-05, Loss_0: 7.206e-10, Loss_r: 3.649e-05, lambda_0 : 7.559e+02, Time: 0.15, Learning Rate: 0.00048\n",
            "It: 1070, Loss: 3.599e-05, Loss_0: 6.795e-10, Loss_r: 3.599e-05, lambda_0 : 7.781e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1080, Loss: 3.549e-05, Loss_0: 6.414e-10, Loss_r: 3.549e-05, lambda_0 : 8.018e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1090, Loss: 3.498e-05, Loss_0: 6.163e-10, Loss_r: 3.498e-05, lambda_0 : 8.422e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1100, Loss: 3.535e-05, Loss_0: 2.773e-09, Loss_r: 3.535e-05, lambda_0 : 1.480e+04, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1110, Loss: 3.476e-05, Loss_0: 5.429e-09, Loss_r: 3.475e-05, lambda_0 : 7.823e+03, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1120, Loss: 3.365e-05, Loss_0: 2.490e-09, Loss_r: 3.365e-05, lambda_0 : 6.431e+03, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1130, Loss: 3.303e-05, Loss_0: 8.212e-10, Loss_r: 3.302e-05, lambda_0 : 1.714e+03, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1140, Loss: 3.252e-05, Loss_0: 5.425e-10, Loss_r: 3.252e-05, lambda_0 : 7.550e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1150, Loss: 3.201e-05, Loss_0: 6.751e-10, Loss_r: 3.201e-05, lambda_0 : 1.563e+03, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1160, Loss: 3.150e-05, Loss_0: 6.897e-09, Loss_r: 3.150e-05, lambda_0 : 7.962e+02, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1170, Loss: 2.623e-04, Loss_0: 2.309e-04, Loss_r: 3.143e-05, lambda_0 : 1.046e+01, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1180, Loss: 5.496e-05, Loss_0: 2.117e-05, Loss_r: 3.378e-05, lambda_0 : 8.127e+01, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1190, Loss: 3.808e-05, Loss_0: 3.262e-06, Loss_r: 3.482e-05, lambda_0 : 4.483e+01, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1200, Loss: 3.532e-05, Loss_0: 2.739e-07, Loss_r: 3.505e-05, lambda_0 : 6.059e+01, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1210, Loss: 3.587e-05, Loss_0: 9.295e-07, Loss_r: 3.494e-05, lambda_0 : 2.314e+01, Time: 0.16, Learning Rate: 0.00048\n",
            "It: 1220, Loss: 3.510e-05, Loss_0: 5.045e-07, Loss_r: 3.460e-05, lambda_0 : 6.746e+01, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1230, Loss: 3.435e-05, Loss_0: 1.299e-07, Loss_r: 3.422e-05, lambda_0 : 5.629e+01, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1240, Loss: 3.380e-05, Loss_0: 2.562e-09, Loss_r: 3.380e-05, lambda_0 : 4.041e+02, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1250, Loss: 3.338e-05, Loss_0: 1.925e-08, Loss_r: 3.336e-05, lambda_0 : 1.908e+02, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1260, Loss: 3.291e-05, Loss_0: 1.126e-09, Loss_r: 3.291e-05, lambda_0 : 5.901e+02, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1270, Loss: 3.246e-05, Loss_0: 3.796e-09, Loss_r: 3.246e-05, lambda_0 : 3.248e+02, Time: 0.16, Learning Rate: 0.00043\n",
            "It: 1280, Loss: 3.200e-05, Loss_0: 1.469e-09, Loss_r: 3.200e-05, lambda_0 : 5.464e+02, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1290, Loss: 3.158e-05, Loss_0: 7.746e-10, Loss_r: 3.157e-05, lambda_0 : 7.274e+02, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1300, Loss: 3.115e-05, Loss_0: 4.334e-10, Loss_r: 3.115e-05, lambda_0 : 1.055e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1310, Loss: 3.071e-05, Loss_0: 4.795e-10, Loss_r: 3.071e-05, lambda_0 : 1.055e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1320, Loss: 3.027e-05, Loss_0: 4.050e-10, Loss_r: 3.027e-05, lambda_0 : 1.117e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1330, Loss: 2.983e-05, Loss_0: 4.023e-10, Loss_r: 2.983e-05, lambda_0 : 1.340e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1340, Loss: 2.937e-05, Loss_0: 4.008e-10, Loss_r: 2.937e-05, lambda_0 : 1.279e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1350, Loss: 2.891e-05, Loss_0: 3.870e-10, Loss_r: 2.891e-05, lambda_0 : 1.331e+03, Time: 0.15, Learning Rate: 0.00039\n",
            "It: 1360, Loss: 2.844e-05, Loss_0: 3.761e-10, Loss_r: 2.844e-05, lambda_0 : 1.372e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1370, Loss: 2.797e-05, Loss_0: 3.657e-10, Loss_r: 2.797e-05, lambda_0 : 1.346e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1380, Loss: 2.749e-05, Loss_0: 3.587e-10, Loss_r: 2.749e-05, lambda_0 : 1.337e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1390, Loss: 2.700e-05, Loss_0: 3.553e-10, Loss_r: 2.700e-05, lambda_0 : 1.353e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1400, Loss: 2.650e-05, Loss_0: 3.503e-10, Loss_r: 2.650e-05, lambda_0 : 1.344e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1410, Loss: 2.600e-05, Loss_0: 3.462e-10, Loss_r: 2.600e-05, lambda_0 : 1.334e+03, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1420, Loss: 2.550e-05, Loss_0: 3.364e-10, Loss_r: 2.550e-05, lambda_0 : 1.287e+03, Time: 0.15, Learning Rate: 0.00039\n",
            "It: 1430, Loss: 2.564e-05, Loss_0: 1.202e-10, Loss_r: 2.564e-05, lambda_0 : 8.195e+04, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1440, Loss: 2.812e-05, Loss_0: 2.365e-06, Loss_r: 2.576e-05, lambda_0 : 7.734e+02, Time: 0.16, Learning Rate: 0.00039\n",
            "It: 1450, Loss: 3.121e-05, Loss_0: 5.382e-06, Loss_r: 2.582e-05, lambda_0 : 2.786e+02, Time: 0.17, Learning Rate: 0.00039\n",
            "It: 1460, Loss: 4.214e-05, Loss_0: 1.505e-05, Loss_r: 2.710e-05, lambda_0 : 1.488e+02, Time: 0.17, Learning Rate: 0.00039\n",
            "It: 1470, Loss: 3.074e-05, Loss_0: 3.249e-06, Loss_r: 2.749e-05, lambda_0 : 5.189e+01, Time: 0.17, Learning Rate: 0.00039\n",
            "It: 1480, Loss: 2.941e-05, Loss_0: 1.865e-06, Loss_r: 2.754e-05, lambda_0 : 1.717e+01, Time: 0.17, Learning Rate: 0.00035\n",
            "It: 1490, Loss: 2.795e-05, Loss_0: 6.516e-07, Loss_r: 2.730e-05, lambda_0 : 6.006e+01, Time: 0.17, Learning Rate: 0.00035\n",
            "It: 1500, Loss: 2.696e-05, Loss_0: 2.052e-09, Loss_r: 2.696e-05, lambda_0 : 1.096e+03, Time: 0.18, Learning Rate: 0.00035\n",
            "It: 1510, Loss: 2.658e-05, Loss_0: 2.334e-08, Loss_r: 2.656e-05, lambda_0 : 2.353e+02, Time: 0.18, Learning Rate: 0.00035\n",
            "It: 1520, Loss: 2.615e-05, Loss_0: 1.295e-08, Loss_r: 2.614e-05, lambda_0 : 1.917e+02, Time: 0.16, Learning Rate: 0.00035\n",
            "It: 1530, Loss: 2.571e-05, Loss_0: 7.047e-10, Loss_r: 2.571e-05, lambda_0 : 7.452e+02, Time: 0.17, Learning Rate: 0.00035\n",
            "It: 1540, Loss: 2.528e-05, Loss_0: 2.915e-09, Loss_r: 2.528e-05, lambda_0 : 4.101e+02, Time: 0.17, Learning Rate: 0.00031\n",
            "It: 1550, Loss: 2.488e-05, Loss_0: 2.031e-09, Loss_r: 2.488e-05, lambda_0 : 4.236e+02, Time: 0.17, Learning Rate: 0.00031\n",
            "It: 1560, Loss: 2.449e-05, Loss_0: 9.736e-10, Loss_r: 2.449e-05, lambda_0 : 6.175e+02, Time: 0.17, Learning Rate: 0.00031\n",
            "It: 1570, Loss: 2.409e-05, Loss_0: 3.976e-10, Loss_r: 2.409e-05, lambda_0 : 1.172e+03, Time: 0.18, Learning Rate: 0.00031\n",
            "It: 1580, Loss: 2.369e-05, Loss_0: 5.010e-10, Loss_r: 2.369e-05, lambda_0 : 8.780e+02, Time: 0.17, Learning Rate: 0.00031\n",
            "It: 1590, Loss: 2.329e-05, Loss_0: 3.860e-10, Loss_r: 2.328e-05, lambda_0 : 1.153e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1600, Loss: 2.288e-05, Loss_0: 3.607e-10, Loss_r: 2.288e-05, lambda_0 : 1.304e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1610, Loss: 2.248e-05, Loss_0: 3.569e-10, Loss_r: 2.248e-05, lambda_0 : 1.288e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1620, Loss: 2.207e-05, Loss_0: 3.545e-10, Loss_r: 2.207e-05, lambda_0 : 1.271e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1630, Loss: 2.167e-05, Loss_0: 3.445e-10, Loss_r: 2.167e-05, lambda_0 : 1.257e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1640, Loss: 2.126e-05, Loss_0: 3.405e-10, Loss_r: 2.126e-05, lambda_0 : 1.228e+03, Time: 0.17, Learning Rate: 0.00031\n",
            "It: 1650, Loss: 2.085e-05, Loss_0: 3.395e-10, Loss_r: 2.085e-05, lambda_0 : 1.243e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1660, Loss: 2.045e-05, Loss_0: 3.365e-10, Loss_r: 2.045e-05, lambda_0 : 1.223e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1670, Loss: 2.004e-05, Loss_0: 3.339e-10, Loss_r: 2.004e-05, lambda_0 : 1.203e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1680, Loss: 1.964e-05, Loss_0: 3.320e-10, Loss_r: 1.964e-05, lambda_0 : 1.190e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1690, Loss: 1.924e-05, Loss_0: 3.305e-10, Loss_r: 1.924e-05, lambda_0 : 1.180e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1700, Loss: 1.884e-05, Loss_0: 3.284e-10, Loss_r: 1.884e-05, lambda_0 : 1.159e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1710, Loss: 1.844e-05, Loss_0: 3.252e-10, Loss_r: 1.844e-05, lambda_0 : 1.107e+03, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1720, Loss: 1.806e-05, Loss_0: 1.985e-08, Loss_r: 1.804e-05, lambda_0 : 1.436e+02, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1730, Loss: 1.181e-04, Loss_0: 9.980e-05, Loss_r: 1.826e-05, lambda_0 : 6.141e+00, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1740, Loss: 9.751e-05, Loss_0: 7.655e-05, Loss_r: 2.096e-05, lambda_0 : 3.950e+00, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1750, Loss: 3.110e-05, Loss_0: 8.707e-06, Loss_r: 2.239e-05, lambda_0 : 6.572e+00, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1760, Loss: 2.765e-05, Loss_0: 4.708e-06, Loss_r: 2.294e-05, lambda_0 : 1.671e+01, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1770, Loss: 2.435e-05, Loss_0: 1.407e-06, Loss_r: 2.294e-05, lambda_0 : 1.796e+01, Time: 0.16, Learning Rate: 0.00031\n",
            "It: 1780, Loss: 2.279e-05, Loss_0: 6.802e-08, Loss_r: 2.272e-05, lambda_0 : 8.263e+01, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1790, Loss: 2.245e-05, Loss_0: 3.743e-08, Loss_r: 2.242e-05, lambda_0 : 1.037e+02, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1800, Loss: 2.208e-05, Loss_0: 4.049e-09, Loss_r: 2.208e-05, lambda_0 : 3.015e+02, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1810, Loss: 2.172e-05, Loss_0: 2.817e-09, Loss_r: 2.172e-05, lambda_0 : 4.065e+02, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1820, Loss: 2.137e-05, Loss_0: 1.002e-08, Loss_r: 2.136e-05, lambda_0 : 2.390e+02, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1830, Loss: 2.100e-05, Loss_0: 4.613e-09, Loss_r: 2.099e-05, lambda_0 : 3.252e+02, Time: 0.16, Learning Rate: 0.00028\n",
            "It: 1840, Loss: 2.063e-05, Loss_0: 3.936e-10, Loss_r: 2.063e-05, lambda_0 : 1.039e+03, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1850, Loss: 2.030e-05, Loss_0: 7.503e-10, Loss_r: 2.030e-05, lambda_0 : 6.465e+02, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1860, Loss: 1.997e-05, Loss_0: 5.671e-10, Loss_r: 1.997e-05, lambda_0 : 7.530e+02, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1870, Loss: 1.965e-05, Loss_0: 3.269e-10, Loss_r: 1.965e-05, lambda_0 : 1.268e+03, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1880, Loss: 1.933e-05, Loss_0: 3.343e-10, Loss_r: 1.933e-05, lambda_0 : 1.052e+03, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1890, Loss: 1.900e-05, Loss_0: 3.268e-10, Loss_r: 1.900e-05, lambda_0 : 1.298e+03, Time: 0.16, Learning Rate: 0.00025\n",
            "It: 1900, Loss: 1.868e-05, Loss_0: 3.242e-10, Loss_r: 1.868e-05, lambda_0 : 1.288e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1910, Loss: 1.840e-05, Loss_0: 3.153e-10, Loss_r: 1.840e-05, lambda_0 : 1.211e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1920, Loss: 1.811e-05, Loss_0: 3.140e-10, Loss_r: 1.811e-05, lambda_0 : 1.226e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1930, Loss: 1.783e-05, Loss_0: 3.137e-10, Loss_r: 1.783e-05, lambda_0 : 1.249e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1940, Loss: 1.754e-05, Loss_0: 3.097e-10, Loss_r: 1.754e-05, lambda_0 : 1.212e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1950, Loss: 1.726e-05, Loss_0: 3.080e-10, Loss_r: 1.726e-05, lambda_0 : 1.212e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1960, Loss: 1.698e-05, Loss_0: 3.055e-10, Loss_r: 1.698e-05, lambda_0 : 1.199e+03, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 1970, Loss: 1.671e-05, Loss_0: 3.032e-10, Loss_r: 1.671e-05, lambda_0 : 1.190e+03, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 1980, Loss: 1.643e-05, Loss_0: 3.008e-10, Loss_r: 1.643e-05, lambda_0 : 1.178e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 1990, Loss: 1.616e-05, Loss_0: 2.986e-10, Loss_r: 1.616e-05, lambda_0 : 1.170e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2000, Loss: 1.589e-05, Loss_0: 2.961e-10, Loss_r: 1.589e-05, lambda_0 : 1.158e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2010, Loss: 1.562e-05, Loss_0: 2.936e-10, Loss_r: 1.562e-05, lambda_0 : 1.148e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2020, Loss: 1.535e-05, Loss_0: 2.912e-10, Loss_r: 1.535e-05, lambda_0 : 1.138e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2030, Loss: 1.509e-05, Loss_0: 2.886e-10, Loss_r: 1.509e-05, lambda_0 : 1.129e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2040, Loss: 1.482e-05, Loss_0: 2.861e-10, Loss_r: 1.482e-05, lambda_0 : 1.119e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2050, Loss: 1.456e-05, Loss_0: 2.835e-10, Loss_r: 1.456e-05, lambda_0 : 1.109e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2060, Loss: 1.431e-05, Loss_0: 2.808e-10, Loss_r: 1.431e-05, lambda_0 : 1.099e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2070, Loss: 1.405e-05, Loss_0: 2.782e-10, Loss_r: 1.405e-05, lambda_0 : 1.090e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2080, Loss: 1.380e-05, Loss_0: 2.754e-10, Loss_r: 1.380e-05, lambda_0 : 1.080e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2090, Loss: 1.355e-05, Loss_0: 2.726e-10, Loss_r: 1.355e-05, lambda_0 : 1.071e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2100, Loss: 1.330e-05, Loss_0: 2.698e-10, Loss_r: 1.330e-05, lambda_0 : 1.062e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2110, Loss: 1.306e-05, Loss_0: 2.670e-10, Loss_r: 1.306e-05, lambda_0 : 1.054e+03, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2120, Loss: 1.281e-05, Loss_0: 2.642e-10, Loss_r: 1.281e-05, lambda_0 : 1.044e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2130, Loss: 1.258e-05, Loss_0: 2.613e-10, Loss_r: 1.258e-05, lambda_0 : 1.036e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2140, Loss: 1.234e-05, Loss_0: 2.585e-10, Loss_r: 1.234e-05, lambda_0 : 1.029e+03, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2150, Loss: 1.211e-05, Loss_0: 2.563e-10, Loss_r: 1.211e-05, lambda_0 : 1.032e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2160, Loss: 1.188e-05, Loss_0: 2.621e-10, Loss_r: 1.188e-05, lambda_0 : 1.538e+03, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2170, Loss: 1.197e-05, Loss_0: 5.262e-10, Loss_r: 1.196e-05, lambda_0 : 3.198e+04, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2180, Loss: 1.146e-05, Loss_0: 1.728e-10, Loss_r: 1.146e-05, lambda_0 : 1.773e+04, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2190, Loss: 1.124e-05, Loss_0: 3.253e-10, Loss_r: 1.124e-05, lambda_0 : 1.130e+04, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2200, Loss: 1.104e-05, Loss_0: 3.644e-10, Loss_r: 1.104e-05, lambda_0 : 1.163e+04, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2210, Loss: 1.081e-05, Loss_0: 6.554e-09, Loss_r: 1.080e-05, lambda_0 : 7.654e+02, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2220, Loss: 4.196e-05, Loss_0: 3.138e-05, Loss_r: 1.059e-05, lambda_0 : 1.903e+00, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2230, Loss: 1.166e-05, Loss_0: 1.034e-06, Loss_r: 1.063e-05, lambda_0 : 3.185e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2240, Loss: 1.836e-05, Loss_0: 7.649e-06, Loss_r: 1.071e-05, lambda_0 : 1.844e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2250, Loss: 1.163e-05, Loss_0: 9.332e-07, Loss_r: 1.070e-05, lambda_0 : 4.445e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2260, Loss: 1.059e-05, Loss_0: 2.335e-10, Loss_r: 1.059e-05, lambda_0 : 1.921e+03, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2270, Loss: 1.043e-05, Loss_0: 2.092e-09, Loss_r: 1.043e-05, lambda_0 : 4.955e+02, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2280, Loss: 1.032e-05, Loss_0: 7.933e-08, Loss_r: 1.024e-05, lambda_0 : 7.039e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2290, Loss: 1.013e-05, Loss_0: 7.970e-08, Loss_r: 1.005e-05, lambda_0 : 4.951e+01, Time: 0.18, Learning Rate: 0.00023\n",
            "It: 2300, Loss: 9.913e-06, Loss_0: 5.928e-08, Loss_r: 9.854e-06, lambda_0 : 5.362e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2310, Loss: 9.662e-06, Loss_0: 2.824e-09, Loss_r: 9.659e-06, lambda_0 : 2.111e+02, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2320, Loss: 9.466e-06, Loss_0: 3.032e-10, Loss_r: 9.466e-06, lambda_0 : 6.326e+02, Time: 0.18, Learning Rate: 0.00023\n",
            "It: 2330, Loss: 9.385e-06, Loss_0: 1.103e-07, Loss_r: 9.275e-06, lambda_0 : 3.605e+01, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2340, Loss: 1.807e-04, Loss_0: 1.714e-04, Loss_r: 9.259e-06, lambda_0 : 1.789e+00, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2350, Loss: 6.358e-05, Loss_0: 5.332e-05, Loss_r: 1.025e-05, lambda_0 : 3.156e+00, Time: 0.17, Learning Rate: 0.00023\n",
            "It: 2360, Loss: 2.146e-05, Loss_0: 1.060e-05, Loss_r: 1.086e-05, lambda_0 : 5.851e+00, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2370, Loss: 1.552e-05, Loss_0: 4.491e-06, Loss_r: 1.103e-05, lambda_0 : 1.366e+01, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2380, Loss: 1.218e-05, Loss_0: 1.192e-06, Loss_r: 1.099e-05, lambda_0 : 1.698e+01, Time: 0.16, Learning Rate: 0.00023\n",
            "It: 2390, Loss: 1.104e-05, Loss_0: 2.099e-07, Loss_r: 1.083e-05, lambda_0 : 3.541e+01, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2400, Loss: 1.090e-05, Loss_0: 2.494e-07, Loss_r: 1.065e-05, lambda_0 : 2.922e+01, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2410, Loss: 1.051e-05, Loss_0: 4.155e-08, Loss_r: 1.047e-05, lambda_0 : 7.758e+01, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2420, Loss: 1.028e-05, Loss_0: 9.739e-09, Loss_r: 1.027e-05, lambda_0 : 1.629e+02, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2430, Loss: 1.009e-05, Loss_0: 8.554e-09, Loss_r: 1.008e-05, lambda_0 : 1.408e+02, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2440, Loss: 9.893e-06, Loss_0: 3.002e-09, Loss_r: 9.890e-06, lambda_0 : 2.218e+02, Time: 0.16, Learning Rate: 0.00021\n",
            "It: 2450, Loss: 9.704e-06, Loss_0: 2.007e-10, Loss_r: 9.703e-06, lambda_0 : 1.015e+03, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2460, Loss: 9.538e-06, Loss_0: 5.348e-10, Loss_r: 9.538e-06, lambda_0 : 4.935e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2470, Loss: 9.375e-06, Loss_0: 2.461e-10, Loss_r: 9.374e-06, lambda_0 : 7.948e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2480, Loss: 9.214e-06, Loss_0: 2.671e-10, Loss_r: 9.214e-06, lambda_0 : 7.217e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2490, Loss: 9.055e-06, Loss_0: 2.047e-10, Loss_r: 9.055e-06, lambda_0 : 1.037e+03, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2500, Loss: 8.899e-06, Loss_0: 2.018e-10, Loss_r: 8.899e-06, lambda_0 : 9.095e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2510, Loss: 8.745e-06, Loss_0: 2.006e-10, Loss_r: 8.745e-06, lambda_0 : 9.059e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2520, Loss: 8.593e-06, Loss_0: 1.992e-10, Loss_r: 8.593e-06, lambda_0 : 9.190e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2530, Loss: 8.444e-06, Loss_0: 1.981e-10, Loss_r: 8.443e-06, lambda_0 : 9.256e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2540, Loss: 8.296e-06, Loss_0: 1.970e-10, Loss_r: 8.296e-06, lambda_0 : 9.270e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2550, Loss: 8.151e-06, Loss_0: 1.960e-10, Loss_r: 8.151e-06, lambda_0 : 9.277e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2560, Loss: 8.009e-06, Loss_0: 1.949e-10, Loss_r: 8.008e-06, lambda_0 : 9.262e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2570, Loss: 7.868e-06, Loss_0: 1.936e-10, Loss_r: 7.868e-06, lambda_0 : 9.172e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2580, Loss: 7.730e-06, Loss_0: 1.920e-10, Loss_r: 7.729e-06, lambda_0 : 9.031e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2590, Loss: 7.594e-06, Loss_0: 1.905e-10, Loss_r: 7.593e-06, lambda_0 : 8.915e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2600, Loss: 7.460e-06, Loss_0: 1.892e-10, Loss_r: 7.459e-06, lambda_0 : 8.843e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2610, Loss: 7.328e-06, Loss_0: 1.879e-10, Loss_r: 7.328e-06, lambda_0 : 8.758e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2620, Loss: 7.199e-06, Loss_0: 1.865e-10, Loss_r: 7.198e-06, lambda_0 : 8.661e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2630, Loss: 7.071e-06, Loss_0: 1.851e-10, Loss_r: 7.071e-06, lambda_0 : 8.554e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2640, Loss: 6.946e-06, Loss_0: 1.838e-10, Loss_r: 6.946e-06, lambda_0 : 8.459e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2650, Loss: 6.824e-06, Loss_0: 1.826e-10, Loss_r: 6.823e-06, lambda_0 : 8.359e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2660, Loss: 6.703e-06, Loss_0: 1.813e-10, Loss_r: 6.703e-06, lambda_0 : 8.252e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2670, Loss: 6.585e-06, Loss_0: 1.800e-10, Loss_r: 6.584e-06, lambda_0 : 8.144e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2680, Loss: 6.468e-06, Loss_0: 1.782e-10, Loss_r: 6.468e-06, lambda_0 : 7.844e+02, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2690, Loss: 6.356e-06, Loss_0: 1.679e-10, Loss_r: 6.355e-06, lambda_0 : 4.050e+03, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2700, Loss: 6.615e-06, Loss_0: 4.374e-11, Loss_r: 6.615e-06, lambda_0 : 1.153e+05, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2710, Loss: 6.141e-06, Loss_0: 1.625e-10, Loss_r: 6.141e-06, lambda_0 : 7.795e+03, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2720, Loss: 6.049e-06, Loss_0: 1.353e-10, Loss_r: 6.049e-06, lambda_0 : 1.444e+04, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2730, Loss: 5.945e-06, Loss_0: 1.349e-10, Loss_r: 5.945e-06, lambda_0 : 1.555e+04, Time: 0.16, Learning Rate: 0.00019\n",
            "It: 2740, Loss: 5.835e-06, Loss_0: 1.549e-10, Loss_r: 5.835e-06, lambda_0 : 9.002e+03, Time: 0.19, Learning Rate: 0.00019\n",
            "It: 2750, Loss: 5.734e-06, Loss_0: 1.585e-09, Loss_r: 5.732e-06, lambda_0 : 1.223e+03, Time: 0.23, Learning Rate: 0.00019\n",
            "It: 2760, Loss: 9.046e-06, Loss_0: 3.414e-06, Loss_r: 5.632e-06, lambda_0 : 1.556e+01, Time: 0.24, Learning Rate: 0.00019\n",
            "It: 2770, Loss: 8.863e-05, Loss_0: 8.276e-05, Loss_r: 5.865e-06, lambda_0 : 2.042e+00, Time: 0.24, Learning Rate: 0.00019\n",
            "It: 2780, Loss: 2.637e-05, Loss_0: 2.010e-05, Loss_r: 6.266e-06, lambda_0 : 4.152e+00, Time: 0.18, Learning Rate: 0.00019\n",
            "It: 2790, Loss: 1.044e-05, Loss_0: 3.991e-06, Loss_r: 6.452e-06, lambda_0 : 7.155e+00, Time: 0.15, Learning Rate: 0.00019\n",
            "It: 2800, Loss: 7.711e-06, Loss_0: 1.240e-06, Loss_r: 6.470e-06, lambda_0 : 9.685e+00, Time: 0.15, Learning Rate: 0.00019\n",
            "It: 2810, Loss: 7.023e-06, Loss_0: 6.100e-07, Loss_r: 6.413e-06, lambda_0 : 1.319e+01, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2820, Loss: 6.512e-06, Loss_0: 1.803e-07, Loss_r: 6.332e-06, lambda_0 : 2.871e+01, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2830, Loss: 6.369e-06, Loss_0: 1.322e-07, Loss_r: 6.237e-06, lambda_0 : 2.965e+01, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2840, Loss: 6.184e-06, Loss_0: 4.577e-08, Loss_r: 6.139e-06, lambda_0 : 4.916e+01, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2850, Loss: 6.055e-06, Loss_0: 1.489e-08, Loss_r: 6.041e-06, lambda_0 : 8.739e+01, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2860, Loss: 5.949e-06, Loss_0: 5.746e-09, Loss_r: 5.944e-06, lambda_0 : 1.342e+02, Time: 0.16, Learning Rate: 0.00017\n",
            "It: 2870, Loss: 5.851e-06, Loss_0: 2.312e-09, Loss_r: 5.849e-06, lambda_0 : 1.923e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2880, Loss: 5.765e-06, Loss_0: 5.203e-10, Loss_r: 5.764e-06, lambda_0 : 3.463e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2890, Loss: 5.682e-06, Loss_0: 1.671e-10, Loss_r: 5.682e-06, lambda_0 : 6.902e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2900, Loss: 5.601e-06, Loss_0: 1.763e-10, Loss_r: 5.600e-06, lambda_0 : 7.038e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2910, Loss: 5.521e-06, Loss_0: 1.758e-10, Loss_r: 5.521e-06, lambda_0 : 6.922e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2920, Loss: 5.442e-06, Loss_0: 1.684e-10, Loss_r: 5.442e-06, lambda_0 : 7.474e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2930, Loss: 5.365e-06, Loss_0: 1.641e-10, Loss_r: 5.365e-06, lambda_0 : 7.579e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2940, Loss: 5.289e-06, Loss_0: 1.624e-10, Loss_r: 5.289e-06, lambda_0 : 7.149e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2950, Loss: 5.215e-06, Loss_0: 1.618e-10, Loss_r: 5.214e-06, lambda_0 : 6.925e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 2960, Loss: 5.141e-06, Loss_0: 1.614e-10, Loss_r: 5.141e-06, lambda_0 : 6.897e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 2970, Loss: 5.069e-06, Loss_0: 1.612e-10, Loss_r: 5.069e-06, lambda_0 : 6.959e+02, Time: 0.18, Learning Rate: 0.00015\n",
            "It: 2980, Loss: 4.999e-06, Loss_0: 1.609e-10, Loss_r: 4.999e-06, lambda_0 : 6.934e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 2990, Loss: 4.929e-06, Loss_0: 1.600e-10, Loss_r: 4.929e-06, lambda_0 : 6.768e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3000, Loss: 4.861e-06, Loss_0: 1.593e-10, Loss_r: 4.861e-06, lambda_0 : 6.647e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3010, Loss: 4.795e-06, Loss_0: 1.589e-10, Loss_r: 4.794e-06, lambda_0 : 6.598e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3020, Loss: 4.729e-06, Loss_0: 1.583e-10, Loss_r: 4.729e-06, lambda_0 : 6.482e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3030, Loss: 4.665e-06, Loss_0: 1.577e-10, Loss_r: 4.665e-06, lambda_0 : 6.386e+02, Time: 0.18, Learning Rate: 0.00015\n",
            "It: 3040, Loss: 4.602e-06, Loss_0: 1.571e-10, Loss_r: 4.601e-06, lambda_0 : 6.296e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3050, Loss: 4.540e-06, Loss_0: 1.565e-10, Loss_r: 4.540e-06, lambda_0 : 6.204e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3060, Loss: 4.479e-06, Loss_0: 1.559e-10, Loss_r: 4.479e-06, lambda_0 : 6.103e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3070, Loss: 4.420e-06, Loss_0: 1.554e-10, Loss_r: 4.419e-06, lambda_0 : 6.008e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3080, Loss: 4.361e-06, Loss_0: 1.549e-10, Loss_r: 4.361e-06, lambda_0 : 5.920e+02, Time: 0.17, Learning Rate: 0.00015\n",
            "It: 3090, Loss: 4.304e-06, Loss_0: 1.543e-10, Loss_r: 4.304e-06, lambda_0 : 5.825e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3100, Loss: 4.248e-06, Loss_0: 1.538e-10, Loss_r: 4.248e-06, lambda_0 : 5.729e+02, Time: 0.15, Learning Rate: 0.00015\n",
            "It: 3110, Loss: 4.194e-06, Loss_0: 1.532e-10, Loss_r: 4.193e-06, lambda_0 : 5.639e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3120, Loss: 4.140e-06, Loss_0: 1.527e-10, Loss_r: 4.140e-06, lambda_0 : 5.544e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3130, Loss: 4.087e-06, Loss_0: 1.522e-10, Loss_r: 4.087e-06, lambda_0 : 5.447e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3140, Loss: 4.036e-06, Loss_0: 1.517e-10, Loss_r: 4.036e-06, lambda_0 : 5.355e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3150, Loss: 3.985e-06, Loss_0: 1.512e-10, Loss_r: 3.985e-06, lambda_0 : 5.261e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3160, Loss: 3.936e-06, Loss_0: 1.507e-10, Loss_r: 3.936e-06, lambda_0 : 5.171e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3170, Loss: 3.888e-06, Loss_0: 1.502e-10, Loss_r: 3.888e-06, lambda_0 : 5.080e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3180, Loss: 3.841e-06, Loss_0: 1.497e-10, Loss_r: 3.841e-06, lambda_0 : 4.985e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3190, Loss: 3.795e-06, Loss_0: 1.492e-10, Loss_r: 3.794e-06, lambda_0 : 4.892e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3200, Loss: 3.749e-06, Loss_0: 1.487e-10, Loss_r: 3.749e-06, lambda_0 : 4.804e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3210, Loss: 3.705e-06, Loss_0: 1.483e-10, Loss_r: 3.705e-06, lambda_0 : 4.733e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3220, Loss: 3.662e-06, Loss_0: 1.493e-10, Loss_r: 3.662e-06, lambda_0 : 4.839e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3230, Loss: 3.620e-06, Loss_0: 6.820e-10, Loss_r: 3.620e-06, lambda_0 : 1.545e+02, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3240, Loss: 5.144e-06, Loss_0: 1.567e-06, Loss_r: 3.577e-06, lambda_0 : 3.692e+00, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3250, Loss: 4.704e-06, Loss_0: 1.154e-06, Loss_r: 3.550e-06, lambda_0 : 4.109e+01, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3260, Loss: 1.121e-04, Loss_0: 1.085e-04, Loss_r: 3.567e-06, lambda_0 : 8.912e+00, Time: 0.15, Learning Rate: 0.00015\n",
            "It: 3270, Loss: 2.323e-05, Loss_0: 1.945e-05, Loss_r: 3.781e-06, lambda_0 : 3.135e+00, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3280, Loss: 4.118e-06, Loss_0: 2.189e-07, Loss_r: 3.899e-06, lambda_0 : 1.588e+01, Time: 0.16, Learning Rate: 0.00015\n",
            "It: 3290, Loss: 6.957e-06, Loss_0: 3.028e-06, Loss_r: 3.929e-06, lambda_0 : 7.639e+00, Time: 0.16, Learning Rate: 0.00014\n",
            "It: 3300, Loss: 4.726e-06, Loss_0: 8.082e-07, Loss_r: 3.918e-06, lambda_0 : 7.859e+00, Time: 0.16, Learning Rate: 0.00014\n",
            "It: 3310, Loss: 3.978e-06, Loss_0: 9.177e-08, Loss_r: 3.887e-06, lambda_0 : 2.037e+01, Time: 0.16, Learning Rate: 0.00014\n",
            "It: 3320, Loss: 3.927e-06, Loss_0: 7.990e-08, Loss_r: 3.847e-06, lambda_0 : 1.918e+01, Time: 0.16, Learning Rate: 0.00014\n",
            "It: 3330, Loss: 3.852e-06, Loss_0: 4.718e-08, Loss_r: 3.804e-06, lambda_0 : 2.489e+01, Time: 0.17, Learning Rate: 0.00014\n",
            "It: 3340, Loss: 3.775e-06, Loss_0: 1.321e-08, Loss_r: 3.762e-06, lambda_0 : 4.507e+01, Time: 0.16, Learning Rate: 0.00014\n",
            "It: 3350, Loss: 3.720e-06, Loss_0: 2.867e-10, Loss_r: 3.719e-06, lambda_0 : 2.712e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3360, Loss: 3.682e-06, Loss_0: 6.536e-10, Loss_r: 3.682e-06, lambda_0 : 1.774e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3370, Loss: 3.645e-06, Loss_0: 4.723e-10, Loss_r: 3.645e-06, lambda_0 : 2.021e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3380, Loss: 3.609e-06, Loss_0: 2.805e-10, Loss_r: 3.609e-06, lambda_0 : 2.600e+02, Time: 0.15, Learning Rate: 0.00012\n",
            "It: 3390, Loss: 3.573e-06, Loss_0: 1.868e-10, Loss_r: 3.573e-06, lambda_0 : 3.397e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3400, Loss: 3.538e-06, Loss_0: 1.508e-10, Loss_r: 3.538e-06, lambda_0 : 4.283e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3410, Loss: 3.504e-06, Loss_0: 1.402e-10, Loss_r: 3.504e-06, lambda_0 : 4.824e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3420, Loss: 3.471e-06, Loss_0: 1.387e-10, Loss_r: 3.470e-06, lambda_0 : 4.394e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3430, Loss: 3.438e-06, Loss_0: 1.386e-10, Loss_r: 3.437e-06, lambda_0 : 4.245e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3440, Loss: 3.405e-06, Loss_0: 1.382e-10, Loss_r: 3.405e-06, lambda_0 : 4.278e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3450, Loss: 3.374e-06, Loss_0: 1.384e-10, Loss_r: 3.374e-06, lambda_0 : 4.353e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3460, Loss: 3.343e-06, Loss_0: 1.383e-10, Loss_r: 3.343e-06, lambda_0 : 4.313e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3470, Loss: 3.312e-06, Loss_0: 1.378e-10, Loss_r: 3.312e-06, lambda_0 : 4.201e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3480, Loss: 3.282e-06, Loss_0: 1.373e-10, Loss_r: 3.282e-06, lambda_0 : 4.107e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3490, Loss: 3.253e-06, Loss_0: 1.372e-10, Loss_r: 3.253e-06, lambda_0 : 4.064e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3500, Loss: 3.225e-06, Loss_0: 1.369e-10, Loss_r: 3.225e-06, lambda_0 : 4.008e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3510, Loss: 3.197e-06, Loss_0: 1.365e-10, Loss_r: 3.197e-06, lambda_0 : 3.930e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3520, Loss: 3.169e-06, Loss_0: 1.362e-10, Loss_r: 3.169e-06, lambda_0 : 3.877e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3530, Loss: 3.142e-06, Loss_0: 1.359e-10, Loss_r: 3.142e-06, lambda_0 : 3.811e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3540, Loss: 3.116e-06, Loss_0: 1.356e-10, Loss_r: 3.116e-06, lambda_0 : 3.756e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3550, Loss: 3.090e-06, Loss_0: 1.353e-10, Loss_r: 3.090e-06, lambda_0 : 3.702e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3560, Loss: 3.065e-06, Loss_0: 1.350e-10, Loss_r: 3.065e-06, lambda_0 : 3.642e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3570, Loss: 3.040e-06, Loss_0: 1.347e-10, Loss_r: 3.040e-06, lambda_0 : 3.582e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3580, Loss: 3.016e-06, Loss_0: 1.344e-10, Loss_r: 3.016e-06, lambda_0 : 3.523e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3590, Loss: 2.992e-06, Loss_0: 1.340e-10, Loss_r: 2.992e-06, lambda_0 : 3.463e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3600, Loss: 2.969e-06, Loss_0: 1.337e-10, Loss_r: 2.969e-06, lambda_0 : 3.417e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3610, Loss: 2.946e-06, Loss_0: 1.334e-10, Loss_r: 2.946e-06, lambda_0 : 3.356e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3620, Loss: 2.924e-06, Loss_0: 1.331e-10, Loss_r: 2.924e-06, lambda_0 : 3.292e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3630, Loss: 2.902e-06, Loss_0: 1.327e-10, Loss_r: 2.902e-06, lambda_0 : 3.249e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3640, Loss: 2.881e-06, Loss_0: 1.324e-10, Loss_r: 2.881e-06, lambda_0 : 3.186e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3650, Loss: 2.860e-06, Loss_0: 1.320e-10, Loss_r: 2.860e-06, lambda_0 : 3.130e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3660, Loss: 2.840e-06, Loss_0: 1.317e-10, Loss_r: 2.840e-06, lambda_0 : 3.088e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3670, Loss: 2.820e-06, Loss_0: 1.314e-10, Loss_r: 2.820e-06, lambda_0 : 3.043e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3680, Loss: 2.800e-06, Loss_0: 1.311e-10, Loss_r: 2.800e-06, lambda_0 : 3.003e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3690, Loss: 2.781e-06, Loss_0: 1.310e-10, Loss_r: 2.781e-06, lambda_0 : 3.071e+02, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3700, Loss: 2.763e-06, Loss_0: 1.335e-10, Loss_r: 2.762e-06, lambda_0 : 1.807e+03, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3710, Loss: 2.794e-06, Loss_0: 1.809e-10, Loss_r: 2.794e-06, lambda_0 : 2.293e+04, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3720, Loss: 2.796e-06, Loss_0: 7.722e-11, Loss_r: 2.796e-06, lambda_0 : 4.034e+04, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3730, Loss: 2.728e-06, Loss_0: 1.044e-10, Loss_r: 2.727e-06, lambda_0 : 1.771e+04, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3740, Loss: 2.698e-06, Loss_0: 1.121e-10, Loss_r: 2.698e-06, lambda_0 : 9.119e+03, Time: 0.18, Learning Rate: 0.00012\n",
            "It: 3750, Loss: 2.678e-06, Loss_0: 1.207e-10, Loss_r: 2.677e-06, lambda_0 : 5.104e+03, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3760, Loss: 2.660e-06, Loss_0: 1.243e-10, Loss_r: 2.660e-06, lambda_0 : 1.988e+03, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3770, Loss: 2.644e-06, Loss_0: 1.272e-10, Loss_r: 2.644e-06, lambda_0 : 2.296e+02, Time: 0.18, Learning Rate: 0.00012\n",
            "It: 3780, Loss: 2.629e-06, Loss_0: 1.290e-10, Loss_r: 2.629e-06, lambda_0 : 6.930e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3790, Loss: 2.614e-06, Loss_0: 1.939e-10, Loss_r: 2.614e-06, lambda_0 : 5.024e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3800, Loss: 2.617e-06, Loss_0: 1.771e-08, Loss_r: 2.599e-06, lambda_0 : 2.510e+01, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3810, Loss: 1.448e-05, Loss_0: 1.189e-05, Loss_r: 2.589e-06, lambda_0 : 2.054e+00, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3820, Loss: 2.587e-06, Loss_0: 1.125e-09, Loss_r: 2.586e-06, lambda_0 : 4.795e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3830, Loss: 2.599e-06, Loss_0: 1.036e-09, Loss_r: 2.598e-06, lambda_0 : 5.257e+02, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3840, Loss: 4.974e-06, Loss_0: 2.358e-06, Loss_r: 2.616e-06, lambda_0 : 7.681e+00, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3850, Loss: 2.918e-06, Loss_0: 3.010e-07, Loss_r: 2.617e-06, lambda_0 : 4.547e+01, Time: 0.17, Learning Rate: 0.00012\n",
            "It: 3860, Loss: 2.893e-06, Loss_0: 2.834e-07, Loss_r: 2.610e-06, lambda_0 : 8.890e+00, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3870, Loss: 2.728e-06, Loss_0: 1.307e-07, Loss_r: 2.598e-06, lambda_0 : 1.050e+01, Time: 0.16, Learning Rate: 0.00012\n",
            "It: 3880, Loss: 2.657e-06, Loss_0: 7.287e-08, Loss_r: 2.584e-06, lambda_0 : 1.539e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3890, Loss: 2.573e-06, Loss_0: 3.089e-09, Loss_r: 2.570e-06, lambda_0 : 4.064e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3900, Loss: 2.565e-06, Loss_0: 7.704e-09, Loss_r: 2.557e-06, lambda_0 : 2.490e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3910, Loss: 2.545e-06, Loss_0: 5.860e-10, Loss_r: 2.544e-06, lambda_0 : 8.621e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3920, Loss: 2.533e-06, Loss_0: 1.305e-09, Loss_r: 2.532e-06, lambda_0 : 6.055e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3930, Loss: 2.519e-06, Loss_0: 2.013e-10, Loss_r: 2.519e-06, lambda_0 : 1.325e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3940, Loss: 2.507e-06, Loss_0: 1.393e-10, Loss_r: 2.507e-06, lambda_0 : 1.844e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3950, Loss: 2.495e-06, Loss_0: 1.582e-10, Loss_r: 2.495e-06, lambda_0 : 1.574e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3960, Loss: 2.483e-06, Loss_0: 1.409e-10, Loss_r: 2.483e-06, lambda_0 : 1.732e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3970, Loss: 2.471e-06, Loss_0: 1.280e-10, Loss_r: 2.471e-06, lambda_0 : 1.947e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3980, Loss: 2.460e-06, Loss_0: 1.217e-10, Loss_r: 2.460e-06, lambda_0 : 2.140e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 3990, Loss: 2.449e-06, Loss_0: 1.190e-10, Loss_r: 2.449e-06, lambda_0 : 2.089e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4000, Loss: 2.438e-06, Loss_0: 1.181e-10, Loss_r: 2.438e-06, lambda_0 : 1.982e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4010, Loss: 2.427e-06, Loss_0: 1.178e-10, Loss_r: 2.427e-06, lambda_0 : 1.933e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4020, Loss: 2.417e-06, Loss_0: 1.178e-10, Loss_r: 2.417e-06, lambda_0 : 1.950e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4030, Loss: 2.406e-06, Loss_0: 1.176e-10, Loss_r: 2.406e-06, lambda_0 : 1.920e+02, Time: 0.17, Learning Rate: 0.00011\n",
            "It: 4040, Loss: 2.396e-06, Loss_0: 1.170e-10, Loss_r: 2.396e-06, lambda_0 : 1.851e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4050, Loss: 2.386e-06, Loss_0: 1.169e-10, Loss_r: 2.386e-06, lambda_0 : 1.838e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4060, Loss: 2.377e-06, Loss_0: 1.164e-10, Loss_r: 2.377e-06, lambda_0 : 1.793e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4070, Loss: 2.367e-06, Loss_0: 1.162e-10, Loss_r: 2.367e-06, lambda_0 : 1.754e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4080, Loss: 2.358e-06, Loss_0: 1.159e-10, Loss_r: 2.358e-06, lambda_0 : 1.718e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4090, Loss: 2.349e-06, Loss_0: 1.156e-10, Loss_r: 2.349e-06, lambda_0 : 1.698e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4100, Loss: 2.340e-06, Loss_0: 1.152e-10, Loss_r: 2.340e-06, lambda_0 : 1.658e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4110, Loss: 2.331e-06, Loss_0: 1.151e-10, Loss_r: 2.331e-06, lambda_0 : 1.627e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4120, Loss: 2.322e-06, Loss_0: 1.328e-10, Loss_r: 2.322e-06, lambda_0 : 1.257e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4130, Loss: 2.325e-06, Loss_0: 1.119e-08, Loss_r: 2.313e-06, lambda_0 : 1.267e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4140, Loss: 1.931e-05, Loss_0: 1.701e-05, Loss_r: 2.302e-06, lambda_0 : 4.256e+00, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4150, Loss: 2.732e-06, Loss_0: 3.605e-07, Loss_r: 2.371e-06, lambda_0 : 4.852e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4160, Loss: 6.872e-06, Loss_0: 4.522e-06, Loss_r: 2.350e-06, lambda_0 : 8.585e+01, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4170, Loss: 2.473e-06, Loss_0: 1.412e-07, Loss_r: 2.332e-06, lambda_0 : 1.620e+02, Time: 0.16, Learning Rate: 0.00011\n",
            "It: 4180, Loss: 2.472e-06, Loss_0: 1.444e-07, Loss_r: 2.328e-06, lambda_0 : 1.251e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4190, Loss: 2.431e-06, Loss_0: 1.094e-07, Loss_r: 2.321e-06, lambda_0 : 7.184e+01, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4200, Loss: 2.314e-06, Loss_0: 2.209e-10, Loss_r: 2.313e-06, lambda_0 : 6.607e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4210, Loss: 2.320e-06, Loss_0: 1.471e-08, Loss_r: 2.306e-06, lambda_0 : 8.471e+01, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4220, Loss: 2.298e-06, Loss_0: 1.866e-10, Loss_r: 2.298e-06, lambda_0 : 1.163e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4230, Loss: 2.292e-06, Loss_0: 1.962e-09, Loss_r: 2.290e-06, lambda_0 : 9.303e+01, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4240, Loss: 2.283e-06, Loss_0: 3.049e-10, Loss_r: 2.282e-06, lambda_0 : 1.795e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4250, Loss: 2.275e-06, Loss_0: 1.388e-10, Loss_r: 2.275e-06, lambda_0 : 1.976e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4260, Loss: 2.267e-06, Loss_0: 1.814e-10, Loss_r: 2.267e-06, lambda_0 : 1.115e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4270, Loss: 2.260e-06, Loss_0: 1.465e-10, Loss_r: 2.260e-06, lambda_0 : 1.063e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4280, Loss: 2.253e-06, Loss_0: 1.233e-10, Loss_r: 2.253e-06, lambda_0 : 1.247e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4290, Loss: 2.246e-06, Loss_0: 1.143e-10, Loss_r: 2.246e-06, lambda_0 : 1.397e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4300, Loss: 2.239e-06, Loss_0: 1.105e-10, Loss_r: 2.239e-06, lambda_0 : 1.499e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4310, Loss: 2.232e-06, Loss_0: 1.085e-10, Loss_r: 2.232e-06, lambda_0 : 1.521e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4320, Loss: 2.225e-06, Loss_0: 1.073e-10, Loss_r: 2.225e-06, lambda_0 : 1.454e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4330, Loss: 2.219e-06, Loss_0: 1.068e-10, Loss_r: 2.219e-06, lambda_0 : 1.413e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4340, Loss: 2.212e-06, Loss_0: 1.066e-10, Loss_r: 2.212e-06, lambda_0 : 1.409e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4350, Loss: 2.206e-06, Loss_0: 1.065e-10, Loss_r: 2.206e-06, lambda_0 : 1.414e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4360, Loss: 2.200e-06, Loss_0: 1.062e-10, Loss_r: 2.199e-06, lambda_0 : 1.397e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4370, Loss: 2.193e-06, Loss_0: 1.059e-10, Loss_r: 2.193e-06, lambda_0 : 1.380e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4380, Loss: 2.187e-06, Loss_0: 1.057e-10, Loss_r: 2.187e-06, lambda_0 : 1.373e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4390, Loss: 2.181e-06, Loss_0: 1.053e-10, Loss_r: 2.181e-06, lambda_0 : 1.356e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4400, Loss: 2.175e-06, Loss_0: 1.051e-10, Loss_r: 2.175e-06, lambda_0 : 1.350e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4410, Loss: 2.170e-06, Loss_0: 1.048e-10, Loss_r: 2.169e-06, lambda_0 : 1.333e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4420, Loss: 2.164e-06, Loss_0: 1.045e-10, Loss_r: 2.164e-06, lambda_0 : 1.320e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4430, Loss: 2.158e-06, Loss_0: 1.043e-10, Loss_r: 2.158e-06, lambda_0 : 1.313e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4440, Loss: 2.153e-06, Loss_0: 1.041e-10, Loss_r: 2.153e-06, lambda_0 : 1.312e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4450, Loss: 2.147e-06, Loss_0: 1.063e-10, Loss_r: 2.147e-06, lambda_0 : 1.318e+02, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4460, Loss: 2.142e-06, Loss_0: 5.254e-10, Loss_r: 2.142e-06, lambda_0 : 4.113e+01, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4470, Loss: 2.405e-06, Loss_0: 2.688e-07, Loss_r: 2.136e-06, lambda_0 : 2.609e+00, Time: 0.16, Learning Rate: 0.00010\n",
            "It: 4480, Loss: 2.269e-06, Loss_0: 1.372e-07, Loss_r: 2.132e-06, lambda_0 : 4.251e+01, Time: 0.17, Learning Rate: 0.00010\n",
            "It: 4490, Loss: 6.361e-05, Loss_0: 6.147e-05, Loss_r: 2.139e-06, lambda_0 : 4.869e+00, Time: 0.18, Learning Rate: 0.00010\n",
            "It: 4500, Loss: 6.159e-06, Loss_0: 4.000e-06, Loss_r: 2.159e-06, lambda_0 : 2.798e+00, Time: 0.17, Learning Rate: 0.00010\n",
            "It: 4510, Loss: 4.463e-06, Loss_0: 2.278e-06, Loss_r: 2.185e-06, lambda_0 : 3.361e+01, Time: 0.17, Learning Rate: 0.00010\n",
            "It: 4520, Loss: 3.398e-06, Loss_0: 1.162e-06, Loss_r: 2.236e-06, lambda_0 : 2.189e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4530, Loss: 2.286e-06, Loss_0: 9.046e-08, Loss_r: 2.195e-06, lambda_0 : 3.219e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4540, Loss: 2.300e-06, Loss_0: 1.157e-07, Loss_r: 2.184e-06, lambda_0 : 1.200e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4550, Loss: 2.194e-06, Loss_0: 1.457e-08, Loss_r: 2.179e-06, lambda_0 : 4.951e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4560, Loss: 2.188e-06, Loss_0: 1.700e-08, Loss_r: 2.171e-06, lambda_0 : 1.248e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4570, Loss: 2.165e-06, Loss_0: 2.715e-10, Loss_r: 2.165e-06, lambda_0 : 4.202e+02, Time: 0.17, Learning Rate: 0.00009\n",
            "It: 4580, Loss: 2.161e-06, Loss_0: 2.406e-09, Loss_r: 2.158e-06, lambda_0 : 2.090e+02, Time: 0.17, Learning Rate: 0.00008\n",
            "It: 4590, Loss: 2.153e-06, Loss_0: 6.559e-10, Loss_r: 2.153e-06, lambda_0 : 3.241e+02, Time: 0.18, Learning Rate: 0.00008\n",
            "It: 4600, Loss: 2.147e-06, Loss_0: 1.437e-10, Loss_r: 2.147e-06, lambda_0 : 2.582e+02, Time: 0.17, Learning Rate: 0.00008\n",
            "It: 4610, Loss: 2.142e-06, Loss_0: 1.027e-10, Loss_r: 2.142e-06, lambda_0 : 4.590e+02, Time: 0.17, Learning Rate: 0.00008\n",
            "It: 4620, Loss: 2.137e-06, Loss_0: 1.256e-10, Loss_r: 2.136e-06, lambda_0 : 1.201e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4630, Loss: 2.131e-06, Loss_0: 1.127e-10, Loss_r: 2.131e-06, lambda_0 : 2.115e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4640, Loss: 2.126e-06, Loss_0: 9.780e-11, Loss_r: 2.126e-06, lambda_0 : 2.792e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4650, Loss: 2.121e-06, Loss_0: 9.587e-11, Loss_r: 2.121e-06, lambda_0 : 1.926e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4660, Loss: 2.116e-06, Loss_0: 9.581e-11, Loss_r: 2.116e-06, lambda_0 : 2.360e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4670, Loss: 2.111e-06, Loss_0: 9.587e-11, Loss_r: 2.111e-06, lambda_0 : 2.211e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4680, Loss: 2.106e-06, Loss_0: 9.609e-11, Loss_r: 2.106e-06, lambda_0 : 2.141e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4690, Loss: 2.101e-06, Loss_0: 9.568e-11, Loss_r: 2.101e-06, lambda_0 : 2.069e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4700, Loss: 2.097e-06, Loss_0: 9.544e-11, Loss_r: 2.097e-06, lambda_0 : 2.004e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4710, Loss: 2.092e-06, Loss_0: 9.541e-11, Loss_r: 2.092e-06, lambda_0 : 1.952e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4720, Loss: 2.087e-06, Loss_0: 9.532e-11, Loss_r: 2.087e-06, lambda_0 : 1.893e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4730, Loss: 2.083e-06, Loss_0: 9.512e-11, Loss_r: 2.083e-06, lambda_0 : 1.804e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4740, Loss: 2.078e-06, Loss_0: 9.499e-11, Loss_r: 2.078e-06, lambda_0 : 1.743e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4750, Loss: 2.074e-06, Loss_0: 9.485e-11, Loss_r: 2.074e-06, lambda_0 : 1.683e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4760, Loss: 2.070e-06, Loss_0: 9.469e-11, Loss_r: 2.070e-06, lambda_0 : 1.625e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4770, Loss: 2.065e-06, Loss_0: 9.455e-11, Loss_r: 2.065e-06, lambda_0 : 1.611e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4780, Loss: 2.061e-06, Loss_0: 9.440e-11, Loss_r: 2.061e-06, lambda_0 : 1.595e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4790, Loss: 2.057e-06, Loss_0: 9.422e-11, Loss_r: 2.057e-06, lambda_0 : 1.496e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4800, Loss: 2.053e-06, Loss_0: 9.405e-11, Loss_r: 2.053e-06, lambda_0 : 1.532e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4810, Loss: 2.049e-06, Loss_0: 9.388e-11, Loss_r: 2.049e-06, lambda_0 : 1.427e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4820, Loss: 2.045e-06, Loss_0: 9.368e-11, Loss_r: 2.045e-06, lambda_0 : 1.347e+02, Time: 0.26, Learning Rate: 0.00008\n",
            "It: 4830, Loss: 2.041e-06, Loss_0: 9.352e-11, Loss_r: 2.041e-06, lambda_0 : 1.337e+02, Time: 0.24, Learning Rate: 0.00008\n",
            "It: 4840, Loss: 2.037e-06, Loss_0: 9.334e-11, Loss_r: 2.037e-06, lambda_0 : 1.303e+02, Time: 0.23, Learning Rate: 0.00008\n",
            "It: 4850, Loss: 2.033e-06, Loss_0: 9.314e-11, Loss_r: 2.033e-06, lambda_0 : 1.247e+02, Time: 0.19, Learning Rate: 0.00008\n",
            "It: 4860, Loss: 2.029e-06, Loss_0: 9.293e-11, Loss_r: 2.029e-06, lambda_0 : 1.219e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4870, Loss: 2.026e-06, Loss_0: 9.275e-11, Loss_r: 2.025e-06, lambda_0 : 1.228e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4880, Loss: 2.022e-06, Loss_0: 9.256e-11, Loss_r: 2.022e-06, lambda_0 : 1.249e+02, Time: 0.17, Learning Rate: 0.00008\n",
            "It: 4890, Loss: 2.018e-06, Loss_0: 9.236e-11, Loss_r: 2.018e-06, lambda_0 : 1.248e+02, Time: 0.23, Learning Rate: 0.00008\n",
            "It: 4900, Loss: 2.015e-06, Loss_0: 9.215e-11, Loss_r: 2.014e-06, lambda_0 : 1.268e+02, Time: 0.25, Learning Rate: 0.00008\n",
            "It: 4910, Loss: 2.011e-06, Loss_0: 9.193e-11, Loss_r: 2.011e-06, lambda_0 : 1.234e+02, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 4920, Loss: 2.007e-06, Loss_0: 9.171e-11, Loss_r: 2.007e-06, lambda_0 : 1.183e+02, Time: 0.18, Learning Rate: 0.00008\n",
            "It: 4930, Loss: 2.004e-06, Loss_0: 9.150e-11, Loss_r: 2.004e-06, lambda_0 : 1.144e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4940, Loss: 2.000e-06, Loss_0: 9.130e-11, Loss_r: 2.000e-06, lambda_0 : 1.306e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4950, Loss: 1.997e-06, Loss_0: 9.108e-11, Loss_r: 1.997e-06, lambda_0 : 1.233e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4960, Loss: 1.994e-06, Loss_0: 9.086e-11, Loss_r: 1.993e-06, lambda_0 : 1.248e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4970, Loss: 1.990e-06, Loss_0: 9.064e-11, Loss_r: 1.990e-06, lambda_0 : 1.275e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4980, Loss: 1.987e-06, Loss_0: 9.043e-11, Loss_r: 1.987e-06, lambda_0 : 1.320e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "It: 4990, Loss: 1.984e-06, Loss_0: 9.019e-11, Loss_r: 1.983e-06, lambda_0 : 1.309e+02, Time: 0.16, Learning Rate: 0.00008\n",
            "Training time: 82.5375\n",
            "[1, 128, 128, 128, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.149e-03, Loss_0: 7.059e-04, Loss_r: 4.432e-04, lambda_0 : 5.035e+01, Time: 1.37, Learning Rate: 0.00100\n",
            "It: 10, Loss: 5.498e-04, Loss_0: 4.478e-04, Loss_r: 1.021e-04, lambda_0 : 6.843e+00, Time: 0.35, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.497e-04, Loss_0: 4.115e-05, Loss_r: 1.086e-04, lambda_0 : 2.830e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 30, Loss: 8.093e-05, Loss_0: 1.045e-05, Loss_r: 7.048e-05, lambda_0 : 2.813e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.515e-05, Loss_0: 1.639e-05, Loss_r: 5.876e-05, lambda_0 : 4.132e+00, Time: 0.33, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.518e-05, Loss_0: 6.151e-06, Loss_r: 5.903e-05, lambda_0 : 7.927e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 60, Loss: 6.051e-05, Loss_0: 1.037e-06, Loss_r: 5.948e-05, lambda_0 : 2.950e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.960e-05, Loss_0: 3.999e-07, Loss_r: 5.920e-05, lambda_0 : 2.707e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.913e-05, Loss_0: 2.221e-07, Loss_r: 5.890e-05, lambda_0 : 1.710e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.889e-05, Loss_0: 1.323e-07, Loss_r: 5.875e-05, lambda_0 : 1.171e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.872e-05, Loss_0: 8.198e-08, Loss_r: 5.863e-05, lambda_0 : 7.848e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.856e-05, Loss_0: 4.200e-08, Loss_r: 5.852e-05, lambda_0 : 1.110e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.841e-05, Loss_0: 1.468e-08, Loss_r: 5.840e-05, lambda_0 : 1.911e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.827e-05, Loss_0: 5.743e-09, Loss_r: 5.826e-05, lambda_0 : 2.866e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.812e-05, Loss_0: 6.725e-09, Loss_r: 5.811e-05, lambda_0 : 2.819e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.796e-05, Loss_0: 1.117e-08, Loss_r: 5.794e-05, lambda_0 : 2.378e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.779e-05, Loss_0: 1.216e-08, Loss_r: 5.778e-05, lambda_0 : 2.381e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.761e-05, Loss_0: 1.017e-08, Loss_r: 5.760e-05, lambda_0 : 2.602e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.742e-05, Loss_0: 9.887e-09, Loss_r: 5.741e-05, lambda_0 : 2.642e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.723e-05, Loss_0: 1.073e-08, Loss_r: 5.722e-05, lambda_0 : 2.589e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.702e-05, Loss_0: 1.047e-08, Loss_r: 5.701e-05, lambda_0 : 2.619e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.680e-05, Loss_0: 1.032e-08, Loss_r: 5.679e-05, lambda_0 : 2.673e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.657e-05, Loss_0: 1.051e-08, Loss_r: 5.656e-05, lambda_0 : 2.688e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.632e-05, Loss_0: 1.039e-08, Loss_r: 5.631e-05, lambda_0 : 2.760e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.610e-05, Loss_0: 4.743e-08, Loss_r: 5.605e-05, lambda_0 : 9.674e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.872e-05, Loss_0: 2.441e-06, Loss_r: 5.627e-05, lambda_0 : 1.081e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 260, Loss: 7.574e-05, Loss_0: 1.850e-05, Loss_r: 5.725e-05, lambda_0 : 2.741e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 270, Loss: 6.255e-05, Loss_0: 4.828e-06, Loss_r: 5.772e-05, lambda_0 : 1.841e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.995e-05, Loss_0: 2.027e-06, Loss_r: 5.792e-05, lambda_0 : 4.623e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.819e-05, Loss_0: 3.046e-07, Loss_r: 5.788e-05, lambda_0 : 6.089e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.830e-05, Loss_0: 5.151e-07, Loss_r: 5.779e-05, lambda_0 : 4.704e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.777e-05, Loss_0: 5.526e-08, Loss_r: 5.771e-05, lambda_0 : 1.447e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.767e-05, Loss_0: 5.923e-08, Loss_r: 5.761e-05, lambda_0 : 1.418e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.750e-05, Loss_0: 1.185e-08, Loss_r: 5.749e-05, lambda_0 : 3.342e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.737e-05, Loss_0: 1.104e-08, Loss_r: 5.736e-05, lambda_0 : 3.309e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.724e-05, Loss_0: 1.236e-08, Loss_r: 5.723e-05, lambda_0 : 3.284e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.709e-05, Loss_0: 1.068e-08, Loss_r: 5.708e-05, lambda_0 : 3.884e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.694e-05, Loss_0: 1.041e-08, Loss_r: 5.693e-05, lambda_0 : 4.672e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.677e-05, Loss_0: 1.033e-08, Loss_r: 5.676e-05, lambda_0 : 4.291e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.659e-05, Loss_0: 1.031e-08, Loss_r: 5.658e-05, lambda_0 : 4.630e+01, Time: 0.33, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.639e-05, Loss_0: 1.018e-08, Loss_r: 5.638e-05, lambda_0 : 5.119e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.617e-05, Loss_0: 1.018e-08, Loss_r: 5.616e-05, lambda_0 : 5.079e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.593e-05, Loss_0: 1.014e-08, Loss_r: 5.592e-05, lambda_0 : 5.401e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.566e-05, Loss_0: 1.007e-08, Loss_r: 5.565e-05, lambda_0 : 5.568e+01, Time: 0.33, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.536e-05, Loss_0: 1.002e-08, Loss_r: 5.535e-05, lambda_0 : 6.403e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.502e-05, Loss_0: 9.937e-09, Loss_r: 5.501e-05, lambda_0 : 7.867e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.463e-05, Loss_0: 9.832e-09, Loss_r: 5.462e-05, lambda_0 : 9.763e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.418e-05, Loss_0: 9.698e-09, Loss_r: 5.417e-05, lambda_0 : 1.222e+02, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.365e-05, Loss_0: 9.519e-09, Loss_r: 5.364e-05, lambda_0 : 1.541e+02, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.302e-05, Loss_0: 9.283e-09, Loss_r: 5.301e-05, lambda_0 : 1.955e+02, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.226e-05, Loss_0: 8.969e-09, Loss_r: 5.225e-05, lambda_0 : 2.484e+02, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.135e-05, Loss_0: 8.552e-09, Loss_r: 5.134e-05, lambda_0 : 3.157e+02, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.025e-05, Loss_0: 8.005e-09, Loss_r: 5.025e-05, lambda_0 : 3.978e+02, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.896e-05, Loss_0: 7.304e-09, Loss_r: 4.896e-05, lambda_0 : 4.892e+02, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.797e-05, Loss_0: 4.774e-07, Loss_r: 4.750e-05, lambda_0 : 6.347e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 550, Loss: 1.705e-04, Loss_0: 1.160e-04, Loss_r: 5.449e-05, lambda_0 : 1.359e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 560, Loss: 8.149e-05, Loss_0: 2.321e-05, Loss_r: 5.829e-05, lambda_0 : 2.174e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 570, Loss: 8.988e-05, Loss_0: 3.281e-05, Loss_r: 5.707e-05, lambda_0 : 1.742e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.970e-05, Loss_0: 2.839e-06, Loss_r: 5.686e-05, lambda_0 : 1.003e+02, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.780e-05, Loss_0: 1.473e-06, Loss_r: 5.633e-05, lambda_0 : 1.405e+02, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 600, Loss: 5.698e-05, Loss_0: 1.485e-06, Loss_r: 5.550e-05, lambda_0 : 7.649e+01, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 610, Loss: 5.508e-05, Loss_0: 2.852e-07, Loss_r: 5.479e-05, lambda_0 : 2.704e+02, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 620, Loss: 5.408e-05, Loss_0: 8.746e-09, Loss_r: 5.407e-05, lambda_0 : 6.902e+02, Time: 0.31, Learning Rate: 0.00090\n",
            "It: 630, Loss: 5.345e-05, Loss_0: 3.232e-08, Loss_r: 5.342e-05, lambda_0 : 4.129e+02, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 640, Loss: 5.292e-05, Loss_0: 2.450e-08, Loss_r: 5.290e-05, lambda_0 : 2.994e+02, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 650, Loss: 5.249e-05, Loss_0: 1.081e-08, Loss_r: 5.248e-05, lambda_0 : 4.730e+02, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 660, Loss: 5.216e-05, Loss_0: 2.924e-09, Loss_r: 5.216e-05, lambda_0 : 5.453e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 670, Loss: 5.194e-05, Loss_0: 2.258e-09, Loss_r: 5.194e-05, lambda_0 : 5.429e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 680, Loss: 5.174e-05, Loss_0: 2.624e-09, Loss_r: 5.174e-05, lambda_0 : 5.622e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 690, Loss: 5.156e-05, Loss_0: 1.942e-09, Loss_r: 5.156e-05, lambda_0 : 8.461e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 700, Loss: 5.138e-05, Loss_0: 2.337e-09, Loss_r: 5.137e-05, lambda_0 : 7.551e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 710, Loss: 5.119e-05, Loss_0: 1.943e-09, Loss_r: 5.119e-05, lambda_0 : 8.257e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 720, Loss: 5.100e-05, Loss_0: 1.905e-09, Loss_r: 5.100e-05, lambda_0 : 8.287e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 730, Loss: 5.083e-05, Loss_0: 1.963e-09, Loss_r: 5.082e-05, lambda_0 : 8.604e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 740, Loss: 5.065e-05, Loss_0: 1.912e-09, Loss_r: 5.064e-05, lambda_0 : 8.700e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 750, Loss: 5.046e-05, Loss_0: 1.886e-09, Loss_r: 5.046e-05, lambda_0 : 8.660e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 760, Loss: 5.027e-05, Loss_0: 1.873e-09, Loss_r: 5.026e-05, lambda_0 : 8.848e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 770, Loss: 5.007e-05, Loss_0: 1.850e-09, Loss_r: 5.006e-05, lambda_0 : 8.870e+02, Time: 0.33, Learning Rate: 0.00073\n",
            "It: 780, Loss: 4.986e-05, Loss_0: 1.828e-09, Loss_r: 4.986e-05, lambda_0 : 8.991e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 790, Loss: 4.967e-05, Loss_0: 1.810e-09, Loss_r: 4.966e-05, lambda_0 : 9.043e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 800, Loss: 4.947e-05, Loss_0: 1.787e-09, Loss_r: 4.946e-05, lambda_0 : 9.138e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 810, Loss: 4.926e-05, Loss_0: 1.769e-09, Loss_r: 4.925e-05, lambda_0 : 9.203e+02, Time: 0.33, Learning Rate: 0.00066\n",
            "It: 820, Loss: 4.904e-05, Loss_0: 1.747e-09, Loss_r: 4.904e-05, lambda_0 : 9.287e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 830, Loss: 4.881e-05, Loss_0: 1.727e-09, Loss_r: 4.881e-05, lambda_0 : 9.362e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 840, Loss: 4.857e-05, Loss_0: 1.706e-09, Loss_r: 4.857e-05, lambda_0 : 9.438e+02, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 850, Loss: 4.835e-05, Loss_0: 1.687e-09, Loss_r: 4.835e-05, lambda_0 : 9.505e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 860, Loss: 4.811e-05, Loss_0: 1.668e-09, Loss_r: 4.811e-05, lambda_0 : 9.580e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 870, Loss: 4.787e-05, Loss_0: 1.649e-09, Loss_r: 4.787e-05, lambda_0 : 9.942e+02, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 880, Loss: 4.761e-05, Loss_0: 1.630e-09, Loss_r: 4.761e-05, lambda_0 : 1.030e+03, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 890, Loss: 4.734e-05, Loss_0: 1.611e-09, Loss_r: 4.734e-05, lambda_0 : 1.064e+03, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 900, Loss: 4.705e-05, Loss_0: 1.593e-09, Loss_r: 4.705e-05, lambda_0 : 1.097e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 910, Loss: 4.675e-05, Loss_0: 1.575e-09, Loss_r: 4.675e-05, lambda_0 : 1.127e+03, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 920, Loss: 4.643e-05, Loss_0: 1.557e-09, Loss_r: 4.643e-05, lambda_0 : 1.155e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 930, Loss: 4.609e-05, Loss_0: 1.540e-09, Loss_r: 4.609e-05, lambda_0 : 1.177e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 940, Loss: 4.573e-05, Loss_0: 1.524e-09, Loss_r: 4.572e-05, lambda_0 : 1.194e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 950, Loss: 4.534e-05, Loss_0: 1.509e-09, Loss_r: 4.534e-05, lambda_0 : 1.204e+03, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 960, Loss: 4.492e-05, Loss_0: 1.496e-09, Loss_r: 4.492e-05, lambda_0 : 1.203e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 970, Loss: 4.448e-05, Loss_0: 1.484e-09, Loss_r: 4.448e-05, lambda_0 : 1.189e+03, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 980, Loss: 4.401e-05, Loss_0: 1.474e-09, Loss_r: 4.400e-05, lambda_0 : 1.161e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 990, Loss: 4.349e-05, Loss_0: 1.467e-09, Loss_r: 4.349e-05, lambda_0 : 1.114e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1000, Loss: 4.294e-05, Loss_0: 1.463e-09, Loss_r: 4.294e-05, lambda_0 : 1.045e+03, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1010, Loss: 4.234e-05, Loss_0: 1.462e-09, Loss_r: 4.234e-05, lambda_0 : 9.489e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1020, Loss: 4.170e-05, Loss_0: 1.465e-09, Loss_r: 4.170e-05, lambda_0 : 9.215e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1030, Loss: 4.101e-05, Loss_0: 1.474e-09, Loss_r: 4.100e-05, lambda_0 : 8.241e+02, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 1040, Loss: 2.103e-04, Loss_0: 1.699e-04, Loss_r: 4.037e-05, lambda_0 : 3.306e+00, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1050, Loss: 7.222e-05, Loss_0: 2.801e-05, Loss_r: 4.421e-05, lambda_0 : 1.006e+01, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1060, Loss: 5.844e-05, Loss_0: 1.180e-05, Loss_r: 4.663e-05, lambda_0 : 3.611e+01, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1070, Loss: 5.206e-05, Loss_0: 4.555e-06, Loss_r: 4.750e-05, lambda_0 : 5.912e+01, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 1080, Loss: 4.835e-05, Loss_0: 7.972e-07, Loss_r: 4.756e-05, lambda_0 : 4.442e+01, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 1090, Loss: 4.736e-05, Loss_0: 1.768e-09, Loss_r: 4.735e-05, lambda_0 : 7.812e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 1100, Loss: 4.720e-05, Loss_0: 1.489e-07, Loss_r: 4.705e-05, lambda_0 : 1.179e+02, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1110, Loss: 4.675e-05, Loss_0: 5.389e-08, Loss_r: 4.670e-05, lambda_0 : 1.590e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 1120, Loss: 4.633e-05, Loss_0: 7.944e-09, Loss_r: 4.632e-05, lambda_0 : 4.132e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 1130, Loss: 4.591e-05, Loss_0: 3.199e-09, Loss_r: 4.590e-05, lambda_0 : 6.040e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 1140, Loss: 4.547e-05, Loss_0: 1.854e-09, Loss_r: 4.546e-05, lambda_0 : 7.836e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 1150, Loss: 4.500e-05, Loss_0: 2.887e-09, Loss_r: 4.499e-05, lambda_0 : 6.433e+02, Time: 0.33, Learning Rate: 0.00048\n",
            "It: 1160, Loss: 4.455e-05, Loss_0: 1.439e-09, Loss_r: 4.455e-05, lambda_0 : 8.965e+02, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1170, Loss: 4.408e-05, Loss_0: 1.726e-09, Loss_r: 4.408e-05, lambda_0 : 1.018e+03, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1180, Loss: 4.358e-05, Loss_0: 1.396e-09, Loss_r: 4.358e-05, lambda_0 : 1.126e+03, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1190, Loss: 4.306e-05, Loss_0: 1.473e-09, Loss_r: 4.305e-05, lambda_0 : 1.034e+03, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1200, Loss: 4.251e-05, Loss_0: 1.342e-09, Loss_r: 4.251e-05, lambda_0 : 1.062e+03, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1210, Loss: 4.194e-05, Loss_0: 1.390e-09, Loss_r: 4.193e-05, lambda_0 : 1.042e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1220, Loss: 4.140e-05, Loss_0: 1.335e-09, Loss_r: 4.140e-05, lambda_0 : 1.061e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1230, Loss: 4.084e-05, Loss_0: 1.319e-09, Loss_r: 4.084e-05, lambda_0 : 1.057e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1240, Loss: 4.027e-05, Loss_0: 1.288e-09, Loss_r: 4.027e-05, lambda_0 : 1.059e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1250, Loss: 3.969e-05, Loss_0: 1.248e-09, Loss_r: 3.968e-05, lambda_0 : 1.069e+03, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1260, Loss: 3.909e-05, Loss_0: 1.209e-09, Loss_r: 3.909e-05, lambda_0 : 1.070e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1270, Loss: 3.848e-05, Loss_0: 1.160e-09, Loss_r: 3.848e-05, lambda_0 : 1.096e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1280, Loss: 3.786e-05, Loss_0: 1.112e-09, Loss_r: 3.786e-05, lambda_0 : 1.184e+03, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1290, Loss: 3.723e-05, Loss_0: 1.058e-09, Loss_r: 3.723e-05, lambda_0 : 1.292e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1300, Loss: 3.664e-05, Loss_0: 9.127e-10, Loss_r: 3.664e-05, lambda_0 : 1.205e+04, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1310, Loss: 4.125e-05, Loss_0: 3.829e-09, Loss_r: 4.125e-05, lambda_0 : 5.612e+04, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1320, Loss: 3.659e-05, Loss_0: 2.257e-09, Loss_r: 3.659e-05, lambda_0 : 3.083e+04, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1330, Loss: 3.522e-05, Loss_0: 5.174e-10, Loss_r: 3.522e-05, lambda_0 : 3.053e+04, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1340, Loss: 3.487e-05, Loss_0: 3.833e-10, Loss_r: 3.487e-05, lambda_0 : 5.074e+04, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1350, Loss: 3.397e-05, Loss_0: 9.010e-10, Loss_r: 3.397e-05, lambda_0 : 3.896e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1360, Loss: 3.340e-05, Loss_0: 8.729e-10, Loss_r: 3.340e-05, lambda_0 : 5.203e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1370, Loss: 3.283e-05, Loss_0: 7.857e-10, Loss_r: 3.283e-05, lambda_0 : 5.293e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1380, Loss: 5.130e-05, Loss_0: 1.903e-05, Loss_r: 3.227e-05, lambda_0 : 6.202e+00, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1390, Loss: 3.483e-05, Loss_0: 2.510e-07, Loss_r: 3.458e-05, lambda_0 : 4.377e+02, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1400, Loss: 4.331e-05, Loss_0: 6.307e-06, Loss_r: 3.700e-05, lambda_0 : 3.764e+01, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1410, Loss: 3.783e-05, Loss_0: 6.154e-09, Loss_r: 3.782e-05, lambda_0 : 5.505e+02, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1420, Loss: 3.786e-05, Loss_0: 7.414e-10, Loss_r: 3.786e-05, lambda_0 : 1.245e+03, Time: 0.32, Learning Rate: 0.00043\n",
            "It: 1430, Loss: 3.756e-05, Loss_0: 9.104e-09, Loss_r: 3.755e-05, lambda_0 : 3.414e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1440, Loss: 3.716e-05, Loss_0: 1.240e-08, Loss_r: 3.715e-05, lambda_0 : 3.739e+02, Time: 0.32, Learning Rate: 0.00039\n",
            "It: 1450, Loss: 3.668e-05, Loss_0: 2.385e-09, Loss_r: 3.668e-05, lambda_0 : 7.398e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1460, Loss: 3.618e-05, Loss_0: 6.804e-09, Loss_r: 3.618e-05, lambda_0 : 4.074e+02, Time: 0.32, Learning Rate: 0.00039\n",
            "It: 1470, Loss: 3.568e-05, Loss_0: 1.378e-08, Loss_r: 3.567e-05, lambda_0 : 2.786e+02, Time: 0.32, Learning Rate: 0.00039\n",
            "It: 1480, Loss: 3.515e-05, Loss_0: 7.616e-10, Loss_r: 3.515e-05, lambda_0 : 1.471e+03, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1490, Loss: 3.463e-05, Loss_0: 1.680e-09, Loss_r: 3.463e-05, lambda_0 : 7.558e+02, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1500, Loss: 3.416e-05, Loss_0: 1.065e-09, Loss_r: 3.416e-05, lambda_0 : 9.558e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1510, Loss: 3.369e-05, Loss_0: 6.373e-10, Loss_r: 3.369e-05, lambda_0 : 1.542e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1520, Loss: 3.321e-05, Loss_0: 6.058e-10, Loss_r: 3.321e-05, lambda_0 : 1.534e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1530, Loss: 3.273e-05, Loss_0: 5.799e-10, Loss_r: 3.273e-05, lambda_0 : 1.580e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1540, Loss: 3.225e-05, Loss_0: 5.836e-10, Loss_r: 3.225e-05, lambda_0 : 1.631e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1550, Loss: 3.176e-05, Loss_0: 5.329e-10, Loss_r: 3.176e-05, lambda_0 : 1.706e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1560, Loss: 3.127e-05, Loss_0: 5.078e-10, Loss_r: 3.127e-05, lambda_0 : 1.715e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1570, Loss: 3.078e-05, Loss_0: 4.877e-10, Loss_r: 3.077e-05, lambda_0 : 1.774e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1580, Loss: 3.027e-05, Loss_0: 4.704e-10, Loss_r: 3.027e-05, lambda_0 : 1.846e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1590, Loss: 2.977e-05, Loss_0: 4.520e-10, Loss_r: 2.977e-05, lambda_0 : 1.886e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1600, Loss: 2.925e-05, Loss_0: 4.333e-10, Loss_r: 2.925e-05, lambda_0 : 1.901e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1610, Loss: 2.874e-05, Loss_0: 4.179e-10, Loss_r: 2.874e-05, lambda_0 : 1.941e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1620, Loss: 2.821e-05, Loss_0: 4.037e-10, Loss_r: 2.821e-05, lambda_0 : 1.977e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1630, Loss: 2.768e-05, Loss_0: 3.933e-10, Loss_r: 2.768e-05, lambda_0 : 2.065e+03, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1640, Loss: 2.765e-05, Loss_0: 6.601e-10, Loss_r: 2.765e-05, lambda_0 : 4.712e+04, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1650, Loss: 2.818e-05, Loss_0: 1.414e-09, Loss_r: 2.818e-05, lambda_0 : 4.838e+04, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1660, Loss: 2.157e-04, Loss_0: 1.893e-04, Loss_r: 2.643e-05, lambda_0 : 1.131e+01, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1670, Loss: 3.459e-05, Loss_0: 4.359e-06, Loss_r: 3.024e-05, lambda_0 : 4.721e+02, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1680, Loss: 4.648e-05, Loss_0: 1.401e-05, Loss_r: 3.248e-05, lambda_0 : 5.533e+01, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1690, Loss: 3.397e-05, Loss_0: 5.967e-07, Loss_r: 3.338e-05, lambda_0 : 3.542e+02, Time: 0.32, Learning Rate: 0.00035\n",
            "It: 1700, Loss: 3.463e-05, Loss_0: 1.213e-06, Loss_r: 3.342e-05, lambda_0 : 1.281e+02, Time: 0.31, Learning Rate: 0.00031\n",
            "It: 1710, Loss: 3.363e-05, Loss_0: 4.331e-07, Loss_r: 3.319e-05, lambda_0 : 1.698e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1720, Loss: 3.296e-05, Loss_0: 1.284e-07, Loss_r: 3.283e-05, lambda_0 : 2.428e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1730, Loss: 3.252e-05, Loss_0: 1.131e-07, Loss_r: 3.241e-05, lambda_0 : 1.962e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1740, Loss: 3.198e-05, Loss_0: 1.015e-08, Loss_r: 3.197e-05, lambda_0 : 3.304e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1750, Loss: 3.152e-05, Loss_0: 6.425e-10, Loss_r: 3.152e-05, lambda_0 : 1.464e+03, Time: 0.31, Learning Rate: 0.00031\n",
            "It: 1760, Loss: 3.106e-05, Loss_0: 6.973e-10, Loss_r: 3.106e-05, lambda_0 : 1.175e+03, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1770, Loss: 3.064e-05, Loss_0: 1.262e-09, Loss_r: 3.064e-05, lambda_0 : 8.566e+02, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1780, Loss: 3.023e-05, Loss_0: 9.040e-10, Loss_r: 3.023e-05, lambda_0 : 1.011e+03, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1790, Loss: 2.981e-05, Loss_0: 6.138e-10, Loss_r: 2.981e-05, lambda_0 : 1.269e+03, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1800, Loss: 2.939e-05, Loss_0: 5.069e-10, Loss_r: 2.939e-05, lambda_0 : 1.587e+03, Time: 0.31, Learning Rate: 0.00028\n",
            "It: 1810, Loss: 2.896e-05, Loss_0: 5.047e-10, Loss_r: 2.896e-05, lambda_0 : 1.830e+03, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1820, Loss: 2.854e-05, Loss_0: 4.964e-10, Loss_r: 2.854e-05, lambda_0 : 1.762e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1830, Loss: 2.816e-05, Loss_0: 4.658e-10, Loss_r: 2.815e-05, lambda_0 : 1.943e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1840, Loss: 2.777e-05, Loss_0: 4.443e-10, Loss_r: 2.777e-05, lambda_0 : 1.870e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1850, Loss: 2.738e-05, Loss_0: 4.308e-10, Loss_r: 2.738e-05, lambda_0 : 1.866e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1860, Loss: 2.699e-05, Loss_0: 4.211e-10, Loss_r: 2.699e-05, lambda_0 : 1.937e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1870, Loss: 2.660e-05, Loss_0: 4.102e-10, Loss_r: 2.660e-05, lambda_0 : 1.965e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1880, Loss: 2.620e-05, Loss_0: 3.988e-10, Loss_r: 2.620e-05, lambda_0 : 1.963e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1890, Loss: 2.580e-05, Loss_0: 3.900e-10, Loss_r: 2.580e-05, lambda_0 : 1.996e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1900, Loss: 2.540e-05, Loss_0: 3.806e-10, Loss_r: 2.540e-05, lambda_0 : 2.002e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1910, Loss: 2.500e-05, Loss_0: 3.726e-10, Loss_r: 2.500e-05, lambda_0 : 2.019e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1920, Loss: 2.460e-05, Loss_0: 3.650e-10, Loss_r: 2.460e-05, lambda_0 : 2.028e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1930, Loss: 2.419e-05, Loss_0: 3.581e-10, Loss_r: 2.419e-05, lambda_0 : 2.034e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1940, Loss: 2.379e-05, Loss_0: 3.519e-10, Loss_r: 2.379e-05, lambda_0 : 2.038e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1950, Loss: 2.338e-05, Loss_0: 3.463e-10, Loss_r: 2.338e-05, lambda_0 : 2.040e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1960, Loss: 2.297e-05, Loss_0: 3.412e-10, Loss_r: 2.297e-05, lambda_0 : 2.038e+03, Time: 0.33, Learning Rate: 0.00025\n",
            "It: 1970, Loss: 2.256e-05, Loss_0: 3.366e-10, Loss_r: 2.256e-05, lambda_0 : 2.034e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1980, Loss: 2.214e-05, Loss_0: 3.326e-10, Loss_r: 2.214e-05, lambda_0 : 2.026e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1990, Loss: 2.173e-05, Loss_0: 3.291e-10, Loss_r: 2.173e-05, lambda_0 : 2.015e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2000, Loss: 2.132e-05, Loss_0: 3.261e-10, Loss_r: 2.132e-05, lambda_0 : 2.001e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2010, Loss: 2.091e-05, Loss_0: 3.235e-10, Loss_r: 2.091e-05, lambda_0 : 1.985e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2020, Loss: 2.049e-05, Loss_0: 3.214e-10, Loss_r: 2.049e-05, lambda_0 : 1.965e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2030, Loss: 2.008e-05, Loss_0: 3.196e-10, Loss_r: 2.008e-05, lambda_0 : 1.943e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2040, Loss: 1.967e-05, Loss_0: 3.183e-10, Loss_r: 1.967e-05, lambda_0 : 1.918e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2050, Loss: 1.926e-05, Loss_0: 3.173e-10, Loss_r: 1.926e-05, lambda_0 : 1.891e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2060, Loss: 1.885e-05, Loss_0: 3.167e-10, Loss_r: 1.885e-05, lambda_0 : 1.862e+03, Time: 0.35, Learning Rate: 0.00025\n",
            "It: 2070, Loss: 1.844e-05, Loss_0: 3.168e-10, Loss_r: 1.844e-05, lambda_0 : 1.842e+03, Time: 0.34, Learning Rate: 0.00025\n",
            "It: 2080, Loss: 1.804e-05, Loss_0: 2.123e-09, Loss_r: 1.804e-05, lambda_0 : 5.234e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2090, Loss: 6.106e-04, Loss_0: 5.928e-04, Loss_r: 1.775e-05, lambda_0 : 9.430e-01, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2100, Loss: 3.732e-05, Loss_0: 1.625e-05, Loss_r: 2.107e-05, lambda_0 : 1.365e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2110, Loss: 4.269e-05, Loss_0: 1.962e-05, Loss_r: 2.307e-05, lambda_0 : 1.171e+01, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2120, Loss: 2.610e-05, Loss_0: 2.198e-06, Loss_r: 2.390e-05, lambda_0 : 1.063e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2130, Loss: 2.468e-05, Loss_0: 6.373e-07, Loss_r: 2.404e-05, lambda_0 : 4.264e+01, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 2140, Loss: 2.401e-05, Loss_0: 1.722e-07, Loss_r: 2.384e-05, lambda_0 : 1.168e+02, Time: 0.32, Learning Rate: 0.00023\n",
            "It: 2150, Loss: 2.376e-05, Loss_0: 2.443e-07, Loss_r: 2.352e-05, lambda_0 : 6.252e+01, Time: 0.34, Learning Rate: 0.00023\n",
            "It: 2160, Loss: 2.326e-05, Loss_0: 1.202e-07, Loss_r: 2.314e-05, lambda_0 : 9.927e+01, Time: 0.35, Learning Rate: 0.00023\n",
            "It: 2170, Loss: 2.279e-05, Loss_0: 4.397e-08, Loss_r: 2.275e-05, lambda_0 : 1.419e+02, Time: 0.35, Learning Rate: 0.00023\n",
            "It: 2180, Loss: 2.236e-05, Loss_0: 1.212e-08, Loss_r: 2.234e-05, lambda_0 : 2.702e+02, Time: 0.33, Learning Rate: 0.00023\n",
            "It: 2190, Loss: 2.194e-05, Loss_0: 1.262e-09, Loss_r: 2.194e-05, lambda_0 : 7.600e+02, Time: 0.36, Learning Rate: 0.00023\n",
            "It: 2200, Loss: 2.153e-05, Loss_0: 9.340e-10, Loss_r: 2.153e-05, lambda_0 : 8.800e+02, Time: 0.35, Learning Rate: 0.00021\n",
            "It: 2210, Loss: 2.117e-05, Loss_0: 8.212e-10, Loss_r: 2.117e-05, lambda_0 : 9.322e+02, Time: 0.36, Learning Rate: 0.00021\n",
            "It: 2220, Loss: 2.080e-05, Loss_0: 6.219e-10, Loss_r: 2.080e-05, lambda_0 : 1.053e+03, Time: 0.35, Learning Rate: 0.00021\n",
            "It: 2230, Loss: 2.044e-05, Loss_0: 4.105e-10, Loss_r: 2.044e-05, lambda_0 : 1.424e+03, Time: 0.33, Learning Rate: 0.00021\n",
            "It: 2240, Loss: 2.008e-05, Loss_0: 4.002e-10, Loss_r: 2.008e-05, lambda_0 : 1.690e+03, Time: 0.35, Learning Rate: 0.00021\n",
            "It: 2250, Loss: 1.971e-05, Loss_0: 3.933e-10, Loss_r: 1.971e-05, lambda_0 : 1.698e+03, Time: 0.36, Learning Rate: 0.00021\n",
            "It: 2260, Loss: 1.936e-05, Loss_0: 3.729e-10, Loss_r: 1.935e-05, lambda_0 : 1.639e+03, Time: 0.33, Learning Rate: 0.00019\n",
            "It: 2270, Loss: 1.903e-05, Loss_0: 3.692e-10, Loss_r: 1.903e-05, lambda_0 : 1.658e+03, Time: 0.38, Learning Rate: 0.00019\n",
            "It: 2280, Loss: 1.871e-05, Loss_0: 3.677e-10, Loss_r: 1.871e-05, lambda_0 : 1.728e+03, Time: 0.35, Learning Rate: 0.00019\n",
            "It: 2290, Loss: 1.839e-05, Loss_0: 3.647e-10, Loss_r: 1.839e-05, lambda_0 : 1.718e+03, Time: 0.35, Learning Rate: 0.00019\n",
            "It: 2300, Loss: 1.807e-05, Loss_0: 3.607e-10, Loss_r: 1.807e-05, lambda_0 : 1.671e+03, Time: 0.35, Learning Rate: 0.00019\n",
            "It: 2310, Loss: 1.776e-05, Loss_0: 3.587e-10, Loss_r: 1.776e-05, lambda_0 : 1.674e+03, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 2320, Loss: 1.744e-05, Loss_0: 3.564e-10, Loss_r: 1.744e-05, lambda_0 : 1.655e+03, Time: 0.34, Learning Rate: 0.00019\n",
            "It: 2330, Loss: 1.713e-05, Loss_0: 3.543e-10, Loss_r: 1.713e-05, lambda_0 : 1.637e+03, Time: 0.35, Learning Rate: 0.00019\n",
            "It: 2340, Loss: 1.682e-05, Loss_0: 3.524e-10, Loss_r: 1.682e-05, lambda_0 : 1.619e+03, Time: 0.34, Learning Rate: 0.00019\n",
            "It: 2350, Loss: 1.651e-05, Loss_0: 3.508e-10, Loss_r: 1.651e-05, lambda_0 : 1.600e+03, Time: 0.39, Learning Rate: 0.00019\n",
            "It: 2360, Loss: 1.621e-05, Loss_0: 3.492e-10, Loss_r: 1.621e-05, lambda_0 : 1.578e+03, Time: 0.36, Learning Rate: 0.00019\n",
            "It: 2370, Loss: 1.591e-05, Loss_0: 3.479e-10, Loss_r: 1.590e-05, lambda_0 : 1.559e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2380, Loss: 1.560e-05, Loss_0: 3.467e-10, Loss_r: 1.560e-05, lambda_0 : 1.537e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2390, Loss: 1.531e-05, Loss_0: 3.455e-10, Loss_r: 1.531e-05, lambda_0 : 1.514e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2400, Loss: 1.501e-05, Loss_0: 3.445e-10, Loss_r: 1.501e-05, lambda_0 : 1.491e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 2410, Loss: 1.472e-05, Loss_0: 3.435e-10, Loss_r: 1.472e-05, lambda_0 : 1.467e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2420, Loss: 1.443e-05, Loss_0: 3.425e-10, Loss_r: 1.443e-05, lambda_0 : 1.442e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2430, Loss: 1.415e-05, Loss_0: 3.415e-10, Loss_r: 1.415e-05, lambda_0 : 1.414e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2440, Loss: 1.387e-05, Loss_0: 3.387e-10, Loss_r: 1.387e-05, lambda_0 : 1.312e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2450, Loss: 1.368e-05, Loss_0: 2.757e-10, Loss_r: 1.368e-05, lambda_0 : 3.380e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2460, Loss: 1.417e-05, Loss_0: 6.529e-10, Loss_r: 1.417e-05, lambda_0 : 6.345e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2470, Loss: 1.378e-05, Loss_0: 6.657e-10, Loss_r: 1.378e-05, lambda_0 : 5.229e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2480, Loss: 1.317e-05, Loss_0: 1.925e-10, Loss_r: 1.317e-05, lambda_0 : 7.156e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2490, Loss: 1.262e-05, Loss_0: 3.842e-10, Loss_r: 1.261e-05, lambda_0 : 1.389e+04, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 2500, Loss: 1.237e-05, Loss_0: 3.832e-10, Loss_r: 1.237e-05, lambda_0 : 1.012e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2510, Loss: 1.213e-05, Loss_0: 2.986e-10, Loss_r: 1.213e-05, lambda_0 : 1.270e+04, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2520, Loss: 1.188e-05, Loss_0: 3.263e-10, Loss_r: 1.188e-05, lambda_0 : 3.759e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2530, Loss: 1.165e-05, Loss_0: 3.432e-10, Loss_r: 1.165e-05, lambda_0 : 2.955e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2540, Loss: 1.142e-05, Loss_0: 3.370e-10, Loss_r: 1.142e-05, lambda_0 : 2.154e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2550, Loss: 1.119e-05, Loss_0: 3.313e-10, Loss_r: 1.119e-05, lambda_0 : 1.265e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2560, Loss: 1.097e-05, Loss_0: 3.271e-10, Loss_r: 1.097e-05, lambda_0 : 1.169e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2570, Loss: 1.075e-05, Loss_0: 3.246e-10, Loss_r: 1.075e-05, lambda_0 : 1.126e+03, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2580, Loss: 1.054e-05, Loss_0: 6.643e-10, Loss_r: 1.054e-05, lambda_0 : 5.858e+02, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2590, Loss: 1.441e-05, Loss_0: 4.076e-06, Loss_r: 1.034e-05, lambda_0 : 8.551e+00, Time: 0.32, Learning Rate: 0.00019\n",
            "It: 2600, Loss: 6.300e-05, Loss_0: 5.227e-05, Loss_r: 1.074e-05, lambda_0 : 5.524e+00, Time: 0.33, Learning Rate: 0.00019\n",
            "It: 2610, Loss: 3.498e-05, Loss_0: 2.346e-05, Loss_r: 1.152e-05, lambda_0 : 3.265e+01, Time: 0.34, Learning Rate: 0.00019\n",
            "It: 2620, Loss: 1.797e-05, Loss_0: 6.108e-06, Loss_r: 1.187e-05, lambda_0 : 5.179e+01, Time: 0.36, Learning Rate: 0.00019\n",
            "It: 2630, Loss: 1.214e-05, Loss_0: 2.439e-07, Loss_r: 1.190e-05, lambda_0 : 1.934e+02, Time: 0.35, Learning Rate: 0.00019\n",
            "It: 2640, Loss: 1.191e-05, Loss_0: 1.365e-07, Loss_r: 1.177e-05, lambda_0 : 7.708e+01, Time: 0.37, Learning Rate: 0.00017\n",
            "It: 2650, Loss: 1.184e-05, Loss_0: 2.396e-07, Loss_r: 1.160e-05, lambda_0 : 4.638e+01, Time: 0.33, Learning Rate: 0.00017\n",
            "It: 2660, Loss: 1.142e-05, Loss_0: 1.979e-08, Loss_r: 1.140e-05, lambda_0 : 1.245e+02, Time: 0.35, Learning Rate: 0.00017\n",
            "It: 2670, Loss: 1.123e-05, Loss_0: 4.259e-08, Loss_r: 1.119e-05, lambda_0 : 8.789e+01, Time: 0.37, Learning Rate: 0.00017\n",
            "It: 2680, Loss: 1.099e-05, Loss_0: 6.963e-09, Loss_r: 1.098e-05, lambda_0 : 2.093e+02, Time: 0.43, Learning Rate: 0.00017\n",
            "It: 2690, Loss: 1.078e-05, Loss_0: 5.253e-10, Loss_r: 1.078e-05, lambda_0 : 7.147e+02, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 2700, Loss: 1.058e-05, Loss_0: 2.969e-10, Loss_r: 1.058e-05, lambda_0 : 1.247e+03, Time: 0.34, Learning Rate: 0.00015\n",
            "It: 2710, Loss: 1.040e-05, Loss_0: 7.332e-10, Loss_r: 1.040e-05, lambda_0 : 5.657e+02, Time: 0.37, Learning Rate: 0.00015\n",
            "It: 2720, Loss: 1.022e-05, Loss_0: 4.134e-10, Loss_r: 1.022e-05, lambda_0 : 7.438e+02, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2730, Loss: 1.005e-05, Loss_0: 2.992e-10, Loss_r: 1.005e-05, lambda_0 : 1.019e+03, Time: 0.37, Learning Rate: 0.00015\n",
            "It: 2740, Loss: 9.876e-06, Loss_0: 2.946e-10, Loss_r: 9.875e-06, lambda_0 : 1.124e+03, Time: 0.35, Learning Rate: 0.00015\n",
            "It: 2750, Loss: 9.707e-06, Loss_0: 2.940e-10, Loss_r: 9.707e-06, lambda_0 : 1.111e+03, Time: 0.33, Learning Rate: 0.00015\n",
            "It: 2760, Loss: 9.542e-06, Loss_0: 2.922e-10, Loss_r: 9.542e-06, lambda_0 : 1.069e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2770, Loss: 9.379e-06, Loss_0: 2.909e-10, Loss_r: 9.379e-06, lambda_0 : 1.035e+03, Time: 0.37, Learning Rate: 0.00015\n",
            "It: 2780, Loss: 9.219e-06, Loss_0: 2.897e-10, Loss_r: 9.219e-06, lambda_0 : 1.024e+03, Time: 0.35, Learning Rate: 0.00015\n",
            "It: 2790, Loss: 9.062e-06, Loss_0: 2.887e-10, Loss_r: 9.062e-06, lambda_0 : 1.032e+03, Time: 0.33, Learning Rate: 0.00015\n",
            "It: 2800, Loss: 8.907e-06, Loss_0: 2.877e-10, Loss_r: 8.907e-06, lambda_0 : 1.041e+03, Time: 0.34, Learning Rate: 0.00015\n",
            "It: 2810, Loss: 8.756e-06, Loss_0: 2.856e-10, Loss_r: 8.755e-06, lambda_0 : 1.058e+03, Time: 0.33, Learning Rate: 0.00015\n",
            "It: 2820, Loss: 8.606e-06, Loss_0: 2.840e-10, Loss_r: 8.606e-06, lambda_0 : 1.080e+03, Time: 0.34, Learning Rate: 0.00015\n",
            "It: 2830, Loss: 8.460e-06, Loss_0: 2.823e-10, Loss_r: 8.460e-06, lambda_0 : 1.104e+03, Time: 0.34, Learning Rate: 0.00015\n",
            "It: 2840, Loss: 8.316e-06, Loss_0: 2.804e-10, Loss_r: 8.316e-06, lambda_0 : 1.112e+03, Time: 0.36, Learning Rate: 0.00015\n",
            "It: 2850, Loss: 8.175e-06, Loss_0: 2.785e-10, Loss_r: 8.175e-06, lambda_0 : 1.128e+03, Time: 0.35, Learning Rate: 0.00015\n",
            "It: 2860, Loss: 8.036e-06, Loss_0: 2.765e-10, Loss_r: 8.036e-06, lambda_0 : 1.145e+03, Time: 0.33, Learning Rate: 0.00015\n",
            "It: 2870, Loss: 7.901e-06, Loss_0: 2.745e-10, Loss_r: 7.900e-06, lambda_0 : 1.144e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2880, Loss: 7.767e-06, Loss_0: 2.723e-10, Loss_r: 7.767e-06, lambda_0 : 1.150e+03, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2890, Loss: 7.636e-06, Loss_0: 2.702e-10, Loss_r: 7.636e-06, lambda_0 : 1.159e+03, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2900, Loss: 7.508e-06, Loss_0: 2.681e-10, Loss_r: 7.508e-06, lambda_0 : 1.142e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2910, Loss: 7.382e-06, Loss_0: 2.658e-10, Loss_r: 7.382e-06, lambda_0 : 1.164e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2920, Loss: 7.259e-06, Loss_0: 2.636e-10, Loss_r: 7.259e-06, lambda_0 : 1.173e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2930, Loss: 7.138e-06, Loss_0: 2.613e-10, Loss_r: 7.138e-06, lambda_0 : 1.214e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2940, Loss: 7.020e-06, Loss_0: 2.582e-10, Loss_r: 7.020e-06, lambda_0 : 1.728e+03, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2950, Loss: 6.910e-06, Loss_0: 2.429e-10, Loss_r: 6.910e-06, lambda_0 : 1.045e+04, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2960, Loss: 8.851e-06, Loss_0: 4.283e-10, Loss_r: 8.851e-06, lambda_0 : 9.648e+04, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2970, Loss: 7.131e-06, Loss_0: 3.441e-07, Loss_r: 6.787e-06, lambda_0 : 8.721e+02, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2980, Loss: 9.964e-05, Loss_0: 9.274e-05, Loss_r: 6.906e-06, lambda_0 : 5.338e+01, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 2990, Loss: 3.236e-05, Loss_0: 2.524e-05, Loss_r: 7.123e-06, lambda_0 : 7.479e+01, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 3000, Loss: 1.308e-05, Loss_0: 5.845e-06, Loss_r: 7.231e-06, lambda_0 : 5.846e+01, Time: 0.32, Learning Rate: 0.00015\n",
            "It: 3010, Loss: 8.225e-06, Loss_0: 9.530e-07, Loss_r: 7.272e-06, lambda_0 : 2.006e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3020, Loss: 7.515e-06, Loss_0: 3.014e-07, Loss_r: 7.213e-06, lambda_0 : 1.686e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3030, Loss: 7.182e-06, Loss_0: 5.510e-08, Loss_r: 7.127e-06, lambda_0 : 2.085e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3040, Loss: 7.030e-06, Loss_0: 2.625e-10, Loss_r: 7.030e-06, lambda_0 : 1.758e+03, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3050, Loss: 6.943e-06, Loss_0: 1.446e-08, Loss_r: 6.929e-06, lambda_0 : 1.531e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3060, Loss: 6.842e-06, Loss_0: 1.462e-08, Loss_r: 6.827e-06, lambda_0 : 1.608e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3070, Loss: 6.728e-06, Loss_0: 9.322e-10, Loss_r: 6.727e-06, lambda_0 : 3.915e+02, Time: 0.33, Learning Rate: 0.00014\n",
            "It: 3080, Loss: 6.630e-06, Loss_0: 1.768e-09, Loss_r: 6.628e-06, lambda_0 : 2.433e+02, Time: 0.33, Learning Rate: 0.00014\n",
            "It: 3090, Loss: 6.532e-06, Loss_0: 2.885e-10, Loss_r: 6.532e-06, lambda_0 : 6.732e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3100, Loss: 6.437e-06, Loss_0: 4.425e-10, Loss_r: 6.437e-06, lambda_0 : 4.781e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3110, Loss: 6.344e-06, Loss_0: 3.034e-10, Loss_r: 6.344e-06, lambda_0 : 6.052e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3120, Loss: 6.253e-06, Loss_0: 2.331e-10, Loss_r: 6.253e-06, lambda_0 : 7.348e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3130, Loss: 6.164e-06, Loss_0: 2.310e-10, Loss_r: 6.164e-06, lambda_0 : 8.751e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3140, Loss: 6.077e-06, Loss_0: 2.260e-10, Loss_r: 6.076e-06, lambda_0 : 7.773e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3150, Loss: 5.991e-06, Loss_0: 2.280e-10, Loss_r: 5.991e-06, lambda_0 : 8.533e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3160, Loss: 5.907e-06, Loss_0: 2.236e-10, Loss_r: 5.907e-06, lambda_0 : 7.758e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3170, Loss: 5.825e-06, Loss_0: 2.227e-10, Loss_r: 5.825e-06, lambda_0 : 7.805e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3180, Loss: 5.744e-06, Loss_0: 2.221e-10, Loss_r: 5.744e-06, lambda_0 : 7.828e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3190, Loss: 5.666e-06, Loss_0: 2.207e-10, Loss_r: 5.665e-06, lambda_0 : 7.685e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3200, Loss: 5.588e-06, Loss_0: 2.192e-10, Loss_r: 5.588e-06, lambda_0 : 7.546e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3210, Loss: 5.513e-06, Loss_0: 2.178e-10, Loss_r: 5.513e-06, lambda_0 : 7.438e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3220, Loss: 5.439e-06, Loss_0: 2.165e-10, Loss_r: 5.439e-06, lambda_0 : 7.343e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3230, Loss: 5.366e-06, Loss_0: 2.150e-10, Loss_r: 5.366e-06, lambda_0 : 7.240e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3240, Loss: 5.295e-06, Loss_0: 2.136e-10, Loss_r: 5.295e-06, lambda_0 : 7.134e+02, Time: 0.33, Learning Rate: 0.00014\n",
            "It: 3250, Loss: 5.226e-06, Loss_0: 2.121e-10, Loss_r: 5.226e-06, lambda_0 : 7.043e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3260, Loss: 5.158e-06, Loss_0: 2.107e-10, Loss_r: 5.158e-06, lambda_0 : 6.962e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3270, Loss: 5.092e-06, Loss_0: 2.092e-10, Loss_r: 5.091e-06, lambda_0 : 6.877e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3280, Loss: 5.027e-06, Loss_0: 2.078e-10, Loss_r: 5.026e-06, lambda_0 : 6.794e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3290, Loss: 4.963e-06, Loss_0: 2.069e-10, Loss_r: 4.963e-06, lambda_0 : 6.797e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3300, Loss: 4.901e-06, Loss_0: 2.929e-10, Loss_r: 4.901e-06, lambda_0 : 4.340e+02, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3310, Loss: 5.112e-06, Loss_0: 2.728e-07, Loss_r: 4.839e-06, lambda_0 : 1.474e+01, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3320, Loss: 4.628e-05, Loss_0: 4.137e-05, Loss_r: 4.903e-06, lambda_0 : 1.399e+01, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3330, Loss: 6.426e-06, Loss_0: 1.098e-06, Loss_r: 5.328e-06, lambda_0 : 6.286e+01, Time: 0.32, Learning Rate: 0.00014\n",
            "It: 3340, Loss: 6.473e-06, Loss_0: 8.857e-07, Loss_r: 5.587e-06, lambda_0 : 5.036e+01, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3350, Loss: 5.713e-06, Loss_0: 4.172e-08, Loss_r: 5.671e-06, lambda_0 : 1.996e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 3360, Loss: 5.714e-06, Loss_0: 5.505e-08, Loss_r: 5.659e-06, lambda_0 : 5.327e+01, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 3370, Loss: 5.994e-06, Loss_0: 3.852e-07, Loss_r: 5.609e-06, lambda_0 : 1.848e+01, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 3380, Loss: 5.544e-06, Loss_0: 1.903e-10, Loss_r: 5.544e-06, lambda_0 : 7.953e+02, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 3390, Loss: 5.518e-06, Loss_0: 4.466e-08, Loss_r: 5.473e-06, lambda_0 : 5.603e+01, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 3400, Loss: 5.412e-06, Loss_0: 1.043e-08, Loss_r: 5.401e-06, lambda_0 : 1.182e+02, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 3410, Loss: 5.331e-06, Loss_0: 1.841e-10, Loss_r: 5.331e-06, lambda_0 : 8.348e+02, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 3420, Loss: 5.263e-06, Loss_0: 9.958e-10, Loss_r: 5.262e-06, lambda_0 : 2.805e+02, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 3430, Loss: 5.202e-06, Loss_0: 4.872e-10, Loss_r: 5.201e-06, lambda_0 : 3.890e+02, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 3440, Loss: 5.142e-06, Loss_0: 3.170e-10, Loss_r: 5.142e-06, lambda_0 : 4.747e+02, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 3450, Loss: 5.084e-06, Loss_0: 2.269e-10, Loss_r: 5.084e-06, lambda_0 : 5.711e+02, Time: 0.33, Learning Rate: 0.00011\n",
            "It: 3460, Loss: 5.027e-06, Loss_0: 1.967e-10, Loss_r: 5.027e-06, lambda_0 : 7.706e+02, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 3470, Loss: 4.971e-06, Loss_0: 2.040e-10, Loss_r: 4.971e-06, lambda_0 : 6.980e+02, Time: 0.33, Learning Rate: 0.00011\n",
            "It: 3480, Loss: 4.917e-06, Loss_0: 1.928e-10, Loss_r: 4.917e-06, lambda_0 : 7.913e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3490, Loss: 4.868e-06, Loss_0: 1.892e-10, Loss_r: 4.868e-06, lambda_0 : 7.048e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3500, Loss: 4.821e-06, Loss_0: 1.892e-10, Loss_r: 4.821e-06, lambda_0 : 7.282e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3510, Loss: 4.774e-06, Loss_0: 1.896e-10, Loss_r: 4.774e-06, lambda_0 : 7.306e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3520, Loss: 4.729e-06, Loss_0: 1.886e-10, Loss_r: 4.728e-06, lambda_0 : 7.059e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3530, Loss: 4.684e-06, Loss_0: 1.880e-10, Loss_r: 4.683e-06, lambda_0 : 6.913e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3540, Loss: 4.639e-06, Loss_0: 1.876e-10, Loss_r: 4.639e-06, lambda_0 : 6.843e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3550, Loss: 4.596e-06, Loss_0: 1.872e-10, Loss_r: 4.596e-06, lambda_0 : 6.771e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3560, Loss: 4.553e-06, Loss_0: 1.866e-10, Loss_r: 4.553e-06, lambda_0 : 6.675e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 3570, Loss: 4.511e-06, Loss_0: 1.859e-10, Loss_r: 4.511e-06, lambda_0 : 6.582e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3580, Loss: 4.470e-06, Loss_0: 1.852e-10, Loss_r: 4.470e-06, lambda_0 : 6.496e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3590, Loss: 4.429e-06, Loss_0: 1.846e-10, Loss_r: 4.429e-06, lambda_0 : 6.410e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 3600, Loss: 4.389e-06, Loss_0: 1.839e-10, Loss_r: 4.389e-06, lambda_0 : 6.327e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3610, Loss: 4.350e-06, Loss_0: 1.832e-10, Loss_r: 4.350e-06, lambda_0 : 6.244e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3620, Loss: 4.312e-06, Loss_0: 1.825e-10, Loss_r: 4.312e-06, lambda_0 : 6.163e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3630, Loss: 4.274e-06, Loss_0: 1.817e-10, Loss_r: 4.274e-06, lambda_0 : 6.089e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3640, Loss: 4.237e-06, Loss_0: 1.810e-10, Loss_r: 4.237e-06, lambda_0 : 6.010e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3650, Loss: 4.201e-06, Loss_0: 1.803e-10, Loss_r: 4.200e-06, lambda_0 : 5.930e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3660, Loss: 4.165e-06, Loss_0: 1.795e-10, Loss_r: 4.165e-06, lambda_0 : 5.857e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3670, Loss: 4.130e-06, Loss_0: 1.788e-10, Loss_r: 4.130e-06, lambda_0 : 5.786e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3680, Loss: 4.095e-06, Loss_0: 1.781e-10, Loss_r: 4.095e-06, lambda_0 : 5.719e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3690, Loss: 4.062e-06, Loss_0: 1.774e-10, Loss_r: 4.061e-06, lambda_0 : 5.650e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3700, Loss: 4.028e-06, Loss_0: 1.766e-10, Loss_r: 4.028e-06, lambda_0 : 5.575e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3710, Loss: 3.996e-06, Loss_0: 1.759e-10, Loss_r: 3.996e-06, lambda_0 : 5.501e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3720, Loss: 3.964e-06, Loss_0: 1.752e-10, Loss_r: 3.964e-06, lambda_0 : 5.424e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3730, Loss: 3.932e-06, Loss_0: 1.745e-10, Loss_r: 3.932e-06, lambda_0 : 5.349e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3740, Loss: 3.901e-06, Loss_0: 1.738e-10, Loss_r: 3.901e-06, lambda_0 : 5.282e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3750, Loss: 3.871e-06, Loss_0: 1.731e-10, Loss_r: 3.871e-06, lambda_0 : 5.211e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3760, Loss: 3.842e-06, Loss_0: 1.725e-10, Loss_r: 3.841e-06, lambda_0 : 5.136e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3770, Loss: 3.812e-06, Loss_0: 1.718e-10, Loss_r: 3.812e-06, lambda_0 : 5.066e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3780, Loss: 3.784e-06, Loss_0: 1.711e-10, Loss_r: 3.784e-06, lambda_0 : 4.996e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3790, Loss: 3.756e-06, Loss_0: 1.705e-10, Loss_r: 3.756e-06, lambda_0 : 4.924e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3800, Loss: 3.728e-06, Loss_0: 1.698e-10, Loss_r: 3.728e-06, lambda_0 : 4.845e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3810, Loss: 3.701e-06, Loss_0: 1.692e-10, Loss_r: 3.701e-06, lambda_0 : 4.777e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3820, Loss: 3.675e-06, Loss_0: 1.686e-10, Loss_r: 3.675e-06, lambda_0 : 4.714e+02, Time: 0.33, Learning Rate: 0.00010\n",
            "It: 3830, Loss: 3.649e-06, Loss_0: 1.679e-10, Loss_r: 3.649e-06, lambda_0 : 4.629e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3840, Loss: 3.624e-06, Loss_0: 1.673e-10, Loss_r: 3.623e-06, lambda_0 : 4.576e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3850, Loss: 3.599e-06, Loss_0: 1.668e-10, Loss_r: 3.598e-06, lambda_0 : 4.511e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3860, Loss: 3.574e-06, Loss_0: 1.662e-10, Loss_r: 3.574e-06, lambda_0 : 4.445e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3870, Loss: 3.550e-06, Loss_0: 1.657e-10, Loss_r: 3.550e-06, lambda_0 : 4.463e+02, Time: 0.33, Learning Rate: 0.00010\n",
            "It: 3880, Loss: 3.527e-06, Loss_0: 1.656e-10, Loss_r: 3.526e-06, lambda_0 : 8.469e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3890, Loss: 3.506e-06, Loss_0: 1.712e-10, Loss_r: 3.505e-06, lambda_0 : 6.694e+03, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3900, Loss: 3.884e-06, Loss_0: 2.703e-10, Loss_r: 3.884e-06, lambda_0 : 6.842e+04, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3910, Loss: 3.788e-06, Loss_0: 8.612e-11, Loss_r: 3.788e-06, lambda_0 : 1.102e+05, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3920, Loss: 3.559e-06, Loss_0: 1.101e-10, Loss_r: 3.559e-06, lambda_0 : 6.105e+04, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3930, Loss: 3.459e-06, Loss_0: 1.300e-10, Loss_r: 3.459e-06, lambda_0 : 2.979e+04, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3940, Loss: 3.411e-06, Loss_0: 1.431e-10, Loss_r: 3.411e-06, lambda_0 : 1.603e+04, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3950, Loss: 3.381e-06, Loss_0: 1.518e-10, Loss_r: 3.381e-06, lambda_0 : 8.302e+03, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 3960, Loss: 3.358e-06, Loss_0: 1.583e-10, Loss_r: 3.358e-06, lambda_0 : 2.260e+03, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 3970, Loss: 3.339e-06, Loss_0: 1.615e-10, Loss_r: 3.338e-06, lambda_0 : 1.475e+03, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 3980, Loss: 3.320e-06, Loss_0: 1.625e-10, Loss_r: 3.320e-06, lambda_0 : 2.479e+03, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 3990, Loss: 3.301e-06, Loss_0: 1.603e-10, Loss_r: 3.301e-06, lambda_0 : 1.180e+03, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4000, Loss: 3.283e-06, Loss_0: 1.579e-10, Loss_r: 3.283e-06, lambda_0 : 5.933e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4010, Loss: 3.265e-06, Loss_0: 1.575e-10, Loss_r: 3.265e-06, lambda_0 : 4.272e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 4020, Loss: 3.247e-06, Loss_0: 1.594e-10, Loss_r: 3.247e-06, lambda_0 : 5.184e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4030, Loss: 3.231e-06, Loss_0: 9.678e-10, Loss_r: 3.230e-06, lambda_0 : 1.032e+02, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4040, Loss: 3.827e-06, Loss_0: 6.136e-07, Loss_r: 3.214e-06, lambda_0 : 5.403e+00, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 4050, Loss: 4.554e-06, Loss_0: 1.353e-06, Loss_r: 3.200e-06, lambda_0 : 1.206e+01, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4060, Loss: 2.578e-05, Loss_0: 2.258e-05, Loss_r: 3.198e-06, lambda_0 : 5.305e+00, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4070, Loss: 8.576e-06, Loss_0: 5.365e-06, Loss_r: 3.211e-06, lambda_0 : 4.739e+00, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 4080, Loss: 3.360e-06, Loss_0: 1.473e-07, Loss_r: 3.213e-06, lambda_0 : 1.571e+01, Time: 0.32, Learning Rate: 0.00010\n",
            "It: 4090, Loss: 3.613e-06, Loss_0: 4.067e-07, Loss_r: 3.206e-06, lambda_0 : 2.552e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4100, Loss: 3.221e-06, Loss_0: 2.599e-08, Loss_r: 3.195e-06, lambda_0 : 7.574e+01, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4110, Loss: 3.268e-06, Loss_0: 8.668e-08, Loss_r: 3.181e-06, lambda_0 : 3.574e+01, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4120, Loss: 3.189e-06, Loss_0: 2.276e-08, Loss_r: 3.167e-06, lambda_0 : 5.369e+01, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4130, Loss: 3.154e-06, Loss_0: 2.174e-09, Loss_r: 3.152e-06, lambda_0 : 1.256e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4140, Loss: 3.137e-06, Loss_0: 2.274e-10, Loss_r: 3.137e-06, lambda_0 : 3.310e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4150, Loss: 3.123e-06, Loss_0: 1.739e-10, Loss_r: 3.123e-06, lambda_0 : 3.784e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4160, Loss: 3.109e-06, Loss_0: 2.419e-10, Loss_r: 3.108e-06, lambda_0 : 2.616e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4170, Loss: 3.095e-06, Loss_0: 3.080e-10, Loss_r: 3.094e-06, lambda_0 : 2.091e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4180, Loss: 3.081e-06, Loss_0: 2.327e-10, Loss_r: 3.080e-06, lambda_0 : 2.418e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4190, Loss: 3.067e-06, Loss_0: 1.498e-10, Loss_r: 3.067e-06, lambda_0 : 3.979e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4200, Loss: 3.053e-06, Loss_0: 1.567e-10, Loss_r: 3.053e-06, lambda_0 : 3.054e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4210, Loss: 3.040e-06, Loss_0: 1.506e-10, Loss_r: 3.040e-06, lambda_0 : 3.975e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4220, Loss: 3.027e-06, Loss_0: 1.490e-10, Loss_r: 3.027e-06, lambda_0 : 3.791e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4230, Loss: 3.014e-06, Loss_0: 1.478e-10, Loss_r: 3.014e-06, lambda_0 : 3.707e+02, Time: 0.33, Learning Rate: 0.00009\n",
            "It: 4240, Loss: 3.001e-06, Loss_0: 1.478e-10, Loss_r: 3.001e-06, lambda_0 : 3.753e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4250, Loss: 2.989e-06, Loss_0: 1.478e-10, Loss_r: 2.989e-06, lambda_0 : 3.624e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4260, Loss: 2.976e-06, Loss_0: 1.470e-10, Loss_r: 2.976e-06, lambda_0 : 3.830e+02, Time: 0.33, Learning Rate: 0.00009\n",
            "It: 4270, Loss: 2.964e-06, Loss_0: 1.464e-10, Loss_r: 2.964e-06, lambda_0 : 3.638e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4280, Loss: 2.952e-06, Loss_0: 1.464e-10, Loss_r: 2.952e-06, lambda_0 : 2.918e+02, Time: 0.33, Learning Rate: 0.00009\n",
            "It: 4290, Loss: 2.941e-06, Loss_0: 3.964e-10, Loss_r: 2.940e-06, lambda_0 : 1.011e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4300, Loss: 3.058e-06, Loss_0: 1.294e-07, Loss_r: 2.929e-06, lambda_0 : 1.707e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4310, Loss: 6.169e-05, Loss_0: 5.875e-05, Loss_r: 2.933e-06, lambda_0 : 1.724e+00, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4320, Loss: 8.798e-06, Loss_0: 5.833e-06, Loss_r: 2.965e-06, lambda_0 : 4.047e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4330, Loss: 7.387e-06, Loss_0: 4.054e-06, Loss_r: 3.333e-06, lambda_0 : 4.342e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 4340, Loss: 5.922e-06, Loss_0: 2.676e-06, Loss_r: 3.246e-06, lambda_0 : 4.397e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 4350, Loss: 3.975e-06, Loss_0: 9.231e-07, Loss_r: 3.052e-06, lambda_0 : 3.092e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 4360, Loss: 3.031e-06, Loss_0: 2.689e-08, Loss_r: 3.004e-06, lambda_0 : 1.069e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 4370, Loss: 3.091e-06, Loss_0: 9.179e-08, Loss_r: 2.999e-06, lambda_0 : 4.148e+02, Time: 0.31, Learning Rate: 0.00008\n",
            "It: 4380, Loss: 3.007e-06, Loss_0: 2.344e-08, Loss_r: 2.983e-06, lambda_0 : 3.482e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 4390, Loss: 2.971e-06, Loss_0: 1.114e-09, Loss_r: 2.970e-06, lambda_0 : 4.338e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 4400, Loss: 2.963e-06, Loss_0: 4.648e-09, Loss_r: 2.958e-06, lambda_0 : 5.161e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 4410, Loss: 2.947e-06, Loss_0: 1.767e-09, Loss_r: 2.946e-06, lambda_0 : 2.282e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4420, Loss: 2.936e-06, Loss_0: 6.913e-10, Loss_r: 2.935e-06, lambda_0 : 2.114e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4430, Loss: 2.925e-06, Loss_0: 1.570e-10, Loss_r: 2.925e-06, lambda_0 : 2.532e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4440, Loss: 2.914e-06, Loss_0: 1.711e-10, Loss_r: 2.914e-06, lambda_0 : 4.693e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4450, Loss: 2.904e-06, Loss_0: 1.581e-10, Loss_r: 2.904e-06, lambda_0 : 2.544e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4460, Loss: 2.894e-06, Loss_0: 1.394e-10, Loss_r: 2.894e-06, lambda_0 : 5.385e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4470, Loss: 2.884e-06, Loss_0: 1.444e-10, Loss_r: 2.884e-06, lambda_0 : 3.871e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4480, Loss: 2.874e-06, Loss_0: 1.390e-10, Loss_r: 2.874e-06, lambda_0 : 4.493e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4490, Loss: 2.864e-06, Loss_0: 1.382e-10, Loss_r: 2.864e-06, lambda_0 : 4.001e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4500, Loss: 2.855e-06, Loss_0: 1.384e-10, Loss_r: 2.855e-06, lambda_0 : 4.124e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4510, Loss: 2.845e-06, Loss_0: 1.383e-10, Loss_r: 2.845e-06, lambda_0 : 4.105e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4520, Loss: 2.836e-06, Loss_0: 1.377e-10, Loss_r: 2.836e-06, lambda_0 : 3.942e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4530, Loss: 2.827e-06, Loss_0: 1.376e-10, Loss_r: 2.826e-06, lambda_0 : 3.956e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4540, Loss: 2.817e-06, Loss_0: 1.374e-10, Loss_r: 2.817e-06, lambda_0 : 3.955e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4550, Loss: 2.808e-06, Loss_0: 1.370e-10, Loss_r: 2.808e-06, lambda_0 : 3.871e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4560, Loss: 2.799e-06, Loss_0: 1.368e-10, Loss_r: 2.799e-06, lambda_0 : 3.839e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4570, Loss: 2.790e-06, Loss_0: 1.365e-10, Loss_r: 2.790e-06, lambda_0 : 3.810e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4580, Loss: 2.781e-06, Loss_0: 1.363e-10, Loss_r: 2.781e-06, lambda_0 : 3.793e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4590, Loss: 2.773e-06, Loss_0: 1.360e-10, Loss_r: 2.772e-06, lambda_0 : 3.741e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4600, Loss: 2.764e-06, Loss_0: 1.357e-10, Loss_r: 2.764e-06, lambda_0 : 3.690e+02, Time: 0.33, Learning Rate: 0.00007\n",
            "It: 4610, Loss: 2.755e-06, Loss_0: 1.354e-10, Loss_r: 2.755e-06, lambda_0 : 3.655e+02, Time: 0.33, Learning Rate: 0.00007\n",
            "It: 4620, Loss: 2.747e-06, Loss_0: 1.351e-10, Loss_r: 2.747e-06, lambda_0 : 3.642e+02, Time: 0.33, Learning Rate: 0.00007\n",
            "It: 4630, Loss: 2.739e-06, Loss_0: 1.349e-10, Loss_r: 2.738e-06, lambda_0 : 3.659e+02, Time: 0.39, Learning Rate: 0.00007\n",
            "It: 4640, Loss: 2.730e-06, Loss_0: 1.346e-10, Loss_r: 2.730e-06, lambda_0 : 3.551e+02, Time: 0.36, Learning Rate: 0.00007\n",
            "It: 4650, Loss: 2.722e-06, Loss_0: 1.343e-10, Loss_r: 2.722e-06, lambda_0 : 3.535e+02, Time: 0.34, Learning Rate: 0.00007\n",
            "It: 4660, Loss: 2.714e-06, Loss_0: 1.340e-10, Loss_r: 2.714e-06, lambda_0 : 3.531e+02, Time: 0.38, Learning Rate: 0.00007\n",
            "It: 4670, Loss: 2.706e-06, Loss_0: 1.337e-10, Loss_r: 2.706e-06, lambda_0 : 3.487e+02, Time: 0.35, Learning Rate: 0.00007\n",
            "It: 4680, Loss: 2.698e-06, Loss_0: 1.334e-10, Loss_r: 2.698e-06, lambda_0 : 3.459e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4690, Loss: 2.690e-06, Loss_0: 1.332e-10, Loss_r: 2.690e-06, lambda_0 : 3.468e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4700, Loss: 2.682e-06, Loss_0: 1.328e-10, Loss_r: 2.682e-06, lambda_0 : 3.398e+02, Time: 0.34, Learning Rate: 0.00007\n",
            "It: 4710, Loss: 2.674e-06, Loss_0: 1.326e-10, Loss_r: 2.674e-06, lambda_0 : 3.297e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4720, Loss: 2.667e-06, Loss_0: 1.323e-10, Loss_r: 2.667e-06, lambda_0 : 3.275e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4730, Loss: 2.659e-06, Loss_0: 1.320e-10, Loss_r: 2.659e-06, lambda_0 : 3.292e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4740, Loss: 2.651e-06, Loss_0: 1.317e-10, Loss_r: 2.651e-06, lambda_0 : 3.186e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4750, Loss: 2.644e-06, Loss_0: 1.314e-10, Loss_r: 2.644e-06, lambda_0 : 3.056e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4760, Loss: 2.637e-06, Loss_0: 1.311e-10, Loss_r: 2.636e-06, lambda_0 : 3.079e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4770, Loss: 2.629e-06, Loss_0: 1.308e-10, Loss_r: 2.629e-06, lambda_0 : 3.171e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4780, Loss: 2.622e-06, Loss_0: 1.305e-10, Loss_r: 2.622e-06, lambda_0 : 2.993e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4790, Loss: 2.615e-06, Loss_0: 1.302e-10, Loss_r: 2.615e-06, lambda_0 : 2.961e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4800, Loss: 2.608e-06, Loss_0: 1.299e-10, Loss_r: 2.607e-06, lambda_0 : 2.991e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4810, Loss: 2.601e-06, Loss_0: 1.296e-10, Loss_r: 2.600e-06, lambda_0 : 2.872e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4820, Loss: 2.593e-06, Loss_0: 1.293e-10, Loss_r: 2.593e-06, lambda_0 : 2.795e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4830, Loss: 2.586e-06, Loss_0: 1.290e-10, Loss_r: 2.586e-06, lambda_0 : 2.860e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4840, Loss: 2.580e-06, Loss_0: 1.287e-10, Loss_r: 2.579e-06, lambda_0 : 2.615e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4850, Loss: 2.573e-06, Loss_0: 1.284e-10, Loss_r: 2.573e-06, lambda_0 : 2.518e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4860, Loss: 2.566e-06, Loss_0: 1.281e-10, Loss_r: 2.566e-06, lambda_0 : 2.412e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4870, Loss: 2.559e-06, Loss_0: 1.278e-10, Loss_r: 2.559e-06, lambda_0 : 2.528e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4880, Loss: 2.552e-06, Loss_0: 1.275e-10, Loss_r: 2.552e-06, lambda_0 : 2.291e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4890, Loss: 2.546e-06, Loss_0: 1.269e-10, Loss_r: 2.546e-06, lambda_0 : 2.734e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4900, Loss: 2.539e-06, Loss_0: 1.243e-10, Loss_r: 2.539e-06, lambda_0 : 2.599e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4910, Loss: 2.612e-06, Loss_0: 9.023e-11, Loss_r: 2.612e-06, lambda_0 : 5.499e+04, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4920, Loss: 2.657e-06, Loss_0: 3.720e-10, Loss_r: 2.656e-06, lambda_0 : 2.432e+04, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4930, Loss: 2.565e-06, Loss_0: 2.389e-08, Loss_r: 2.541e-06, lambda_0 : 1.349e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4940, Loss: 9.806e-06, Loss_0: 7.282e-06, Loss_r: 2.524e-06, lambda_0 : 5.990e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 4950, Loss: 1.527e-05, Loss_0: 1.274e-05, Loss_r: 2.535e-06, lambda_0 : 3.830e+01, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 4960, Loss: 4.630e-06, Loss_0: 2.092e-06, Loss_r: 2.538e-06, lambda_0 : 6.537e+01, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4970, Loss: 2.918e-06, Loss_0: 3.829e-07, Loss_r: 2.535e-06, lambda_0 : 5.456e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4980, Loss: 2.562e-06, Loss_0: 2.967e-08, Loss_r: 2.532e-06, lambda_0 : 7.082e+01, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4990, Loss: 2.527e-06, Loss_0: 1.193e-10, Loss_r: 2.527e-06, lambda_0 : 1.857e+03, Time: 0.32, Learning Rate: 0.00006\n",
            "Training time: 162.3163\n",
            "[1, 256, 256, 256, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 4.606e-03, Loss_0: 2.061e-03, Loss_r: 2.545e-03, lambda_0 : 7.289e+01, Time: 1.16, Learning Rate: 0.00100\n",
            "It: 10, Loss: 7.501e-04, Loss_0: 3.603e-04, Loss_r: 3.898e-04, lambda_0 : 1.902e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 20, Loss: 2.943e-04, Loss_0: 9.920e-05, Loss_r: 1.951e-04, lambda_0 : 2.173e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 30, Loss: 9.784e-05, Loss_0: 1.574e-06, Loss_r: 9.627e-05, lambda_0 : 8.713e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 40, Loss: 9.935e-05, Loss_0: 3.524e-05, Loss_r: 6.410e-05, lambda_0 : 8.098e+00, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 50, Loss: 7.482e-05, Loss_0: 1.645e-05, Loss_r: 5.837e-05, lambda_0 : 4.862e+00, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.923e-05, Loss_0: 1.120e-06, Loss_r: 5.811e-05, lambda_0 : 3.772e+00, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.888e-05, Loss_0: 3.580e-07, Loss_r: 5.852e-05, lambda_0 : 1.573e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.883e-05, Loss_0: 4.504e-07, Loss_r: 5.838e-05, lambda_0 : 9.750e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.840e-05, Loss_0: 1.631e-07, Loss_r: 5.824e-05, lambda_0 : 1.571e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.812e-05, Loss_0: 4.737e-08, Loss_r: 5.807e-05, lambda_0 : 1.720e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.795e-05, Loss_0: 4.425e-09, Loss_r: 5.795e-05, lambda_0 : 5.469e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.783e-05, Loss_0: 6.543e-10, Loss_r: 5.783e-05, lambda_0 : 1.170e+02, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.772e-05, Loss_0: 8.362e-09, Loss_r: 5.771e-05, lambda_0 : 3.701e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.762e-05, Loss_0: 1.431e-08, Loss_r: 5.760e-05, lambda_0 : 2.661e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.751e-05, Loss_0: 1.565e-08, Loss_r: 5.749e-05, lambda_0 : 2.552e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.739e-05, Loss_0: 1.279e-08, Loss_r: 5.738e-05, lambda_0 : 2.808e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.728e-05, Loss_0: 1.005e-08, Loss_r: 5.727e-05, lambda_0 : 3.244e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.715e-05, Loss_0: 9.339e-09, Loss_r: 5.714e-05, lambda_0 : 3.370e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.702e-05, Loss_0: 9.962e-09, Loss_r: 5.701e-05, lambda_0 : 3.253e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.688e-05, Loss_0: 1.062e-08, Loss_r: 5.687e-05, lambda_0 : 3.155e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.674e-05, Loss_0: 1.046e-08, Loss_r: 5.673e-05, lambda_0 : 3.172e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.659e-05, Loss_0: 1.000e-08, Loss_r: 5.658e-05, lambda_0 : 3.043e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 230, Loss: 6.107e-05, Loss_0: 4.579e-06, Loss_r: 5.649e-05, lambda_0 : 1.283e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.646e-05, Loss_0: 7.451e-08, Loss_r: 5.638e-05, lambda_0 : 1.151e+01, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.878e-05, Loss_0: 2.329e-06, Loss_r: 5.645e-05, lambda_0 : 1.886e+00, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.833e-05, Loss_0: 2.025e-06, Loss_r: 5.630e-05, lambda_0 : 1.970e+00, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.627e-05, Loss_0: 2.731e-08, Loss_r: 5.624e-05, lambda_0 : 1.736e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.652e-05, Loss_0: 4.402e-07, Loss_r: 5.608e-05, lambda_0 : 4.004e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.596e-05, Loss_0: 1.470e-08, Loss_r: 5.595e-05, lambda_0 : 2.305e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 300, Loss: 7.955e-05, Loss_0: 2.361e-05, Loss_r: 5.594e-05, lambda_0 : 5.630e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 310, Loss: 6.067e-05, Loss_0: 4.876e-06, Loss_r: 5.580e-05, lambda_0 : 1.333e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.571e-05, Loss_0: 1.002e-07, Loss_r: 5.561e-05, lambda_0 : 8.434e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.612e-05, Loss_0: 6.700e-07, Loss_r: 5.545e-05, lambda_0 : 3.418e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.554e-05, Loss_0: 2.239e-07, Loss_r: 5.531e-05, lambda_0 : 7.521e+00, Time: 0.80, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.512e-05, Loss_0: 1.778e-08, Loss_r: 5.511e-05, lambda_0 : 2.175e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 360, Loss: 1.063e-04, Loss_0: 5.156e-05, Loss_r: 5.472e-05, lambda_0 : 3.871e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 370, Loss: 6.698e-05, Loss_0: 1.205e-05, Loss_r: 5.493e-05, lambda_0 : 8.517e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.862e-05, Loss_0: 3.527e-06, Loss_r: 5.509e-05, lambda_0 : 1.745e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.562e-05, Loss_0: 6.642e-07, Loss_r: 5.495e-05, lambda_0 : 4.426e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.478e-05, Loss_0: 1.714e-08, Loss_r: 5.476e-05, lambda_0 : 3.144e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.478e-05, Loss_0: 1.879e-07, Loss_r: 5.460e-05, lambda_0 : 7.303e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.566e-05, Loss_0: 1.325e-06, Loss_r: 5.433e-05, lambda_0 : 2.368e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.419e-05, Loss_0: 4.323e-08, Loss_r: 5.414e-05, lambda_0 : 1.381e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 440, Loss: 1.658e-04, Loss_0: 1.121e-04, Loss_r: 5.367e-05, lambda_0 : 3.670e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.820e-05, Loss_0: 3.664e-06, Loss_r: 5.453e-05, lambda_0 : 1.558e+00, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.545e-05, Loss_0: 5.382e-07, Loss_r: 5.491e-05, lambda_0 : 3.989e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.724e-05, Loss_0: 2.199e-06, Loss_r: 5.504e-05, lambda_0 : 2.289e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.515e-05, Loss_0: 2.919e-07, Loss_r: 5.486e-05, lambda_0 : 5.673e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.473e-05, Loss_0: 3.265e-08, Loss_r: 5.469e-05, lambda_0 : 1.699e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.449e-05, Loss_0: 2.084e-08, Loss_r: 5.447e-05, lambda_0 : 2.067e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.425e-05, Loss_0: 1.670e-08, Loss_r: 5.423e-05, lambda_0 : 2.067e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.402e-05, Loss_0: 4.999e-08, Loss_r: 5.397e-05, lambda_0 : 1.226e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.370e-05, Loss_0: 2.293e-08, Loss_r: 5.368e-05, lambda_0 : 1.665e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 540, Loss: 6.338e-05, Loss_0: 9.927e-06, Loss_r: 5.345e-05, lambda_0 : 8.825e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.518e-05, Loss_0: 1.957e-06, Loss_r: 5.322e-05, lambda_0 : 1.923e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.374e-05, Loss_0: 6.933e-07, Loss_r: 5.305e-05, lambda_0 : 3.327e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.305e-05, Loss_0: 2.855e-07, Loss_r: 5.276e-05, lambda_0 : 5.193e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.250e-05, Loss_0: 8.149e-08, Loss_r: 5.242e-05, lambda_0 : 9.502e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.206e-05, Loss_0: 1.505e-08, Loss_r: 5.204e-05, lambda_0 : 2.333e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 600, Loss: 1.322e-03, Loss_0: 1.269e-03, Loss_r: 5.267e-05, lambda_0 : 6.247e-01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 610, Loss: 3.096e-04, Loss_0: 2.499e-04, Loss_r: 5.974e-05, lambda_0 : 4.882e+00, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 620, Loss: 1.538e-04, Loss_0: 9.214e-05, Loss_r: 6.168e-05, lambda_0 : 3.720e+00, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 630, Loss: 6.961e-05, Loss_0: 8.564e-06, Loss_r: 6.104e-05, lambda_0 : 1.501e+01, Time: 0.79, Learning Rate: 0.00100\n",
            "It: 640, Loss: 6.271e-05, Loss_0: 2.182e-06, Loss_r: 6.053e-05, lambda_0 : 3.319e+01, Time: 0.78, Learning Rate: 0.00100\n",
            "It: 650, Loss: 6.260e-05, Loss_0: 2.137e-06, Loss_r: 6.046e-05, lambda_0 : 1.100e+01, Time: 0.78, Learning Rate: 0.00090\n",
            "It: 660, Loss: 6.145e-05, Loss_0: 1.263e-06, Loss_r: 6.018e-05, lambda_0 : 2.953e+00, Time: 0.78, Learning Rate: 0.00090\n",
            "It: 670, Loss: 6.028e-05, Loss_0: 3.643e-08, Loss_r: 6.024e-05, lambda_0 : 5.420e+01, Time: 0.79, Learning Rate: 0.00090\n",
            "It: 680, Loss: 6.028e-05, Loss_0: 1.773e-07, Loss_r: 6.011e-05, lambda_0 : 1.124e+01, Time: 0.78, Learning Rate: 0.00090\n",
            "It: 690, Loss: 6.010e-05, Loss_0: 1.527e-08, Loss_r: 6.009e-05, lambda_0 : 2.418e+01, Time: 0.78, Learning Rate: 0.00090\n",
            "It: 700, Loss: 6.005e-05, Loss_0: 1.360e-08, Loss_r: 6.003e-05, lambda_0 : 2.610e+01, Time: 0.79, Learning Rate: 0.00090\n",
            "It: 710, Loss: 6.000e-05, Loss_0: 1.912e-08, Loss_r: 5.998e-05, lambda_0 : 2.348e+01, Time: 0.79, Learning Rate: 0.00081\n",
            "It: 720, Loss: 5.995e-05, Loss_0: 8.338e-09, Loss_r: 5.994e-05, lambda_0 : 4.723e+01, Time: 0.79, Learning Rate: 0.00081\n",
            "It: 730, Loss: 5.990e-05, Loss_0: 1.227e-08, Loss_r: 5.989e-05, lambda_0 : 3.708e+01, Time: 0.79, Learning Rate: 0.00081\n",
            "It: 740, Loss: 5.986e-05, Loss_0: 9.184e-09, Loss_r: 5.985e-05, lambda_0 : 4.349e+01, Time: 0.78, Learning Rate: 0.00081\n",
            "It: 750, Loss: 5.981e-05, Loss_0: 1.122e-08, Loss_r: 5.980e-05, lambda_0 : 3.710e+01, Time: 0.78, Learning Rate: 0.00081\n",
            "It: 760, Loss: 5.977e-05, Loss_0: 1.017e-08, Loss_r: 5.976e-05, lambda_0 : 4.198e+01, Time: 0.78, Learning Rate: 0.00081\n",
            "It: 770, Loss: 5.972e-05, Loss_0: 1.018e-08, Loss_r: 5.971e-05, lambda_0 : 4.271e+01, Time: 0.78, Learning Rate: 0.00073\n",
            "It: 780, Loss: 5.968e-05, Loss_0: 1.043e-08, Loss_r: 5.967e-05, lambda_0 : 4.099e+01, Time: 0.79, Learning Rate: 0.00073\n",
            "It: 790, Loss: 5.964e-05, Loss_0: 1.028e-08, Loss_r: 5.963e-05, lambda_0 : 4.073e+01, Time: 0.78, Learning Rate: 0.00073\n",
            "It: 800, Loss: 5.959e-05, Loss_0: 1.035e-08, Loss_r: 5.958e-05, lambda_0 : 4.076e+01, Time: 0.78, Learning Rate: 0.00073\n",
            "It: 810, Loss: 5.955e-05, Loss_0: 1.030e-08, Loss_r: 5.954e-05, lambda_0 : 4.096e+01, Time: 0.78, Learning Rate: 0.00073\n",
            "It: 820, Loss: 5.951e-05, Loss_0: 1.029e-08, Loss_r: 5.950e-05, lambda_0 : 4.079e+01, Time: 0.78, Learning Rate: 0.00073\n",
            "It: 830, Loss: 5.946e-05, Loss_0: 1.032e-08, Loss_r: 5.945e-05, lambda_0 : 4.052e+01, Time: 0.78, Learning Rate: 0.00066\n",
            "It: 840, Loss: 5.942e-05, Loss_0: 1.030e-08, Loss_r: 5.941e-05, lambda_0 : 4.044e+01, Time: 0.78, Learning Rate: 0.00066\n",
            "It: 850, Loss: 5.938e-05, Loss_0: 1.030e-08, Loss_r: 5.937e-05, lambda_0 : 4.035e+01, Time: 0.78, Learning Rate: 0.00066\n",
            "It: 860, Loss: 5.934e-05, Loss_0: 1.029e-08, Loss_r: 5.933e-05, lambda_0 : 4.024e+01, Time: 0.78, Learning Rate: 0.00066\n",
            "It: 870, Loss: 5.930e-05, Loss_0: 1.029e-08, Loss_r: 5.929e-05, lambda_0 : 4.012e+01, Time: 0.79, Learning Rate: 0.00066\n",
            "It: 880, Loss: 5.926e-05, Loss_0: 1.028e-08, Loss_r: 5.924e-05, lambda_0 : 4.013e+01, Time: 0.79, Learning Rate: 0.00066\n",
            "It: 890, Loss: 5.921e-05, Loss_0: 1.028e-08, Loss_r: 5.920e-05, lambda_0 : 4.056e+01, Time: 0.78, Learning Rate: 0.00059\n",
            "It: 900, Loss: 5.917e-05, Loss_0: 1.028e-08, Loss_r: 5.916e-05, lambda_0 : 4.097e+01, Time: 0.78, Learning Rate: 0.00059\n",
            "It: 910, Loss: 5.913e-05, Loss_0: 1.027e-08, Loss_r: 5.912e-05, lambda_0 : 4.140e+01, Time: 0.79, Learning Rate: 0.00059\n",
            "It: 920, Loss: 5.909e-05, Loss_0: 1.027e-08, Loss_r: 5.908e-05, lambda_0 : 4.185e+01, Time: 0.78, Learning Rate: 0.00059\n",
            "It: 930, Loss: 5.904e-05, Loss_0: 1.026e-08, Loss_r: 5.903e-05, lambda_0 : 4.231e+01, Time: 0.78, Learning Rate: 0.00059\n",
            "It: 940, Loss: 5.900e-05, Loss_0: 1.026e-08, Loss_r: 5.899e-05, lambda_0 : 4.280e+01, Time: 0.78, Learning Rate: 0.00059\n",
            "It: 950, Loss: 5.895e-05, Loss_0: 1.025e-08, Loss_r: 5.894e-05, lambda_0 : 4.330e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 960, Loss: 5.891e-05, Loss_0: 1.025e-08, Loss_r: 5.890e-05, lambda_0 : 4.378e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 970, Loss: 5.886e-05, Loss_0: 1.024e-08, Loss_r: 5.885e-05, lambda_0 : 4.427e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 980, Loss: 5.882e-05, Loss_0: 1.023e-08, Loss_r: 5.881e-05, lambda_0 : 4.479e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 990, Loss: 5.877e-05, Loss_0: 1.023e-08, Loss_r: 5.876e-05, lambda_0 : 4.532e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 1000, Loss: 5.872e-05, Loss_0: 1.022e-08, Loss_r: 5.871e-05, lambda_0 : 4.588e+01, Time: 0.78, Learning Rate: 0.00053\n",
            "It: 1010, Loss: 5.867e-05, Loss_0: 1.021e-08, Loss_r: 5.866e-05, lambda_0 : 4.645e+01, Time: 0.78, Learning Rate: 0.00048\n",
            "It: 1020, Loss: 5.863e-05, Loss_0: 1.021e-08, Loss_r: 5.862e-05, lambda_0 : 4.699e+01, Time: 0.79, Learning Rate: 0.00048\n",
            "It: 1030, Loss: 5.858e-05, Loss_0: 1.020e-08, Loss_r: 5.857e-05, lambda_0 : 4.754e+01, Time: 0.79, Learning Rate: 0.00048\n",
            "It: 1040, Loss: 5.853e-05, Loss_0: 1.019e-08, Loss_r: 5.852e-05, lambda_0 : 4.811e+01, Time: 0.79, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 5.848e-05, Loss_0: 1.018e-08, Loss_r: 5.847e-05, lambda_0 : 4.871e+01, Time: 0.78, Learning Rate: 0.00048\n",
            "It: 1060, Loss: 5.843e-05, Loss_0: 1.017e-08, Loss_r: 5.842e-05, lambda_0 : 4.932e+01, Time: 0.78, Learning Rate: 0.00048\n",
            "It: 1070, Loss: 5.837e-05, Loss_0: 1.017e-08, Loss_r: 5.836e-05, lambda_0 : 4.996e+01, Time: 0.78, Learning Rate: 0.00043\n",
            "It: 1080, Loss: 5.832e-05, Loss_0: 1.016e-08, Loss_r: 5.831e-05, lambda_0 : 5.056e+01, Time: 0.78, Learning Rate: 0.00043\n",
            "It: 1090, Loss: 5.826e-05, Loss_0: 1.015e-08, Loss_r: 5.825e-05, lambda_0 : 5.118e+01, Time: 0.79, Learning Rate: 0.00043\n",
            "It: 1100, Loss: 5.821e-05, Loss_0: 1.013e-08, Loss_r: 5.820e-05, lambda_0 : 5.182e+01, Time: 0.79, Learning Rate: 0.00043\n",
            "It: 1110, Loss: 5.815e-05, Loss_0: 1.012e-08, Loss_r: 5.814e-05, lambda_0 : 5.247e+01, Time: 0.78, Learning Rate: 0.00043\n",
            "It: 1120, Loss: 5.809e-05, Loss_0: 1.011e-08, Loss_r: 5.808e-05, lambda_0 : 5.315e+01, Time: 0.79, Learning Rate: 0.00043\n",
            "It: 1130, Loss: 5.802e-05, Loss_0: 1.010e-08, Loss_r: 5.801e-05, lambda_0 : 5.385e+01, Time: 0.79, Learning Rate: 0.00039\n",
            "It: 1140, Loss: 5.797e-05, Loss_0: 1.008e-08, Loss_r: 5.796e-05, lambda_0 : 5.451e+01, Time: 0.78, Learning Rate: 0.00039\n",
            "It: 1150, Loss: 5.790e-05, Loss_0: 1.007e-08, Loss_r: 5.789e-05, lambda_0 : 5.518e+01, Time: 0.78, Learning Rate: 0.00039\n",
            "It: 1160, Loss: 5.784e-05, Loss_0: 1.005e-08, Loss_r: 5.783e-05, lambda_0 : 5.588e+01, Time: 0.78, Learning Rate: 0.00039\n",
            "It: 1170, Loss: 5.777e-05, Loss_0: 1.004e-08, Loss_r: 5.776e-05, lambda_0 : 5.659e+01, Time: 0.79, Learning Rate: 0.00039\n",
            "It: 1180, Loss: 5.770e-05, Loss_0: 1.002e-08, Loss_r: 5.769e-05, lambda_0 : 5.732e+01, Time: 0.79, Learning Rate: 0.00039\n",
            "It: 1190, Loss: 5.762e-05, Loss_0: 9.998e-09, Loss_r: 5.761e-05, lambda_0 : 5.808e+01, Time: 0.79, Learning Rate: 0.00035\n",
            "It: 1200, Loss: 5.755e-05, Loss_0: 9.979e-09, Loss_r: 5.754e-05, lambda_0 : 5.878e+01, Time: 0.79, Learning Rate: 0.00035\n",
            "It: 1210, Loss: 5.748e-05, Loss_0: 9.958e-09, Loss_r: 5.747e-05, lambda_0 : 5.949e+01, Time: 0.79, Learning Rate: 0.00035\n",
            "It: 1220, Loss: 5.740e-05, Loss_0: 9.935e-09, Loss_r: 5.739e-05, lambda_0 : 6.022e+01, Time: 0.78, Learning Rate: 0.00035\n",
            "It: 1230, Loss: 5.732e-05, Loss_0: 9.911e-09, Loss_r: 5.731e-05, lambda_0 : 6.097e+01, Time: 0.79, Learning Rate: 0.00035\n",
            "It: 1240, Loss: 5.723e-05, Loss_0: 9.884e-09, Loss_r: 5.722e-05, lambda_0 : 6.174e+01, Time: 0.79, Learning Rate: 0.00035\n",
            "It: 1250, Loss: 5.714e-05, Loss_0: 9.855e-09, Loss_r: 5.713e-05, lambda_0 : 6.449e+01, Time: 0.79, Learning Rate: 0.00031\n",
            "It: 1260, Loss: 5.705e-05, Loss_0: 9.827e-09, Loss_r: 5.704e-05, lambda_0 : 6.859e+01, Time: 0.78, Learning Rate: 0.00031\n",
            "It: 1270, Loss: 5.696e-05, Loss_0: 9.797e-09, Loss_r: 5.695e-05, lambda_0 : 7.307e+01, Time: 0.79, Learning Rate: 0.00031\n",
            "It: 1280, Loss: 5.686e-05, Loss_0: 9.763e-09, Loss_r: 5.685e-05, lambda_0 : 7.793e+01, Time: 0.78, Learning Rate: 0.00031\n",
            "It: 1290, Loss: 5.676e-05, Loss_0: 9.727e-09, Loss_r: 5.675e-05, lambda_0 : 8.321e+01, Time: 0.79, Learning Rate: 0.00031\n",
            "It: 1300, Loss: 5.666e-05, Loss_0: 9.688e-09, Loss_r: 5.665e-05, lambda_0 : 8.895e+01, Time: 0.79, Learning Rate: 0.00031\n",
            "It: 1310, Loss: 5.654e-05, Loss_0: 9.646e-09, Loss_r: 5.653e-05, lambda_0 : 9.520e+01, Time: 0.79, Learning Rate: 0.00028\n",
            "It: 1320, Loss: 5.643e-05, Loss_0: 9.604e-09, Loss_r: 5.642e-05, lambda_0 : 1.013e+02, Time: 0.78, Learning Rate: 0.00028\n",
            "It: 1330, Loss: 5.632e-05, Loss_0: 9.559e-09, Loss_r: 5.631e-05, lambda_0 : 1.079e+02, Time: 0.79, Learning Rate: 0.00028\n",
            "It: 1340, Loss: 5.620e-05, Loss_0: 9.511e-09, Loss_r: 5.619e-05, lambda_0 : 1.150e+02, Time: 0.80, Learning Rate: 0.00028\n",
            "It: 1350, Loss: 5.607e-05, Loss_0: 9.458e-09, Loss_r: 5.606e-05, lambda_0 : 1.227e+02, Time: 0.79, Learning Rate: 0.00028\n",
            "It: 1360, Loss: 5.594e-05, Loss_0: 9.401e-09, Loss_r: 5.593e-05, lambda_0 : 1.310e+02, Time: 0.79, Learning Rate: 0.00028\n",
            "It: 1370, Loss: 5.579e-05, Loss_0: 9.338e-09, Loss_r: 5.578e-05, lambda_0 : 1.400e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1380, Loss: 5.566e-05, Loss_0: 9.278e-09, Loss_r: 5.565e-05, lambda_0 : 1.487e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1390, Loss: 5.551e-05, Loss_0: 9.212e-09, Loss_r: 5.550e-05, lambda_0 : 1.581e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1400, Loss: 5.536e-05, Loss_0: 9.141e-09, Loss_r: 5.535e-05, lambda_0 : 1.682e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1410, Loss: 5.520e-05, Loss_0: 9.064e-09, Loss_r: 5.519e-05, lambda_0 : 1.789e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1420, Loss: 5.503e-05, Loss_0: 8.981e-09, Loss_r: 5.502e-05, lambda_0 : 1.905e+02, Time: 0.79, Learning Rate: 0.00025\n",
            "It: 1430, Loss: 5.484e-05, Loss_0: 8.892e-09, Loss_r: 5.483e-05, lambda_0 : 2.028e+02, Time: 0.79, Learning Rate: 0.00023\n",
            "It: 1440, Loss: 5.467e-05, Loss_0: 8.805e-09, Loss_r: 5.466e-05, lambda_0 : 2.146e+02, Time: 0.78, Learning Rate: 0.00023\n",
            "It: 1450, Loss: 5.449e-05, Loss_0: 8.712e-09, Loss_r: 5.448e-05, lambda_0 : 2.271e+02, Time: 0.79, Learning Rate: 0.00023\n",
            "It: 1460, Loss: 5.430e-05, Loss_0: 8.611e-09, Loss_r: 5.429e-05, lambda_0 : 2.403e+02, Time: 0.79, Learning Rate: 0.00023\n",
            "It: 1470, Loss: 5.409e-05, Loss_0: 8.504e-09, Loss_r: 5.408e-05, lambda_0 : 2.542e+02, Time: 0.79, Learning Rate: 0.00023\n",
            "It: 1480, Loss: 5.388e-05, Loss_0: 8.388e-09, Loss_r: 5.387e-05, lambda_0 : 2.689e+02, Time: 0.79, Learning Rate: 0.00023\n",
            "It: 1490, Loss: 5.365e-05, Loss_0: 8.265e-09, Loss_r: 5.364e-05, lambda_0 : 2.843e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1500, Loss: 5.344e-05, Loss_0: 8.146e-09, Loss_r: 5.343e-05, lambda_0 : 2.987e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1510, Loss: 5.322e-05, Loss_0: 8.020e-09, Loss_r: 5.321e-05, lambda_0 : 3.137e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1520, Loss: 5.298e-05, Loss_0: 7.887e-09, Loss_r: 5.298e-05, lambda_0 : 3.292e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1530, Loss: 5.274e-05, Loss_0: 7.745e-09, Loss_r: 5.273e-05, lambda_0 : 3.451e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1540, Loss: 5.249e-05, Loss_0: 7.595e-09, Loss_r: 5.248e-05, lambda_0 : 3.614e+02, Time: 0.79, Learning Rate: 0.00021\n",
            "It: 1550, Loss: 5.222e-05, Loss_0: 7.437e-09, Loss_r: 5.221e-05, lambda_0 : 3.780e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1560, Loss: 5.198e-05, Loss_0: 7.287e-09, Loss_r: 5.197e-05, lambda_0 : 3.929e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1570, Loss: 5.172e-05, Loss_0: 7.131e-09, Loss_r: 5.171e-05, lambda_0 : 4.079e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1580, Loss: 5.146e-05, Loss_0: 6.968e-09, Loss_r: 5.145e-05, lambda_0 : 4.226e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1590, Loss: 5.119e-05, Loss_0: 6.798e-09, Loss_r: 5.118e-05, lambda_0 : 4.370e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1600, Loss: 5.091e-05, Loss_0: 6.623e-09, Loss_r: 5.091e-05, lambda_0 : 4.509e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1610, Loss: 5.063e-05, Loss_0: 6.441e-09, Loss_r: 5.062e-05, lambda_0 : 4.639e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1620, Loss: 5.034e-05, Loss_0: 6.255e-09, Loss_r: 5.034e-05, lambda_0 : 4.760e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1630, Loss: 5.005e-05, Loss_0: 6.063e-09, Loss_r: 5.004e-05, lambda_0 : 4.869e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1640, Loss: 4.976e-05, Loss_0: 5.868e-09, Loss_r: 4.975e-05, lambda_0 : 4.961e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1650, Loss: 4.946e-05, Loss_0: 5.669e-09, Loss_r: 4.945e-05, lambda_0 : 5.036e+02, Time: 0.80, Learning Rate: 0.00019\n",
            "It: 1660, Loss: 4.916e-05, Loss_0: 5.469e-09, Loss_r: 4.916e-05, lambda_0 : 5.088e+02, Time: 0.80, Learning Rate: 0.00019\n",
            "It: 1670, Loss: 4.887e-05, Loss_0: 5.267e-09, Loss_r: 4.886e-05, lambda_0 : 5.115e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1680, Loss: 4.857e-05, Loss_0: 5.065e-09, Loss_r: 4.857e-05, lambda_0 : 5.115e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1690, Loss: 4.828e-05, Loss_0: 4.864e-09, Loss_r: 4.828e-05, lambda_0 : 5.084e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1700, Loss: 4.799e-05, Loss_0: 4.665e-09, Loss_r: 4.799e-05, lambda_0 : 5.021e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1710, Loss: 4.771e-05, Loss_0: 4.468e-09, Loss_r: 4.771e-05, lambda_0 : 4.921e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1720, Loss: 4.744e-05, Loss_0: 4.276e-09, Loss_r: 4.743e-05, lambda_0 : 4.785e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1730, Loss: 4.717e-05, Loss_0: 4.089e-09, Loss_r: 4.716e-05, lambda_0 : 4.610e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1740, Loss: 4.691e-05, Loss_0: 3.909e-09, Loss_r: 4.690e-05, lambda_0 : 4.397e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1750, Loss: 4.665e-05, Loss_0: 3.735e-09, Loss_r: 4.665e-05, lambda_0 : 4.145e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1760, Loss: 4.641e-05, Loss_0: 3.569e-09, Loss_r: 4.640e-05, lambda_0 : 3.856e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1770, Loss: 4.617e-05, Loss_0: 3.410e-09, Loss_r: 4.616e-05, lambda_0 : 3.531e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1780, Loss: 4.594e-05, Loss_0: 3.261e-09, Loss_r: 4.593e-05, lambda_0 : 3.173e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1790, Loss: 4.571e-05, Loss_0: 3.120e-09, Loss_r: 4.571e-05, lambda_0 : 2.785e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1800, Loss: 4.550e-05, Loss_0: 2.987e-09, Loss_r: 4.549e-05, lambda_0 : 2.371e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1810, Loss: 4.528e-05, Loss_0: 2.864e-09, Loss_r: 4.528e-05, lambda_0 : 2.319e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1820, Loss: 4.508e-05, Loss_0: 2.750e-09, Loss_r: 4.508e-05, lambda_0 : 2.425e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1830, Loss: 4.488e-05, Loss_0: 2.644e-09, Loss_r: 4.487e-05, lambda_0 : 2.531e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1840, Loss: 4.468e-05, Loss_0: 2.547e-09, Loss_r: 4.467e-05, lambda_0 : 2.636e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1850, Loss: 4.448e-05, Loss_0: 2.457e-09, Loss_r: 4.448e-05, lambda_0 : 2.739e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1860, Loss: 4.428e-05, Loss_0: 2.375e-09, Loss_r: 4.428e-05, lambda_0 : 2.840e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1870, Loss: 4.409e-05, Loss_0: 2.301e-09, Loss_r: 4.409e-05, lambda_0 : 2.937e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1880, Loss: 4.390e-05, Loss_0: 2.233e-09, Loss_r: 4.389e-05, lambda_0 : 3.030e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1890, Loss: 4.370e-05, Loss_0: 2.171e-09, Loss_r: 4.370e-05, lambda_0 : 3.119e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1900, Loss: 4.350e-05, Loss_0: 2.115e-09, Loss_r: 4.350e-05, lambda_0 : 3.204e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1910, Loss: 4.330e-05, Loss_0: 2.064e-09, Loss_r: 4.330e-05, lambda_0 : 3.283e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1920, Loss: 4.310e-05, Loss_0: 2.019e-09, Loss_r: 4.310e-05, lambda_0 : 3.357e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1930, Loss: 4.290e-05, Loss_0: 1.977e-09, Loss_r: 4.290e-05, lambda_0 : 3.426e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1940, Loss: 4.269e-05, Loss_0: 1.940e-09, Loss_r: 4.269e-05, lambda_0 : 3.644e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1950, Loss: 4.248e-05, Loss_0: 1.907e-09, Loss_r: 4.248e-05, lambda_0 : 3.954e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1960, Loss: 4.227e-05, Loss_0: 1.877e-09, Loss_r: 4.226e-05, lambda_0 : 4.240e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1970, Loss: 4.205e-05, Loss_0: 1.851e-09, Loss_r: 4.204e-05, lambda_0 : 4.500e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 1980, Loss: 4.182e-05, Loss_0: 1.827e-09, Loss_r: 4.182e-05, lambda_0 : 4.738e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 1990, Loss: 4.159e-05, Loss_0: 1.806e-09, Loss_r: 4.159e-05, lambda_0 : 4.950e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2000, Loss: 4.136e-05, Loss_0: 1.788e-09, Loss_r: 4.135e-05, lambda_0 : 5.137e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2010, Loss: 4.112e-05, Loss_0: 1.772e-09, Loss_r: 4.111e-05, lambda_0 : 5.300e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2020, Loss: 4.087e-05, Loss_0: 1.758e-09, Loss_r: 4.087e-05, lambda_0 : 5.438e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2030, Loss: 4.062e-05, Loss_0: 1.746e-09, Loss_r: 4.061e-05, lambda_0 : 5.551e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2040, Loss: 4.036e-05, Loss_0: 1.736e-09, Loss_r: 4.035e-05, lambda_0 : 5.638e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2050, Loss: 4.009e-05, Loss_0: 1.728e-09, Loss_r: 4.009e-05, lambda_0 : 5.701e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2060, Loss: 3.981e-05, Loss_0: 1.723e-09, Loss_r: 3.981e-05, lambda_0 : 5.735e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2070, Loss: 3.953e-05, Loss_0: 1.719e-09, Loss_r: 3.953e-05, lambda_0 : 5.742e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2080, Loss: 3.924e-05, Loss_0: 1.717e-09, Loss_r: 3.924e-05, lambda_0 : 5.720e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2090, Loss: 3.894e-05, Loss_0: 1.717e-09, Loss_r: 3.894e-05, lambda_0 : 5.666e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2100, Loss: 3.864e-05, Loss_0: 1.720e-09, Loss_r: 3.864e-05, lambda_0 : 5.582e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2110, Loss: 3.832e-05, Loss_0: 1.724e-09, Loss_r: 3.832e-05, lambda_0 : 5.476e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2120, Loss: 3.800e-05, Loss_0: 2.041e-09, Loss_r: 3.800e-05, lambda_0 : 4.259e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2130, Loss: 9.483e-05, Loss_0: 5.719e-05, Loss_r: 3.764e-05, lambda_0 : 1.483e+00, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2140, Loss: 5.577e-05, Loss_0: 1.788e-05, Loss_r: 3.788e-05, lambda_0 : 8.146e+00, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2150, Loss: 4.043e-05, Loss_0: 2.472e-06, Loss_r: 3.796e-05, lambda_0 : 1.748e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2160, Loss: 3.973e-05, Loss_0: 1.878e-06, Loss_r: 3.785e-05, lambda_0 : 1.225e+01, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2170, Loss: 3.780e-05, Loss_0: 1.537e-07, Loss_r: 3.765e-05, lambda_0 : 3.914e+01, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2180, Loss: 3.774e-05, Loss_0: 3.575e-07, Loss_r: 3.738e-05, lambda_0 : 2.923e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2190, Loss: 3.722e-05, Loss_0: 1.547e-07, Loss_r: 3.707e-05, lambda_0 : 3.375e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2200, Loss: 3.677e-05, Loss_0: 3.413e-08, Loss_r: 3.674e-05, lambda_0 : 8.183e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2210, Loss: 3.641e-05, Loss_0: 1.144e-08, Loss_r: 3.639e-05, lambda_0 : 1.224e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2220, Loss: 3.604e-05, Loss_0: 2.224e-09, Loss_r: 3.604e-05, lambda_0 : 2.514e+02, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2230, Loss: 3.568e-05, Loss_0: 5.554e-09, Loss_r: 3.568e-05, lambda_0 : 1.511e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2240, Loss: 3.531e-05, Loss_0: 4.050e-09, Loss_r: 3.530e-05, lambda_0 : 1.688e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2250, Loss: 3.493e-05, Loss_0: 1.830e-09, Loss_r: 3.492e-05, lambda_0 : 3.465e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2260, Loss: 3.454e-05, Loss_0: 2.053e-09, Loss_r: 3.453e-05, lambda_0 : 2.555e+02, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2270, Loss: 6.866e-05, Loss_0: 3.449e-05, Loss_r: 3.417e-05, lambda_0 : 2.415e+00, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2280, Loss: 5.706e-05, Loss_0: 2.256e-05, Loss_r: 3.451e-05, lambda_0 : 7.722e+00, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2290, Loss: 3.933e-05, Loss_0: 4.573e-06, Loss_r: 3.475e-05, lambda_0 : 5.872e+00, Time: 0.79, Learning Rate: 0.00019\n",
            "It: 2300, Loss: 3.561e-05, Loss_0: 9.123e-07, Loss_r: 3.470e-05, lambda_0 : 1.389e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2310, Loss: 3.486e-05, Loss_0: 3.980e-07, Loss_r: 3.446e-05, lambda_0 : 1.978e+01, Time: 0.78, Learning Rate: 0.00019\n",
            "It: 2320, Loss: 3.420e-05, Loss_0: 5.708e-08, Loss_r: 3.415e-05, lambda_0 : 5.496e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2330, Loss: 3.392e-05, Loss_0: 1.033e-07, Loss_r: 3.382e-05, lambda_0 : 4.092e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2340, Loss: 3.351e-05, Loss_0: 4.358e-08, Loss_r: 3.346e-05, lambda_0 : 5.183e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2350, Loss: 3.311e-05, Loss_0: 1.423e-08, Loss_r: 3.310e-05, lambda_0 : 9.672e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2360, Loss: 3.273e-05, Loss_0: 9.413e-09, Loss_r: 3.272e-05, lambda_0 : 1.040e+02, Time: 0.78, Learning Rate: 0.00017\n",
            "It: 2370, Loss: 3.234e-05, Loss_0: 1.794e-09, Loss_r: 3.234e-05, lambda_0 : 3.212e+02, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2380, Loss: 3.195e-05, Loss_0: 2.106e-09, Loss_r: 3.195e-05, lambda_0 : 2.353e+02, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2390, Loss: 3.155e-05, Loss_0: 1.793e-09, Loss_r: 3.155e-05, lambda_0 : 3.263e+02, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2400, Loss: 3.115e-05, Loss_0: 1.995e-09, Loss_r: 3.115e-05, lambda_0 : 2.694e+02, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2410, Loss: 3.074e-05, Loss_0: 2.360e-09, Loss_r: 3.074e-05, lambda_0 : 2.181e+02, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2420, Loss: 3.057e-05, Loss_0: 2.493e-07, Loss_r: 3.032e-05, lambda_0 : 2.050e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2430, Loss: 3.185e-05, Loss_0: 1.564e-06, Loss_r: 3.029e-05, lambda_0 : 7.940e+00, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2440, Loss: 5.973e-05, Loss_0: 2.864e-05, Loss_r: 3.110e-05, lambda_0 : 5.950e+00, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2450, Loss: 3.436e-05, Loss_0: 2.915e-06, Loss_r: 3.145e-05, lambda_0 : 6.380e+00, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2460, Loss: 3.383e-05, Loss_0: 2.411e-06, Loss_r: 3.142e-05, lambda_0 : 1.384e+01, Time: 0.79, Learning Rate: 0.00017\n",
            "It: 2470, Loss: 3.219e-05, Loss_0: 1.012e-06, Loss_r: 3.117e-05, lambda_0 : 1.873e+01, Time: 0.78, Learning Rate: 0.00017\n",
            "It: 2480, Loss: 3.119e-05, Loss_0: 3.494e-07, Loss_r: 3.084e-05, lambda_0 : 2.045e+01, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2490, Loss: 3.050e-05, Loss_0: 2.273e-09, Loss_r: 3.050e-05, lambda_0 : 2.220e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2500, Loss: 3.016e-05, Loss_0: 1.918e-08, Loss_r: 3.014e-05, lambda_0 : 7.085e+01, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2510, Loss: 2.978e-05, Loss_0: 1.200e-08, Loss_r: 2.977e-05, lambda_0 : 8.717e+01, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2520, Loss: 2.939e-05, Loss_0: 3.313e-09, Loss_r: 2.939e-05, lambda_0 : 1.650e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2530, Loss: 2.901e-05, Loss_0: 1.947e-09, Loss_r: 2.901e-05, lambda_0 : 2.297e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2540, Loss: 2.862e-05, Loss_0: 2.555e-09, Loss_r: 2.862e-05, lambda_0 : 1.856e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2550, Loss: 2.823e-05, Loss_0: 1.772e-09, Loss_r: 2.823e-05, lambda_0 : 2.746e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2560, Loss: 2.784e-05, Loss_0: 1.609e-09, Loss_r: 2.784e-05, lambda_0 : 3.061e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2570, Loss: 2.744e-05, Loss_0: 1.599e-09, Loss_r: 2.744e-05, lambda_0 : 3.100e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2580, Loss: 2.704e-05, Loss_0: 1.580e-09, Loss_r: 2.704e-05, lambda_0 : 2.831e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2590, Loss: 2.664e-05, Loss_0: 1.564e-09, Loss_r: 2.664e-05, lambda_0 : 3.045e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2600, Loss: 2.623e-05, Loss_0: 1.548e-09, Loss_r: 2.623e-05, lambda_0 : 3.037e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2610, Loss: 2.582e-05, Loss_0: 1.523e-09, Loss_r: 2.582e-05, lambda_0 : 3.134e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2620, Loss: 2.541e-05, Loss_0: 1.502e-09, Loss_r: 2.541e-05, lambda_0 : 3.307e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2630, Loss: 2.500e-05, Loss_0: 1.484e-09, Loss_r: 2.500e-05, lambda_0 : 3.574e+02, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2640, Loss: 2.459e-05, Loss_0: 1.481e-09, Loss_r: 2.459e-05, lambda_0 : 3.962e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2650, Loss: 2.424e-05, Loss_0: 6.187e-08, Loss_r: 2.418e-05, lambda_0 : 4.785e+01, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2660, Loss: 1.165e-04, Loss_0: 9.236e-05, Loss_r: 2.414e-05, lambda_0 : 9.479e+00, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2670, Loss: 7.308e-05, Loss_0: 4.742e-05, Loss_r: 2.566e-05, lambda_0 : 1.231e+01, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2680, Loss: 3.822e-05, Loss_0: 1.165e-05, Loss_r: 2.657e-05, lambda_0 : 3.949e+01, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2690, Loss: 2.844e-05, Loss_0: 1.728e-06, Loss_r: 2.671e-05, lambda_0 : 4.724e+01, Time: 0.79, Learning Rate: 0.00015\n",
            "It: 2700, Loss: 2.678e-05, Loss_0: 2.109e-07, Loss_r: 2.657e-05, lambda_0 : 1.066e+02, Time: 0.78, Learning Rate: 0.00015\n",
            "It: 2710, Loss: 2.629e-05, Loss_0: 4.438e-09, Loss_r: 2.628e-05, lambda_0 : 4.119e+02, Time: 0.78, Learning Rate: 0.00014\n",
            "It: 2720, Loss: 2.600e-05, Loss_0: 2.837e-08, Loss_r: 2.597e-05, lambda_0 : 6.433e+01, Time: 0.79, Learning Rate: 0.00014\n",
            "It: 2730, Loss: 2.566e-05, Loss_0: 2.115e-08, Loss_r: 2.564e-05, lambda_0 : 6.788e+01, Time: 0.79, Learning Rate: 0.00014\n",
            "It: 2740, Loss: 2.530e-05, Loss_0: 4.914e-09, Loss_r: 2.529e-05, lambda_0 : 1.341e+02, Time: 0.79, Learning Rate: 0.00014\n",
            "It: 2750, Loss: 2.495e-05, Loss_0: 1.352e-09, Loss_r: 2.495e-05, lambda_0 : 3.260e+02, Time: 0.79, Learning Rate: 0.00014\n",
            "It: 2760, Loss: 2.460e-05, Loss_0: 3.643e-09, Loss_r: 2.460e-05, lambda_0 : 1.886e+02, Time: 0.79, Learning Rate: 0.00014\n",
            "It: 2770, Loss: 2.425e-05, Loss_0: 2.311e-09, Loss_r: 2.425e-05, lambda_0 : 2.685e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2780, Loss: 2.394e-05, Loss_0: 1.577e-09, Loss_r: 2.394e-05, lambda_0 : 3.712e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2790, Loss: 2.363e-05, Loss_0: 1.216e-09, Loss_r: 2.363e-05, lambda_0 : 4.841e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2800, Loss: 2.331e-05, Loss_0: 1.198e-09, Loss_r: 2.331e-05, lambda_0 : 4.756e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2810, Loss: 2.300e-05, Loss_0: 1.206e-09, Loss_r: 2.300e-05, lambda_0 : 5.211e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2820, Loss: 2.269e-05, Loss_0: 1.154e-09, Loss_r: 2.269e-05, lambda_0 : 5.238e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2830, Loss: 2.238e-05, Loss_0: 1.132e-09, Loss_r: 2.238e-05, lambda_0 : 5.117e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2840, Loss: 2.207e-05, Loss_0: 1.114e-09, Loss_r: 2.207e-05, lambda_0 : 5.216e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2850, Loss: 2.176e-05, Loss_0: 1.096e-09, Loss_r: 2.176e-05, lambda_0 : 5.304e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2860, Loss: 2.146e-05, Loss_0: 1.078e-09, Loss_r: 2.145e-05, lambda_0 : 5.343e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2870, Loss: 2.115e-05, Loss_0: 1.059e-09, Loss_r: 2.115e-05, lambda_0 : 5.352e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2880, Loss: 2.085e-05, Loss_0: 1.041e-09, Loss_r: 2.085e-05, lambda_0 : 5.343e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2890, Loss: 2.055e-05, Loss_0: 1.023e-09, Loss_r: 2.054e-05, lambda_0 : 5.324e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2900, Loss: 2.025e-05, Loss_0: 1.005e-09, Loss_r: 2.024e-05, lambda_0 : 5.298e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2910, Loss: 1.995e-05, Loss_0: 9.866e-10, Loss_r: 1.995e-05, lambda_0 : 5.262e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2920, Loss: 1.965e-05, Loss_0: 9.686e-10, Loss_r: 1.965e-05, lambda_0 : 5.194e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2930, Loss: 1.936e-05, Loss_0: 9.511e-10, Loss_r: 1.936e-05, lambda_0 : 5.133e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2940, Loss: 1.907e-05, Loss_0: 9.337e-10, Loss_r: 1.907e-05, lambda_0 : 5.040e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2950, Loss: 1.878e-05, Loss_0: 9.167e-10, Loss_r: 1.878e-05, lambda_0 : 4.960e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2960, Loss: 1.850e-05, Loss_0: 9.000e-10, Loss_r: 1.850e-05, lambda_0 : 4.846e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2970, Loss: 1.821e-05, Loss_0: 8.835e-10, Loss_r: 1.821e-05, lambda_0 : 4.741e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 2980, Loss: 1.793e-05, Loss_0: 8.674e-10, Loss_r: 1.793e-05, lambda_0 : 4.603e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 2990, Loss: 1.766e-05, Loss_0: 8.518e-10, Loss_r: 1.766e-05, lambda_0 : 4.471e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 3000, Loss: 1.738e-05, Loss_0: 8.364e-10, Loss_r: 1.738e-05, lambda_0 : 4.331e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3010, Loss: 1.711e-05, Loss_0: 8.217e-10, Loss_r: 1.711e-05, lambda_0 : 4.178e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 3020, Loss: 1.684e-05, Loss_0: 8.071e-10, Loss_r: 1.684e-05, lambda_0 : 4.010e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3030, Loss: 1.658e-05, Loss_0: 7.931e-10, Loss_r: 1.658e-05, lambda_0 : 3.842e+02, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 3040, Loss: 1.632e-05, Loss_0: 7.786e-10, Loss_r: 1.632e-05, lambda_0 : 3.643e+02, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3050, Loss: 1.606e-05, Loss_0: 5.172e-09, Loss_r: 1.606e-05, lambda_0 : 9.755e+01, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3060, Loss: 3.401e-04, Loss_0: 3.242e-04, Loss_r: 1.593e-05, lambda_0 : 4.415e+00, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3070, Loss: 2.821e-05, Loss_0: 1.122e-05, Loss_r: 1.699e-05, lambda_0 : 2.987e+01, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3080, Loss: 2.027e-05, Loss_0: 2.485e-06, Loss_r: 1.778e-05, lambda_0 : 4.012e+01, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 3090, Loss: 2.582e-05, Loss_0: 7.789e-06, Loss_r: 1.803e-05, lambda_0 : 7.814e+00, Time: 0.79, Learning Rate: 0.00012\n",
            "It: 3100, Loss: 1.997e-05, Loss_0: 1.977e-06, Loss_r: 1.799e-05, lambda_0 : 6.479e+00, Time: 0.78, Learning Rate: 0.00012\n",
            "It: 3110, Loss: 1.788e-05, Loss_0: 4.400e-08, Loss_r: 1.783e-05, lambda_0 : 7.900e+01, Time: 0.79, Learning Rate: 0.00011\n",
            "It: 3120, Loss: 1.765e-05, Loss_0: 9.950e-09, Loss_r: 1.764e-05, lambda_0 : 8.786e+01, Time: 0.78, Learning Rate: 0.00011\n",
            "It: 3130, Loss: 1.748e-05, Loss_0: 5.911e-08, Loss_r: 1.742e-05, lambda_0 : 3.222e+01, Time: 0.78, Learning Rate: 0.00011\n",
            "It: 3140, Loss: 1.724e-05, Loss_0: 4.094e-08, Loss_r: 1.720e-05, lambda_0 : 4.355e+01, Time: 0.79, Learning Rate: 0.00011\n",
            "It: 3150, Loss: 1.697e-05, Loss_0: 2.867e-09, Loss_r: 1.697e-05, lambda_0 : 1.378e+02, Time: 0.78, Learning Rate: 0.00011\n",
            "It: 3160, Loss: 1.674e-05, Loss_0: 4.434e-09, Loss_r: 1.674e-05, lambda_0 : 1.103e+02, Time: 0.78, Learning Rate: 0.00011\n",
            "It: 3170, Loss: 1.651e-05, Loss_0: 1.092e-09, Loss_r: 1.651e-05, lambda_0 : 2.280e+02, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3180, Loss: 1.631e-05, Loss_0: 6.641e-10, Loss_r: 1.631e-05, lambda_0 : 3.783e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3190, Loss: 1.611e-05, Loss_0: 8.324e-10, Loss_r: 1.611e-05, lambda_0 : 2.877e+02, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3200, Loss: 1.591e-05, Loss_0: 6.799e-10, Loss_r: 1.591e-05, lambda_0 : 3.344e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3210, Loss: 1.571e-05, Loss_0: 6.425e-10, Loss_r: 1.571e-05, lambda_0 : 3.814e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3220, Loss: 1.552e-05, Loss_0: 6.473e-10, Loss_r: 1.552e-05, lambda_0 : 4.214e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3230, Loss: 1.532e-05, Loss_0: 6.339e-10, Loss_r: 1.532e-05, lambda_0 : 4.167e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3240, Loss: 1.513e-05, Loss_0: 6.228e-10, Loss_r: 1.513e-05, lambda_0 : 4.030e+02, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3250, Loss: 1.494e-05, Loss_0: 6.165e-10, Loss_r: 1.494e-05, lambda_0 : 3.988e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3260, Loss: 1.475e-05, Loss_0: 6.112e-10, Loss_r: 1.475e-05, lambda_0 : 3.990e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3270, Loss: 1.456e-05, Loss_0: 6.061e-10, Loss_r: 1.456e-05, lambda_0 : 3.997e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3280, Loss: 1.438e-05, Loss_0: 6.012e-10, Loss_r: 1.438e-05, lambda_0 : 3.995e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3290, Loss: 1.419e-05, Loss_0: 5.965e-10, Loss_r: 1.419e-05, lambda_0 : 3.987e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3300, Loss: 1.401e-05, Loss_0: 5.920e-10, Loss_r: 1.401e-05, lambda_0 : 3.979e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3310, Loss: 1.383e-05, Loss_0: 5.876e-10, Loss_r: 1.383e-05, lambda_0 : 3.967e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3320, Loss: 1.365e-05, Loss_0: 5.835e-10, Loss_r: 1.365e-05, lambda_0 : 3.951e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3330, Loss: 1.348e-05, Loss_0: 5.794e-10, Loss_r: 1.347e-05, lambda_0 : 3.929e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3340, Loss: 1.330e-05, Loss_0: 5.756e-10, Loss_r: 1.330e-05, lambda_0 : 3.905e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3350, Loss: 1.313e-05, Loss_0: 5.721e-10, Loss_r: 1.313e-05, lambda_0 : 3.887e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3360, Loss: 1.296e-05, Loss_0: 5.687e-10, Loss_r: 1.296e-05, lambda_0 : 3.861e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3370, Loss: 1.279e-05, Loss_0: 5.655e-10, Loss_r: 1.279e-05, lambda_0 : 3.834e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3380, Loss: 1.262e-05, Loss_0: 5.624e-10, Loss_r: 1.262e-05, lambda_0 : 3.803e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3390, Loss: 1.246e-05, Loss_0: 5.597e-10, Loss_r: 1.246e-05, lambda_0 : 3.763e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3400, Loss: 1.230e-05, Loss_0: 5.584e-10, Loss_r: 1.230e-05, lambda_0 : 3.600e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3410, Loss: 1.215e-05, Loss_0: 5.880e-10, Loss_r: 1.215e-05, lambda_0 : 3.049e+03, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3420, Loss: 1.245e-05, Loss_0: 9.350e-10, Loss_r: 1.245e-05, lambda_0 : 1.382e+04, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3430, Loss: 1.196e-05, Loss_0: 1.072e-09, Loss_r: 1.195e-05, lambda_0 : 5.825e+03, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3440, Loss: 1.168e-05, Loss_0: 1.021e-09, Loss_r: 1.168e-05, lambda_0 : 2.864e+02, Time: 0.80, Learning Rate: 0.00010\n",
            "It: 3450, Loss: 1.155e-05, Loss_0: 6.799e-10, Loss_r: 1.155e-05, lambda_0 : 2.403e+03, Time: 0.80, Learning Rate: 0.00010\n",
            "It: 3460, Loss: 1.140e-05, Loss_0: 5.247e-10, Loss_r: 1.140e-05, lambda_0 : 2.989e+03, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3470, Loss: 1.126e-05, Loss_0: 5.229e-10, Loss_r: 1.126e-05, lambda_0 : 2.187e+03, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3480, Loss: 1.111e-05, Loss_0: 5.341e-10, Loss_r: 1.111e-05, lambda_0 : 1.358e+03, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3490, Loss: 1.097e-05, Loss_0: 5.593e-10, Loss_r: 1.097e-05, lambda_0 : 5.192e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3500, Loss: 1.084e-05, Loss_0: 7.203e-10, Loss_r: 1.084e-05, lambda_0 : 2.110e+02, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3510, Loss: 1.075e-05, Loss_0: 4.587e-08, Loss_r: 1.070e-05, lambda_0 : 2.356e+01, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3520, Loss: 7.868e-05, Loss_0: 6.810e-05, Loss_r: 1.058e-05, lambda_0 : 6.063e+00, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3530, Loss: 3.509e-05, Loss_0: 2.430e-05, Loss_r: 1.078e-05, lambda_0 : 3.170e+01, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3540, Loss: 1.614e-05, Loss_0: 5.321e-06, Loss_r: 1.082e-05, lambda_0 : 1.963e+01, Time: 0.79, Learning Rate: 0.00010\n",
            "It: 3550, Loss: 1.306e-05, Loss_0: 2.246e-06, Loss_r: 1.081e-05, lambda_0 : 2.982e+01, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3560, Loss: 1.102e-05, Loss_0: 2.943e-07, Loss_r: 1.073e-05, lambda_0 : 1.551e+01, Time: 0.78, Learning Rate: 0.00010\n",
            "It: 3570, Loss: 1.092e-05, Loss_0: 3.029e-07, Loss_r: 1.062e-05, lambda_0 : 1.718e+01, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3580, Loss: 1.065e-05, Loss_0: 1.326e-07, Loss_r: 1.051e-05, lambda_0 : 1.728e+01, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3590, Loss: 1.045e-05, Loss_0: 4.820e-08, Loss_r: 1.040e-05, lambda_0 : 3.030e+01, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3600, Loss: 1.030e-05, Loss_0: 1.268e-08, Loss_r: 1.029e-05, lambda_0 : 5.285e+01, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3610, Loss: 1.018e-05, Loss_0: 1.039e-09, Loss_r: 1.018e-05, lambda_0 : 1.608e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3620, Loss: 1.007e-05, Loss_0: 1.739e-09, Loss_r: 1.007e-05, lambda_0 : 1.182e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3630, Loss: 9.961e-06, Loss_0: 1.082e-09, Loss_r: 9.960e-06, lambda_0 : 1.516e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3640, Loss: 9.854e-06, Loss_0: 6.594e-10, Loss_r: 9.853e-06, lambda_0 : 2.029e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3650, Loss: 9.749e-06, Loss_0: 5.100e-10, Loss_r: 9.748e-06, lambda_0 : 3.241e+02, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3660, Loss: 9.646e-06, Loss_0: 5.284e-10, Loss_r: 9.645e-06, lambda_0 : 2.892e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3670, Loss: 9.544e-06, Loss_0: 5.157e-10, Loss_r: 9.544e-06, lambda_0 : 3.074e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3680, Loss: 9.444e-06, Loss_0: 5.048e-10, Loss_r: 9.444e-06, lambda_0 : 3.052e+02, Time: 0.80, Learning Rate: 0.00009\n",
            "It: 3690, Loss: 9.347e-06, Loss_0: 5.017e-10, Loss_r: 9.346e-06, lambda_0 : 2.888e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3700, Loss: 9.250e-06, Loss_0: 5.010e-10, Loss_r: 9.250e-06, lambda_0 : 2.859e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3710, Loss: 9.156e-06, Loss_0: 5.027e-10, Loss_r: 9.156e-06, lambda_0 : 2.917e+02, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3720, Loss: 9.063e-06, Loss_0: 4.999e-10, Loss_r: 9.063e-06, lambda_0 : 2.790e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3730, Loss: 8.972e-06, Loss_0: 5.003e-10, Loss_r: 8.972e-06, lambda_0 : 2.776e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3740, Loss: 8.883e-06, Loss_0: 5.031e-10, Loss_r: 8.883e-06, lambda_0 : 2.514e+02, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3750, Loss: 8.827e-06, Loss_0: 7.493e-10, Loss_r: 8.826e-06, lambda_0 : 3.737e+03, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3760, Loss: 8.970e-06, Loss_0: 2.350e-07, Loss_r: 8.735e-06, lambda_0 : 1.824e+02, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3770, Loss: 1.256e-04, Loss_0: 1.170e-04, Loss_r: 8.665e-06, lambda_0 : 5.235e+00, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3780, Loss: 2.613e-05, Loss_0: 1.721e-05, Loss_r: 8.921e-06, lambda_0 : 3.479e+01, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3790, Loss: 1.228e-05, Loss_0: 3.290e-06, Loss_r: 8.986e-06, lambda_0 : 3.572e+01, Time: 0.79, Learning Rate: 0.00009\n",
            "It: 3800, Loss: 1.152e-05, Loss_0: 2.509e-06, Loss_r: 9.008e-06, lambda_0 : 2.331e+01, Time: 0.78, Learning Rate: 0.00009\n",
            "It: 3810, Loss: 1.035e-05, Loss_0: 1.388e-06, Loss_r: 8.965e-06, lambda_0 : 2.107e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3820, Loss: 8.939e-06, Loss_0: 3.226e-08, Loss_r: 8.907e-06, lambda_0 : 1.033e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3830, Loss: 8.968e-06, Loss_0: 1.302e-07, Loss_r: 8.838e-06, lambda_0 : 2.644e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3840, Loss: 8.793e-06, Loss_0: 2.877e-08, Loss_r: 8.765e-06, lambda_0 : 6.038e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3850, Loss: 8.706e-06, Loss_0: 1.588e-08, Loss_r: 8.690e-06, lambda_0 : 3.145e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3860, Loss: 8.618e-06, Loss_0: 1.518e-09, Loss_r: 8.616e-06, lambda_0 : 1.356e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3870, Loss: 8.547e-06, Loss_0: 3.331e-09, Loss_r: 8.544e-06, lambda_0 : 7.261e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3880, Loss: 8.473e-06, Loss_0: 1.190e-09, Loss_r: 8.472e-06, lambda_0 : 1.122e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3890, Loss: 8.402e-06, Loss_0: 4.960e-10, Loss_r: 8.401e-06, lambda_0 : 2.167e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3900, Loss: 8.332e-06, Loss_0: 4.661e-10, Loss_r: 8.332e-06, lambda_0 : 2.631e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3910, Loss: 8.264e-06, Loss_0: 4.682e-10, Loss_r: 8.264e-06, lambda_0 : 2.654e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3920, Loss: 8.197e-06, Loss_0: 4.657e-10, Loss_r: 8.197e-06, lambda_0 : 2.577e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3930, Loss: 8.131e-06, Loss_0: 4.644e-10, Loss_r: 8.131e-06, lambda_0 : 2.452e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3940, Loss: 8.067e-06, Loss_0: 4.686e-10, Loss_r: 8.066e-06, lambda_0 : 2.557e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3950, Loss: 8.003e-06, Loss_0: 4.634e-10, Loss_r: 8.003e-06, lambda_0 : 2.387e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 3960, Loss: 7.941e-06, Loss_0: 4.633e-10, Loss_r: 7.941e-06, lambda_0 : 2.390e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3970, Loss: 7.880e-06, Loss_0: 4.634e-10, Loss_r: 7.879e-06, lambda_0 : 2.374e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3980, Loss: 7.820e-06, Loss_0: 4.622e-10, Loss_r: 7.819e-06, lambda_0 : 2.320e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 3990, Loss: 7.761e-06, Loss_0: 4.613e-10, Loss_r: 7.760e-06, lambda_0 : 2.269e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4000, Loss: 7.703e-06, Loss_0: 4.605e-10, Loss_r: 7.703e-06, lambda_0 : 2.230e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4010, Loss: 7.646e-06, Loss_0: 4.598e-10, Loss_r: 7.646e-06, lambda_0 : 2.195e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4020, Loss: 7.591e-06, Loss_0: 4.589e-10, Loss_r: 7.590e-06, lambda_0 : 2.155e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4030, Loss: 7.536e-06, Loss_0: 4.580e-10, Loss_r: 7.535e-06, lambda_0 : 2.116e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4040, Loss: 7.482e-06, Loss_0: 4.570e-10, Loss_r: 7.482e-06, lambda_0 : 2.094e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4050, Loss: 7.430e-06, Loss_0: 4.560e-10, Loss_r: 7.429e-06, lambda_0 : 2.067e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4060, Loss: 7.378e-06, Loss_0: 4.549e-10, Loss_r: 7.378e-06, lambda_0 : 2.065e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4070, Loss: 7.328e-06, Loss_0: 4.539e-10, Loss_r: 7.327e-06, lambda_0 : 2.012e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4080, Loss: 7.278e-06, Loss_0: 4.527e-10, Loss_r: 7.278e-06, lambda_0 : 2.016e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4090, Loss: 7.230e-06, Loss_0: 4.515e-10, Loss_r: 7.229e-06, lambda_0 : 1.967e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4100, Loss: 7.182e-06, Loss_0: 4.503e-10, Loss_r: 7.182e-06, lambda_0 : 1.964e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4110, Loss: 7.136e-06, Loss_0: 4.492e-10, Loss_r: 7.135e-06, lambda_0 : 1.943e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4120, Loss: 7.090e-06, Loss_0: 4.498e-10, Loss_r: 7.090e-06, lambda_0 : 2.000e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4130, Loss: 7.045e-06, Loss_0: 7.025e-10, Loss_r: 7.045e-06, lambda_0 : 1.283e+02, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4140, Loss: 7.254e-06, Loss_0: 2.537e-07, Loss_r: 7.000e-06, lambda_0 : 1.797e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4150, Loss: 1.170e-04, Loss_0: 1.101e-04, Loss_r: 6.986e-06, lambda_0 : 4.415e+00, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4160, Loss: 3.912e-05, Loss_0: 3.136e-05, Loss_r: 7.755e-06, lambda_0 : 8.006e+01, Time: 0.79, Learning Rate: 0.00008\n",
            "It: 4170, Loss: 1.620e-05, Loss_0: 8.757e-06, Loss_r: 7.447e-06, lambda_0 : 8.510e+01, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4180, Loss: 7.596e-06, Loss_0: 2.941e-07, Loss_r: 7.302e-06, lambda_0 : 1.741e+02, Time: 0.78, Learning Rate: 0.00008\n",
            "It: 4190, Loss: 7.695e-06, Loss_0: 4.343e-07, Loss_r: 7.261e-06, lambda_0 : 1.015e+01, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4200, Loss: 7.230e-06, Loss_0: 1.818e-09, Loss_r: 7.228e-06, lambda_0 : 2.026e+02, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4210, Loss: 7.244e-06, Loss_0: 5.531e-08, Loss_r: 7.189e-06, lambda_0 : 3.887e+01, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4220, Loss: 7.195e-06, Loss_0: 4.702e-08, Loss_r: 7.148e-06, lambda_0 : 7.469e+01, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4230, Loss: 7.110e-06, Loss_0: 3.674e-09, Loss_r: 7.106e-06, lambda_0 : 1.246e+02, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4240, Loss: 7.067e-06, Loss_0: 3.223e-09, Loss_r: 7.064e-06, lambda_0 : 1.270e+02, Time: 0.79, Learning Rate: 0.00007\n",
            "It: 4250, Loss: 7.025e-06, Loss_0: 1.731e-09, Loss_r: 7.023e-06, lambda_0 : 9.952e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4260, Loss: 6.987e-06, Loss_0: 8.634e-10, Loss_r: 6.986e-06, lambda_0 : 1.456e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4270, Loss: 6.951e-06, Loss_0: 5.322e-10, Loss_r: 6.950e-06, lambda_0 : 1.244e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4280, Loss: 6.916e-06, Loss_0: 4.259e-10, Loss_r: 6.915e-06, lambda_0 : 2.219e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4290, Loss: 6.881e-06, Loss_0: 4.043e-10, Loss_r: 6.880e-06, lambda_0 : 2.248e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4300, Loss: 6.847e-06, Loss_0: 4.070e-10, Loss_r: 6.846e-06, lambda_0 : 2.420e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4310, Loss: 6.813e-06, Loss_0: 4.092e-10, Loss_r: 6.813e-06, lambda_0 : 2.438e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4320, Loss: 6.780e-06, Loss_0: 4.066e-10, Loss_r: 6.779e-06, lambda_0 : 2.313e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4330, Loss: 6.747e-06, Loss_0: 4.037e-10, Loss_r: 6.747e-06, lambda_0 : 2.258e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4340, Loss: 6.715e-06, Loss_0: 4.026e-10, Loss_r: 6.715e-06, lambda_0 : 2.166e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4350, Loss: 6.684e-06, Loss_0: 4.022e-10, Loss_r: 6.683e-06, lambda_0 : 2.138e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4360, Loss: 6.653e-06, Loss_0: 4.021e-10, Loss_r: 6.652e-06, lambda_0 : 2.119e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4370, Loss: 6.622e-06, Loss_0: 4.015e-10, Loss_r: 6.622e-06, lambda_0 : 2.070e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4380, Loss: 6.592e-06, Loss_0: 4.007e-10, Loss_r: 6.592e-06, lambda_0 : 2.037e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4390, Loss: 6.563e-06, Loss_0: 4.000e-10, Loss_r: 6.562e-06, lambda_0 : 1.995e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4400, Loss: 6.534e-06, Loss_0: 3.993e-10, Loss_r: 6.533e-06, lambda_0 : 1.963e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4410, Loss: 6.505e-06, Loss_0: 3.984e-10, Loss_r: 6.505e-06, lambda_0 : 1.933e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4420, Loss: 6.477e-06, Loss_0: 3.976e-10, Loss_r: 6.477e-06, lambda_0 : 1.897e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4430, Loss: 6.450e-06, Loss_0: 3.967e-10, Loss_r: 6.449e-06, lambda_0 : 1.865e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4440, Loss: 6.423e-06, Loss_0: 3.957e-10, Loss_r: 6.422e-06, lambda_0 : 1.898e+02, Time: 0.81, Learning Rate: 0.00006\n",
            "It: 4450, Loss: 6.396e-06, Loss_0: 3.948e-10, Loss_r: 6.395e-06, lambda_0 : 1.977e+02, Time: 0.91, Learning Rate: 0.00006\n",
            "It: 4460, Loss: 6.370e-06, Loss_0: 3.937e-10, Loss_r: 6.369e-06, lambda_0 : 2.047e+02, Time: 0.89, Learning Rate: 0.00006\n",
            "It: 4470, Loss: 6.344e-06, Loss_0: 3.927e-10, Loss_r: 6.343e-06, lambda_0 : 2.118e+02, Time: 0.83, Learning Rate: 0.00006\n",
            "It: 4480, Loss: 6.318e-06, Loss_0: 3.916e-10, Loss_r: 6.318e-06, lambda_0 : 2.181e+02, Time: 0.81, Learning Rate: 0.00006\n",
            "It: 4490, Loss: 6.293e-06, Loss_0: 3.904e-10, Loss_r: 6.293e-06, lambda_0 : 2.239e+02, Time: 0.80, Learning Rate: 0.00006\n",
            "It: 4500, Loss: 6.269e-06, Loss_0: 3.893e-10, Loss_r: 6.268e-06, lambda_0 : 2.308e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4510, Loss: 6.245e-06, Loss_0: 3.880e-10, Loss_r: 6.244e-06, lambda_0 : 2.359e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4520, Loss: 6.221e-06, Loss_0: 3.869e-10, Loss_r: 6.220e-06, lambda_0 : 2.421e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4530, Loss: 6.197e-06, Loss_0: 3.856e-10, Loss_r: 6.197e-06, lambda_0 : 2.468e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4540, Loss: 6.174e-06, Loss_0: 3.843e-10, Loss_r: 6.174e-06, lambda_0 : 2.513e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4550, Loss: 6.152e-06, Loss_0: 3.829e-10, Loss_r: 6.151e-06, lambda_0 : 2.565e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4560, Loss: 6.130e-06, Loss_0: 3.816e-10, Loss_r: 6.129e-06, lambda_0 : 2.604e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4570, Loss: 6.108e-06, Loss_0: 3.803e-10, Loss_r: 6.107e-06, lambda_0 : 2.661e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4580, Loss: 6.086e-06, Loss_0: 3.790e-10, Loss_r: 6.086e-06, lambda_0 : 2.686e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4590, Loss: 6.065e-06, Loss_0: 3.776e-10, Loss_r: 6.065e-06, lambda_0 : 2.721e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4600, Loss: 6.044e-06, Loss_0: 3.762e-10, Loss_r: 6.044e-06, lambda_0 : 2.765e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4610, Loss: 6.024e-06, Loss_0: 3.747e-10, Loss_r: 6.023e-06, lambda_0 : 2.797e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4620, Loss: 6.004e-06, Loss_0: 3.733e-10, Loss_r: 6.003e-06, lambda_0 : 2.801e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4630, Loss: 5.984e-06, Loss_0: 3.718e-10, Loss_r: 5.983e-06, lambda_0 : 2.856e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4640, Loss: 5.964e-06, Loss_0: 3.704e-10, Loss_r: 5.964e-06, lambda_0 : 2.875e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4650, Loss: 5.945e-06, Loss_0: 3.688e-10, Loss_r: 5.945e-06, lambda_0 : 2.837e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4660, Loss: 5.926e-06, Loss_0: 3.673e-10, Loss_r: 5.926e-06, lambda_0 : 2.751e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4670, Loss: 5.907e-06, Loss_0: 3.657e-10, Loss_r: 5.907e-06, lambda_0 : 2.282e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4680, Loss: 5.889e-06, Loss_0: 3.631e-10, Loss_r: 5.889e-06, lambda_0 : 6.026e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4690, Loss: 5.892e-06, Loss_0: 3.483e-10, Loss_r: 5.892e-06, lambda_0 : 5.943e+03, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4700, Loss: 6.301e-06, Loss_0: 5.248e-10, Loss_r: 6.301e-06, lambda_0 : 1.578e+04, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4710, Loss: 6.032e-06, Loss_0: 5.146e-10, Loss_r: 6.032e-06, lambda_0 : 1.085e+04, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4720, Loss: 5.882e-06, Loss_0: 3.761e-10, Loss_r: 5.882e-06, lambda_0 : 8.805e+03, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4730, Loss: 5.824e-06, Loss_0: 3.736e-10, Loss_r: 5.823e-06, lambda_0 : 4.615e+03, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4740, Loss: 5.793e-06, Loss_0: 6.193e-10, Loss_r: 5.792e-06, lambda_0 : 1.553e+03, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4750, Loss: 5.781e-06, Loss_0: 9.774e-09, Loss_r: 5.772e-06, lambda_0 : 1.536e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4760, Loss: 6.856e-06, Loss_0: 1.100e-06, Loss_r: 5.756e-06, lambda_0 : 5.940e+00, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4770, Loss: 5.772e-06, Loss_0: 3.075e-08, Loss_r: 5.741e-06, lambda_0 : 8.730e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4780, Loss: 4.471e-05, Loss_0: 3.899e-05, Loss_r: 5.722e-06, lambda_0 : 4.068e+00, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4790, Loss: 1.713e-05, Loss_0: 1.140e-05, Loss_r: 5.738e-06, lambda_0 : 7.524e+00, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4800, Loss: 5.838e-06, Loss_0: 8.964e-08, Loss_r: 5.748e-06, lambda_0 : 7.600e+01, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4810, Loss: 5.850e-06, Loss_0: 1.072e-07, Loss_r: 5.743e-06, lambda_0 : 3.659e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4820, Loss: 5.900e-06, Loss_0: 1.672e-07, Loss_r: 5.733e-06, lambda_0 : 8.094e+00, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4830, Loss: 5.735e-06, Loss_0: 1.428e-08, Loss_r: 5.721e-06, lambda_0 : 7.235e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4840, Loss: 5.756e-06, Loss_0: 4.813e-08, Loss_r: 5.708e-06, lambda_0 : 1.677e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4850, Loss: 5.706e-06, Loss_0: 1.300e-08, Loss_r: 5.693e-06, lambda_0 : 1.891e+01, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4860, Loss: 5.680e-06, Loss_0: 9.886e-10, Loss_r: 5.679e-06, lambda_0 : 9.574e+01, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4870, Loss: 5.668e-06, Loss_0: 2.765e-09, Loss_r: 5.665e-06, lambda_0 : 6.182e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4880, Loss: 5.652e-06, Loss_0: 3.333e-10, Loss_r: 5.652e-06, lambda_0 : 1.894e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4890, Loss: 5.639e-06, Loss_0: 6.479e-10, Loss_r: 5.638e-06, lambda_0 : 9.599e+01, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4900, Loss: 5.625e-06, Loss_0: 3.799e-10, Loss_r: 5.624e-06, lambda_0 : 1.601e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4910, Loss: 5.612e-06, Loss_0: 3.252e-10, Loss_r: 5.611e-06, lambda_0 : 2.084e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4920, Loss: 5.598e-06, Loss_0: 3.304e-10, Loss_r: 5.598e-06, lambda_0 : 1.997e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4930, Loss: 5.586e-06, Loss_0: 3.244e-10, Loss_r: 5.585e-06, lambda_0 : 2.153e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4940, Loss: 5.573e-06, Loss_0: 3.213e-10, Loss_r: 5.573e-06, lambda_0 : 2.316e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4950, Loss: 5.560e-06, Loss_0: 3.203e-10, Loss_r: 5.560e-06, lambda_0 : 2.394e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4960, Loss: 5.548e-06, Loss_0: 3.195e-10, Loss_r: 5.548e-06, lambda_0 : 2.466e+02, Time: 0.78, Learning Rate: 0.00006\n",
            "It: 4970, Loss: 5.536e-06, Loss_0: 3.186e-10, Loss_r: 5.535e-06, lambda_0 : 2.530e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4980, Loss: 5.523e-06, Loss_0: 3.178e-10, Loss_r: 5.523e-06, lambda_0 : 2.574e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "It: 4990, Loss: 5.511e-06, Loss_0: 3.169e-10, Loss_r: 5.511e-06, lambda_0 : 2.639e+02, Time: 0.79, Learning Rate: 0.00006\n",
            "Training time: 394.7414\n",
            "[1, 64, 64, 64, 64, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.180e-03, Loss_0: 9.792e-04, Loss_r: 2.004e-04, lambda_0 : 2.448e+01, Time: 1.70, Learning Rate: 0.00100\n",
            "It: 10, Loss: 6.069e-04, Loss_0: 5.439e-04, Loss_r: 6.302e-05, lambda_0 : 7.378e+00, Time: 0.23, Learning Rate: 0.00100\n",
            "It: 20, Loss: 6.905e-05, Loss_0: 6.992e-07, Loss_r: 6.835e-05, lambda_0 : 1.884e+02, Time: 0.22, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.033e-04, Loss_0: 4.594e-05, Loss_r: 5.731e-05, lambda_0 : 2.461e-01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.891e-05, Loss_0: 2.100e-05, Loss_r: 5.791e-05, lambda_0 : 5.987e+00, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.586e-05, Loss_0: 7.503e-06, Loss_r: 5.836e-05, lambda_0 : 1.479e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.917e-05, Loss_0: 9.004e-07, Loss_r: 5.827e-05, lambda_0 : 2.859e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.866e-05, Loss_0: 4.449e-07, Loss_r: 5.821e-05, lambda_0 : 1.357e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.803e-05, Loss_0: 8.280e-08, Loss_r: 5.794e-05, lambda_0 : 2.247e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.780e-05, Loss_0: 2.068e-07, Loss_r: 5.760e-05, lambda_0 : 1.424e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.745e-05, Loss_0: 3.917e-09, Loss_r: 5.745e-05, lambda_0 : 2.368e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.826e-05, Loss_0: 1.047e-06, Loss_r: 5.721e-05, lambda_0 : 3.480e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.817e-05, Loss_0: 1.216e-06, Loss_r: 5.695e-05, lambda_0 : 1.608e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.720e-05, Loss_0: 4.898e-07, Loss_r: 5.671e-05, lambda_0 : 2.739e+00, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.651e-05, Loss_0: 9.772e-08, Loss_r: 5.642e-05, lambda_0 : 6.426e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.618e-05, Loss_0: 9.523e-08, Loss_r: 5.609e-05, lambda_0 : 6.952e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.576e-05, Loss_0: 2.850e-08, Loss_r: 5.573e-05, lambda_0 : 1.345e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 170, Loss: 1.285e-04, Loss_0: 7.314e-05, Loss_r: 5.535e-05, lambda_0 : 2.821e-01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 180, Loss: 6.282e-05, Loss_0: 7.334e-06, Loss_r: 5.548e-05, lambda_0 : 1.103e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.638e-05, Loss_0: 9.162e-07, Loss_r: 5.546e-05, lambda_0 : 3.802e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.629e-05, Loss_0: 1.053e-06, Loss_r: 5.524e-05, lambda_0 : 4.202e+00, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.526e-05, Loss_0: 3.659e-07, Loss_r: 5.490e-05, lambda_0 : 7.361e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.457e-05, Loss_0: 1.158e-07, Loss_r: 5.446e-05, lambda_0 : 1.512e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.394e-05, Loss_0: 1.448e-08, Loss_r: 5.393e-05, lambda_0 : 4.872e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.339e-05, Loss_0: 8.263e-08, Loss_r: 5.331e-05, lambda_0 : 2.053e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.259e-05, Loss_0: 2.182e-08, Loss_r: 5.257e-05, lambda_0 : 4.535e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.277e-05, Loss_0: 1.080e-06, Loss_r: 5.169e-05, lambda_0 : 8.241e+00, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 270, Loss: 7.462e-05, Loss_0: 2.260e-05, Loss_r: 5.201e-05, lambda_0 : 3.900e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.719e-05, Loss_0: 4.136e-06, Loss_r: 5.305e-05, lambda_0 : 3.764e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.655e-05, Loss_0: 3.450e-06, Loss_r: 5.310e-05, lambda_0 : 1.241e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.423e-05, Loss_0: 1.605e-06, Loss_r: 5.263e-05, lambda_0 : 1.089e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.249e-05, Loss_0: 5.987e-07, Loss_r: 5.189e-05, lambda_0 : 2.978e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.105e-05, Loss_0: 1.124e-08, Loss_r: 5.104e-05, lambda_0 : 1.778e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.021e-05, Loss_0: 6.722e-08, Loss_r: 5.014e-05, lambda_0 : 7.511e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 340, Loss: 4.933e-05, Loss_0: 3.980e-08, Loss_r: 4.929e-05, lambda_0 : 7.617e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 350, Loss: 4.855e-05, Loss_0: 1.054e-08, Loss_r: 4.854e-05, lambda_0 : 1.159e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 360, Loss: 4.791e-05, Loss_0: 5.328e-09, Loss_r: 4.791e-05, lambda_0 : 1.697e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 370, Loss: 4.737e-05, Loss_0: 3.382e-09, Loss_r: 4.736e-05, lambda_0 : 2.281e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 380, Loss: 4.686e-05, Loss_0: 2.424e-09, Loss_r: 4.685e-05, lambda_0 : 2.990e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 390, Loss: 4.634e-05, Loss_0: 2.146e-09, Loss_r: 4.634e-05, lambda_0 : 3.501e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 400, Loss: 4.581e-05, Loss_0: 2.445e-09, Loss_r: 4.580e-05, lambda_0 : 3.008e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 410, Loss: 4.647e-05, Loss_0: 1.226e-06, Loss_r: 4.525e-05, lambda_0 : 1.380e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 420, Loss: 9.733e-05, Loss_0: 4.807e-05, Loss_r: 4.927e-05, lambda_0 : 2.167e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 430, Loss: 8.834e-05, Loss_0: 3.514e-05, Loss_r: 5.321e-05, lambda_0 : 3.940e+00, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 440, Loss: 6.256e-05, Loss_0: 8.873e-06, Loss_r: 5.368e-05, lambda_0 : 1.838e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.435e-05, Loss_0: 9.431e-07, Loss_r: 5.341e-05, lambda_0 : 2.598e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.317e-05, Loss_0: 1.776e-07, Loss_r: 5.300e-05, lambda_0 : 5.287e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.304e-05, Loss_0: 4.154e-07, Loss_r: 5.262e-05, lambda_0 : 4.400e+01, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.244e-05, Loss_0: 1.126e-07, Loss_r: 5.233e-05, lambda_0 : 7.831e+01, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.210e-05, Loss_0: 5.642e-09, Loss_r: 5.209e-05, lambda_0 : 3.248e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.189e-05, Loss_0: 1.388e-08, Loss_r: 5.188e-05, lambda_0 : 2.129e+02, Time: 0.20, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.168e-05, Loss_0: 1.013e-08, Loss_r: 5.167e-05, lambda_0 : 2.593e+02, Time: 0.21, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.147e-05, Loss_0: 4.442e-09, Loss_r: 5.147e-05, lambda_0 : 3.858e+02, Time: 0.24, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.126e-05, Loss_0: 2.742e-09, Loss_r: 5.125e-05, lambda_0 : 5.537e+02, Time: 0.24, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.104e-05, Loss_0: 2.264e-09, Loss_r: 5.104e-05, lambda_0 : 6.142e+02, Time: 0.22, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.082e-05, Loss_0: 2.187e-09, Loss_r: 5.082e-05, lambda_0 : 6.768e+02, Time: 0.22, Learning Rate: 0.00090\n",
            "It: 560, Loss: 5.061e-05, Loss_0: 2.218e-09, Loss_r: 5.061e-05, lambda_0 : 6.695e+02, Time: 0.21, Learning Rate: 0.00090\n",
            "It: 570, Loss: 5.040e-05, Loss_0: 2.221e-09, Loss_r: 5.040e-05, lambda_0 : 6.653e+02, Time: 0.22, Learning Rate: 0.00090\n",
            "It: 580, Loss: 5.019e-05, Loss_0: 2.207e-09, Loss_r: 5.018e-05, lambda_0 : 6.735e+02, Time: 0.22, Learning Rate: 0.00090\n",
            "It: 590, Loss: 4.996e-05, Loss_0: 2.193e-09, Loss_r: 4.996e-05, lambda_0 : 6.852e+02, Time: 0.21, Learning Rate: 0.00090\n",
            "It: 600, Loss: 4.972e-05, Loss_0: 2.180e-09, Loss_r: 4.972e-05, lambda_0 : 6.776e+02, Time: 0.22, Learning Rate: 0.00090\n",
            "It: 610, Loss: 4.948e-05, Loss_0: 2.166e-09, Loss_r: 4.947e-05, lambda_0 : 6.895e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 620, Loss: 4.924e-05, Loss_0: 2.154e-09, Loss_r: 4.924e-05, lambda_0 : 6.947e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 630, Loss: 4.900e-05, Loss_0: 2.141e-09, Loss_r: 4.899e-05, lambda_0 : 6.966e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 640, Loss: 4.874e-05, Loss_0: 2.129e-09, Loss_r: 4.873e-05, lambda_0 : 7.012e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 650, Loss: 4.846e-05, Loss_0: 2.115e-09, Loss_r: 4.846e-05, lambda_0 : 7.059e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 660, Loss: 4.816e-05, Loss_0: 2.103e-09, Loss_r: 4.816e-05, lambda_0 : 7.097e+02, Time: 0.21, Learning Rate: 0.00081\n",
            "It: 670, Loss: 4.785e-05, Loss_0: 2.092e-09, Loss_r: 4.785e-05, lambda_0 : 7.131e+02, Time: 0.21, Learning Rate: 0.00073\n",
            "It: 680, Loss: 4.754e-05, Loss_0: 2.084e-09, Loss_r: 4.754e-05, lambda_0 : 7.159e+02, Time: 0.21, Learning Rate: 0.00073\n",
            "It: 690, Loss: 4.722e-05, Loss_0: 2.077e-09, Loss_r: 4.721e-05, lambda_0 : 7.178e+02, Time: 0.21, Learning Rate: 0.00073\n",
            "It: 700, Loss: 4.686e-05, Loss_0: 2.072e-09, Loss_r: 4.686e-05, lambda_0 : 7.193e+02, Time: 0.21, Learning Rate: 0.00073\n",
            "It: 710, Loss: 4.648e-05, Loss_0: 2.069e-09, Loss_r: 4.648e-05, lambda_0 : 7.202e+02, Time: 0.22, Learning Rate: 0.00073\n",
            "It: 720, Loss: 4.606e-05, Loss_0: 2.069e-09, Loss_r: 4.606e-05, lambda_0 : 7.201e+02, Time: 0.21, Learning Rate: 0.00073\n",
            "It: 730, Loss: 4.561e-05, Loss_0: 2.072e-09, Loss_r: 4.561e-05, lambda_0 : 7.192e+02, Time: 0.20, Learning Rate: 0.00066\n",
            "It: 740, Loss: 4.517e-05, Loss_0: 2.077e-09, Loss_r: 4.517e-05, lambda_0 : 7.180e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 750, Loss: 4.469e-05, Loss_0: 2.082e-09, Loss_r: 4.469e-05, lambda_0 : 7.159e+02, Time: 0.20, Learning Rate: 0.00066\n",
            "It: 760, Loss: 4.418e-05, Loss_0: 2.088e-09, Loss_r: 4.417e-05, lambda_0 : 7.137e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 770, Loss: 4.362e-05, Loss_0: 2.092e-09, Loss_r: 4.362e-05, lambda_0 : 7.120e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 780, Loss: 4.302e-05, Loss_0: 2.092e-09, Loss_r: 4.302e-05, lambda_0 : 7.110e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 790, Loss: 4.238e-05, Loss_0: 2.085e-09, Loss_r: 4.237e-05, lambda_0 : 7.117e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 800, Loss: 4.169e-05, Loss_0: 2.067e-09, Loss_r: 4.169e-05, lambda_0 : 7.128e+02, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 810, Loss: 4.154e-05, Loss_0: 5.850e-07, Loss_r: 4.096e-05, lambda_0 : 3.401e+01, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 820, Loss: 2.351e-04, Loss_0: 1.892e-04, Loss_r: 4.593e-05, lambda_0 : 6.921e-01, Time: 0.20, Learning Rate: 0.00066\n",
            "It: 830, Loss: 5.574e-05, Loss_0: 5.054e-06, Loss_r: 5.069e-05, lambda_0 : 1.226e+01, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 840, Loss: 6.016e-05, Loss_0: 8.284e-06, Loss_r: 5.188e-05, lambda_0 : 7.654e+00, Time: 0.21, Learning Rate: 0.00066\n",
            "It: 850, Loss: 5.546e-05, Loss_0: 3.488e-06, Loss_r: 5.198e-05, lambda_0 : 1.271e+01, Time: 0.22, Learning Rate: 0.00066\n",
            "It: 860, Loss: 5.329e-05, Loss_0: 1.451e-06, Loss_r: 5.183e-05, lambda_0 : 2.245e+01, Time: 0.20, Learning Rate: 0.00066\n",
            "It: 870, Loss: 5.227e-05, Loss_0: 5.824e-07, Loss_r: 5.169e-05, lambda_0 : 3.869e+01, Time: 0.20, Learning Rate: 0.00059\n",
            "It: 880, Loss: 5.177e-05, Loss_0: 1.996e-07, Loss_r: 5.157e-05, lambda_0 : 7.497e+01, Time: 0.22, Learning Rate: 0.00059\n",
            "It: 890, Loss: 5.153e-05, Loss_0: 6.825e-08, Loss_r: 5.146e-05, lambda_0 : 1.346e+02, Time: 0.21, Learning Rate: 0.00059\n",
            "It: 900, Loss: 5.137e-05, Loss_0: 2.485e-08, Loss_r: 5.135e-05, lambda_0 : 2.030e+02, Time: 0.21, Learning Rate: 0.00059\n",
            "It: 910, Loss: 5.125e-05, Loss_0: 1.036e-08, Loss_r: 5.124e-05, lambda_0 : 2.808e+02, Time: 0.20, Learning Rate: 0.00059\n",
            "It: 920, Loss: 5.113e-05, Loss_0: 4.924e-09, Loss_r: 5.112e-05, lambda_0 : 3.826e+02, Time: 0.21, Learning Rate: 0.00059\n",
            "It: 930, Loss: 5.101e-05, Loss_0: 2.891e-09, Loss_r: 5.101e-05, lambda_0 : 4.861e+02, Time: 0.21, Learning Rate: 0.00053\n",
            "It: 940, Loss: 5.090e-05, Loss_0: 2.259e-09, Loss_r: 5.090e-05, lambda_0 : 5.619e+02, Time: 0.20, Learning Rate: 0.00053\n",
            "It: 950, Loss: 5.079e-05, Loss_0: 2.140e-09, Loss_r: 5.079e-05, lambda_0 : 6.069e+02, Time: 0.21, Learning Rate: 0.00053\n",
            "It: 960, Loss: 5.068e-05, Loss_0: 2.029e-09, Loss_r: 5.068e-05, lambda_0 : 6.487e+02, Time: 0.21, Learning Rate: 0.00053\n",
            "It: 970, Loss: 5.057e-05, Loss_0: 1.993e-09, Loss_r: 5.056e-05, lambda_0 : 6.705e+02, Time: 0.21, Learning Rate: 0.00053\n",
            "It: 980, Loss: 5.045e-05, Loss_0: 2.004e-09, Loss_r: 5.045e-05, lambda_0 : 6.769e+02, Time: 0.21, Learning Rate: 0.00053\n",
            "It: 990, Loss: 5.033e-05, Loss_0: 1.998e-09, Loss_r: 5.033e-05, lambda_0 : 6.816e+02, Time: 0.21, Learning Rate: 0.00048\n",
            "It: 1000, Loss: 5.022e-05, Loss_0: 1.982e-09, Loss_r: 5.021e-05, lambda_0 : 6.882e+02, Time: 0.21, Learning Rate: 0.00048\n",
            "It: 1010, Loss: 5.010e-05, Loss_0: 1.977e-09, Loss_r: 5.010e-05, lambda_0 : 6.899e+02, Time: 0.20, Learning Rate: 0.00048\n",
            "It: 1020, Loss: 4.998e-05, Loss_0: 1.970e-09, Loss_r: 4.998e-05, lambda_0 : 6.907e+02, Time: 0.21, Learning Rate: 0.00048\n",
            "It: 1030, Loss: 4.986e-05, Loss_0: 1.960e-09, Loss_r: 4.986e-05, lambda_0 : 6.915e+02, Time: 0.21, Learning Rate: 0.00048\n",
            "It: 1040, Loss: 4.973e-05, Loss_0: 1.953e-09, Loss_r: 4.973e-05, lambda_0 : 6.916e+02, Time: 0.21, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 4.960e-05, Loss_0: 1.944e-09, Loss_r: 4.960e-05, lambda_0 : 6.917e+02, Time: 0.21, Learning Rate: 0.00043\n",
            "It: 1060, Loss: 4.948e-05, Loss_0: 1.936e-09, Loss_r: 4.948e-05, lambda_0 : 6.917e+02, Time: 0.21, Learning Rate: 0.00043\n",
            "It: 1070, Loss: 4.936e-05, Loss_0: 1.928e-09, Loss_r: 4.935e-05, lambda_0 : 6.916e+02, Time: 0.21, Learning Rate: 0.00043\n",
            "It: 1080, Loss: 4.922e-05, Loss_0: 1.920e-09, Loss_r: 4.922e-05, lambda_0 : 6.916e+02, Time: 0.21, Learning Rate: 0.00043\n",
            "It: 1090, Loss: 4.909e-05, Loss_0: 1.912e-09, Loss_r: 4.909e-05, lambda_0 : 6.917e+02, Time: 0.22, Learning Rate: 0.00043\n",
            "It: 1100, Loss: 4.895e-05, Loss_0: 1.903e-09, Loss_r: 4.894e-05, lambda_0 : 6.918e+02, Time: 0.22, Learning Rate: 0.00043\n",
            "It: 1110, Loss: 4.880e-05, Loss_0: 1.895e-09, Loss_r: 4.880e-05, lambda_0 : 6.917e+02, Time: 0.22, Learning Rate: 0.00039\n",
            "It: 1120, Loss: 4.866e-05, Loss_0: 1.887e-09, Loss_r: 4.866e-05, lambda_0 : 6.916e+02, Time: 0.22, Learning Rate: 0.00039\n",
            "It: 1130, Loss: 4.852e-05, Loss_0: 1.878e-09, Loss_r: 4.851e-05, lambda_0 : 6.916e+02, Time: 0.22, Learning Rate: 0.00039\n",
            "It: 1140, Loss: 4.837e-05, Loss_0: 1.870e-09, Loss_r: 4.837e-05, lambda_0 : 6.917e+02, Time: 0.22, Learning Rate: 0.00039\n",
            "It: 1150, Loss: 4.821e-05, Loss_0: 1.861e-09, Loss_r: 4.821e-05, lambda_0 : 6.919e+02, Time: 0.22, Learning Rate: 0.00039\n",
            "It: 1160, Loss: 4.805e-05, Loss_0: 1.852e-09, Loss_r: 4.805e-05, lambda_0 : 6.921e+02, Time: 0.23, Learning Rate: 0.00039\n",
            "It: 1170, Loss: 4.788e-05, Loss_0: 1.842e-09, Loss_r: 4.788e-05, lambda_0 : 6.923e+02, Time: 0.22, Learning Rate: 0.00035\n",
            "It: 1180, Loss: 4.772e-05, Loss_0: 1.833e-09, Loss_r: 4.772e-05, lambda_0 : 6.927e+02, Time: 0.22, Learning Rate: 0.00035\n",
            "It: 1190, Loss: 4.755e-05, Loss_0: 1.824e-09, Loss_r: 4.755e-05, lambda_0 : 6.934e+02, Time: 0.21, Learning Rate: 0.00035\n",
            "It: 1200, Loss: 4.738e-05, Loss_0: 1.814e-09, Loss_r: 4.738e-05, lambda_0 : 6.943e+02, Time: 0.21, Learning Rate: 0.00035\n",
            "It: 1210, Loss: 4.720e-05, Loss_0: 1.803e-09, Loss_r: 4.720e-05, lambda_0 : 6.954e+02, Time: 0.21, Learning Rate: 0.00035\n",
            "It: 1220, Loss: 4.701e-05, Loss_0: 1.791e-09, Loss_r: 4.701e-05, lambda_0 : 6.967e+02, Time: 0.21, Learning Rate: 0.00035\n",
            "It: 1230, Loss: 4.682e-05, Loss_0: 1.779e-09, Loss_r: 4.682e-05, lambda_0 : 6.984e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1240, Loss: 4.664e-05, Loss_0: 1.767e-09, Loss_r: 4.663e-05, lambda_0 : 7.004e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1250, Loss: 4.645e-05, Loss_0: 1.754e-09, Loss_r: 4.645e-05, lambda_0 : 7.027e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1260, Loss: 4.625e-05, Loss_0: 1.740e-09, Loss_r: 4.625e-05, lambda_0 : 7.050e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1270, Loss: 4.605e-05, Loss_0: 1.724e-09, Loss_r: 4.604e-05, lambda_0 : 7.076e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1280, Loss: 4.583e-05, Loss_0: 1.708e-09, Loss_r: 4.583e-05, lambda_0 : 7.107e+02, Time: 0.21, Learning Rate: 0.00031\n",
            "It: 1290, Loss: 4.562e-05, Loss_0: 1.690e-09, Loss_r: 4.561e-05, lambda_0 : 7.183e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1300, Loss: 4.541e-05, Loss_0: 1.673e-09, Loss_r: 4.541e-05, lambda_0 : 7.261e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1310, Loss: 4.520e-05, Loss_0: 1.655e-09, Loss_r: 4.520e-05, lambda_0 : 7.344e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1320, Loss: 4.498e-05, Loss_0: 1.635e-09, Loss_r: 4.498e-05, lambda_0 : 7.430e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1330, Loss: 4.476e-05, Loss_0: 1.615e-09, Loss_r: 4.476e-05, lambda_0 : 7.520e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1340, Loss: 4.453e-05, Loss_0: 1.593e-09, Loss_r: 4.453e-05, lambda_0 : 7.629e+02, Time: 0.21, Learning Rate: 0.00028\n",
            "It: 1350, Loss: 4.429e-05, Loss_0: 1.570e-09, Loss_r: 4.429e-05, lambda_0 : 7.767e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1360, Loss: 4.407e-05, Loss_0: 1.549e-09, Loss_r: 4.407e-05, lambda_0 : 7.893e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1370, Loss: 4.384e-05, Loss_0: 1.526e-09, Loss_r: 4.384e-05, lambda_0 : 8.020e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1380, Loss: 4.361e-05, Loss_0: 1.503e-09, Loss_r: 4.361e-05, lambda_0 : 8.150e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1390, Loss: 4.337e-05, Loss_0: 1.479e-09, Loss_r: 4.337e-05, lambda_0 : 8.279e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1400, Loss: 4.313e-05, Loss_0: 1.454e-09, Loss_r: 4.313e-05, lambda_0 : 8.406e+02, Time: 0.21, Learning Rate: 0.00025\n",
            "It: 1410, Loss: 4.288e-05, Loss_0: 1.428e-09, Loss_r: 4.288e-05, lambda_0 : 8.535e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1420, Loss: 4.265e-05, Loss_0: 1.405e-09, Loss_r: 4.265e-05, lambda_0 : 8.650e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1430, Loss: 4.242e-05, Loss_0: 1.380e-09, Loss_r: 4.242e-05, lambda_0 : 8.759e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1440, Loss: 4.218e-05, Loss_0: 1.356e-09, Loss_r: 4.218e-05, lambda_0 : 8.868e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1450, Loss: 4.194e-05, Loss_0: 1.331e-09, Loss_r: 4.194e-05, lambda_0 : 8.978e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1460, Loss: 4.169e-05, Loss_0: 1.306e-09, Loss_r: 4.169e-05, lambda_0 : 9.088e+02, Time: 0.21, Learning Rate: 0.00023\n",
            "It: 1470, Loss: 4.144e-05, Loss_0: 1.280e-09, Loss_r: 4.144e-05, lambda_0 : 9.199e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1480, Loss: 4.121e-05, Loss_0: 1.257e-09, Loss_r: 4.120e-05, lambda_0 : 9.299e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1490, Loss: 4.097e-05, Loss_0: 1.234e-09, Loss_r: 4.097e-05, lambda_0 : 9.400e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1500, Loss: 4.073e-05, Loss_0: 1.210e-09, Loss_r: 4.073e-05, lambda_0 : 9.501e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1510, Loss: 4.049e-05, Loss_0: 1.187e-09, Loss_r: 4.049e-05, lambda_0 : 9.604e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1520, Loss: 4.025e-05, Loss_0: 1.163e-09, Loss_r: 4.024e-05, lambda_0 : 9.706e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1530, Loss: 4.000e-05, Loss_0: 1.140e-09, Loss_r: 4.000e-05, lambda_0 : 9.808e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1540, Loss: 3.974e-05, Loss_0: 1.116e-09, Loss_r: 3.974e-05, lambda_0 : 9.910e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1550, Loss: 3.949e-05, Loss_0: 1.093e-09, Loss_r: 3.949e-05, lambda_0 : 1.001e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1560, Loss: 3.923e-05, Loss_0: 1.070e-09, Loss_r: 3.923e-05, lambda_0 : 1.010e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1570, Loss: 3.896e-05, Loss_0: 1.047e-09, Loss_r: 3.896e-05, lambda_0 : 1.020e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1580, Loss: 3.870e-05, Loss_0: 1.025e-09, Loss_r: 3.870e-05, lambda_0 : 1.030e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1590, Loss: 3.843e-05, Loss_0: 1.003e-09, Loss_r: 3.843e-05, lambda_0 : 1.039e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1600, Loss: 3.815e-05, Loss_0: 9.809e-10, Loss_r: 3.815e-05, lambda_0 : 1.049e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1610, Loss: 3.787e-05, Loss_0: 9.595e-10, Loss_r: 3.787e-05, lambda_0 : 1.059e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1620, Loss: 3.759e-05, Loss_0: 9.386e-10, Loss_r: 3.759e-05, lambda_0 : 1.068e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1630, Loss: 3.730e-05, Loss_0: 9.182e-10, Loss_r: 3.730e-05, lambda_0 : 1.078e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1640, Loss: 3.701e-05, Loss_0: 8.983e-10, Loss_r: 3.701e-05, lambda_0 : 1.087e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1650, Loss: 3.672e-05, Loss_0: 8.790e-10, Loss_r: 3.672e-05, lambda_0 : 1.096e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1660, Loss: 3.642e-05, Loss_0: 8.602e-10, Loss_r: 3.642e-05, lambda_0 : 1.105e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1670, Loss: 3.612e-05, Loss_0: 8.421e-10, Loss_r: 3.612e-05, lambda_0 : 1.113e+03, Time: 0.23, Learning Rate: 0.00021\n",
            "It: 1680, Loss: 3.581e-05, Loss_0: 8.246e-10, Loss_r: 3.581e-05, lambda_0 : 1.121e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1690, Loss: 3.550e-05, Loss_0: 8.078e-10, Loss_r: 3.550e-05, lambda_0 : 1.129e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1700, Loss: 3.518e-05, Loss_0: 7.917e-10, Loss_r: 3.518e-05, lambda_0 : 1.137e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1710, Loss: 3.486e-05, Loss_0: 7.763e-10, Loss_r: 3.486e-05, lambda_0 : 1.144e+03, Time: 0.23, Learning Rate: 0.00021\n",
            "It: 1720, Loss: 3.453e-05, Loss_0: 7.615e-10, Loss_r: 3.453e-05, lambda_0 : 1.151e+03, Time: 0.23, Learning Rate: 0.00021\n",
            "It: 1730, Loss: 3.419e-05, Loss_0: 7.475e-10, Loss_r: 3.419e-05, lambda_0 : 1.158e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1740, Loss: 3.386e-05, Loss_0: 7.343e-10, Loss_r: 3.386e-05, lambda_0 : 1.165e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1750, Loss: 3.351e-05, Loss_0: 7.217e-10, Loss_r: 3.351e-05, lambda_0 : 1.172e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1760, Loss: 3.316e-05, Loss_0: 7.099e-10, Loss_r: 3.316e-05, lambda_0 : 1.178e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1770, Loss: 3.281e-05, Loss_0: 6.987e-10, Loss_r: 3.281e-05, lambda_0 : 1.184e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1780, Loss: 3.245e-05, Loss_0: 6.884e-10, Loss_r: 3.245e-05, lambda_0 : 1.189e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1790, Loss: 3.208e-05, Loss_0: 6.786e-10, Loss_r: 3.208e-05, lambda_0 : 1.193e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1800, Loss: 3.171e-05, Loss_0: 6.697e-10, Loss_r: 3.171e-05, lambda_0 : 1.197e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1810, Loss: 3.133e-05, Loss_0: 6.613e-10, Loss_r: 3.133e-05, lambda_0 : 1.199e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1820, Loss: 3.095e-05, Loss_0: 6.553e-10, Loss_r: 3.094e-05, lambda_0 : 1.218e+03, Time: 0.22, Learning Rate: 0.00021\n",
            "It: 1830, Loss: 3.060e-05, Loss_0: 8.752e-10, Loss_r: 3.060e-05, lambda_0 : 8.520e+03, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1840, Loss: 6.839e-05, Loss_0: 3.749e-05, Loss_r: 3.090e-05, lambda_0 : 1.037e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1850, Loss: 6.608e-05, Loss_0: 3.504e-05, Loss_r: 3.104e-05, lambda_0 : 5.952e+01, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1860, Loss: 3.191e-05, Loss_0: 5.001e-07, Loss_r: 3.141e-05, lambda_0 : 4.772e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1870, Loss: 3.420e-05, Loss_0: 2.755e-06, Loss_r: 3.144e-05, lambda_0 : 1.220e+02, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1880, Loss: 3.255e-05, Loss_0: 1.260e-06, Loss_r: 3.129e-05, lambda_0 : 9.405e+01, Time: 0.21, Learning Rate: 0.00021\n",
            "It: 1890, Loss: 3.107e-05, Loss_0: 5.599e-08, Loss_r: 3.101e-05, lambda_0 : 1.961e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1900, Loss: 3.073e-05, Loss_0: 1.915e-09, Loss_r: 3.072e-05, lambda_0 : 6.875e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1910, Loss: 3.043e-05, Loss_0: 1.728e-08, Loss_r: 3.041e-05, lambda_0 : 1.949e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1920, Loss: 3.011e-05, Loss_0: 1.917e-08, Loss_r: 3.009e-05, lambda_0 : 1.703e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1930, Loss: 2.976e-05, Loss_0: 1.573e-09, Loss_r: 2.976e-05, lambda_0 : 6.039e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1940, Loss: 2.942e-05, Loss_0: 1.216e-09, Loss_r: 2.942e-05, lambda_0 : 6.563e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1950, Loss: 2.908e-05, Loss_0: 1.091e-09, Loss_r: 2.908e-05, lambda_0 : 7.121e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1960, Loss: 2.874e-05, Loss_0: 6.738e-10, Loss_r: 2.874e-05, lambda_0 : 9.772e+02, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1970, Loss: 2.839e-05, Loss_0: 6.465e-10, Loss_r: 2.839e-05, lambda_0 : 1.166e+03, Time: 0.20, Learning Rate: 0.00019\n",
            "It: 1980, Loss: 2.805e-05, Loss_0: 6.597e-10, Loss_r: 2.805e-05, lambda_0 : 1.102e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 1990, Loss: 2.770e-05, Loss_0: 6.050e-10, Loss_r: 2.770e-05, lambda_0 : 1.135e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2000, Loss: 2.734e-05, Loss_0: 6.125e-10, Loss_r: 2.734e-05, lambda_0 : 1.252e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2010, Loss: 2.699e-05, Loss_0: 5.925e-10, Loss_r: 2.699e-05, lambda_0 : 1.251e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2020, Loss: 2.663e-05, Loss_0: 5.884e-10, Loss_r: 2.663e-05, lambda_0 : 1.230e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2030, Loss: 2.628e-05, Loss_0: 5.870e-10, Loss_r: 2.627e-05, lambda_0 : 1.242e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2040, Loss: 2.592e-05, Loss_0: 5.861e-10, Loss_r: 2.591e-05, lambda_0 : 1.253e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2050, Loss: 2.555e-05, Loss_0: 5.915e-10, Loss_r: 2.555e-05, lambda_0 : 1.262e+03, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2060, Loss: 2.521e-05, Loss_0: 1.368e-08, Loss_r: 2.519e-05, lambda_0 : 1.770e+02, Time: 0.20, Learning Rate: 0.00019\n",
            "It: 2070, Loss: 1.749e-04, Loss_0: 1.499e-04, Loss_r: 2.499e-05, lambda_0 : 8.765e-01, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2080, Loss: 3.294e-05, Loss_0: 6.702e-06, Loss_r: 2.624e-05, lambda_0 : 5.692e+01, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2090, Loss: 3.970e-05, Loss_0: 1.274e-05, Loss_r: 2.697e-05, lambda_0 : 5.558e+00, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2100, Loss: 2.990e-05, Loss_0: 2.753e-06, Loss_r: 2.715e-05, lambda_0 : 1.548e+01, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2110, Loss: 2.797e-05, Loss_0: 9.262e-07, Loss_r: 2.705e-05, lambda_0 : 2.805e+01, Time: 0.21, Learning Rate: 0.00019\n",
            "It: 2120, Loss: 2.720e-05, Loss_0: 4.168e-07, Loss_r: 2.679e-05, lambda_0 : 4.087e+01, Time: 0.20, Learning Rate: 0.00017\n",
            "It: 2130, Loss: 2.655e-05, Loss_0: 4.648e-08, Loss_r: 2.650e-05, lambda_0 : 1.210e+02, Time: 0.21, Learning Rate: 0.00017\n",
            "It: 2140, Loss: 2.620e-05, Loss_0: 7.360e-09, Loss_r: 2.619e-05, lambda_0 : 2.569e+02, Time: 0.21, Learning Rate: 0.00017\n",
            "It: 2150, Loss: 2.587e-05, Loss_0: 6.310e-09, Loss_r: 2.586e-05, lambda_0 : 2.968e+02, Time: 0.20, Learning Rate: 0.00017\n",
            "It: 2160, Loss: 2.554e-05, Loss_0: 8.305e-09, Loss_r: 2.554e-05, lambda_0 : 2.415e+02, Time: 0.21, Learning Rate: 0.00017\n",
            "It: 2170, Loss: 2.521e-05, Loss_0: 2.740e-09, Loss_r: 2.521e-05, lambda_0 : 4.262e+02, Time: 0.21, Learning Rate: 0.00017\n",
            "It: 2180, Loss: 2.487e-05, Loss_0: 8.581e-10, Loss_r: 2.487e-05, lambda_0 : 7.625e+02, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2190, Loss: 2.457e-05, Loss_0: 9.747e-10, Loss_r: 2.457e-05, lambda_0 : 7.151e+02, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2200, Loss: 2.427e-05, Loss_0: 5.805e-10, Loss_r: 2.427e-05, lambda_0 : 1.089e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2210, Loss: 2.397e-05, Loss_0: 5.796e-10, Loss_r: 2.397e-05, lambda_0 : 1.081e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2220, Loss: 2.367e-05, Loss_0: 5.531e-10, Loss_r: 2.367e-05, lambda_0 : 1.291e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2230, Loss: 2.337e-05, Loss_0: 5.567e-10, Loss_r: 2.337e-05, lambda_0 : 1.317e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2240, Loss: 2.307e-05, Loss_0: 5.529e-10, Loss_r: 2.307e-05, lambda_0 : 1.301e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2250, Loss: 2.277e-05, Loss_0: 5.483e-10, Loss_r: 2.277e-05, lambda_0 : 1.274e+03, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2260, Loss: 2.247e-05, Loss_0: 5.447e-10, Loss_r: 2.247e-05, lambda_0 : 1.249e+03, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2270, Loss: 2.217e-05, Loss_0: 5.425e-10, Loss_r: 2.217e-05, lambda_0 : 1.244e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2280, Loss: 2.187e-05, Loss_0: 5.409e-10, Loss_r: 2.187e-05, lambda_0 : 1.248e+03, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2290, Loss: 2.157e-05, Loss_0: 5.375e-10, Loss_r: 2.157e-05, lambda_0 : 1.230e+03, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2300, Loss: 2.128e-05, Loss_0: 5.350e-10, Loss_r: 2.128e-05, lambda_0 : 1.228e+03, Time: 0.23, Learning Rate: 0.00015\n",
            "It: 2310, Loss: 2.098e-05, Loss_0: 5.317e-10, Loss_r: 2.098e-05, lambda_0 : 1.217e+03, Time: 0.23, Learning Rate: 0.00015\n",
            "It: 2320, Loss: 2.068e-05, Loss_0: 5.287e-10, Loss_r: 2.068e-05, lambda_0 : 1.210e+03, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2330, Loss: 2.038e-05, Loss_0: 5.275e-10, Loss_r: 2.038e-05, lambda_0 : 1.250e+03, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2340, Loss: 2.016e-05, Loss_0: 5.958e-10, Loss_r: 2.016e-05, lambda_0 : 1.627e+04, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2350, Loss: 2.035e-05, Loss_0: 2.941e-10, Loss_r: 2.035e-05, lambda_0 : 6.874e+04, Time: 0.22, Learning Rate: 0.00015\n",
            "It: 2360, Loss: 1.957e-05, Loss_0: 4.445e-10, Loss_r: 1.956e-05, lambda_0 : 1.488e+04, Time: 0.23, Learning Rate: 0.00015\n",
            "It: 2370, Loss: 1.932e-05, Loss_0: 1.137e-09, Loss_r: 1.932e-05, lambda_0 : 7.654e+03, Time: 0.27, Learning Rate: 0.00015\n",
            "It: 2380, Loss: 3.248e-05, Loss_0: 1.350e-05, Loss_r: 1.898e-05, lambda_0 : 6.146e+00, Time: 0.24, Learning Rate: 0.00015\n",
            "It: 2390, Loss: 3.897e-05, Loss_0: 1.970e-05, Loss_r: 1.927e-05, lambda_0 : 1.992e+01, Time: 0.21, Learning Rate: 0.00015\n",
            "It: 2400, Loss: 3.350e-05, Loss_0: 1.386e-05, Loss_r: 1.964e-05, lambda_0 : 4.782e+00, Time: 0.20, Learning Rate: 0.00015\n",
            "It: 2410, Loss: 2.401e-05, Loss_0: 4.286e-06, Loss_r: 1.972e-05, lambda_0 : 1.479e+01, Time: 0.27, Learning Rate: 0.00015\n",
            "It: 2420, Loss: 1.967e-05, Loss_0: 6.571e-08, Loss_r: 1.961e-05, lambda_0 : 7.082e+01, Time: 0.24, Learning Rate: 0.00015\n",
            "It: 2430, Loss: 1.992e-05, Loss_0: 5.207e-07, Loss_r: 1.940e-05, lambda_0 : 2.422e+01, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2440, Loss: 1.938e-05, Loss_0: 2.063e-07, Loss_r: 1.917e-05, lambda_0 : 4.096e+01, Time: 0.20, Learning Rate: 0.00014\n",
            "It: 2450, Loss: 1.901e-05, Loss_0: 7.741e-08, Loss_r: 1.893e-05, lambda_0 : 6.351e+01, Time: 0.26, Learning Rate: 0.00014\n",
            "It: 2460, Loss: 1.869e-05, Loss_0: 3.878e-09, Loss_r: 1.869e-05, lambda_0 : 2.686e+02, Time: 0.26, Learning Rate: 0.00014\n",
            "It: 2470, Loss: 1.845e-05, Loss_0: 9.386e-09, Loss_r: 1.844e-05, lambda_0 : 1.851e+02, Time: 0.24, Learning Rate: 0.00014\n",
            "It: 2480, Loss: 1.819e-05, Loss_0: 4.738e-10, Loss_r: 1.819e-05, lambda_0 : 1.173e+03, Time: 0.20, Learning Rate: 0.00014\n",
            "It: 2490, Loss: 1.794e-05, Loss_0: 9.792e-10, Loss_r: 1.794e-05, lambda_0 : 5.338e+02, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2500, Loss: 1.770e-05, Loss_0: 6.226e-10, Loss_r: 1.770e-05, lambda_0 : 7.140e+02, Time: 0.20, Learning Rate: 0.00014\n",
            "It: 2510, Loss: 1.745e-05, Loss_0: 4.704e-10, Loss_r: 1.745e-05, lambda_0 : 1.113e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2520, Loss: 1.721e-05, Loss_0: 5.658e-10, Loss_r: 1.721e-05, lambda_0 : 8.125e+02, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2530, Loss: 1.697e-05, Loss_0: 4.756e-10, Loss_r: 1.697e-05, lambda_0 : 1.128e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2540, Loss: 1.673e-05, Loss_0: 4.669e-10, Loss_r: 1.673e-05, lambda_0 : 1.134e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2550, Loss: 1.649e-05, Loss_0: 4.699e-10, Loss_r: 1.649e-05, lambda_0 : 1.114e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2560, Loss: 1.625e-05, Loss_0: 4.592e-10, Loss_r: 1.625e-05, lambda_0 : 1.095e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2570, Loss: 1.601e-05, Loss_0: 4.594e-10, Loss_r: 1.601e-05, lambda_0 : 1.108e+03, Time: 0.20, Learning Rate: 0.00014\n",
            "It: 2580, Loss: 1.578e-05, Loss_0: 4.645e-10, Loss_r: 1.578e-05, lambda_0 : 1.052e+03, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2590, Loss: 1.555e-05, Loss_0: 1.104e-08, Loss_r: 1.554e-05, lambda_0 : 1.352e+02, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2600, Loss: 1.503e-04, Loss_0: 1.350e-04, Loss_r: 1.533e-05, lambda_0 : 7.383e-01, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2610, Loss: 4.961e-05, Loss_0: 3.381e-05, Loss_r: 1.580e-05, lambda_0 : 2.227e+00, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2620, Loss: 2.284e-05, Loss_0: 6.765e-06, Loss_r: 1.608e-05, lambda_0 : 5.965e+00, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2630, Loss: 1.666e-05, Loss_0: 5.616e-07, Loss_r: 1.610e-05, lambda_0 : 2.238e+01, Time: 0.21, Learning Rate: 0.00014\n",
            "It: 2640, Loss: 1.634e-05, Loss_0: 3.559e-07, Loss_r: 1.599e-05, lambda_0 : 2.681e+01, Time: 0.20, Learning Rate: 0.00014\n",
            "It: 2650, Loss: 1.629e-05, Loss_0: 4.905e-07, Loss_r: 1.580e-05, lambda_0 : 2.240e+01, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2660, Loss: 1.568e-05, Loss_0: 7.370e-08, Loss_r: 1.561e-05, lambda_0 : 5.813e+01, Time: 0.20, Learning Rate: 0.00012\n",
            "It: 2670, Loss: 1.541e-05, Loss_0: 5.548e-09, Loss_r: 1.540e-05, lambda_0 : 1.909e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2680, Loss: 1.522e-05, Loss_0: 3.209e-08, Loss_r: 1.519e-05, lambda_0 : 8.118e+01, Time: 0.20, Learning Rate: 0.00012\n",
            "It: 2690, Loss: 1.498e-05, Loss_0: 4.559e-10, Loss_r: 1.498e-05, lambda_0 : 8.563e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2700, Loss: 1.477e-05, Loss_0: 3.043e-09, Loss_r: 1.477e-05, lambda_0 : 2.583e+02, Time: 0.20, Learning Rate: 0.00012\n",
            "It: 2710, Loss: 1.456e-05, Loss_0: 2.007e-09, Loss_r: 1.456e-05, lambda_0 : 2.994e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2720, Loss: 1.436e-05, Loss_0: 9.495e-10, Loss_r: 1.436e-05, lambda_0 : 4.473e+02, Time: 0.20, Learning Rate: 0.00012\n",
            "It: 2730, Loss: 1.415e-05, Loss_0: 5.300e-10, Loss_r: 1.415e-05, lambda_0 : 7.042e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2740, Loss: 1.395e-05, Loss_0: 4.476e-10, Loss_r: 1.395e-05, lambda_0 : 9.038e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2750, Loss: 1.375e-05, Loss_0: 4.284e-10, Loss_r: 1.375e-05, lambda_0 : 8.851e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2760, Loss: 1.355e-05, Loss_0: 4.235e-10, Loss_r: 1.355e-05, lambda_0 : 8.963e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2770, Loss: 1.335e-05, Loss_0: 4.152e-10, Loss_r: 1.335e-05, lambda_0 : 9.525e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2780, Loss: 1.315e-05, Loss_0: 4.122e-10, Loss_r: 1.315e-05, lambda_0 : 1.018e+03, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2790, Loss: 1.296e-05, Loss_0: 4.100e-10, Loss_r: 1.296e-05, lambda_0 : 1.013e+03, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2800, Loss: 1.276e-05, Loss_0: 4.072e-10, Loss_r: 1.276e-05, lambda_0 : 1.004e+03, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2810, Loss: 1.257e-05, Loss_0: 4.040e-10, Loss_r: 1.257e-05, lambda_0 : 9.865e+02, Time: 0.20, Learning Rate: 0.00012\n",
            "It: 2820, Loss: 1.239e-05, Loss_0: 4.014e-10, Loss_r: 1.238e-05, lambda_0 : 9.522e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2830, Loss: 1.221e-05, Loss_0: 1.046e-08, Loss_r: 1.220e-05, lambda_0 : 1.223e+02, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2840, Loss: 2.749e-04, Loss_0: 2.628e-04, Loss_r: 1.211e-05, lambda_0 : 4.084e-01, Time: 0.22, Learning Rate: 0.00012\n",
            "It: 2850, Loss: 4.832e-05, Loss_0: 3.540e-05, Loss_r: 1.293e-05, lambda_0 : 2.058e+00, Time: 0.22, Learning Rate: 0.00012\n",
            "It: 2860, Loss: 1.726e-05, Loss_0: 3.783e-06, Loss_r: 1.347e-05, lambda_0 : 7.105e+00, Time: 0.21, Learning Rate: 0.00012\n",
            "It: 2870, Loss: 1.659e-05, Loss_0: 2.940e-06, Loss_r: 1.365e-05, lambda_0 : 9.236e+00, Time: 0.22, Learning Rate: 0.00012\n",
            "It: 2880, Loss: 1.516e-05, Loss_0: 1.572e-06, Loss_r: 1.359e-05, lambda_0 : 1.204e+01, Time: 0.22, Learning Rate: 0.00012\n",
            "It: 2890, Loss: 1.393e-05, Loss_0: 4.782e-07, Loss_r: 1.345e-05, lambda_0 : 2.187e+01, Time: 0.22, Learning Rate: 0.00011\n",
            "It: 2900, Loss: 1.329e-05, Loss_0: 1.148e-08, Loss_r: 1.328e-05, lambda_0 : 1.216e+02, Time: 0.22, Learning Rate: 0.00011\n",
            "It: 2910, Loss: 1.316e-05, Loss_0: 5.624e-08, Loss_r: 1.311e-05, lambda_0 : 5.545e+01, Time: 0.22, Learning Rate: 0.00011\n",
            "It: 2920, Loss: 1.296e-05, Loss_0: 2.920e-08, Loss_r: 1.293e-05, lambda_0 : 8.204e+01, Time: 0.21, Learning Rate: 0.00011\n",
            "It: 2930, Loss: 1.276e-05, Loss_0: 3.298e-09, Loss_r: 1.275e-05, lambda_0 : 2.250e+02, Time: 0.21, Learning Rate: 0.00011\n",
            "It: 2940, Loss: 1.258e-05, Loss_0: 3.029e-09, Loss_r: 1.258e-05, lambda_0 : 2.196e+02, Time: 0.20, Learning Rate: 0.00011\n",
            "It: 2950, Loss: 1.240e-05, Loss_0: 2.092e-09, Loss_r: 1.240e-05, lambda_0 : 2.607e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 2960, Loss: 1.225e-05, Loss_0: 5.823e-10, Loss_r: 1.225e-05, lambda_0 : 5.384e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 2970, Loss: 1.209e-05, Loss_0: 4.878e-10, Loss_r: 1.209e-05, lambda_0 : 6.182e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 2980, Loss: 1.194e-05, Loss_0: 4.219e-10, Loss_r: 1.194e-05, lambda_0 : 7.979e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 2990, Loss: 1.179e-05, Loss_0: 4.161e-10, Loss_r: 1.179e-05, lambda_0 : 8.080e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3000, Loss: 1.164e-05, Loss_0: 3.847e-10, Loss_r: 1.164e-05, lambda_0 : 9.860e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3010, Loss: 1.149e-05, Loss_0: 3.801e-10, Loss_r: 1.149e-05, lambda_0 : 9.476e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3020, Loss: 1.135e-05, Loss_0: 3.784e-10, Loss_r: 1.134e-05, lambda_0 : 9.363e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3030, Loss: 1.120e-05, Loss_0: 3.766e-10, Loss_r: 1.120e-05, lambda_0 : 9.373e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3040, Loss: 1.106e-05, Loss_0: 3.750e-10, Loss_r: 1.105e-05, lambda_0 : 9.372e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3050, Loss: 1.091e-05, Loss_0: 3.735e-10, Loss_r: 1.091e-05, lambda_0 : 9.356e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3060, Loss: 1.077e-05, Loss_0: 3.719e-10, Loss_r: 1.077e-05, lambda_0 : 9.315e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3070, Loss: 1.063e-05, Loss_0: 3.701e-10, Loss_r: 1.063e-05, lambda_0 : 9.240e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3080, Loss: 1.049e-05, Loss_0: 3.683e-10, Loss_r: 1.049e-05, lambda_0 : 9.150e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3090, Loss: 1.036e-05, Loss_0: 3.667e-10, Loss_r: 1.036e-05, lambda_0 : 9.082e+02, Time: 0.20, Learning Rate: 0.00010\n",
            "It: 3100, Loss: 1.022e-05, Loss_0: 3.651e-10, Loss_r: 1.022e-05, lambda_0 : 9.024e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3110, Loss: 1.009e-05, Loss_0: 3.635e-10, Loss_r: 1.009e-05, lambda_0 : 8.937e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3120, Loss: 9.954e-06, Loss_0: 3.619e-10, Loss_r: 9.953e-06, lambda_0 : 8.869e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3130, Loss: 9.823e-06, Loss_0: 3.603e-10, Loss_r: 9.823e-06, lambda_0 : 8.791e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3140, Loss: 9.693e-06, Loss_0: 3.588e-10, Loss_r: 9.693e-06, lambda_0 : 8.725e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3150, Loss: 9.565e-06, Loss_0: 3.573e-10, Loss_r: 9.565e-06, lambda_0 : 8.643e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3160, Loss: 9.439e-06, Loss_0: 3.558e-10, Loss_r: 9.439e-06, lambda_0 : 8.567e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3170, Loss: 9.314e-06, Loss_0: 3.544e-10, Loss_r: 9.314e-06, lambda_0 : 8.504e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3180, Loss: 9.191e-06, Loss_0: 3.530e-10, Loss_r: 9.191e-06, lambda_0 : 8.425e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3190, Loss: 9.069e-06, Loss_0: 3.516e-10, Loss_r: 9.069e-06, lambda_0 : 8.335e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3200, Loss: 8.949e-06, Loss_0: 3.502e-10, Loss_r: 8.949e-06, lambda_0 : 8.264e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3210, Loss: 8.830e-06, Loss_0: 3.488e-10, Loss_r: 8.830e-06, lambda_0 : 8.183e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3220, Loss: 8.713e-06, Loss_0: 3.474e-10, Loss_r: 8.713e-06, lambda_0 : 8.076e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3230, Loss: 8.598e-06, Loss_0: 3.449e-10, Loss_r: 8.598e-06, lambda_0 : 8.812e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3240, Loss: 8.506e-06, Loss_0: 3.202e-10, Loss_r: 8.506e-06, lambda_0 : 1.324e+04, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3250, Loss: 8.416e-06, Loss_0: 5.173e-10, Loss_r: 8.415e-06, lambda_0 : 9.532e+03, Time: 0.20, Learning Rate: 0.00010\n",
            "It: 3260, Loss: 8.539e-06, Loss_0: 2.549e-07, Loss_r: 8.284e-06, lambda_0 : 2.471e+02, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3270, Loss: 3.011e-05, Loss_0: 2.181e-05, Loss_r: 8.303e-06, lambda_0 : 2.287e+01, Time: 0.20, Learning Rate: 0.00010\n",
            "It: 3280, Loss: 8.678e-06, Loss_0: 3.346e-09, Loss_r: 8.675e-06, lambda_0 : 1.026e+03, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3290, Loss: 1.441e-05, Loss_0: 5.538e-06, Loss_r: 8.876e-06, lambda_0 : 3.972e+00, Time: 0.21, Learning Rate: 0.00010\n",
            "It: 3300, Loss: 9.377e-06, Loss_0: 4.661e-07, Loss_r: 8.911e-06, lambda_0 : 3.490e+01, Time: 0.20, Learning Rate: 0.00010\n",
            "It: 3310, Loss: 9.415e-06, Loss_0: 5.578e-07, Loss_r: 8.857e-06, lambda_0 : 2.396e+01, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3320, Loss: 8.776e-06, Loss_0: 2.920e-09, Loss_r: 8.773e-06, lambda_0 : 1.770e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3330, Loss: 8.744e-06, Loss_0: 6.801e-08, Loss_r: 8.676e-06, lambda_0 : 3.257e+01, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3340, Loss: 8.620e-06, Loss_0: 4.373e-08, Loss_r: 8.576e-06, lambda_0 : 4.369e+01, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3350, Loss: 8.492e-06, Loss_0: 1.708e-08, Loss_r: 8.475e-06, lambda_0 : 6.498e+01, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3360, Loss: 8.381e-06, Loss_0: 6.357e-09, Loss_r: 8.375e-06, lambda_0 : 1.054e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3370, Loss: 8.278e-06, Loss_0: 2.399e-09, Loss_r: 8.276e-06, lambda_0 : 1.678e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3380, Loss: 8.178e-06, Loss_0: 7.840e-10, Loss_r: 8.178e-06, lambda_0 : 3.019e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3390, Loss: 8.081e-06, Loss_0: 3.363e-10, Loss_r: 8.081e-06, lambda_0 : 7.929e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3400, Loss: 7.985e-06, Loss_0: 3.878e-10, Loss_r: 7.985e-06, lambda_0 : 5.083e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3410, Loss: 7.891e-06, Loss_0: 3.481e-10, Loss_r: 7.890e-06, lambda_0 : 5.987e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3420, Loss: 7.797e-06, Loss_0: 3.369e-10, Loss_r: 7.797e-06, lambda_0 : 7.333e+02, Time: 0.23, Learning Rate: 0.00009\n",
            "It: 3430, Loss: 7.705e-06, Loss_0: 3.310e-10, Loss_r: 7.705e-06, lambda_0 : 7.607e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3440, Loss: 7.614e-06, Loss_0: 3.280e-10, Loss_r: 7.614e-06, lambda_0 : 7.068e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3450, Loss: 7.525e-06, Loss_0: 3.276e-10, Loss_r: 7.524e-06, lambda_0 : 7.435e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3460, Loss: 7.436e-06, Loss_0: 3.254e-10, Loss_r: 7.436e-06, lambda_0 : 7.240e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3470, Loss: 7.349e-06, Loss_0: 3.248e-10, Loss_r: 7.349e-06, lambda_0 : 7.255e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3480, Loss: 7.263e-06, Loss_0: 3.235e-10, Loss_r: 7.263e-06, lambda_0 : 7.127e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3490, Loss: 7.178e-06, Loss_0: 3.228e-10, Loss_r: 7.178e-06, lambda_0 : 7.103e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3500, Loss: 7.095e-06, Loss_0: 3.217e-10, Loss_r: 7.095e-06, lambda_0 : 7.008e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3510, Loss: 7.013e-06, Loss_0: 3.208e-10, Loss_r: 7.012e-06, lambda_0 : 6.934e+02, Time: 0.22, Learning Rate: 0.00009\n",
            "It: 3520, Loss: 6.932e-06, Loss_0: 3.199e-10, Loss_r: 6.931e-06, lambda_0 : 6.870e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3530, Loss: 6.852e-06, Loss_0: 3.190e-10, Loss_r: 6.852e-06, lambda_0 : 6.805e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3540, Loss: 6.773e-06, Loss_0: 3.181e-10, Loss_r: 6.773e-06, lambda_0 : 6.725e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3550, Loss: 6.696e-06, Loss_0: 3.172e-10, Loss_r: 6.695e-06, lambda_0 : 6.652e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3560, Loss: 6.620e-06, Loss_0: 3.163e-10, Loss_r: 6.619e-06, lambda_0 : 6.581e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3570, Loss: 6.544e-06, Loss_0: 3.154e-10, Loss_r: 6.544e-06, lambda_0 : 6.511e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3580, Loss: 6.471e-06, Loss_0: 3.146e-10, Loss_r: 6.470e-06, lambda_0 : 6.439e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3590, Loss: 6.398e-06, Loss_0: 3.137e-10, Loss_r: 6.398e-06, lambda_0 : 6.352e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3600, Loss: 6.326e-06, Loss_0: 3.129e-10, Loss_r: 6.326e-06, lambda_0 : 6.307e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3610, Loss: 6.256e-06, Loss_0: 3.121e-10, Loss_r: 6.256e-06, lambda_0 : 6.241e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3620, Loss: 6.187e-06, Loss_0: 3.125e-10, Loss_r: 6.186e-06, lambda_0 : 6.203e+02, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3630, Loss: 6.119e-06, Loss_0: 6.508e-10, Loss_r: 6.118e-06, lambda_0 : 2.438e+02, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3640, Loss: 6.917e-06, Loss_0: 8.668e-07, Loss_r: 6.051e-06, lambda_0 : 6.442e+00, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3650, Loss: 3.121e-05, Loss_0: 2.518e-05, Loss_r: 6.027e-06, lambda_0 : 1.471e+00, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3660, Loss: 1.375e-05, Loss_0: 7.653e-06, Loss_r: 6.100e-06, lambda_0 : 2.451e+00, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3670, Loss: 7.892e-06, Loss_0: 1.785e-06, Loss_r: 6.107e-06, lambda_0 : 4.918e+00, Time: 0.20, Learning Rate: 0.00009\n",
            "It: 3680, Loss: 6.581e-06, Loss_0: 5.045e-07, Loss_r: 6.076e-06, lambda_0 : 9.935e+00, Time: 0.21, Learning Rate: 0.00009\n",
            "It: 3690, Loss: 6.364e-06, Loss_0: 3.383e-07, Loss_r: 6.025e-06, lambda_0 : 1.169e+01, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3700, Loss: 6.015e-06, Loss_0: 4.412e-08, Loss_r: 5.971e-06, lambda_0 : 3.228e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3710, Loss: 6.004e-06, Loss_0: 8.957e-08, Loss_r: 5.914e-06, lambda_0 : 2.033e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3720, Loss: 5.857e-06, Loss_0: 5.784e-10, Loss_r: 5.857e-06, lambda_0 : 2.569e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3730, Loss: 5.809e-06, Loss_0: 9.725e-09, Loss_r: 5.800e-06, lambda_0 : 5.626e+01, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3740, Loss: 5.749e-06, Loss_0: 5.616e-09, Loss_r: 5.743e-06, lambda_0 : 7.249e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3750, Loss: 5.690e-06, Loss_0: 2.321e-09, Loss_r: 5.687e-06, lambda_0 : 1.105e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3760, Loss: 5.633e-06, Loss_0: 8.208e-10, Loss_r: 5.632e-06, lambda_0 : 1.903e+02, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3770, Loss: 5.578e-06, Loss_0: 2.965e-10, Loss_r: 5.578e-06, lambda_0 : 5.572e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3780, Loss: 5.525e-06, Loss_0: 4.074e-10, Loss_r: 5.525e-06, lambda_0 : 2.998e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3790, Loss: 5.473e-06, Loss_0: 3.115e-10, Loss_r: 5.472e-06, lambda_0 : 4.788e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3800, Loss: 5.421e-06, Loss_0: 2.977e-10, Loss_r: 5.421e-06, lambda_0 : 5.201e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3810, Loss: 5.371e-06, Loss_0: 3.059e-10, Loss_r: 5.370e-06, lambda_0 : 4.959e+02, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3820, Loss: 5.321e-06, Loss_0: 2.977e-10, Loss_r: 5.320e-06, lambda_0 : 4.887e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3830, Loss: 5.272e-06, Loss_0: 3.582e-10, Loss_r: 5.271e-06, lambda_0 : 3.196e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3840, Loss: 5.236e-06, Loss_0: 1.219e-08, Loss_r: 5.223e-06, lambda_0 : 4.651e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3850, Loss: 1.533e-05, Loss_0: 1.015e-05, Loss_r: 5.180e-06, lambda_0 : 1.533e+00, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3860, Loss: 5.464e-06, Loss_0: 3.094e-07, Loss_r: 5.154e-06, lambda_0 : 8.836e+00, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3870, Loss: 6.461e-06, Loss_0: 1.334e-06, Loss_r: 5.127e-06, lambda_0 : 4.492e+00, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3880, Loss: 5.331e-06, Loss_0: 2.391e-07, Loss_r: 5.092e-06, lambda_0 : 1.043e+01, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3890, Loss: 5.227e-06, Loss_0: 1.752e-07, Loss_r: 5.052e-06, lambda_0 : 1.179e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3900, Loss: 5.017e-06, Loss_0: 6.721e-09, Loss_r: 5.010e-06, lambda_0 : 6.141e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3910, Loss: 2.170e-05, Loss_0: 1.674e-05, Loss_r: 4.965e-06, lambda_0 : 1.091e+00, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3920, Loss: 5.578e-06, Loss_0: 6.066e-07, Loss_r: 4.971e-06, lambda_0 : 1.640e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3930, Loss: 5.200e-06, Loss_0: 2.153e-07, Loss_r: 4.985e-06, lambda_0 : 7.568e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3940, Loss: 9.399e-06, Loss_0: 4.395e-06, Loss_r: 5.005e-06, lambda_0 : 4.977e+01, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3950, Loss: 4.987e-06, Loss_0: 5.111e-09, Loss_r: 4.981e-06, lambda_0 : 6.315e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3960, Loss: 5.534e-06, Loss_0: 5.806e-07, Loss_r: 4.953e-06, lambda_0 : 6.492e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 3970, Loss: 4.920e-06, Loss_0: 5.634e-09, Loss_r: 4.914e-06, lambda_0 : 5.048e+02, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3980, Loss: 4.901e-06, Loss_0: 2.895e-08, Loss_r: 4.872e-06, lambda_0 : 7.234e+01, Time: 0.20, Learning Rate: 0.00008\n",
            "It: 3990, Loss: 4.846e-06, Loss_0: 1.416e-08, Loss_r: 4.831e-06, lambda_0 : 1.153e+02, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 4000, Loss: 4.792e-06, Loss_0: 2.287e-09, Loss_r: 4.790e-06, lambda_0 : 9.150e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 4010, Loss: 4.767e-06, Loss_0: 1.703e-08, Loss_r: 4.750e-06, lambda_0 : 8.926e+01, Time: 0.22, Learning Rate: 0.00008\n",
            "It: 4020, Loss: 5.120e-06, Loss_0: 4.094e-07, Loss_r: 4.711e-06, lambda_0 : 5.266e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 4030, Loss: 3.665e-05, Loss_0: 3.194e-05, Loss_r: 4.706e-06, lambda_0 : 2.299e+01, Time: 0.21, Learning Rate: 0.00008\n",
            "It: 4040, Loss: 4.879e-06, Loss_0: 1.437e-07, Loss_r: 4.736e-06, lambda_0 : 8.277e+01, Time: 0.22, Learning Rate: 0.00008\n",
            "It: 4050, Loss: 4.783e-06, Loss_0: 2.701e-08, Loss_r: 4.756e-06, lambda_0 : 1.821e+02, Time: 0.23, Learning Rate: 0.00008\n",
            "It: 4060, Loss: 5.085e-06, Loss_0: 3.398e-07, Loss_r: 4.745e-06, lambda_0 : 3.742e+01, Time: 0.22, Learning Rate: 0.00008\n",
            "It: 4070, Loss: 4.995e-06, Loss_0: 2.775e-07, Loss_r: 4.718e-06, lambda_0 : 2.847e+01, Time: 0.22, Learning Rate: 0.00007\n",
            "It: 4080, Loss: 4.730e-06, Loss_0: 4.284e-08, Loss_r: 4.687e-06, lambda_0 : 5.369e+01, Time: 0.22, Learning Rate: 0.00007\n",
            "It: 4090, Loss: 4.654e-06, Loss_0: 5.305e-10, Loss_r: 4.653e-06, lambda_0 : 2.752e+02, Time: 0.22, Learning Rate: 0.00007\n",
            "It: 4100, Loss: 4.631e-06, Loss_0: 1.259e-08, Loss_r: 4.619e-06, lambda_0 : 3.473e+01, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4110, Loss: 4.589e-06, Loss_0: 4.001e-09, Loss_r: 4.585e-06, lambda_0 : 6.509e+01, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4120, Loss: 4.552e-06, Loss_0: 9.477e-10, Loss_r: 4.551e-06, lambda_0 : 1.300e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4130, Loss: 4.518e-06, Loss_0: 6.698e-10, Loss_r: 4.517e-06, lambda_0 : 1.547e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4140, Loss: 4.485e-06, Loss_0: 5.681e-10, Loss_r: 4.485e-06, lambda_0 : 1.701e+02, Time: 0.20, Learning Rate: 0.00007\n",
            "It: 4150, Loss: 4.453e-06, Loss_0: 3.158e-10, Loss_r: 4.453e-06, lambda_0 : 2.973e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4160, Loss: 4.421e-06, Loss_0: 2.770e-10, Loss_r: 4.421e-06, lambda_0 : 4.216e+02, Time: 0.20, Learning Rate: 0.00007\n",
            "It: 4170, Loss: 4.390e-06, Loss_0: 2.744e-10, Loss_r: 4.390e-06, lambda_0 : 4.166e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4180, Loss: 4.360e-06, Loss_0: 2.752e-10, Loss_r: 4.359e-06, lambda_0 : 4.123e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4190, Loss: 4.330e-06, Loss_0: 2.752e-10, Loss_r: 4.329e-06, lambda_0 : 4.070e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4200, Loss: 4.300e-06, Loss_0: 2.726e-10, Loss_r: 4.300e-06, lambda_0 : 4.022e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4210, Loss: 4.272e-06, Loss_0: 2.712e-10, Loss_r: 4.271e-06, lambda_0 : 3.890e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4220, Loss: 4.243e-06, Loss_0: 2.708e-10, Loss_r: 4.243e-06, lambda_0 : 3.879e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4230, Loss: 4.215e-06, Loss_0: 2.705e-10, Loss_r: 4.215e-06, lambda_0 : 3.851e+02, Time: 0.20, Learning Rate: 0.00007\n",
            "It: 4240, Loss: 4.188e-06, Loss_0: 2.698e-10, Loss_r: 4.188e-06, lambda_0 : 3.786e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4250, Loss: 4.161e-06, Loss_0: 2.694e-10, Loss_r: 4.161e-06, lambda_0 : 3.739e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4260, Loss: 4.135e-06, Loss_0: 2.690e-10, Loss_r: 4.135e-06, lambda_0 : 3.716e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4270, Loss: 4.109e-06, Loss_0: 2.697e-10, Loss_r: 4.109e-06, lambda_0 : 3.674e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4280, Loss: 4.084e-06, Loss_0: 3.748e-10, Loss_r: 4.084e-06, lambda_0 : 1.953e+02, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4290, Loss: 4.118e-06, Loss_0: 5.896e-08, Loss_r: 4.059e-06, lambda_0 : 1.303e+01, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4300, Loss: 6.774e-05, Loss_0: 6.370e-05, Loss_r: 4.036e-06, lambda_0 : 9.320e-01, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4310, Loss: 4.154e-06, Loss_0: 5.097e-08, Loss_r: 4.103e-06, lambda_0 : 2.845e+01, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4320, Loss: 1.267e-05, Loss_0: 8.516e-06, Loss_r: 4.155e-06, lambda_0 : 3.311e+00, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4330, Loss: 6.811e-06, Loss_0: 2.643e-06, Loss_r: 4.168e-06, lambda_0 : 2.108e+00, Time: 0.21, Learning Rate: 0.00007\n",
            "It: 4340, Loss: 4.406e-06, Loss_0: 2.438e-07, Loss_r: 4.162e-06, lambda_0 : 8.182e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4350, Loss: 4.282e-06, Loss_0: 1.374e-07, Loss_r: 4.144e-06, lambda_0 : 1.045e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4360, Loss: 4.153e-06, Loss_0: 2.989e-08, Loss_r: 4.123e-06, lambda_0 : 2.117e+01, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4370, Loss: 4.146e-06, Loss_0: 4.589e-08, Loss_r: 4.100e-06, lambda_0 : 1.651e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4380, Loss: 4.097e-06, Loss_0: 1.999e-08, Loss_r: 4.077e-06, lambda_0 : 2.288e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4390, Loss: 4.060e-06, Loss_0: 6.570e-09, Loss_r: 4.053e-06, lambda_0 : 4.110e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4400, Loss: 4.033e-06, Loss_0: 2.481e-09, Loss_r: 4.030e-06, lambda_0 : 6.104e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4410, Loss: 4.008e-06, Loss_0: 1.095e-09, Loss_r: 4.007e-06, lambda_0 : 9.473e+01, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4420, Loss: 3.985e-06, Loss_0: 5.915e-10, Loss_r: 3.985e-06, lambda_0 : 1.307e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4430, Loss: 3.963e-06, Loss_0: 3.186e-10, Loss_r: 3.963e-06, lambda_0 : 2.088e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4440, Loss: 3.941e-06, Loss_0: 2.588e-10, Loss_r: 3.941e-06, lambda_0 : 3.483e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4450, Loss: 3.920e-06, Loss_0: 2.710e-10, Loss_r: 3.920e-06, lambda_0 : 2.922e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4460, Loss: 3.899e-06, Loss_0: 2.592e-10, Loss_r: 3.899e-06, lambda_0 : 3.052e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4470, Loss: 3.879e-06, Loss_0: 2.571e-10, Loss_r: 3.879e-06, lambda_0 : 3.216e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4480, Loss: 3.859e-06, Loss_0: 2.575e-10, Loss_r: 3.859e-06, lambda_0 : 3.310e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4490, Loss: 3.839e-06, Loss_0: 2.569e-10, Loss_r: 3.839e-06, lambda_0 : 3.260e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4500, Loss: 3.820e-06, Loss_0: 2.561e-10, Loss_r: 3.819e-06, lambda_0 : 3.218e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4510, Loss: 3.801e-06, Loss_0: 2.556e-10, Loss_r: 3.800e-06, lambda_0 : 3.155e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4520, Loss: 3.782e-06, Loss_0: 2.552e-10, Loss_r: 3.782e-06, lambda_0 : 3.116e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4530, Loss: 3.764e-06, Loss_0: 2.548e-10, Loss_r: 3.763e-06, lambda_0 : 3.073e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4540, Loss: 3.746e-06, Loss_0: 2.545e-10, Loss_r: 3.745e-06, lambda_0 : 3.037e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4550, Loss: 3.728e-06, Loss_0: 2.541e-10, Loss_r: 3.728e-06, lambda_0 : 3.018e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4560, Loss: 3.711e-06, Loss_0: 2.538e-10, Loss_r: 3.710e-06, lambda_0 : 3.020e+02, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4570, Loss: 3.694e-06, Loss_0: 2.536e-10, Loss_r: 3.693e-06, lambda_0 : 3.147e+02, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4580, Loss: 3.677e-06, Loss_0: 2.551e-10, Loss_r: 3.677e-06, lambda_0 : 1.654e+03, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4590, Loss: 3.686e-06, Loss_0: 2.724e-10, Loss_r: 3.686e-06, lambda_0 : 1.423e+04, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4600, Loss: 3.662e-06, Loss_0: 2.356e-10, Loss_r: 3.662e-06, lambda_0 : 1.300e+04, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4610, Loss: 3.631e-06, Loss_0: 2.571e-10, Loss_r: 3.631e-06, lambda_0 : 3.860e+03, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4620, Loss: 3.617e-06, Loss_0: 2.591e-10, Loss_r: 3.617e-06, lambda_0 : 5.136e+03, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4630, Loss: 3.599e-06, Loss_0: 2.521e-10, Loss_r: 3.599e-06, lambda_0 : 6.155e+02, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4640, Loss: 3.585e-06, Loss_0: 2.489e-10, Loss_r: 3.584e-06, lambda_0 : 2.019e+03, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4650, Loss: 3.570e-06, Loss_0: 2.752e-10, Loss_r: 3.570e-06, lambda_0 : 3.776e+02, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4660, Loss: 3.560e-06, Loss_0: 4.744e-09, Loss_r: 3.555e-06, lambda_0 : 8.626e+01, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4670, Loss: 6.198e-06, Loss_0: 2.657e-06, Loss_r: 3.541e-06, lambda_0 : 1.435e+00, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4680, Loss: 3.910e-06, Loss_0: 3.785e-07, Loss_r: 3.532e-06, lambda_0 : 7.015e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4690, Loss: 3.868e-06, Loss_0: 3.470e-07, Loss_r: 3.521e-06, lambda_0 : 6.300e+00, Time: 0.22, Learning Rate: 0.00006\n",
            "It: 4700, Loss: 3.630e-06, Loss_0: 1.213e-07, Loss_r: 3.508e-06, lambda_0 : 9.263e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4710, Loss: 3.518e-06, Loss_0: 2.173e-08, Loss_r: 3.496e-06, lambda_0 : 1.672e+01, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4720, Loss: 7.809e-06, Loss_0: 4.324e-06, Loss_r: 3.485e-06, lambda_0 : 1.970e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4730, Loss: 3.698e-06, Loss_0: 2.221e-07, Loss_r: 3.476e-06, lambda_0 : 4.984e+00, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4740, Loss: 3.655e-06, Loss_0: 1.887e-07, Loss_r: 3.467e-06, lambda_0 : 5.531e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4750, Loss: 3.524e-06, Loss_0: 6.828e-08, Loss_r: 3.456e-06, lambda_0 : 1.100e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4760, Loss: 3.472e-06, Loss_0: 2.855e-08, Loss_r: 3.444e-06, lambda_0 : 1.571e+01, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4770, Loss: 3.440e-06, Loss_0: 8.262e-09, Loss_r: 3.432e-06, lambda_0 : 3.608e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4780, Loss: 2.127e-04, Loss_0: 2.093e-04, Loss_r: 3.419e-06, lambda_0 : 2.373e-01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4790, Loss: 5.898e-06, Loss_0: 2.313e-06, Loss_r: 3.585e-06, lambda_0 : 1.046e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4800, Loss: 1.567e-05, Loss_0: 1.195e-05, Loss_r: 3.719e-06, lambda_0 : 1.682e+00, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4810, Loss: 4.868e-06, Loss_0: 1.090e-06, Loss_r: 3.778e-06, lambda_0 : 6.814e+00, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4820, Loss: 3.901e-06, Loss_0: 1.165e-07, Loss_r: 3.785e-06, lambda_0 : 5.250e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4830, Loss: 4.309e-06, Loss_0: 5.278e-07, Loss_r: 3.781e-06, lambda_0 : 8.938e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4840, Loss: 3.770e-06, Loss_0: 6.144e-09, Loss_r: 3.764e-06, lambda_0 : 4.560e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4850, Loss: 3.832e-06, Loss_0: 8.532e-08, Loss_r: 3.746e-06, lambda_0 : 7.111e+01, Time: 0.20, Learning Rate: 0.00006\n",
            "It: 4860, Loss: 3.730e-06, Loss_0: 2.584e-09, Loss_r: 3.728e-06, lambda_0 : 1.701e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4870, Loss: 3.719e-06, Loss_0: 9.936e-09, Loss_r: 3.709e-06, lambda_0 : 2.699e+01, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4880, Loss: 3.692e-06, Loss_0: 4.844e-10, Loss_r: 3.691e-06, lambda_0 : 1.453e+02, Time: 0.21, Learning Rate: 0.00006\n",
            "It: 4890, Loss: 3.675e-06, Loss_0: 1.534e-09, Loss_r: 3.673e-06, lambda_0 : 7.885e+01, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4900, Loss: 3.658e-06, Loss_0: 5.710e-10, Loss_r: 3.657e-06, lambda_0 : 1.109e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4910, Loss: 3.642e-06, Loss_0: 2.361e-10, Loss_r: 3.642e-06, lambda_0 : 2.615e+02, Time: 0.20, Learning Rate: 0.00005\n",
            "It: 4920, Loss: 3.627e-06, Loss_0: 2.492e-10, Loss_r: 3.626e-06, lambda_0 : 2.342e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4930, Loss: 3.612e-06, Loss_0: 2.547e-10, Loss_r: 3.611e-06, lambda_0 : 2.392e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4940, Loss: 3.597e-06, Loss_0: 2.312e-10, Loss_r: 3.597e-06, lambda_0 : 2.888e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4950, Loss: 3.582e-06, Loss_0: 2.312e-10, Loss_r: 3.582e-06, lambda_0 : 2.876e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4960, Loss: 3.569e-06, Loss_0: 2.320e-10, Loss_r: 3.569e-06, lambda_0 : 3.030e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4970, Loss: 3.557e-06, Loss_0: 2.307e-10, Loss_r: 3.556e-06, lambda_0 : 2.919e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4980, Loss: 3.544e-06, Loss_0: 2.312e-10, Loss_r: 3.544e-06, lambda_0 : 2.958e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "It: 4990, Loss: 3.532e-06, Loss_0: 2.306e-10, Loss_r: 3.531e-06, lambda_0 : 2.900e+02, Time: 0.21, Learning Rate: 0.00005\n",
            "Training time: 106.9072\n",
            "[1, 256, 256, 256, 256, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 7.461e-03, Loss_0: 3.104e-03, Loss_r: 4.357e-03, lambda_0 : 6.608e+01, Time: 1.38, Learning Rate: 0.00100\n",
            "It: 10, Loss: 1.997e-03, Loss_0: 1.785e-03, Loss_r: 2.119e-04, lambda_0 : 4.701e+00, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 20, Loss: 5.876e-04, Loss_0: 3.969e-04, Loss_r: 1.907e-04, lambda_0 : 1.266e+01, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.767e-04, Loss_0: 1.008e-04, Loss_r: 7.588e-05, lambda_0 : 9.636e+00, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 40, Loss: 7.185e-05, Loss_0: 1.941e-06, Loss_r: 6.991e-05, lambda_0 : 4.141e+01, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.390e-05, Loss_0: 9.639e-07, Loss_r: 6.294e-05, lambda_0 : 3.152e+01, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 60, Loss: 6.128e-05, Loss_0: 6.566e-07, Loss_r: 6.062e-05, lambda_0 : 2.459e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 70, Loss: 6.052e-05, Loss_0: 9.169e-07, Loss_r: 5.960e-05, lambda_0 : 5.179e+00, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.989e-05, Loss_0: 5.190e-07, Loss_r: 5.937e-05, lambda_0 : 8.919e+00, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.938e-05, Loss_0: 2.104e-07, Loss_r: 5.917e-05, lambda_0 : 7.886e+00, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.900e-05, Loss_0: 1.729e-08, Loss_r: 5.898e-05, lambda_0 : 2.277e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.882e-05, Loss_0: 3.541e-08, Loss_r: 5.879e-05, lambda_0 : 1.815e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.869e-05, Loss_0: 4.886e-08, Loss_r: 5.864e-05, lambda_0 : 1.403e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.854e-05, Loss_0: 8.340e-09, Loss_r: 5.853e-05, lambda_0 : 3.257e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.857e-05, Loss_0: 1.846e-07, Loss_r: 5.839e-05, lambda_0 : 6.153e+00, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 150, Loss: 1.256e-04, Loss_0: 6.734e-05, Loss_r: 5.826e-05, lambda_0 : 3.511e-01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.826e-05, Loss_0: 7.056e-08, Loss_r: 5.819e-05, lambda_0 : 1.070e+01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.829e-05, Loss_0: 1.628e-07, Loss_r: 5.813e-05, lambda_0 : 7.303e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.831e-05, Loss_0: 2.798e-07, Loss_r: 5.803e-05, lambda_0 : 5.934e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.851e-05, Loss_0: 5.878e-07, Loss_r: 5.792e-05, lambda_0 : 3.933e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.781e-05, Loss_0: 2.814e-08, Loss_r: 5.778e-05, lambda_0 : 1.703e+01, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.776e-05, Loss_0: 1.162e-07, Loss_r: 5.765e-05, lambda_0 : 8.149e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.751e-05, Loss_0: 1.607e-08, Loss_r: 5.749e-05, lambda_0 : 2.277e+01, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 230, Loss: 6.036e-05, Loss_0: 3.023e-06, Loss_r: 5.734e-05, lambda_0 : 1.776e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.899e-05, Loss_0: 1.807e-06, Loss_r: 5.718e-05, lambda_0 : 2.217e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.773e-05, Loss_0: 7.082e-07, Loss_r: 5.702e-05, lambda_0 : 3.579e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.713e-05, Loss_0: 2.843e-07, Loss_r: 5.684e-05, lambda_0 : 5.628e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.678e-05, Loss_0: 1.288e-07, Loss_r: 5.665e-05, lambda_0 : 8.188e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.646e-05, Loss_0: 1.334e-08, Loss_r: 5.644e-05, lambda_0 : 3.308e+01, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 290, Loss: 2.510e-04, Loss_0: 1.926e-04, Loss_r: 5.834e-05, lambda_0 : 3.727e-01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 300, Loss: 3.343e-04, Loss_0: 2.750e-04, Loss_r: 5.931e-05, lambda_0 : 3.513e-01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 310, Loss: 1.248e-04, Loss_0: 6.534e-05, Loss_r: 5.946e-05, lambda_0 : 6.134e-01, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 320, Loss: 6.617e-05, Loss_0: 6.546e-06, Loss_r: 5.962e-05, lambda_0 : 1.876e+00, Time: 1.11, Learning Rate: 0.00100\n",
            "It: 330, Loss: 6.151e-05, Loss_0: 1.978e-06, Loss_r: 5.953e-05, lambda_0 : 5.106e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 340, Loss: 6.125e-05, Loss_0: 1.955e-06, Loss_r: 5.930e-05, lambda_0 : 3.819e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.990e-05, Loss_0: 7.542e-07, Loss_r: 5.914e-05, lambda_0 : 4.790e+00, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.937e-05, Loss_0: 2.925e-07, Loss_r: 5.908e-05, lambda_0 : 7.230e+00, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.918e-05, Loss_0: 1.146e-07, Loss_r: 5.907e-05, lambda_0 : 1.472e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.912e-05, Loss_0: 2.173e-08, Loss_r: 5.910e-05, lambda_0 : 2.820e+01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.907e-05, Loss_0: 4.109e-09, Loss_r: 5.906e-05, lambda_0 : 5.516e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.902e-05, Loss_0: 1.939e-08, Loss_r: 5.900e-05, lambda_0 : 3.053e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.898e-05, Loss_0: 5.866e-09, Loss_r: 5.897e-05, lambda_0 : 5.672e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.893e-05, Loss_0: 1.355e-08, Loss_r: 5.892e-05, lambda_0 : 3.456e+01, Time: 1.10, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.888e-05, Loss_0: 8.880e-09, Loss_r: 5.887e-05, lambda_0 : 4.540e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.883e-05, Loss_0: 1.093e-08, Loss_r: 5.882e-05, lambda_0 : 3.851e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.877e-05, Loss_0: 1.020e-08, Loss_r: 5.876e-05, lambda_0 : 4.118e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.872e-05, Loss_0: 1.021e-08, Loss_r: 5.871e-05, lambda_0 : 4.019e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.866e-05, Loss_0: 1.045e-08, Loss_r: 5.865e-05, lambda_0 : 3.976e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.859e-05, Loss_0: 1.017e-08, Loss_r: 5.858e-05, lambda_0 : 4.025e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.852e-05, Loss_0: 1.038e-08, Loss_r: 5.851e-05, lambda_0 : 3.934e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.845e-05, Loss_0: 1.022e-08, Loss_r: 5.844e-05, lambda_0 : 3.963e+01, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.837e-05, Loss_0: 1.030e-08, Loss_r: 5.836e-05, lambda_0 : 3.920e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.829e-05, Loss_0: 1.027e-08, Loss_r: 5.827e-05, lambda_0 : 3.901e+01, Time: 1.08, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.819e-05, Loss_0: 1.024e-08, Loss_r: 5.818e-05, lambda_0 : 3.882e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.810e-05, Loss_0: 1.026e-08, Loss_r: 5.809e-05, lambda_0 : 3.853e+01, Time: 1.09, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.799e-05, Loss_0: 1.023e-08, Loss_r: 5.798e-05, lambda_0 : 3.829e+01, Time: 1.09, Learning Rate: 0.00090\n",
            "It: 560, Loss: 5.788e-05, Loss_0: 1.022e-08, Loss_r: 5.787e-05, lambda_0 : 3.805e+01, Time: 1.09, Learning Rate: 0.00090\n",
            "It: 570, Loss: 5.777e-05, Loss_0: 1.020e-08, Loss_r: 5.776e-05, lambda_0 : 3.780e+01, Time: 1.09, Learning Rate: 0.00090\n",
            "It: 580, Loss: 5.764e-05, Loss_0: 1.018e-08, Loss_r: 5.763e-05, lambda_0 : 3.756e+01, Time: 1.08, Learning Rate: 0.00090\n",
            "It: 590, Loss: 5.751e-05, Loss_0: 1.016e-08, Loss_r: 5.750e-05, lambda_0 : 3.730e+01, Time: 1.08, Learning Rate: 0.00090\n",
            "It: 600, Loss: 5.736e-05, Loss_0: 1.013e-08, Loss_r: 5.735e-05, lambda_0 : 3.704e+01, Time: 1.09, Learning Rate: 0.00090\n",
            "It: 610, Loss: 5.719e-05, Loss_0: 1.010e-08, Loss_r: 5.718e-05, lambda_0 : 3.677e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 620, Loss: 5.703e-05, Loss_0: 1.006e-08, Loss_r: 5.702e-05, lambda_0 : 3.653e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 630, Loss: 5.684e-05, Loss_0: 1.002e-08, Loss_r: 5.683e-05, lambda_0 : 3.647e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 640, Loss: 5.664e-05, Loss_0: 9.970e-09, Loss_r: 5.663e-05, lambda_0 : 3.648e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 650, Loss: 5.641e-05, Loss_0: 9.910e-09, Loss_r: 5.640e-05, lambda_0 : 3.770e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 660, Loss: 5.615e-05, Loss_0: 9.836e-09, Loss_r: 5.614e-05, lambda_0 : 3.907e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 670, Loss: 5.585e-05, Loss_0: 9.747e-09, Loss_r: 5.584e-05, lambda_0 : 4.062e+01, Time: 1.08, Learning Rate: 0.00081\n",
            "It: 680, Loss: 5.551e-05, Loss_0: 9.638e-09, Loss_r: 5.550e-05, lambda_0 : 4.946e+01, Time: 1.08, Learning Rate: 0.00081\n",
            "It: 690, Loss: 5.512e-05, Loss_0: 9.501e-09, Loss_r: 5.511e-05, lambda_0 : 6.055e+01, Time: 1.08, Learning Rate: 0.00081\n",
            "It: 700, Loss: 5.467e-05, Loss_0: 9.331e-09, Loss_r: 5.466e-05, lambda_0 : 7.445e+01, Time: 1.08, Learning Rate: 0.00081\n",
            "It: 710, Loss: 5.414e-05, Loss_0: 9.117e-09, Loss_r: 5.414e-05, lambda_0 : 9.186e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 720, Loss: 5.352e-05, Loss_0: 8.846e-09, Loss_r: 5.351e-05, lambda_0 : 1.135e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 730, Loss: 5.279e-05, Loss_0: 8.503e-09, Loss_r: 5.278e-05, lambda_0 : 1.402e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 740, Loss: 5.192e-05, Loss_0: 8.071e-09, Loss_r: 5.192e-05, lambda_0 : 1.724e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 750, Loss: 5.092e-05, Loss_0: 7.530e-09, Loss_r: 5.091e-05, lambda_0 : 2.093e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 760, Loss: 4.976e-05, Loss_0: 6.871e-09, Loss_r: 4.975e-05, lambda_0 : 2.484e+02, Time: 1.10, Learning Rate: 0.00081\n",
            "It: 770, Loss: 4.849e-05, Loss_0: 6.094e-09, Loss_r: 4.848e-05, lambda_0 : 2.832e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 780, Loss: 4.715e-05, Loss_0: 5.232e-09, Loss_r: 4.715e-05, lambda_0 : 3.010e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 790, Loss: 4.608e-05, Loss_0: 2.276e-07, Loss_r: 4.585e-05, lambda_0 : 3.840e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 800, Loss: 6.767e-05, Loss_0: 7.782e-06, Loss_r: 5.989e-05, lambda_0 : 1.332e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 810, Loss: 1.285e-04, Loss_0: 6.910e-05, Loss_r: 5.944e-05, lambda_0 : 1.319e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 820, Loss: 7.277e-05, Loss_0: 1.281e-05, Loss_r: 5.996e-05, lambda_0 : 4.384e+01, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 830, Loss: 7.787e-05, Loss_0: 1.929e-05, Loss_r: 5.858e-05, lambda_0 : 7.459e+00, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 840, Loss: 5.874e-05, Loss_0: 2.786e-08, Loss_r: 5.871e-05, lambda_0 : 3.041e+02, Time: 1.09, Learning Rate: 0.00081\n",
            "It: 850, Loss: 6.073e-05, Loss_0: 2.039e-06, Loss_r: 5.869e-05, lambda_0 : 2.884e+01, Time: 1.09, Learning Rate: 0.00073\n",
            "It: 860, Loss: 5.878e-05, Loss_0: 2.971e-07, Loss_r: 5.848e-05, lambda_0 : 3.497e+01, Time: 1.09, Learning Rate: 0.00073\n",
            "It: 870, Loss: 5.841e-05, Loss_0: 4.368e-08, Loss_r: 5.836e-05, lambda_0 : 2.730e+01, Time: 1.10, Learning Rate: 0.00073\n",
            "It: 880, Loss: 5.833e-05, Loss_0: 1.095e-07, Loss_r: 5.823e-05, lambda_0 : 1.911e+01, Time: 1.10, Learning Rate: 0.00073\n",
            "It: 890, Loss: 5.811e-05, Loss_0: 2.634e-08, Loss_r: 5.809e-05, lambda_0 : 3.794e+01, Time: 1.10, Learning Rate: 0.00073\n",
            "It: 900, Loss: 5.794e-05, Loss_0: 8.079e-09, Loss_r: 5.794e-05, lambda_0 : 7.959e+01, Time: 1.09, Learning Rate: 0.00073\n",
            "It: 910, Loss: 5.776e-05, Loss_0: 1.182e-08, Loss_r: 5.775e-05, lambda_0 : 8.065e+01, Time: 1.10, Learning Rate: 0.00066\n",
            "It: 920, Loss: 5.757e-05, Loss_0: 8.343e-09, Loss_r: 5.756e-05, lambda_0 : 1.177e+02, Time: 1.09, Learning Rate: 0.00066\n",
            "It: 930, Loss: 5.736e-05, Loss_0: 9.393e-09, Loss_r: 5.735e-05, lambda_0 : 1.451e+02, Time: 1.09, Learning Rate: 0.00066\n",
            "It: 940, Loss: 5.712e-05, Loss_0: 9.481e-09, Loss_r: 5.711e-05, lambda_0 : 1.660e+02, Time: 1.09, Learning Rate: 0.00066\n",
            "It: 950, Loss: 5.684e-05, Loss_0: 8.814e-09, Loss_r: 5.683e-05, lambda_0 : 2.002e+02, Time: 1.09, Learning Rate: 0.00066\n",
            "It: 960, Loss: 5.652e-05, Loss_0: 8.475e-09, Loss_r: 5.651e-05, lambda_0 : 2.196e+02, Time: 1.09, Learning Rate: 0.00066\n",
            "It: 970, Loss: 5.616e-05, Loss_0: 8.033e-09, Loss_r: 5.615e-05, lambda_0 : 2.516e+02, Time: 1.09, Learning Rate: 0.00059\n",
            "It: 980, Loss: 5.578e-05, Loss_0: 7.899e-09, Loss_r: 5.578e-05, lambda_0 : 2.875e+02, Time: 1.10, Learning Rate: 0.00059\n",
            "It: 990, Loss: 5.536e-05, Loss_0: 7.596e-09, Loss_r: 5.535e-05, lambda_0 : 3.317e+02, Time: 1.10, Learning Rate: 0.00059\n",
            "It: 1000, Loss: 5.489e-05, Loss_0: 7.274e-09, Loss_r: 5.489e-05, lambda_0 : 3.744e+02, Time: 1.10, Learning Rate: 0.00059\n",
            "It: 1010, Loss: 5.438e-05, Loss_0: 6.840e-09, Loss_r: 5.437e-05, lambda_0 : 4.190e+02, Time: 1.09, Learning Rate: 0.00059\n",
            "It: 1020, Loss: 5.382e-05, Loss_0: 6.383e-09, Loss_r: 5.382e-05, lambda_0 : 4.623e+02, Time: 1.09, Learning Rate: 0.00059\n",
            "It: 1030, Loss: 5.324e-05, Loss_0: 5.883e-09, Loss_r: 5.324e-05, lambda_0 : 5.010e+02, Time: 1.10, Learning Rate: 0.00053\n",
            "It: 1040, Loss: 5.271e-05, Loss_0: 5.415e-09, Loss_r: 5.271e-05, lambda_0 : 5.265e+02, Time: 1.10, Learning Rate: 0.00053\n",
            "It: 1050, Loss: 5.219e-05, Loss_0: 4.930e-09, Loss_r: 5.218e-05, lambda_0 : 5.383e+02, Time: 1.10, Learning Rate: 0.00053\n",
            "It: 1060, Loss: 5.169e-05, Loss_0: 4.449e-09, Loss_r: 5.169e-05, lambda_0 : 5.309e+02, Time: 1.10, Learning Rate: 0.00053\n",
            "It: 1070, Loss: 5.124e-05, Loss_0: 3.991e-09, Loss_r: 5.123e-05, lambda_0 : 5.017e+02, Time: 1.09, Learning Rate: 0.00053\n",
            "It: 1080, Loss: 5.083e-05, Loss_0: 3.573e-09, Loss_r: 5.083e-05, lambda_0 : 4.495e+02, Time: 1.09, Learning Rate: 0.00053\n",
            "It: 1090, Loss: 5.047e-05, Loss_0: 3.204e-09, Loss_r: 5.047e-05, lambda_0 : 3.763e+02, Time: 1.10, Learning Rate: 0.00048\n",
            "It: 1100, Loss: 5.018e-05, Loss_0: 2.921e-09, Loss_r: 5.018e-05, lambda_0 : 2.973e+02, Time: 1.10, Learning Rate: 0.00048\n",
            "It: 1110, Loss: 4.993e-05, Loss_0: 2.686e-09, Loss_r: 4.992e-05, lambda_0 : 2.728e+02, Time: 1.09, Learning Rate: 0.00048\n",
            "It: 1120, Loss: 4.968e-05, Loss_0: 2.493e-09, Loss_r: 4.968e-05, lambda_0 : 2.954e+02, Time: 1.09, Learning Rate: 0.00048\n",
            "It: 1130, Loss: 4.945e-05, Loss_0: 2.337e-09, Loss_r: 4.945e-05, lambda_0 : 3.162e+02, Time: 1.09, Learning Rate: 0.00048\n",
            "It: 1140, Loss: 4.923e-05, Loss_0: 2.214e-09, Loss_r: 4.922e-05, lambda_0 : 3.348e+02, Time: 1.09, Learning Rate: 0.00048\n",
            "It: 1150, Loss: 4.900e-05, Loss_0: 2.115e-09, Loss_r: 4.900e-05, lambda_0 : 3.510e+02, Time: 1.09, Learning Rate: 0.00043\n",
            "It: 1160, Loss: 4.879e-05, Loss_0: 2.044e-09, Loss_r: 4.879e-05, lambda_0 : 3.634e+02, Time: 1.09, Learning Rate: 0.00043\n",
            "It: 1170, Loss: 4.857e-05, Loss_0: 1.986e-09, Loss_r: 4.857e-05, lambda_0 : 3.741e+02, Time: 1.09, Learning Rate: 0.00043\n",
            "It: 1180, Loss: 4.835e-05, Loss_0: 1.937e-09, Loss_r: 4.835e-05, lambda_0 : 3.833e+02, Time: 1.09, Learning Rate: 0.00043\n",
            "It: 1190, Loss: 4.812e-05, Loss_0: 1.895e-09, Loss_r: 4.812e-05, lambda_0 : 3.915e+02, Time: 1.09, Learning Rate: 0.00043\n",
            "It: 1200, Loss: 4.788e-05, Loss_0: 1.858e-09, Loss_r: 4.788e-05, lambda_0 : 3.988e+02, Time: 1.10, Learning Rate: 0.00043\n",
            "It: 1210, Loss: 4.763e-05, Loss_0: 1.824e-09, Loss_r: 4.763e-05, lambda_0 : 4.055e+02, Time: 1.10, Learning Rate: 0.00039\n",
            "It: 1220, Loss: 4.740e-05, Loss_0: 1.796e-09, Loss_r: 4.740e-05, lambda_0 : 4.110e+02, Time: 1.10, Learning Rate: 0.00039\n",
            "It: 1230, Loss: 4.716e-05, Loss_0: 1.770e-09, Loss_r: 4.716e-05, lambda_0 : 4.163e+02, Time: 1.09, Learning Rate: 0.00039\n",
            "It: 1240, Loss: 4.691e-05, Loss_0: 1.744e-09, Loss_r: 4.691e-05, lambda_0 : 4.214e+02, Time: 1.09, Learning Rate: 0.00039\n",
            "It: 1250, Loss: 4.665e-05, Loss_0: 1.719e-09, Loss_r: 4.664e-05, lambda_0 : 4.348e+02, Time: 1.09, Learning Rate: 0.00039\n",
            "It: 1260, Loss: 4.637e-05, Loss_0: 1.695e-09, Loss_r: 4.637e-05, lambda_0 : 4.577e+02, Time: 1.09, Learning Rate: 0.00039\n",
            "It: 1270, Loss: 4.609e-05, Loss_0: 1.671e-09, Loss_r: 4.608e-05, lambda_0 : 4.803e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1280, Loss: 4.582e-05, Loss_0: 1.650e-09, Loss_r: 4.582e-05, lambda_0 : 5.002e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1290, Loss: 4.554e-05, Loss_0: 1.628e-09, Loss_r: 4.554e-05, lambda_0 : 5.199e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1300, Loss: 4.524e-05, Loss_0: 1.608e-09, Loss_r: 4.524e-05, lambda_0 : 5.390e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1310, Loss: 4.494e-05, Loss_0: 1.587e-09, Loss_r: 4.493e-05, lambda_0 : 5.573e+02, Time: 1.10, Learning Rate: 0.00035\n",
            "It: 1320, Loss: 4.461e-05, Loss_0: 1.567e-09, Loss_r: 4.461e-05, lambda_0 : 5.747e+02, Time: 1.10, Learning Rate: 0.00035\n",
            "It: 1330, Loss: 4.428e-05, Loss_0: 1.547e-09, Loss_r: 4.428e-05, lambda_0 : 5.906e+02, Time: 1.10, Learning Rate: 0.00035\n",
            "It: 1340, Loss: 4.392e-05, Loss_0: 1.528e-09, Loss_r: 4.392e-05, lambda_0 : 6.045e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1350, Loss: 4.355e-05, Loss_0: 1.510e-09, Loss_r: 4.355e-05, lambda_0 : 6.159e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1360, Loss: 4.316e-05, Loss_0: 1.492e-09, Loss_r: 4.315e-05, lambda_0 : 6.240e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1370, Loss: 4.274e-05, Loss_0: 1.476e-09, Loss_r: 4.274e-05, lambda_0 : 6.276e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1380, Loss: 4.230e-05, Loss_0: 1.461e-09, Loss_r: 4.230e-05, lambda_0 : 6.262e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1390, Loss: 4.183e-05, Loss_0: 1.448e-09, Loss_r: 4.183e-05, lambda_0 : 6.178e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1400, Loss: 4.134e-05, Loss_0: 1.436e-09, Loss_r: 4.134e-05, lambda_0 : 6.015e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1410, Loss: 4.081e-05, Loss_0: 1.427e-09, Loss_r: 4.081e-05, lambda_0 : 5.751e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1420, Loss: 4.026e-05, Loss_0: 1.421e-09, Loss_r: 4.025e-05, lambda_0 : 5.365e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1430, Loss: 3.966e-05, Loss_0: 1.417e-09, Loss_r: 3.966e-05, lambda_0 : 4.844e+02, Time: 1.10, Learning Rate: 0.00035\n",
            "It: 1440, Loss: 3.903e-05, Loss_0: 1.416e-09, Loss_r: 3.903e-05, lambda_0 : 4.416e+02, Time: 1.10, Learning Rate: 0.00035\n",
            "It: 1450, Loss: 3.865e-05, Loss_0: 2.799e-07, Loss_r: 3.837e-05, lambda_0 : 2.608e+01, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1460, Loss: 3.038e-04, Loss_0: 2.614e-04, Loss_r: 4.238e-05, lambda_0 : 8.577e-01, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1470, Loss: 8.400e-05, Loss_0: 3.562e-05, Loss_r: 4.838e-05, lambda_0 : 2.876e+00, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1480, Loss: 5.671e-05, Loss_0: 6.173e-06, Loss_r: 5.053e-05, lambda_0 : 1.116e+01, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1490, Loss: 5.117e-05, Loss_0: 2.653e-08, Loss_r: 5.114e-05, lambda_0 : 1.633e+02, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1500, Loss: 5.140e-05, Loss_0: 2.019e-07, Loss_r: 5.120e-05, lambda_0 : 6.771e+01, Time: 1.09, Learning Rate: 0.00035\n",
            "It: 1510, Loss: 5.135e-05, Loss_0: 2.780e-07, Loss_r: 5.107e-05, lambda_0 : 5.096e+01, Time: 1.09, Learning Rate: 0.00031\n",
            "It: 1520, Loss: 5.105e-05, Loss_0: 1.546e-07, Loss_r: 5.089e-05, lambda_0 : 5.493e+01, Time: 1.09, Learning Rate: 0.00031\n",
            "It: 1530, Loss: 5.073e-05, Loss_0: 2.624e-08, Loss_r: 5.070e-05, lambda_0 : 1.183e+02, Time: 1.09, Learning Rate: 0.00031\n",
            "It: 1540, Loss: 5.051e-05, Loss_0: 1.909e-09, Loss_r: 5.050e-05, lambda_0 : 3.775e+02, Time: 1.10, Learning Rate: 0.00031\n",
            "It: 1550, Loss: 5.031e-05, Loss_0: 3.981e-09, Loss_r: 5.030e-05, lambda_0 : 2.560e+02, Time: 1.10, Learning Rate: 0.00031\n",
            "It: 1560, Loss: 5.011e-05, Loss_0: 3.970e-09, Loss_r: 5.010e-05, lambda_0 : 2.603e+02, Time: 1.09, Learning Rate: 0.00031\n",
            "It: 1570, Loss: 4.990e-05, Loss_0: 2.762e-09, Loss_r: 4.990e-05, lambda_0 : 3.241e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1580, Loss: 4.971e-05, Loss_0: 2.040e-09, Loss_r: 4.971e-05, lambda_0 : 4.207e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1590, Loss: 4.952e-05, Loss_0: 1.817e-09, Loss_r: 4.952e-05, lambda_0 : 4.953e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1600, Loss: 4.933e-05, Loss_0: 1.752e-09, Loss_r: 4.932e-05, lambda_0 : 4.754e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1610, Loss: 4.913e-05, Loss_0: 1.705e-09, Loss_r: 4.913e-05, lambda_0 : 4.799e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1620, Loss: 4.892e-05, Loss_0: 1.672e-09, Loss_r: 4.892e-05, lambda_0 : 4.909e+02, Time: 1.09, Learning Rate: 0.00028\n",
            "It: 1630, Loss: 4.871e-05, Loss_0: 1.644e-09, Loss_r: 4.871e-05, lambda_0 : 5.026e+02, Time: 1.09, Learning Rate: 0.00025\n",
            "It: 1640, Loss: 4.852e-05, Loss_0: 1.618e-09, Loss_r: 4.852e-05, lambda_0 : 5.126e+02, Time: 1.09, Learning Rate: 0.00025\n",
            "It: 1650, Loss: 4.832e-05, Loss_0: 1.592e-09, Loss_r: 4.832e-05, lambda_0 : 5.221e+02, Time: 1.10, Learning Rate: 0.00025\n",
            "It: 1660, Loss: 4.811e-05, Loss_0: 1.568e-09, Loss_r: 4.811e-05, lambda_0 : 5.303e+02, Time: 1.10, Learning Rate: 0.00025\n",
            "It: 1670, Loss: 4.790e-05, Loss_0: 1.546e-09, Loss_r: 4.790e-05, lambda_0 : 5.373e+02, Time: 1.09, Learning Rate: 0.00025\n",
            "It: 1680, Loss: 4.768e-05, Loss_0: 1.525e-09, Loss_r: 4.768e-05, lambda_0 : 5.436e+02, Time: 1.09, Learning Rate: 0.00025\n",
            "It: 1690, Loss: 4.746e-05, Loss_0: 1.504e-09, Loss_r: 4.746e-05, lambda_0 : 5.494e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1700, Loss: 4.725e-05, Loss_0: 1.486e-09, Loss_r: 4.725e-05, lambda_0 : 5.544e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1710, Loss: 4.703e-05, Loss_0: 1.468e-09, Loss_r: 4.703e-05, lambda_0 : 5.590e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1720, Loss: 4.681e-05, Loss_0: 1.451e-09, Loss_r: 4.681e-05, lambda_0 : 5.633e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1730, Loss: 4.658e-05, Loss_0: 1.435e-09, Loss_r: 4.658e-05, lambda_0 : 5.673e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1740, Loss: 4.634e-05, Loss_0: 1.418e-09, Loss_r: 4.634e-05, lambda_0 : 5.710e+02, Time: 1.09, Learning Rate: 0.00023\n",
            "It: 1750, Loss: 4.609e-05, Loss_0: 1.403e-09, Loss_r: 4.609e-05, lambda_0 : 5.743e+02, Time: 1.09, Learning Rate: 0.00021\n",
            "It: 1760, Loss: 4.586e-05, Loss_0: 1.389e-09, Loss_r: 4.586e-05, lambda_0 : 5.769e+02, Time: 1.10, Learning Rate: 0.00021\n",
            "It: 1770, Loss: 4.562e-05, Loss_0: 1.375e-09, Loss_r: 4.562e-05, lambda_0 : 5.792e+02, Time: 1.10, Learning Rate: 0.00021\n",
            "It: 1780, Loss: 4.538e-05, Loss_0: 1.362e-09, Loss_r: 4.538e-05, lambda_0 : 5.811e+02, Time: 1.10, Learning Rate: 0.00021\n",
            "It: 1790, Loss: 4.512e-05, Loss_0: 1.349e-09, Loss_r: 4.512e-05, lambda_0 : 5.826e+02, Time: 1.09, Learning Rate: 0.00021\n",
            "It: 1800, Loss: 4.486e-05, Loss_0: 1.337e-09, Loss_r: 4.486e-05, lambda_0 : 5.837e+02, Time: 1.10, Learning Rate: 0.00021\n",
            "It: 1810, Loss: 4.459e-05, Loss_0: 1.325e-09, Loss_r: 4.459e-05, lambda_0 : 5.842e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1820, Loss: 4.433e-05, Loss_0: 1.314e-09, Loss_r: 4.433e-05, lambda_0 : 5.842e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1830, Loss: 4.407e-05, Loss_0: 1.304e-09, Loss_r: 4.407e-05, lambda_0 : 5.838e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1840, Loss: 4.380e-05, Loss_0: 1.294e-09, Loss_r: 4.380e-05, lambda_0 : 5.829e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1850, Loss: 4.352e-05, Loss_0: 1.285e-09, Loss_r: 4.352e-05, lambda_0 : 5.816e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1860, Loss: 4.323e-05, Loss_0: 1.275e-09, Loss_r: 4.323e-05, lambda_0 : 5.797e+02, Time: 1.09, Learning Rate: 0.00019\n",
            "It: 1870, Loss: 4.294e-05, Loss_0: 1.266e-09, Loss_r: 4.293e-05, lambda_0 : 5.773e+02, Time: 1.09, Learning Rate: 0.00017\n",
            "It: 1880, Loss: 4.266e-05, Loss_0: 1.258e-09, Loss_r: 4.266e-05, lambda_0 : 5.747e+02, Time: 1.10, Learning Rate: 0.00017\n",
            "It: 1890, Loss: 4.238e-05, Loss_0: 1.250e-09, Loss_r: 4.238e-05, lambda_0 : 5.718e+02, Time: 1.10, Learning Rate: 0.00017\n",
            "It: 1900, Loss: 4.209e-05, Loss_0: 1.241e-09, Loss_r: 4.209e-05, lambda_0 : 5.686e+02, Time: 1.09, Learning Rate: 0.00017\n",
            "It: 1910, Loss: 4.179e-05, Loss_0: 1.233e-09, Loss_r: 4.179e-05, lambda_0 : 5.650e+02, Time: 1.09, Learning Rate: 0.00017\n",
            "It: 1920, Loss: 4.148e-05, Loss_0: 1.224e-09, Loss_r: 4.148e-05, lambda_0 : 5.611e+02, Time: 1.09, Learning Rate: 0.00017\n",
            "It: 1930, Loss: 4.117e-05, Loss_0: 1.215e-09, Loss_r: 4.117e-05, lambda_0 : 5.570e+02, Time: 1.10, Learning Rate: 0.00015\n",
            "It: 1940, Loss: 4.088e-05, Loss_0: 1.207e-09, Loss_r: 4.088e-05, lambda_0 : 5.532e+02, Time: 1.09, Learning Rate: 0.00015\n",
            "It: 1950, Loss: 4.059e-05, Loss_0: 1.197e-09, Loss_r: 4.059e-05, lambda_0 : 5.494e+02, Time: 1.09, Learning Rate: 0.00015\n",
            "It: 1960, Loss: 4.029e-05, Loss_0: 1.187e-09, Loss_r: 4.029e-05, lambda_0 : 5.457e+02, Time: 1.09, Learning Rate: 0.00015\n",
            "It: 1970, Loss: 3.999e-05, Loss_0: 1.177e-09, Loss_r: 3.998e-05, lambda_0 : 5.420e+02, Time: 1.09, Learning Rate: 0.00015\n",
            "It: 1980, Loss: 3.968e-05, Loss_0: 1.165e-09, Loss_r: 3.967e-05, lambda_0 : 5.386e+02, Time: 1.09, Learning Rate: 0.00015\n",
            "It: 1990, Loss: 3.936e-05, Loss_0: 1.153e-09, Loss_r: 3.936e-05, lambda_0 : 5.354e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2000, Loss: 3.907e-05, Loss_0: 1.140e-09, Loss_r: 3.907e-05, lambda_0 : 5.330e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2010, Loss: 3.878e-05, Loss_0: 1.127e-09, Loss_r: 3.878e-05, lambda_0 : 5.492e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2020, Loss: 3.848e-05, Loss_0: 1.113e-09, Loss_r: 3.848e-05, lambda_0 : 5.966e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2030, Loss: 3.819e-05, Loss_0: 1.097e-09, Loss_r: 3.818e-05, lambda_0 : 6.417e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2040, Loss: 3.788e-05, Loss_0: 1.081e-09, Loss_r: 3.788e-05, lambda_0 : 6.841e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2050, Loss: 3.758e-05, Loss_0: 1.063e-09, Loss_r: 3.757e-05, lambda_0 : 7.234e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2060, Loss: 3.727e-05, Loss_0: 1.045e-09, Loss_r: 3.727e-05, lambda_0 : 7.594e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2070, Loss: 3.695e-05, Loss_0: 1.025e-09, Loss_r: 3.695e-05, lambda_0 : 7.911e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2080, Loss: 3.664e-05, Loss_0: 1.005e-09, Loss_r: 3.664e-05, lambda_0 : 8.188e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2090, Loss: 3.632e-05, Loss_0: 9.834e-10, Loss_r: 3.632e-05, lambda_0 : 8.420e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2100, Loss: 3.600e-05, Loss_0: 9.614e-10, Loss_r: 3.600e-05, lambda_0 : 8.602e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2110, Loss: 3.567e-05, Loss_0: 9.386e-10, Loss_r: 3.567e-05, lambda_0 : 8.736e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2120, Loss: 3.535e-05, Loss_0: 9.153e-10, Loss_r: 3.535e-05, lambda_0 : 8.818e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2130, Loss: 3.502e-05, Loss_0: 8.914e-10, Loss_r: 3.502e-05, lambda_0 : 8.849e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2140, Loss: 3.469e-05, Loss_0: 8.672e-10, Loss_r: 3.469e-05, lambda_0 : 8.827e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2150, Loss: 3.435e-05, Loss_0: 8.429e-10, Loss_r: 3.435e-05, lambda_0 : 8.751e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2160, Loss: 3.401e-05, Loss_0: 8.184e-10, Loss_r: 3.401e-05, lambda_0 : 8.622e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2170, Loss: 3.367e-05, Loss_0: 7.940e-10, Loss_r: 3.367e-05, lambda_0 : 8.441e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2180, Loss: 3.333e-05, Loss_0: 7.695e-10, Loss_r: 3.333e-05, lambda_0 : 8.206e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2190, Loss: 3.299e-05, Loss_0: 7.453e-10, Loss_r: 3.299e-05, lambda_0 : 7.919e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2200, Loss: 3.264e-05, Loss_0: 7.214e-10, Loss_r: 3.264e-05, lambda_0 : 7.582e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2210, Loss: 3.229e-05, Loss_0: 6.976e-10, Loss_r: 3.229e-05, lambda_0 : 7.193e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2220, Loss: 3.194e-05, Loss_0: 6.744e-10, Loss_r: 3.194e-05, lambda_0 : 6.754e+02, Time: 1.11, Learning Rate: 0.00014\n",
            "It: 2230, Loss: 3.159e-05, Loss_0: 6.516e-10, Loss_r: 3.158e-05, lambda_0 : 6.696e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2240, Loss: 3.123e-05, Loss_0: 6.293e-10, Loss_r: 3.123e-05, lambda_0 : 6.848e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2250, Loss: 3.087e-05, Loss_0: 6.076e-10, Loss_r: 3.087e-05, lambda_0 : 7.005e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2260, Loss: 3.051e-05, Loss_0: 5.865e-10, Loss_r: 3.050e-05, lambda_0 : 7.167e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2270, Loss: 3.014e-05, Loss_0: 5.659e-10, Loss_r: 3.014e-05, lambda_0 : 7.334e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2280, Loss: 2.977e-05, Loss_0: 5.460e-10, Loss_r: 2.977e-05, lambda_0 : 7.504e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2290, Loss: 2.940e-05, Loss_0: 5.268e-10, Loss_r: 2.940e-05, lambda_0 : 7.680e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2300, Loss: 2.903e-05, Loss_0: 5.082e-10, Loss_r: 2.903e-05, lambda_0 : 7.858e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2310, Loss: 2.865e-05, Loss_0: 4.903e-10, Loss_r: 2.865e-05, lambda_0 : 8.037e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2320, Loss: 2.827e-05, Loss_0: 4.732e-10, Loss_r: 2.827e-05, lambda_0 : 8.224e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2330, Loss: 2.789e-05, Loss_0: 4.567e-10, Loss_r: 2.789e-05, lambda_0 : 8.409e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2340, Loss: 2.750e-05, Loss_0: 4.409e-10, Loss_r: 2.750e-05, lambda_0 : 8.596e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2350, Loss: 2.712e-05, Loss_0: 4.261e-10, Loss_r: 2.712e-05, lambda_0 : 8.798e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2360, Loss: 2.673e-05, Loss_0: 4.481e-10, Loss_r: 2.673e-05, lambda_0 : 7.824e+02, Time: 1.09, Learning Rate: 0.00014\n",
            "It: 2370, Loss: 2.779e-05, Loss_0: 1.464e-06, Loss_r: 2.633e-05, lambda_0 : 1.082e+01, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2380, Loss: 5.880e-05, Loss_0: 3.160e-05, Loss_r: 2.719e-05, lambda_0 : 1.639e+01, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2390, Loss: 4.665e-05, Loss_0: 1.777e-05, Loss_r: 2.887e-05, lambda_0 : 6.935e+00, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2400, Loss: 2.964e-05, Loss_0: 2.186e-08, Loss_r: 2.962e-05, lambda_0 : 2.968e+02, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2410, Loss: 3.212e-05, Loss_0: 2.387e-06, Loss_r: 2.973e-05, lambda_0 : 1.521e+01, Time: 1.10, Learning Rate: 0.00014\n",
            "It: 2420, Loss: 3.081e-05, Loss_0: 1.249e-06, Loss_r: 2.956e-05, lambda_0 : 1.287e+01, Time: 1.10, Learning Rate: 0.00012\n",
            "It: 2430, Loss: 2.937e-05, Loss_0: 6.230e-08, Loss_r: 2.931e-05, lambda_0 : 5.336e+01, Time: 1.10, Learning Rate: 0.00012\n",
            "It: 2440, Loss: 2.916e-05, Loss_0: 1.671e-07, Loss_r: 2.900e-05, lambda_0 : 3.491e+01, Time: 1.10, Learning Rate: 0.00012\n",
            "It: 2450, Loss: 2.867e-05, Loss_0: 8.021e-09, Loss_r: 2.866e-05, lambda_0 : 1.395e+02, Time: 1.10, Learning Rate: 0.00012\n",
            "It: 2460, Loss: 2.832e-05, Loss_0: 3.193e-09, Loss_r: 2.831e-05, lambda_0 : 2.217e+02, Time: 1.09, Learning Rate: 0.00012\n",
            "It: 2470, Loss: 2.796e-05, Loss_0: 3.407e-09, Loss_r: 2.796e-05, lambda_0 : 2.355e+02, Time: 1.10, Learning Rate: 0.00012\n",
            "It: 2480, Loss: 2.760e-05, Loss_0: 1.438e-09, Loss_r: 2.760e-05, lambda_0 : 3.323e+02, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2490, Loss: 2.728e-05, Loss_0: 1.161e-09, Loss_r: 2.728e-05, lambda_0 : 3.821e+02, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2500, Loss: 2.695e-05, Loss_0: 7.835e-10, Loss_r: 2.695e-05, lambda_0 : 4.668e+02, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2510, Loss: 2.662e-05, Loss_0: 4.807e-10, Loss_r: 2.662e-05, lambda_0 : 6.351e+02, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2520, Loss: 2.629e-05, Loss_0: 4.214e-10, Loss_r: 2.629e-05, lambda_0 : 8.215e+02, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2530, Loss: 2.596e-05, Loss_0: 3.768e-10, Loss_r: 2.596e-05, lambda_0 : 9.602e+02, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2540, Loss: 2.562e-05, Loss_0: 3.667e-10, Loss_r: 2.562e-05, lambda_0 : 8.963e+02, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2550, Loss: 2.529e-05, Loss_0: 3.588e-10, Loss_r: 2.528e-05, lambda_0 : 9.773e+02, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2560, Loss: 2.495e-05, Loss_0: 3.530e-10, Loss_r: 2.495e-05, lambda_0 : 1.003e+03, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2570, Loss: 2.460e-05, Loss_0: 3.446e-10, Loss_r: 2.460e-05, lambda_0 : 1.009e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2580, Loss: 2.426e-05, Loss_0: 3.366e-10, Loss_r: 2.426e-05, lambda_0 : 1.014e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2590, Loss: 2.392e-05, Loss_0: 3.293e-10, Loss_r: 2.392e-05, lambda_0 : 1.044e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2600, Loss: 2.357e-05, Loss_0: 3.223e-10, Loss_r: 2.357e-05, lambda_0 : 1.100e+03, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2610, Loss: 2.322e-05, Loss_0: 3.160e-10, Loss_r: 2.322e-05, lambda_0 : 1.160e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2620, Loss: 2.287e-05, Loss_0: 3.104e-10, Loss_r: 2.287e-05, lambda_0 : 1.218e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2630, Loss: 2.252e-05, Loss_0: 3.049e-10, Loss_r: 2.252e-05, lambda_0 : 1.270e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2640, Loss: 2.217e-05, Loss_0: 2.998e-10, Loss_r: 2.217e-05, lambda_0 : 1.319e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2650, Loss: 2.182e-05, Loss_0: 2.950e-10, Loss_r: 2.182e-05, lambda_0 : 1.369e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2660, Loss: 2.147e-05, Loss_0: 2.905e-10, Loss_r: 2.147e-05, lambda_0 : 1.415e+03, Time: 1.11, Learning Rate: 0.00011\n",
            "It: 2670, Loss: 2.111e-05, Loss_0: 2.863e-10, Loss_r: 2.111e-05, lambda_0 : 1.506e+03, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2680, Loss: 2.076e-05, Loss_0: 2.814e-10, Loss_r: 2.076e-05, lambda_0 : 2.723e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2690, Loss: 2.168e-05, Loss_0: 2.315e-10, Loss_r: 2.168e-05, lambda_0 : 5.711e+04, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2700, Loss: 2.020e-05, Loss_0: 4.021e-09, Loss_r: 2.020e-05, lambda_0 : 2.875e+03, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2710, Loss: 2.565e-05, Loss_0: 5.454e-06, Loss_r: 2.019e-05, lambda_0 : 1.622e+02, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2720, Loss: 4.719e-05, Loss_0: 2.699e-05, Loss_r: 2.020e-05, lambda_0 : 3.678e+01, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2730, Loss: 3.148e-05, Loss_0: 1.065e-05, Loss_r: 2.084e-05, lambda_0 : 3.339e+01, Time: 1.09, Learning Rate: 0.00011\n",
            "It: 2740, Loss: 2.135e-05, Loss_0: 3.505e-07, Loss_r: 2.100e-05, lambda_0 : 8.359e+01, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2750, Loss: 2.097e-05, Loss_0: 3.837e-08, Loss_r: 2.093e-05, lambda_0 : 2.655e+02, Time: 1.10, Learning Rate: 0.00011\n",
            "It: 2760, Loss: 2.105e-05, Loss_0: 3.402e-07, Loss_r: 2.071e-05, lambda_0 : 3.476e+01, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2770, Loss: 2.045e-05, Loss_0: 5.803e-09, Loss_r: 2.045e-05, lambda_0 : 4.087e+02, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2780, Loss: 2.016e-05, Loss_0: 8.671e-10, Loss_r: 2.016e-05, lambda_0 : 3.414e+02, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2790, Loss: 1.986e-05, Loss_0: 7.580e-10, Loss_r: 1.986e-05, lambda_0 : 7.321e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2800, Loss: 1.957e-05, Loss_0: 6.432e-09, Loss_r: 1.956e-05, lambda_0 : 1.501e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2810, Loss: 1.926e-05, Loss_0: 4.724e-09, Loss_r: 1.926e-05, lambda_0 : 2.458e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2820, Loss: 1.896e-05, Loss_0: 4.629e-10, Loss_r: 1.896e-05, lambda_0 : 7.206e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2830, Loss: 1.866e-05, Loss_0: 6.065e-10, Loss_r: 1.865e-05, lambda_0 : 5.487e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2840, Loss: 1.835e-05, Loss_0: 4.647e-10, Loss_r: 1.835e-05, lambda_0 : 6.681e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2850, Loss: 1.805e-05, Loss_0: 3.611e-10, Loss_r: 1.805e-05, lambda_0 : 8.659e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2860, Loss: 1.775e-05, Loss_0: 2.689e-10, Loss_r: 1.775e-05, lambda_0 : 1.255e+03, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2870, Loss: 1.745e-05, Loss_0: 2.725e-10, Loss_r: 1.745e-05, lambda_0 : 1.145e+03, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2880, Loss: 1.715e-05, Loss_0: 2.653e-10, Loss_r: 1.715e-05, lambda_0 : 1.301e+03, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2890, Loss: 1.686e-05, Loss_0: 2.645e-10, Loss_r: 1.686e-05, lambda_0 : 1.286e+03, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2900, Loss: 1.656e-05, Loss_0: 2.631e-10, Loss_r: 1.656e-05, lambda_0 : 1.236e+03, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2910, Loss: 1.627e-05, Loss_0: 2.635e-10, Loss_r: 1.627e-05, lambda_0 : 1.245e+03, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2920, Loss: 1.598e-05, Loss_0: 2.619e-10, Loss_r: 1.598e-05, lambda_0 : 1.195e+03, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 2930, Loss: 1.569e-05, Loss_0: 2.640e-10, Loss_r: 1.569e-05, lambda_0 : 1.053e+03, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2940, Loss: 1.541e-05, Loss_0: 1.085e-08, Loss_r: 1.540e-05, lambda_0 : 1.204e+02, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2950, Loss: 1.889e-04, Loss_0: 1.737e-04, Loss_r: 1.518e-05, lambda_0 : 2.236e+00, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2960, Loss: 4.805e-05, Loss_0: 3.183e-05, Loss_r: 1.622e-05, lambda_0 : 6.664e+01, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2970, Loss: 3.361e-05, Loss_0: 1.724e-05, Loss_r: 1.637e-05, lambda_0 : 2.132e+01, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2980, Loss: 2.011e-05, Loss_0: 3.575e-06, Loss_r: 1.653e-05, lambda_0 : 7.726e+01, Time: 1.09, Learning Rate: 0.00010\n",
            "It: 2990, Loss: 1.647e-05, Loss_0: 4.449e-08, Loss_r: 1.642e-05, lambda_0 : 4.365e+02, Time: 1.10, Learning Rate: 0.00010\n",
            "It: 3000, Loss: 1.698e-05, Loss_0: 7.818e-07, Loss_r: 1.620e-05, lambda_0 : 8.587e+01, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3010, Loss: 1.624e-05, Loss_0: 2.738e-07, Loss_r: 1.596e-05, lambda_0 : 9.504e+01, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3020, Loss: 1.582e-05, Loss_0: 1.086e-07, Loss_r: 1.571e-05, lambda_0 : 1.018e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3030, Loss: 1.549e-05, Loss_0: 2.530e-08, Loss_r: 1.546e-05, lambda_0 : 1.505e+02, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3040, Loss: 1.521e-05, Loss_0: 5.329e-10, Loss_r: 1.521e-05, lambda_0 : 6.245e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3050, Loss: 1.496e-05, Loss_0: 5.368e-09, Loss_r: 1.495e-05, lambda_0 : 1.219e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3060, Loss: 1.470e-05, Loss_0: 6.068e-10, Loss_r: 1.470e-05, lambda_0 : 3.431e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3070, Loss: 1.445e-05, Loss_0: 3.615e-10, Loss_r: 1.445e-05, lambda_0 : 5.814e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3080, Loss: 1.420e-05, Loss_0: 3.377e-10, Loss_r: 1.420e-05, lambda_0 : 5.022e+02, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3090, Loss: 1.396e-05, Loss_0: 2.858e-10, Loss_r: 1.396e-05, lambda_0 : 7.097e+02, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3100, Loss: 1.372e-05, Loss_0: 2.835e-10, Loss_r: 1.372e-05, lambda_0 : 7.245e+02, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3110, Loss: 1.348e-05, Loss_0: 2.780e-10, Loss_r: 1.348e-05, lambda_0 : 7.740e+02, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3120, Loss: 1.326e-05, Loss_0: 2.701e-10, Loss_r: 1.326e-05, lambda_0 : 7.154e+03, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3130, Loss: 1.855e-05, Loss_0: 2.757e-10, Loss_r: 1.855e-05, lambda_0 : 9.999e+04, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3140, Loss: 1.416e-05, Loss_0: 2.868e-10, Loss_r: 1.416e-05, lambda_0 : 5.583e+04, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3150, Loss: 1.261e-05, Loss_0: 2.801e-10, Loss_r: 1.261e-05, lambda_0 : 6.595e+03, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3160, Loss: 1.259e-05, Loss_0: 2.662e-10, Loss_r: 1.259e-05, lambda_0 : 2.350e+04, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3170, Loss: 1.219e-05, Loss_0: 2.748e-10, Loss_r: 1.219e-05, lambda_0 : 5.054e+03, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3180, Loss: 1.198e-05, Loss_0: 2.766e-10, Loss_r: 1.198e-05, lambda_0 : 4.295e+03, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3190, Loss: 1.178e-05, Loss_0: 1.014e-09, Loss_r: 1.177e-05, lambda_0 : 1.503e+03, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3200, Loss: 1.430e-05, Loss_0: 2.730e-06, Loss_r: 1.157e-05, lambda_0 : 1.950e+01, Time: 1.09, Learning Rate: 0.00009\n",
            "It: 3210, Loss: 1.401e-04, Loss_0: 1.283e-04, Loss_r: 1.188e-05, lambda_0 : 1.750e+00, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3220, Loss: 4.910e-05, Loss_0: 3.645e-05, Loss_r: 1.265e-05, lambda_0 : 1.357e+00, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3230, Loss: 1.511e-05, Loss_0: 2.101e-06, Loss_r: 1.301e-05, lambda_0 : 5.724e+00, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3240, Loss: 1.307e-05, Loss_0: 1.484e-08, Loss_r: 1.306e-05, lambda_0 : 7.375e+01, Time: 1.10, Learning Rate: 0.00009\n",
            "It: 3250, Loss: 1.296e-05, Loss_0: 5.867e-09, Loss_r: 1.295e-05, lambda_0 : 9.264e+01, Time: 1.10, Learning Rate: 0.00008\n",
            "It: 3260, Loss: 1.323e-05, Loss_0: 4.298e-07, Loss_r: 1.280e-05, lambda_0 : 1.188e+01, Time: 1.10, Learning Rate: 0.00008\n",
            "It: 3270, Loss: 1.270e-05, Loss_0: 7.077e-08, Loss_r: 1.263e-05, lambda_0 : 2.777e+01, Time: 1.10, Learning Rate: 0.00008\n",
            "It: 3280, Loss: 1.244e-05, Loss_0: 6.054e-10, Loss_r: 1.244e-05, lambda_0 : 2.954e+02, Time: 1.09, Learning Rate: 0.00008\n",
            "It: 3290, Loss: 1.226e-05, Loss_0: 6.826e-09, Loss_r: 1.225e-05, lambda_0 : 9.235e+01, Time: 1.09, Learning Rate: 0.00008\n",
            "It: 3300, Loss: 1.207e-05, Loss_0: 4.288e-09, Loss_r: 1.206e-05, lambda_0 : 1.114e+02, Time: 1.09, Learning Rate: 0.00008\n",
            "It: 3310, Loss: 1.188e-05, Loss_0: 1.522e-09, Loss_r: 1.188e-05, lambda_0 : 1.758e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3320, Loss: 1.171e-05, Loss_0: 9.702e-10, Loss_r: 1.171e-05, lambda_0 : 2.148e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3330, Loss: 1.155e-05, Loss_0: 3.114e-10, Loss_r: 1.155e-05, lambda_0 : 4.399e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3340, Loss: 1.138e-05, Loss_0: 2.663e-10, Loss_r: 1.138e-05, lambda_0 : 6.650e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3350, Loss: 1.122e-05, Loss_0: 2.723e-10, Loss_r: 1.122e-05, lambda_0 : 6.257e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3360, Loss: 1.106e-05, Loss_0: 2.686e-10, Loss_r: 1.106e-05, lambda_0 : 6.439e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3370, Loss: 1.091e-05, Loss_0: 2.656e-10, Loss_r: 1.091e-05, lambda_0 : 6.249e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3380, Loss: 1.075e-05, Loss_0: 2.650e-10, Loss_r: 1.075e-05, lambda_0 : 6.016e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3390, Loss: 1.060e-05, Loss_0: 2.653e-10, Loss_r: 1.060e-05, lambda_0 : 5.875e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3400, Loss: 1.045e-05, Loss_0: 2.657e-10, Loss_r: 1.045e-05, lambda_0 : 5.866e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3410, Loss: 1.030e-05, Loss_0: 2.665e-10, Loss_r: 1.030e-05, lambda_0 : 5.817e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3420, Loss: 1.016e-05, Loss_0: 2.663e-10, Loss_r: 1.016e-05, lambda_0 : 5.701e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3430, Loss: 1.001e-05, Loss_0: 2.660e-10, Loss_r: 1.001e-05, lambda_0 : 5.575e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3440, Loss: 9.869e-06, Loss_0: 2.661e-10, Loss_r: 9.868e-06, lambda_0 : 5.490e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3450, Loss: 9.728e-06, Loss_0: 2.659e-10, Loss_r: 9.728e-06, lambda_0 : 5.385e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3460, Loss: 9.590e-06, Loss_0: 2.657e-10, Loss_r: 9.590e-06, lambda_0 : 5.288e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3470, Loss: 9.454e-06, Loss_0: 2.653e-10, Loss_r: 9.453e-06, lambda_0 : 5.221e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3480, Loss: 9.319e-06, Loss_0: 2.650e-10, Loss_r: 9.319e-06, lambda_0 : 5.550e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3490, Loss: 9.187e-06, Loss_0: 2.645e-10, Loss_r: 9.187e-06, lambda_0 : 5.839e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3500, Loss: 9.057e-06, Loss_0: 2.640e-10, Loss_r: 9.056e-06, lambda_0 : 6.139e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3510, Loss: 8.928e-06, Loss_0: 2.634e-10, Loss_r: 8.928e-06, lambda_0 : 6.415e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3520, Loss: 8.802e-06, Loss_0: 2.627e-10, Loss_r: 8.802e-06, lambda_0 : 6.639e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3530, Loss: 8.678e-06, Loss_0: 2.619e-10, Loss_r: 8.677e-06, lambda_0 : 6.870e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3540, Loss: 8.555e-06, Loss_0: 2.611e-10, Loss_r: 8.555e-06, lambda_0 : 7.165e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3550, Loss: 8.435e-06, Loss_0: 2.602e-10, Loss_r: 8.435e-06, lambda_0 : 7.288e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3560, Loss: 8.316e-06, Loss_0: 2.592e-10, Loss_r: 8.316e-06, lambda_0 : 7.508e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3570, Loss: 8.200e-06, Loss_0: 2.582e-10, Loss_r: 8.200e-06, lambda_0 : 7.770e+02, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3580, Loss: 8.085e-06, Loss_0: 2.570e-10, Loss_r: 8.085e-06, lambda_0 : 8.130e+02, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3590, Loss: 7.972e-06, Loss_0: 2.562e-10, Loss_r: 7.972e-06, lambda_0 : 1.073e+03, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3600, Loss: 7.871e-06, Loss_0: 2.776e-10, Loss_r: 7.870e-06, lambda_0 : 4.738e+03, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3610, Loss: 1.220e-05, Loss_0: 2.465e-08, Loss_r: 1.218e-05, lambda_0 : 6.459e+03, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3620, Loss: 1.131e-04, Loss_0: 1.043e-04, Loss_r: 8.786e-06, lambda_0 : 5.094e+01, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3630, Loss: 3.789e-05, Loss_0: 2.959e-05, Loss_r: 8.297e-06, lambda_0 : 4.508e+01, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3640, Loss: 1.652e-05, Loss_0: 7.965e-06, Loss_r: 8.558e-06, lambda_0 : 6.967e+01, Time: 1.09, Learning Rate: 0.00007\n",
            "It: 3650, Loss: 8.796e-06, Loss_0: 2.726e-07, Loss_r: 8.523e-06, lambda_0 : 5.701e+01, Time: 1.10, Learning Rate: 0.00007\n",
            "It: 3660, Loss: 1.003e-05, Loss_0: 1.509e-06, Loss_r: 8.524e-06, lambda_0 : 4.599e+01, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3670, Loss: 8.557e-06, Loss_0: 1.091e-07, Loss_r: 8.448e-06, lambda_0 : 1.770e+01, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3680, Loss: 8.364e-06, Loss_0: 1.975e-09, Loss_r: 8.362e-06, lambda_0 : 2.272e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3690, Loss: 8.279e-06, Loss_0: 1.245e-08, Loss_r: 8.266e-06, lambda_0 : 1.237e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3700, Loss: 8.174e-06, Loss_0: 6.684e-09, Loss_r: 8.168e-06, lambda_0 : 8.402e+01, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3710, Loss: 8.071e-06, Loss_0: 1.459e-09, Loss_r: 8.069e-06, lambda_0 : 1.868e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3720, Loss: 7.972e-06, Loss_0: 2.228e-10, Loss_r: 7.972e-06, lambda_0 : 5.046e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3730, Loss: 7.886e-06, Loss_0: 8.544e-10, Loss_r: 7.885e-06, lambda_0 : 1.447e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3740, Loss: 7.800e-06, Loss_0: 6.419e-10, Loss_r: 7.799e-06, lambda_0 : 1.651e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3750, Loss: 7.715e-06, Loss_0: 3.572e-10, Loss_r: 7.714e-06, lambda_0 : 2.412e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3760, Loss: 7.631e-06, Loss_0: 2.612e-10, Loss_r: 7.631e-06, lambda_0 : 3.417e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3770, Loss: 7.549e-06, Loss_0: 2.369e-10, Loss_r: 7.548e-06, lambda_0 : 4.086e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3780, Loss: 7.467e-06, Loss_0: 2.303e-10, Loss_r: 7.467e-06, lambda_0 : 4.797e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3790, Loss: 7.387e-06, Loss_0: 2.277e-10, Loss_r: 7.387e-06, lambda_0 : 5.052e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3800, Loss: 7.308e-06, Loss_0: 2.256e-10, Loss_r: 7.307e-06, lambda_0 : 5.405e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3810, Loss: 7.229e-06, Loss_0: 2.239e-10, Loss_r: 7.229e-06, lambda_0 : 5.538e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3820, Loss: 7.152e-06, Loss_0: 2.227e-10, Loss_r: 7.152e-06, lambda_0 : 5.680e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3830, Loss: 7.076e-06, Loss_0: 2.221e-10, Loss_r: 7.076e-06, lambda_0 : 5.835e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3840, Loss: 7.001e-06, Loss_0: 2.217e-10, Loss_r: 7.001e-06, lambda_0 : 5.981e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3850, Loss: 6.927e-06, Loss_0: 2.212e-10, Loss_r: 6.927e-06, lambda_0 : 6.125e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3860, Loss: 6.854e-06, Loss_0: 2.204e-10, Loss_r: 6.854e-06, lambda_0 : 6.230e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3870, Loss: 6.782e-06, Loss_0: 2.196e-10, Loss_r: 6.782e-06, lambda_0 : 6.317e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3880, Loss: 6.711e-06, Loss_0: 2.189e-10, Loss_r: 6.710e-06, lambda_0 : 6.406e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3890, Loss: 6.641e-06, Loss_0: 2.181e-10, Loss_r: 6.640e-06, lambda_0 : 6.480e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3900, Loss: 6.572e-06, Loss_0: 2.172e-10, Loss_r: 6.571e-06, lambda_0 : 6.532e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3910, Loss: 6.503e-06, Loss_0: 2.163e-10, Loss_r: 6.503e-06, lambda_0 : 6.590e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3920, Loss: 6.436e-06, Loss_0: 2.154e-10, Loss_r: 6.436e-06, lambda_0 : 6.613e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3930, Loss: 6.370e-06, Loss_0: 2.143e-10, Loss_r: 6.370e-06, lambda_0 : 6.639e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3940, Loss: 6.305e-06, Loss_0: 2.133e-10, Loss_r: 6.305e-06, lambda_0 : 6.650e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3950, Loss: 6.240e-06, Loss_0: 2.123e-10, Loss_r: 6.240e-06, lambda_0 : 6.639e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3960, Loss: 6.177e-06, Loss_0: 2.112e-10, Loss_r: 6.177e-06, lambda_0 : 6.627e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3970, Loss: 6.115e-06, Loss_0: 2.100e-10, Loss_r: 6.114e-06, lambda_0 : 6.615e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 3980, Loss: 6.053e-06, Loss_0: 2.089e-10, Loss_r: 6.053e-06, lambda_0 : 6.587e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 3990, Loss: 5.992e-06, Loss_0: 2.077e-10, Loss_r: 5.992e-06, lambda_0 : 6.537e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4000, Loss: 5.933e-06, Loss_0: 2.065e-10, Loss_r: 5.933e-06, lambda_0 : 6.479e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4010, Loss: 5.874e-06, Loss_0: 2.053e-10, Loss_r: 5.874e-06, lambda_0 : 6.417e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4020, Loss: 5.816e-06, Loss_0: 2.040e-10, Loss_r: 5.816e-06, lambda_0 : 6.358e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4030, Loss: 5.759e-06, Loss_0: 2.029e-10, Loss_r: 5.759e-06, lambda_0 : 6.311e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4040, Loss: 5.703e-06, Loss_0: 2.017e-10, Loss_r: 5.703e-06, lambda_0 : 6.219e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4050, Loss: 5.647e-06, Loss_0: 2.004e-10, Loss_r: 5.647e-06, lambda_0 : 6.113e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4060, Loss: 5.593e-06, Loss_0: 1.993e-10, Loss_r: 5.593e-06, lambda_0 : 6.055e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4070, Loss: 5.539e-06, Loss_0: 1.978e-10, Loss_r: 5.539e-06, lambda_0 : 5.918e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4080, Loss: 5.487e-06, Loss_0: 1.966e-10, Loss_r: 5.486e-06, lambda_0 : 5.861e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4090, Loss: 5.435e-06, Loss_0: 1.955e-10, Loss_r: 5.435e-06, lambda_0 : 5.752e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4100, Loss: 5.384e-06, Loss_0: 1.952e-10, Loss_r: 5.384e-06, lambda_0 : 5.620e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4110, Loss: 5.334e-06, Loss_0: 2.811e-10, Loss_r: 5.333e-06, lambda_0 : 2.851e+02, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4120, Loss: 5.358e-06, Loss_0: 7.409e-08, Loss_r: 5.284e-06, lambda_0 : 1.179e+01, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4130, Loss: 1.246e-04, Loss_0: 1.194e-04, Loss_r: 5.247e-06, lambda_0 : 3.147e+00, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4140, Loss: 8.602e-06, Loss_0: 3.199e-06, Loss_r: 5.403e-06, lambda_0 : 9.577e+00, Time: 1.10, Learning Rate: 0.00006\n",
            "It: 4150, Loss: 1.052e-05, Loss_0: 5.027e-06, Loss_r: 5.494e-06, lambda_0 : 1.899e+00, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4160, Loss: 5.531e-06, Loss_0: 9.191e-09, Loss_r: 5.521e-06, lambda_0 : 1.439e+02, Time: 1.09, Learning Rate: 0.00006\n",
            "It: 4170, Loss: 5.824e-06, Loss_0: 3.220e-07, Loss_r: 5.502e-06, lambda_0 : 2.048e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4180, Loss: 5.668e-06, Loss_0: 2.002e-07, Loss_r: 5.468e-06, lambda_0 : 1.492e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4190, Loss: 5.530e-06, Loss_0: 1.034e-07, Loss_r: 5.427e-06, lambda_0 : 1.070e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4200, Loss: 5.382e-06, Loss_0: 2.179e-10, Loss_r: 5.382e-06, lambda_0 : 2.195e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4210, Loss: 5.346e-06, Loss_0: 9.285e-09, Loss_r: 5.336e-06, lambda_0 : 2.712e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4220, Loss: 5.297e-06, Loss_0: 6.044e-09, Loss_r: 5.291e-06, lambda_0 : 3.692e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4230, Loss: 5.249e-06, Loss_0: 2.208e-09, Loss_r: 5.246e-06, lambda_0 : 5.528e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4240, Loss: 5.203e-06, Loss_0: 7.432e-10, Loss_r: 5.202e-06, lambda_0 : 9.918e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4250, Loss: 5.159e-06, Loss_0: 4.157e-10, Loss_r: 5.159e-06, lambda_0 : 1.305e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4260, Loss: 5.116e-06, Loss_0: 2.567e-10, Loss_r: 5.116e-06, lambda_0 : 1.777e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4270, Loss: 5.074e-06, Loss_0: 2.117e-10, Loss_r: 5.074e-06, lambda_0 : 2.245e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4280, Loss: 5.032e-06, Loss_0: 1.721e-10, Loss_r: 5.032e-06, lambda_0 : 3.194e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4290, Loss: 4.992e-06, Loss_0: 1.708e-10, Loss_r: 4.991e-06, lambda_0 : 3.309e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4300, Loss: 4.951e-06, Loss_0: 1.738e-10, Loss_r: 4.951e-06, lambda_0 : 3.363e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4310, Loss: 4.912e-06, Loss_0: 1.705e-10, Loss_r: 4.912e-06, lambda_0 : 3.381e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4320, Loss: 4.873e-06, Loss_0: 1.694e-10, Loss_r: 4.873e-06, lambda_0 : 3.301e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4330, Loss: 4.834e-06, Loss_0: 1.690e-10, Loss_r: 4.834e-06, lambda_0 : 3.270e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4340, Loss: 4.797e-06, Loss_0: 1.686e-10, Loss_r: 4.797e-06, lambda_0 : 3.243e+02, Time: 1.11, Learning Rate: 0.00005\n",
            "It: 4350, Loss: 4.760e-06, Loss_0: 1.681e-10, Loss_r: 4.759e-06, lambda_0 : 3.189e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4360, Loss: 4.723e-06, Loss_0: 1.675e-10, Loss_r: 4.723e-06, lambda_0 : 3.340e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4370, Loss: 4.688e-06, Loss_0: 1.664e-10, Loss_r: 4.687e-06, lambda_0 : 1.779e+03, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4380, Loss: 4.840e-06, Loss_0: 1.578e-10, Loss_r: 4.840e-06, lambda_0 : 2.830e+04, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4390, Loss: 4.939e-06, Loss_0: 2.011e-10, Loss_r: 4.939e-06, lambda_0 : 2.479e+04, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4400, Loss: 4.742e-06, Loss_0: 2.961e-10, Loss_r: 4.742e-06, lambda_0 : 1.138e+04, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4410, Loss: 4.697e-06, Loss_0: 1.853e-10, Loss_r: 4.697e-06, lambda_0 : 2.157e+04, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4420, Loss: 4.578e-06, Loss_0: 1.717e-10, Loss_r: 4.578e-06, lambda_0 : 1.540e+04, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4430, Loss: 4.503e-06, Loss_0: 1.664e-10, Loss_r: 4.503e-06, lambda_0 : 6.816e+03, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4440, Loss: 4.462e-06, Loss_0: 1.634e-10, Loss_r: 4.462e-06, lambda_0 : 2.620e+03, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4450, Loss: 4.430e-06, Loss_0: 1.662e-10, Loss_r: 4.430e-06, lambda_0 : 1.070e+03, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4460, Loss: 4.400e-06, Loss_0: 1.728e-10, Loss_r: 4.400e-06, lambda_0 : 6.365e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4470, Loss: 4.371e-06, Loss_0: 4.476e-10, Loss_r: 4.371e-06, lambda_0 : 2.336e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4480, Loss: 4.391e-06, Loss_0: 4.997e-08, Loss_r: 4.341e-06, lambda_0 : 1.972e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4490, Loss: 3.107e-05, Loss_0: 2.676e-05, Loss_r: 4.310e-06, lambda_0 : 1.536e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4500, Loss: 9.669e-06, Loss_0: 5.349e-06, Loss_r: 4.320e-06, lambda_0 : 5.201e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4510, Loss: 5.422e-06, Loss_0: 1.103e-06, Loss_r: 4.319e-06, lambda_0 : 1.087e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4520, Loss: 4.306e-06, Loss_0: 8.659e-10, Loss_r: 4.305e-06, lambda_0 : 6.389e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4530, Loss: 5.626e-06, Loss_0: 1.338e-06, Loss_r: 4.288e-06, lambda_0 : 3.365e+00, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4540, Loss: 4.953e-06, Loss_0: 6.846e-07, Loss_r: 4.268e-06, lambda_0 : 2.596e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4550, Loss: 4.243e-06, Loss_0: 1.599e-10, Loss_r: 4.243e-06, lambda_0 : 3.849e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4560, Loss: 9.266e-06, Loss_0: 5.049e-06, Loss_r: 4.218e-06, lambda_0 : 2.804e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4570, Loss: 5.156e-06, Loss_0: 9.540e-07, Loss_r: 4.202e-06, lambda_0 : 9.989e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4580, Loss: 4.306e-06, Loss_0: 1.236e-07, Loss_r: 4.183e-06, lambda_0 : 1.686e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4590, Loss: 1.084e-05, Loss_0: 6.681e-06, Loss_r: 4.157e-06, lambda_0 : 1.583e+00, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4600, Loss: 5.424e-06, Loss_0: 1.282e-06, Loss_r: 4.143e-06, lambda_0 : 2.149e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4610, Loss: 4.221e-06, Loss_0: 9.691e-08, Loss_r: 4.124e-06, lambda_0 : 2.950e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4620, Loss: 4.221e-06, Loss_0: 1.168e-07, Loss_r: 4.104e-06, lambda_0 : 1.133e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4630, Loss: 4.148e-06, Loss_0: 6.573e-08, Loss_r: 4.082e-06, lambda_0 : 8.869e+00, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4640, Loss: 4.074e-06, Loss_0: 1.395e-08, Loss_r: 4.060e-06, lambda_0 : 1.545e+01, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4650, Loss: 4.042e-06, Loss_0: 3.708e-09, Loss_r: 4.039e-06, lambda_0 : 2.855e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4660, Loss: 4.018e-06, Loss_0: 1.605e-09, Loss_r: 4.017e-06, lambda_0 : 4.032e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4670, Loss: 3.997e-06, Loss_0: 1.731e-09, Loss_r: 3.995e-06, lambda_0 : 3.801e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4680, Loss: 3.975e-06, Loss_0: 6.810e-10, Loss_r: 3.974e-06, lambda_0 : 6.255e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4690, Loss: 3.953e-06, Loss_0: 4.322e-10, Loss_r: 3.953e-06, lambda_0 : 7.582e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4700, Loss: 3.932e-06, Loss_0: 1.989e-10, Loss_r: 3.932e-06, lambda_0 : 1.269e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4710, Loss: 3.912e-06, Loss_0: 2.354e-10, Loss_r: 3.912e-06, lambda_0 : 1.096e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4720, Loss: 3.892e-06, Loss_0: 4.384e-10, Loss_r: 3.892e-06, lambda_0 : 7.277e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4730, Loss: 3.884e-06, Loss_0: 1.201e-08, Loss_r: 3.872e-06, lambda_0 : 1.392e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4740, Loss: 5.859e-06, Loss_0: 2.004e-06, Loss_r: 3.854e-06, lambda_0 : 1.752e+00, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4750, Loss: 3.929e-06, Loss_0: 9.390e-08, Loss_r: 3.835e-06, lambda_0 : 8.809e+00, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4760, Loss: 4.037e-06, Loss_0: 2.189e-07, Loss_r: 3.818e-06, lambda_0 : 2.982e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4770, Loss: 3.871e-06, Loss_0: 7.038e-08, Loss_r: 3.800e-06, lambda_0 : 1.193e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4780, Loss: 3.792e-06, Loss_0: 1.016e-08, Loss_r: 3.782e-06, lambda_0 : 1.415e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4790, Loss: 4.020e-06, Loss_0: 2.565e-07, Loss_r: 3.764e-06, lambda_0 : 1.169e+01, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4800, Loss: 4.131e-05, Loss_0: 3.755e-05, Loss_r: 3.758e-06, lambda_0 : 8.892e+00, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4810, Loss: 4.390e-06, Loss_0: 1.705e-07, Loss_r: 4.220e-06, lambda_0 : 7.287e+02, Time: 1.09, Learning Rate: 0.00005\n",
            "It: 4820, Loss: 5.004e-06, Loss_0: 1.068e-06, Loss_r: 3.936e-06, lambda_0 : 1.485e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4830, Loss: 3.937e-06, Loss_0: 4.822e-08, Loss_r: 3.888e-06, lambda_0 : 5.338e+02, Time: 1.10, Learning Rate: 0.00005\n",
            "It: 4840, Loss: 3.884e-06, Loss_0: 3.967e-08, Loss_r: 3.845e-06, lambda_0 : 3.341e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4850, Loss: 3.813e-06, Loss_0: 1.372e-09, Loss_r: 3.812e-06, lambda_0 : 5.523e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4860, Loss: 3.815e-06, Loss_0: 2.022e-08, Loss_r: 3.794e-06, lambda_0 : 4.040e+01, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4870, Loss: 3.799e-06, Loss_0: 2.114e-08, Loss_r: 3.778e-06, lambda_0 : 7.910e+01, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4880, Loss: 3.767e-06, Loss_0: 5.415e-09, Loss_r: 3.761e-06, lambda_0 : 1.401e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4890, Loss: 3.745e-06, Loss_0: 1.657e-10, Loss_r: 3.745e-06, lambda_0 : 6.570e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4900, Loss: 3.729e-06, Loss_0: 1.171e-09, Loss_r: 3.728e-06, lambda_0 : 1.311e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4910, Loss: 3.712e-06, Loss_0: 1.536e-10, Loss_r: 3.712e-06, lambda_0 : 3.470e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4920, Loss: 3.696e-06, Loss_0: 2.498e-10, Loss_r: 3.696e-06, lambda_0 : 1.490e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4930, Loss: 3.680e-06, Loss_0: 1.337e-10, Loss_r: 3.680e-06, lambda_0 : 2.281e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4940, Loss: 3.664e-06, Loss_0: 1.459e-10, Loss_r: 3.664e-06, lambda_0 : 1.735e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4950, Loss: 3.649e-06, Loss_0: 1.422e-10, Loss_r: 3.649e-06, lambda_0 : 1.807e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4960, Loss: 3.634e-06, Loss_0: 1.361e-10, Loss_r: 3.633e-06, lambda_0 : 2.115e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4970, Loss: 3.619e-06, Loss_0: 1.338e-10, Loss_r: 3.618e-06, lambda_0 : 2.209e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "It: 4980, Loss: 3.604e-06, Loss_0: 1.331e-10, Loss_r: 3.603e-06, lambda_0 : 2.164e+02, Time: 1.09, Learning Rate: 0.00004\n",
            "It: 4990, Loss: 3.589e-06, Loss_0: 1.328e-10, Loss_r: 3.589e-06, lambda_0 : 2.129e+02, Time: 1.10, Learning Rate: 0.00004\n",
            "Training time: 548.6803\n",
            "[1, 128, 128, 128, 128, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.610e-03, Loss_0: 1.247e-03, Loss_r: 3.625e-04, lambda_0 : 2.913e+01, Time: 1.67, Learning Rate: 0.00100\n",
            "It: 10, Loss: 2.944e-04, Loss_0: 2.333e-04, Loss_r: 6.117e-05, lambda_0 : 4.025e+00, Time: 0.45, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.935e-04, Loss_0: 1.382e-04, Loss_r: 5.527e-05, lambda_0 : 1.868e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 30, Loss: 9.268e-05, Loss_0: 3.231e-05, Loss_r: 6.037e-05, lambda_0 : 1.072e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 40, Loss: 5.913e-05, Loss_0: 1.011e-06, Loss_r: 5.812e-05, lambda_0 : 1.662e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.363e-05, Loss_0: 6.102e-06, Loss_r: 5.752e-05, lambda_0 : 5.148e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.812e-05, Loss_0: 3.371e-08, Loss_r: 5.809e-05, lambda_0 : 4.463e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.832e-05, Loss_0: 4.701e-07, Loss_r: 5.785e-05, lambda_0 : 4.911e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.810e-05, Loss_0: 4.147e-07, Loss_r: 5.768e-05, lambda_0 : 1.035e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.770e-05, Loss_0: 1.341e-07, Loss_r: 5.757e-05, lambda_0 : 9.479e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.746e-05, Loss_0: 1.514e-08, Loss_r: 5.745e-05, lambda_0 : 2.052e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 110, Loss: 2.601e-04, Loss_0: 2.031e-04, Loss_r: 5.699e-05, lambda_0 : 2.005e-01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 120, Loss: 7.039e-05, Loss_0: 1.247e-05, Loss_r: 5.793e-05, lambda_0 : 9.539e-01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.809e-05, Loss_0: 2.396e-08, Loss_r: 5.807e-05, lambda_0 : 3.447e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 140, Loss: 6.247e-05, Loss_0: 4.404e-06, Loss_r: 5.807e-05, lambda_0 : 2.958e+00, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.894e-05, Loss_0: 8.543e-07, Loss_r: 5.808e-05, lambda_0 : 3.458e+00, Time: 0.44, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.812e-05, Loss_0: 9.305e-08, Loss_r: 5.803e-05, lambda_0 : 8.668e+00, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.798e-05, Loss_0: 2.577e-08, Loss_r: 5.796e-05, lambda_0 : 1.785e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.789e-05, Loss_0: 3.750e-08, Loss_r: 5.786e-05, lambda_0 : 1.557e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.776e-05, Loss_0: 3.923e-08, Loss_r: 5.772e-05, lambda_0 : 1.266e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.762e-05, Loss_0: 1.477e-08, Loss_r: 5.760e-05, lambda_0 : 2.176e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.747e-05, Loss_0: 1.439e-08, Loss_r: 5.746e-05, lambda_0 : 2.390e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.731e-05, Loss_0: 9.910e-09, Loss_r: 5.730e-05, lambda_0 : 3.400e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.714e-05, Loss_0: 9.939e-09, Loss_r: 5.713e-05, lambda_0 : 3.252e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.696e-05, Loss_0: 9.934e-09, Loss_r: 5.695e-05, lambda_0 : 3.667e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.675e-05, Loss_0: 1.016e-08, Loss_r: 5.674e-05, lambda_0 : 3.854e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.651e-05, Loss_0: 1.000e-08, Loss_r: 5.650e-05, lambda_0 : 4.001e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.625e-05, Loss_0: 9.977e-09, Loss_r: 5.624e-05, lambda_0 : 4.156e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.595e-05, Loss_0: 1.006e-08, Loss_r: 5.594e-05, lambda_0 : 4.442e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.560e-05, Loss_0: 9.919e-09, Loss_r: 5.559e-05, lambda_0 : 4.616e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.520e-05, Loss_0: 9.866e-09, Loss_r: 5.519e-05, lambda_0 : 4.869e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.472e-05, Loss_0: 9.800e-09, Loss_r: 5.471e-05, lambda_0 : 5.178e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.415e-05, Loss_0: 9.946e-09, Loss_r: 5.414e-05, lambda_0 : 5.641e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 330, Loss: 2.611e-04, Loss_0: 2.079e-04, Loss_r: 5.323e-05, lambda_0 : 5.270e-01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 340, Loss: 1.039e-04, Loss_0: 4.671e-05, Loss_r: 5.722e-05, lambda_0 : 3.186e+00, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 350, Loss: 6.041e-05, Loss_0: 1.438e-06, Loss_r: 5.897e-05, lambda_0 : 2.756e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 360, Loss: 6.738e-05, Loss_0: 8.206e-06, Loss_r: 5.917e-05, lambda_0 : 1.231e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.997e-05, Loss_0: 9.641e-07, Loss_r: 5.901e-05, lambda_0 : 2.171e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.925e-05, Loss_0: 6.043e-07, Loss_r: 5.864e-05, lambda_0 : 1.428e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.888e-05, Loss_0: 4.316e-07, Loss_r: 5.845e-05, lambda_0 : 2.192e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.839e-05, Loss_0: 1.727e-09, Loss_r: 5.839e-05, lambda_0 : 2.644e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.822e-05, Loss_0: 1.604e-08, Loss_r: 5.821e-05, lambda_0 : 1.503e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.794e-05, Loss_0: 9.731e-09, Loss_r: 5.793e-05, lambda_0 : 1.998e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.763e-05, Loss_0: 8.960e-09, Loss_r: 5.762e-05, lambda_0 : 2.226e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.726e-05, Loss_0: 1.124e-08, Loss_r: 5.725e-05, lambda_0 : 2.069e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.679e-05, Loss_0: 1.220e-08, Loss_r: 5.678e-05, lambda_0 : 2.466e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 460, Loss: 5.617e-05, Loss_0: 1.097e-08, Loss_r: 5.616e-05, lambda_0 : 3.505e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 470, Loss: 5.536e-05, Loss_0: 8.703e-09, Loss_r: 5.535e-05, lambda_0 : 4.707e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 480, Loss: 5.427e-05, Loss_0: 7.041e-09, Loss_r: 5.426e-05, lambda_0 : 6.379e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 490, Loss: 5.285e-05, Loss_0: 6.157e-09, Loss_r: 5.284e-05, lambda_0 : 7.960e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 500, Loss: 5.116e-05, Loss_0: 5.136e-09, Loss_r: 5.116e-05, lambda_0 : 8.847e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.954e-05, Loss_0: 3.705e-09, Loss_r: 4.953e-05, lambda_0 : 6.995e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.843e-05, Loss_0: 2.426e-09, Loss_r: 4.842e-05, lambda_0 : 6.940e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 530, Loss: 4.771e-05, Loss_0: 1.797e-09, Loss_r: 4.771e-05, lambda_0 : 8.835e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 540, Loss: 4.991e-05, Loss_0: 2.928e-06, Loss_r: 4.698e-05, lambda_0 : 1.685e+01, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 550, Loss: 8.044e-05, Loss_0: 1.804e-05, Loss_r: 6.241e-05, lambda_0 : 2.073e+02, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 560, Loss: 6.011e-05, Loss_0: 9.160e-07, Loss_r: 5.919e-05, lambda_0 : 4.365e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 570, Loss: 6.956e-05, Loss_0: 1.073e-05, Loss_r: 5.883e-05, lambda_0 : 4.147e+01, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 580, Loss: 5.957e-05, Loss_0: 1.035e-06, Loss_r: 5.853e-05, lambda_0 : 2.421e+02, Time: 0.42, Learning Rate: 0.00100\n",
            "It: 590, Loss: 5.856e-05, Loss_0: 5.525e-07, Loss_r: 5.800e-05, lambda_0 : 5.647e+01, Time: 0.43, Learning Rate: 0.00090\n",
            "It: 600, Loss: 5.819e-05, Loss_0: 5.495e-07, Loss_r: 5.764e-05, lambda_0 : 1.957e+02, Time: 0.42, Learning Rate: 0.00090\n",
            "It: 610, Loss: 5.729e-05, Loss_0: 8.453e-08, Loss_r: 5.721e-05, lambda_0 : 3.352e+02, Time: 0.42, Learning Rate: 0.00090\n",
            "It: 620, Loss: 5.678e-05, Loss_0: 8.271e-08, Loss_r: 5.669e-05, lambda_0 : 7.560e+01, Time: 0.43, Learning Rate: 0.00090\n",
            "It: 630, Loss: 5.606e-05, Loss_0: 1.363e-08, Loss_r: 5.604e-05, lambda_0 : 6.234e+02, Time: 0.43, Learning Rate: 0.00090\n",
            "It: 640, Loss: 5.526e-05, Loss_0: 1.293e-08, Loss_r: 5.525e-05, lambda_0 : 5.675e+02, Time: 0.43, Learning Rate: 0.00090\n",
            "It: 650, Loss: 5.435e-05, Loss_0: 4.746e-09, Loss_r: 5.435e-05, lambda_0 : 1.395e+03, Time: 0.43, Learning Rate: 0.00081\n",
            "It: 660, Loss: 5.353e-05, Loss_0: 4.542e-09, Loss_r: 5.353e-05, lambda_0 : 1.068e+03, Time: 0.43, Learning Rate: 0.00081\n",
            "It: 670, Loss: 5.284e-05, Loss_0: 3.769e-09, Loss_r: 5.283e-05, lambda_0 : 1.028e+03, Time: 0.43, Learning Rate: 0.00081\n",
            "It: 680, Loss: 5.234e-05, Loss_0: 3.156e-09, Loss_r: 5.234e-05, lambda_0 : 6.984e+02, Time: 0.43, Learning Rate: 0.00081\n",
            "It: 690, Loss: 5.202e-05, Loss_0: 2.671e-09, Loss_r: 5.202e-05, lambda_0 : 7.184e+02, Time: 0.44, Learning Rate: 0.00081\n",
            "It: 700, Loss: 5.179e-05, Loss_0: 2.325e-09, Loss_r: 5.178e-05, lambda_0 : 9.179e+02, Time: 0.43, Learning Rate: 0.00081\n",
            "It: 710, Loss: 5.156e-05, Loss_0: 2.118e-09, Loss_r: 5.155e-05, lambda_0 : 9.600e+02, Time: 0.43, Learning Rate: 0.00073\n",
            "It: 720, Loss: 5.133e-05, Loss_0: 2.018e-09, Loss_r: 5.133e-05, lambda_0 : 9.435e+02, Time: 0.43, Learning Rate: 0.00073\n",
            "It: 730, Loss: 5.110e-05, Loss_0: 2.000e-09, Loss_r: 5.110e-05, lambda_0 : 9.656e+02, Time: 0.43, Learning Rate: 0.00073\n",
            "It: 740, Loss: 5.086e-05, Loss_0: 2.008e-09, Loss_r: 5.086e-05, lambda_0 : 9.608e+02, Time: 0.43, Learning Rate: 0.00073\n",
            "It: 750, Loss: 5.060e-05, Loss_0: 2.005e-09, Loss_r: 5.060e-05, lambda_0 : 9.484e+02, Time: 0.43, Learning Rate: 0.00073\n",
            "It: 760, Loss: 5.032e-05, Loss_0: 1.981e-09, Loss_r: 5.032e-05, lambda_0 : 9.627e+02, Time: 0.42, Learning Rate: 0.00073\n",
            "It: 770, Loss: 5.003e-05, Loss_0: 1.945e-09, Loss_r: 5.003e-05, lambda_0 : 9.704e+02, Time: 0.43, Learning Rate: 0.00066\n",
            "It: 780, Loss: 4.974e-05, Loss_0: 1.904e-09, Loss_r: 4.974e-05, lambda_0 : 9.862e+02, Time: 0.43, Learning Rate: 0.00066\n",
            "It: 790, Loss: 4.943e-05, Loss_0: 1.867e-09, Loss_r: 4.943e-05, lambda_0 : 1.000e+03, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 800, Loss: 4.910e-05, Loss_0: 1.832e-09, Loss_r: 4.910e-05, lambda_0 : 1.014e+03, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 810, Loss: 4.874e-05, Loss_0: 1.793e-09, Loss_r: 4.873e-05, lambda_0 : 1.030e+03, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 820, Loss: 4.834e-05, Loss_0: 1.756e-09, Loss_r: 4.834e-05, lambda_0 : 1.046e+03, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 830, Loss: 4.790e-05, Loss_0: 1.720e-09, Loss_r: 4.790e-05, lambda_0 : 1.062e+03, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 840, Loss: 4.746e-05, Loss_0: 1.686e-09, Loss_r: 4.746e-05, lambda_0 : 1.075e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 850, Loss: 4.697e-05, Loss_0: 1.653e-09, Loss_r: 4.697e-05, lambda_0 : 1.087e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 860, Loss: 4.643e-05, Loss_0: 1.621e-09, Loss_r: 4.643e-05, lambda_0 : 1.096e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 870, Loss: 4.582e-05, Loss_0: 1.592e-09, Loss_r: 4.582e-05, lambda_0 : 1.101e+03, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 880, Loss: 4.514e-05, Loss_0: 1.565e-09, Loss_r: 4.514e-05, lambda_0 : 1.099e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 890, Loss: 4.437e-05, Loss_0: 1.540e-09, Loss_r: 4.437e-05, lambda_0 : 1.090e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 900, Loss: 4.350e-05, Loss_0: 1.517e-09, Loss_r: 4.350e-05, lambda_0 : 1.070e+03, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 910, Loss: 4.253e-05, Loss_0: 1.491e-09, Loss_r: 4.253e-05, lambda_0 : 1.041e+03, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 920, Loss: 4.147e-05, Loss_0: 1.457e-09, Loss_r: 4.146e-05, lambda_0 : 1.011e+03, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 930, Loss: 4.581e-05, Loss_0: 5.535e-06, Loss_r: 4.028e-05, lambda_0 : 1.310e+01, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 940, Loss: 7.544e-05, Loss_0: 2.350e-05, Loss_r: 5.194e-05, lambda_0 : 7.801e+01, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 950, Loss: 8.655e-05, Loss_0: 3.019e-05, Loss_r: 5.636e-05, lambda_0 : 4.381e+01, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 960, Loss: 7.562e-05, Loss_0: 1.910e-05, Loss_r: 5.652e-05, lambda_0 : 1.158e+02, Time: 0.42, Learning Rate: 0.00059\n",
            "It: 970, Loss: 6.320e-05, Loss_0: 7.286e-06, Loss_r: 5.591e-05, lambda_0 : 3.054e+01, Time: 0.43, Learning Rate: 0.00059\n",
            "It: 980, Loss: 5.775e-05, Loss_0: 2.261e-06, Loss_r: 5.549e-05, lambda_0 : 4.933e+01, Time: 0.43, Learning Rate: 0.00053\n",
            "It: 990, Loss: 5.519e-05, Loss_0: 3.612e-07, Loss_r: 5.483e-05, lambda_0 : 2.272e+02, Time: 0.44, Learning Rate: 0.00053\n",
            "It: 1000, Loss: 5.437e-05, Loss_0: 8.813e-09, Loss_r: 5.436e-05, lambda_0 : 1.362e+03, Time: 0.43, Learning Rate: 0.00053\n",
            "It: 1010, Loss: 5.390e-05, Loss_0: 5.816e-08, Loss_r: 5.384e-05, lambda_0 : 3.597e+02, Time: 0.43, Learning Rate: 0.00053\n",
            "It: 1020, Loss: 5.347e-05, Loss_0: 4.091e-08, Loss_r: 5.343e-05, lambda_0 : 4.563e+02, Time: 0.43, Learning Rate: 0.00053\n",
            "It: 1030, Loss: 5.312e-05, Loss_0: 1.724e-08, Loss_r: 5.310e-05, lambda_0 : 8.452e+02, Time: 0.43, Learning Rate: 0.00053\n",
            "It: 1040, Loss: 5.286e-05, Loss_0: 7.538e-09, Loss_r: 5.285e-05, lambda_0 : 3.686e+02, Time: 0.42, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 5.268e-05, Loss_0: 2.052e-09, Loss_r: 5.268e-05, lambda_0 : 1.138e+03, Time: 0.42, Learning Rate: 0.00048\n",
            "It: 1060, Loss: 5.253e-05, Loss_0: 2.385e-09, Loss_r: 5.253e-05, lambda_0 : 6.925e+02, Time: 0.42, Learning Rate: 0.00048\n",
            "It: 1070, Loss: 5.240e-05, Loss_0: 2.232e-09, Loss_r: 5.240e-05, lambda_0 : 7.827e+02, Time: 0.43, Learning Rate: 0.00048\n",
            "It: 1080, Loss: 5.227e-05, Loss_0: 1.998e-09, Loss_r: 5.227e-05, lambda_0 : 9.326e+02, Time: 0.42, Learning Rate: 0.00048\n",
            "It: 1090, Loss: 5.215e-05, Loss_0: 1.853e-09, Loss_r: 5.215e-05, lambda_0 : 9.910e+02, Time: 0.42, Learning Rate: 0.00048\n",
            "It: 1100, Loss: 5.202e-05, Loss_0: 1.880e-09, Loss_r: 5.202e-05, lambda_0 : 9.697e+02, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1110, Loss: 5.190e-05, Loss_0: 1.829e-09, Loss_r: 5.189e-05, lambda_0 : 1.000e+03, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1120, Loss: 5.177e-05, Loss_0: 1.782e-09, Loss_r: 5.177e-05, lambda_0 : 1.041e+03, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1130, Loss: 5.164e-05, Loss_0: 1.741e-09, Loss_r: 5.164e-05, lambda_0 : 1.069e+03, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1140, Loss: 5.150e-05, Loss_0: 1.719e-09, Loss_r: 5.150e-05, lambda_0 : 1.080e+03, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1150, Loss: 5.136e-05, Loss_0: 1.691e-09, Loss_r: 5.136e-05, lambda_0 : 1.087e+03, Time: 0.42, Learning Rate: 0.00043\n",
            "It: 1160, Loss: 5.121e-05, Loss_0: 1.662e-09, Loss_r: 5.121e-05, lambda_0 : 1.096e+03, Time: 0.43, Learning Rate: 0.00039\n",
            "It: 1170, Loss: 5.107e-05, Loss_0: 1.636e-09, Loss_r: 5.107e-05, lambda_0 : 1.107e+03, Time: 0.42, Learning Rate: 0.00039\n",
            "It: 1180, Loss: 5.093e-05, Loss_0: 1.608e-09, Loss_r: 5.093e-05, lambda_0 : 1.120e+03, Time: 0.42, Learning Rate: 0.00039\n",
            "It: 1190, Loss: 5.078e-05, Loss_0: 1.580e-09, Loss_r: 5.077e-05, lambda_0 : 1.133e+03, Time: 0.43, Learning Rate: 0.00039\n",
            "It: 1200, Loss: 5.062e-05, Loss_0: 1.552e-09, Loss_r: 5.062e-05, lambda_0 : 1.147e+03, Time: 0.43, Learning Rate: 0.00039\n",
            "It: 1210, Loss: 5.045e-05, Loss_0: 1.523e-09, Loss_r: 5.045e-05, lambda_0 : 1.160e+03, Time: 0.42, Learning Rate: 0.00039\n",
            "It: 1220, Loss: 5.028e-05, Loss_0: 1.494e-09, Loss_r: 5.027e-05, lambda_0 : 1.174e+03, Time: 0.42, Learning Rate: 0.00035\n",
            "It: 1230, Loss: 5.011e-05, Loss_0: 1.468e-09, Loss_r: 5.011e-05, lambda_0 : 1.187e+03, Time: 0.43, Learning Rate: 0.00035\n",
            "It: 1240, Loss: 4.994e-05, Loss_0: 1.442e-09, Loss_r: 4.994e-05, lambda_0 : 1.199e+03, Time: 0.42, Learning Rate: 0.00035\n",
            "It: 1250, Loss: 4.976e-05, Loss_0: 1.415e-09, Loss_r: 4.975e-05, lambda_0 : 1.212e+03, Time: 0.42, Learning Rate: 0.00035\n",
            "It: 1260, Loss: 4.956e-05, Loss_0: 1.388e-09, Loss_r: 4.956e-05, lambda_0 : 1.225e+03, Time: 0.43, Learning Rate: 0.00035\n",
            "It: 1270, Loss: 4.936e-05, Loss_0: 1.361e-09, Loss_r: 4.936e-05, lambda_0 : 1.237e+03, Time: 0.43, Learning Rate: 0.00035\n",
            "It: 1280, Loss: 4.915e-05, Loss_0: 1.335e-09, Loss_r: 4.915e-05, lambda_0 : 1.249e+03, Time: 0.43, Learning Rate: 0.00031\n",
            "It: 1290, Loss: 4.894e-05, Loss_0: 1.311e-09, Loss_r: 4.894e-05, lambda_0 : 1.260e+03, Time: 0.43, Learning Rate: 0.00031\n",
            "It: 1300, Loss: 4.873e-05, Loss_0: 1.287e-09, Loss_r: 4.873e-05, lambda_0 : 1.270e+03, Time: 0.43, Learning Rate: 0.00031\n",
            "It: 1310, Loss: 4.850e-05, Loss_0: 1.263e-09, Loss_r: 4.850e-05, lambda_0 : 1.279e+03, Time: 0.42, Learning Rate: 0.00031\n",
            "It: 1320, Loss: 4.826e-05, Loss_0: 1.240e-09, Loss_r: 4.826e-05, lambda_0 : 1.287e+03, Time: 0.42, Learning Rate: 0.00031\n",
            "It: 1330, Loss: 4.801e-05, Loss_0: 1.217e-09, Loss_r: 4.801e-05, lambda_0 : 1.294e+03, Time: 0.42, Learning Rate: 0.00031\n",
            "It: 1340, Loss: 4.774e-05, Loss_0: 1.195e-09, Loss_r: 4.774e-05, lambda_0 : 1.300e+03, Time: 0.42, Learning Rate: 0.00028\n",
            "It: 1350, Loss: 4.749e-05, Loss_0: 1.175e-09, Loss_r: 4.749e-05, lambda_0 : 1.304e+03, Time: 0.43, Learning Rate: 0.00028\n",
            "It: 1360, Loss: 4.722e-05, Loss_0: 1.155e-09, Loss_r: 4.722e-05, lambda_0 : 1.306e+03, Time: 0.42, Learning Rate: 0.00028\n",
            "It: 1370, Loss: 4.694e-05, Loss_0: 1.135e-09, Loss_r: 4.694e-05, lambda_0 : 1.307e+03, Time: 0.43, Learning Rate: 0.00028\n",
            "It: 1380, Loss: 4.665e-05, Loss_0: 1.115e-09, Loss_r: 4.665e-05, lambda_0 : 1.307e+03, Time: 0.42, Learning Rate: 0.00028\n",
            "It: 1390, Loss: 4.634e-05, Loss_0: 1.094e-09, Loss_r: 4.634e-05, lambda_0 : 1.306e+03, Time: 0.42, Learning Rate: 0.00028\n",
            "It: 1400, Loss: 4.602e-05, Loss_0: 1.073e-09, Loss_r: 4.602e-05, lambda_0 : 1.304e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1410, Loss: 4.572e-05, Loss_0: 1.053e-09, Loss_r: 4.571e-05, lambda_0 : 1.303e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1420, Loss: 4.540e-05, Loss_0: 1.031e-09, Loss_r: 4.540e-05, lambda_0 : 1.303e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1430, Loss: 4.507e-05, Loss_0: 1.008e-09, Loss_r: 4.507e-05, lambda_0 : 1.304e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1440, Loss: 4.474e-05, Loss_0: 9.836e-10, Loss_r: 4.474e-05, lambda_0 : 1.307e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1450, Loss: 4.439e-05, Loss_0: 9.571e-10, Loss_r: 4.439e-05, lambda_0 : 1.312e+03, Time: 0.42, Learning Rate: 0.00025\n",
            "It: 1460, Loss: 4.403e-05, Loss_0: 9.289e-10, Loss_r: 4.403e-05, lambda_0 : 1.321e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1470, Loss: 4.370e-05, Loss_0: 9.014e-10, Loss_r: 4.370e-05, lambda_0 : 1.332e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1480, Loss: 4.336e-05, Loss_0: 8.723e-10, Loss_r: 4.336e-05, lambda_0 : 1.347e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1490, Loss: 4.301e-05, Loss_0: 8.418e-10, Loss_r: 4.301e-05, lambda_0 : 1.382e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1500, Loss: 4.265e-05, Loss_0: 8.105e-10, Loss_r: 4.265e-05, lambda_0 : 1.440e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1510, Loss: 4.229e-05, Loss_0: 7.782e-10, Loss_r: 4.229e-05, lambda_0 : 1.482e+03, Time: 0.42, Learning Rate: 0.00023\n",
            "It: 1520, Loss: 4.192e-05, Loss_0: 7.455e-10, Loss_r: 4.192e-05, lambda_0 : 1.504e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1530, Loss: 4.158e-05, Loss_0: 7.157e-10, Loss_r: 4.158e-05, lambda_0 : 1.508e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1540, Loss: 4.123e-05, Loss_0: 6.857e-10, Loss_r: 4.123e-05, lambda_0 : 1.515e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1550, Loss: 4.088e-05, Loss_0: 6.560e-10, Loss_r: 4.088e-05, lambda_0 : 1.554e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1560, Loss: 4.052e-05, Loss_0: 6.267e-10, Loss_r: 4.052e-05, lambda_0 : 1.597e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1570, Loss: 4.016e-05, Loss_0: 5.979e-10, Loss_r: 4.015e-05, lambda_0 : 1.642e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1580, Loss: 3.978e-05, Loss_0: 5.698e-10, Loss_r: 3.978e-05, lambda_0 : 1.691e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1590, Loss: 3.941e-05, Loss_0: 5.423e-10, Loss_r: 3.941e-05, lambda_0 : 1.743e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1600, Loss: 3.902e-05, Loss_0: 5.158e-10, Loss_r: 3.902e-05, lambda_0 : 1.797e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1610, Loss: 3.863e-05, Loss_0: 4.902e-10, Loss_r: 3.863e-05, lambda_0 : 1.854e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1620, Loss: 3.823e-05, Loss_0: 4.656e-10, Loss_r: 3.823e-05, lambda_0 : 1.914e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1630, Loss: 3.783e-05, Loss_0: 4.420e-10, Loss_r: 3.783e-05, lambda_0 : 1.976e+03, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1640, Loss: 3.742e-05, Loss_0: 4.196e-10, Loss_r: 3.742e-05, lambda_0 : 2.039e+03, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1650, Loss: 3.700e-05, Loss_0: 3.984e-10, Loss_r: 3.700e-05, lambda_0 : 2.103e+03, Time: 0.45, Learning Rate: 0.00021\n",
            "It: 1660, Loss: 3.657e-05, Loss_0: 3.783e-10, Loss_r: 3.657e-05, lambda_0 : 2.169e+03, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1670, Loss: 3.613e-05, Loss_0: 3.595e-10, Loss_r: 3.613e-05, lambda_0 : 2.234e+03, Time: 0.48, Learning Rate: 0.00021\n",
            "It: 1680, Loss: 3.569e-05, Loss_0: 3.419e-10, Loss_r: 3.569e-05, lambda_0 : 2.299e+03, Time: 0.47, Learning Rate: 0.00021\n",
            "It: 1690, Loss: 3.524e-05, Loss_0: 3.254e-10, Loss_r: 3.524e-05, lambda_0 : 2.364e+03, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1700, Loss: 3.478e-05, Loss_0: 3.100e-10, Loss_r: 3.478e-05, lambda_0 : 2.425e+03, Time: 0.49, Learning Rate: 0.00021\n",
            "It: 1710, Loss: 3.431e-05, Loss_0: 2.848e-10, Loss_r: 3.431e-05, lambda_0 : 2.337e+03, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1720, Loss: 4.187e-05, Loss_0: 3.705e-09, Loss_r: 4.187e-05, lambda_0 : 5.450e+04, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1730, Loss: 3.456e-05, Loss_0: 4.843e-09, Loss_r: 3.455e-05, lambda_0 : 1.783e+04, Time: 0.49, Learning Rate: 0.00021\n",
            "It: 1740, Loss: 3.373e-05, Loss_0: 1.856e-09, Loss_r: 3.373e-05, lambda_0 : 2.372e+04, Time: 0.44, Learning Rate: 0.00021\n",
            "It: 1750, Loss: 3.266e-05, Loss_0: 2.857e-10, Loss_r: 3.266e-05, lambda_0 : 1.088e+04, Time: 0.44, Learning Rate: 0.00021\n",
            "It: 1760, Loss: 3.230e-05, Loss_0: 2.831e-09, Loss_r: 3.230e-05, lambda_0 : 6.986e+03, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1770, Loss: 3.178e-05, Loss_0: 2.456e-09, Loss_r: 3.177e-05, lambda_0 : 4.337e+03, Time: 0.44, Learning Rate: 0.00021\n",
            "It: 1780, Loss: 4.392e-05, Loss_0: 1.265e-05, Loss_r: 3.127e-05, lambda_0 : 6.652e+00, Time: 0.48, Learning Rate: 0.00021\n",
            "It: 1790, Loss: 4.835e-05, Loss_0: 1.697e-05, Loss_r: 3.138e-05, lambda_0 : 1.316e+01, Time: 0.49, Learning Rate: 0.00021\n",
            "It: 1800, Loss: 3.739e-05, Loss_0: 5.865e-06, Loss_r: 3.153e-05, lambda_0 : 4.001e+01, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1810, Loss: 3.207e-05, Loss_0: 7.272e-07, Loss_r: 3.135e-05, lambda_0 : 1.315e+02, Time: 0.44, Learning Rate: 0.00021\n",
            "It: 1820, Loss: 3.104e-05, Loss_0: 2.014e-08, Loss_r: 3.102e-05, lambda_0 : 2.675e+02, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1830, Loss: 3.070e-05, Loss_0: 9.256e-08, Loss_r: 3.060e-05, lambda_0 : 1.052e+02, Time: 0.46, Learning Rate: 0.00021\n",
            "It: 1840, Loss: 3.024e-05, Loss_0: 8.128e-08, Loss_r: 3.016e-05, lambda_0 : 4.555e+02, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1850, Loss: 2.970e-05, Loss_0: 2.723e-08, Loss_r: 2.967e-05, lambda_0 : 4.782e+02, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1860, Loss: 2.923e-05, Loss_0: 5.216e-08, Loss_r: 2.918e-05, lambda_0 : 3.147e+02, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1870, Loss: 3.413e-05, Loss_0: 5.457e-06, Loss_r: 2.868e-05, lambda_0 : 4.534e+01, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1880, Loss: 2.993e-05, Loss_0: 8.629e-07, Loss_r: 2.907e-05, lambda_0 : 2.006e+02, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1890, Loss: 3.990e-05, Loss_0: 1.028e-05, Loss_r: 2.962e-05, lambda_0 : 1.176e+01, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1900, Loss: 3.318e-05, Loss_0: 3.497e-06, Loss_r: 2.968e-05, lambda_0 : 6.211e+01, Time: 0.42, Learning Rate: 0.00021\n",
            "It: 1910, Loss: 3.064e-05, Loss_0: 1.200e-06, Loss_r: 2.944e-05, lambda_0 : 1.135e+02, Time: 0.43, Learning Rate: 0.00021\n",
            "It: 1920, Loss: 2.921e-05, Loss_0: 1.750e-07, Loss_r: 2.903e-05, lambda_0 : 8.582e+01, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1930, Loss: 2.869e-05, Loss_0: 7.987e-08, Loss_r: 2.861e-05, lambda_0 : 1.353e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1940, Loss: 2.816e-05, Loss_0: 1.422e-09, Loss_r: 2.815e-05, lambda_0 : 9.759e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1950, Loss: 2.771e-05, Loss_0: 2.225e-08, Loss_r: 2.768e-05, lambda_0 : 2.128e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1960, Loss: 2.722e-05, Loss_0: 4.576e-09, Loss_r: 2.721e-05, lambda_0 : 5.508e+02, Time: 0.43, Learning Rate: 0.00019\n",
            "It: 1970, Loss: 2.673e-05, Loss_0: 3.940e-09, Loss_r: 2.673e-05, lambda_0 : 4.701e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1980, Loss: 2.625e-05, Loss_0: 2.935e-10, Loss_r: 2.625e-05, lambda_0 : 1.701e+03, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 1990, Loss: 2.576e-05, Loss_0: 5.885e-10, Loss_r: 2.576e-05, lambda_0 : 1.088e+03, Time: 0.43, Learning Rate: 0.00019\n",
            "It: 2000, Loss: 2.730e-05, Loss_0: 2.857e-09, Loss_r: 2.729e-05, lambda_0 : 3.556e+04, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 2010, Loss: 3.065e-05, Loss_0: 4.334e-06, Loss_r: 2.632e-05, lambda_0 : 8.422e+02, Time: 0.43, Learning Rate: 0.00019\n",
            "It: 2020, Loss: 5.013e-05, Loss_0: 2.445e-05, Loss_r: 2.568e-05, lambda_0 : 1.231e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 2030, Loss: 3.142e-05, Loss_0: 4.981e-06, Loss_r: 2.644e-05, lambda_0 : 4.238e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 2040, Loss: 2.631e-05, Loss_0: 3.156e-07, Loss_r: 2.600e-05, lambda_0 : 2.829e+02, Time: 0.42, Learning Rate: 0.00019\n",
            "It: 2050, Loss: 2.585e-05, Loss_0: 2.062e-09, Loss_r: 2.585e-05, lambda_0 : 6.349e+03, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2060, Loss: 2.568e-05, Loss_0: 1.779e-07, Loss_r: 2.551e-05, lambda_0 : 1.254e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2070, Loss: 2.524e-05, Loss_0: 1.009e-07, Loss_r: 2.514e-05, lambda_0 : 2.023e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2080, Loss: 2.476e-05, Loss_0: 1.564e-08, Loss_r: 2.475e-05, lambda_0 : 5.773e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2090, Loss: 2.434e-05, Loss_0: 6.058e-09, Loss_r: 2.434e-05, lambda_0 : 4.375e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2100, Loss: 2.393e-05, Loss_0: 2.969e-09, Loss_r: 2.393e-05, lambda_0 : 6.637e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2110, Loss: 2.352e-05, Loss_0: 2.561e-09, Loss_r: 2.352e-05, lambda_0 : 4.846e+02, Time: 0.43, Learning Rate: 0.00017\n",
            "It: 2120, Loss: 2.311e-05, Loss_0: 2.752e-10, Loss_r: 2.311e-05, lambda_0 : 1.595e+03, Time: 0.43, Learning Rate: 0.00017\n",
            "It: 2130, Loss: 2.270e-05, Loss_0: 5.797e-10, Loss_r: 2.270e-05, lambda_0 : 1.085e+03, Time: 0.43, Learning Rate: 0.00017\n",
            "It: 2140, Loss: 2.229e-05, Loss_0: 3.442e-10, Loss_r: 2.229e-05, lambda_0 : 1.375e+03, Time: 0.44, Learning Rate: 0.00017\n",
            "It: 2150, Loss: 2.188e-05, Loss_0: 1.747e-10, Loss_r: 2.188e-05, lambda_0 : 2.553e+03, Time: 0.44, Learning Rate: 0.00017\n",
            "It: 2160, Loss: 2.147e-05, Loss_0: 4.679e-10, Loss_r: 2.147e-05, lambda_0 : 1.147e+03, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2170, Loss: 2.137e-05, Loss_0: 3.034e-07, Loss_r: 2.107e-05, lambda_0 : 4.892e+01, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2180, Loss: 6.064e-05, Loss_0: 3.926e-05, Loss_r: 2.139e-05, lambda_0 : 2.702e+01, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2190, Loss: 3.835e-05, Loss_0: 1.536e-05, Loss_r: 2.299e-05, lambda_0 : 1.652e+01, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2200, Loss: 2.413e-05, Loss_0: 3.862e-07, Loss_r: 2.375e-05, lambda_0 : 1.810e+02, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2210, Loss: 2.427e-05, Loss_0: 3.834e-07, Loss_r: 2.388e-05, lambda_0 : 1.078e+02, Time: 0.43, Learning Rate: 0.00017\n",
            "It: 2220, Loss: 2.400e-05, Loss_0: 2.886e-07, Loss_r: 2.371e-05, lambda_0 : 9.875e+01, Time: 0.42, Learning Rate: 0.00017\n",
            "It: 2230, Loss: 2.345e-05, Loss_0: 6.843e-08, Loss_r: 2.338e-05, lambda_0 : 1.085e+02, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2240, Loss: 2.313e-05, Loss_0: 9.329e-08, Loss_r: 2.304e-05, lambda_0 : 9.880e+01, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2250, Loss: 2.271e-05, Loss_0: 4.134e-08, Loss_r: 2.267e-05, lambda_0 : 1.374e+02, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2260, Loss: 2.230e-05, Loss_0: 9.302e-09, Loss_r: 2.229e-05, lambda_0 : 3.173e+02, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2270, Loss: 2.191e-05, Loss_0: 1.769e-10, Loss_r: 2.191e-05, lambda_0 : 2.404e+03, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2280, Loss: 2.153e-05, Loss_0: 2.555e-09, Loss_r: 2.153e-05, lambda_0 : 4.908e+02, Time: 0.42, Learning Rate: 0.00015\n",
            "It: 2290, Loss: 2.115e-05, Loss_0: 1.573e-10, Loss_r: 2.115e-05, lambda_0 : 2.320e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2300, Loss: 2.080e-05, Loss_0: 2.869e-10, Loss_r: 2.080e-05, lambda_0 : 1.442e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2310, Loss: 2.047e-05, Loss_0: 2.078e-10, Loss_r: 2.046e-05, lambda_0 : 1.808e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2320, Loss: 2.013e-05, Loss_0: 2.088e-10, Loss_r: 2.013e-05, lambda_0 : 1.769e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2330, Loss: 1.979e-05, Loss_0: 1.685e-10, Loss_r: 1.979e-05, lambda_0 : 2.217e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2340, Loss: 1.945e-05, Loss_0: 1.583e-10, Loss_r: 1.945e-05, lambda_0 : 2.423e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2350, Loss: 1.912e-05, Loss_0: 1.548e-10, Loss_r: 1.912e-05, lambda_0 : 2.463e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2360, Loss: 1.879e-05, Loss_0: 1.492e-10, Loss_r: 1.879e-05, lambda_0 : 2.566e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2370, Loss: 1.846e-05, Loss_0: 1.442e-10, Loss_r: 1.846e-05, lambda_0 : 2.451e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2380, Loss: 1.813e-05, Loss_0: 1.439e-10, Loss_r: 1.813e-05, lambda_0 : 2.494e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2390, Loss: 1.781e-05, Loss_0: 1.414e-10, Loss_r: 1.781e-05, lambda_0 : 2.460e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2400, Loss: 1.748e-05, Loss_0: 1.401e-10, Loss_r: 1.748e-05, lambda_0 : 2.468e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2410, Loss: 1.716e-05, Loss_0: 1.379e-10, Loss_r: 1.716e-05, lambda_0 : 2.445e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2420, Loss: 1.684e-05, Loss_0: 1.360e-10, Loss_r: 1.684e-05, lambda_0 : 2.431e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2430, Loss: 1.653e-05, Loss_0: 1.342e-10, Loss_r: 1.653e-05, lambda_0 : 2.424e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2440, Loss: 1.621e-05, Loss_0: 1.322e-10, Loss_r: 1.621e-05, lambda_0 : 2.409e+03, Time: 0.44, Learning Rate: 0.00014\n",
            "It: 2450, Loss: 1.590e-05, Loss_0: 1.305e-10, Loss_r: 1.590e-05, lambda_0 : 2.404e+03, Time: 0.43, Learning Rate: 0.00014\n",
            "It: 2460, Loss: 1.559e-05, Loss_0: 1.349e-10, Loss_r: 1.559e-05, lambda_0 : 2.267e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2470, Loss: 1.538e-05, Loss_0: 9.445e-08, Loss_r: 1.529e-05, lambda_0 : 6.928e+01, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2480, Loss: 1.383e-04, Loss_0: 1.211e-04, Loss_r: 1.726e-05, lambda_0 : 1.044e+02, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2490, Loss: 3.210e-05, Loss_0: 1.379e-05, Loss_r: 1.831e-05, lambda_0 : 2.016e+02, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2500, Loss: 3.070e-05, Loss_0: 1.151e-05, Loss_r: 1.919e-05, lambda_0 : 8.183e+01, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2510, Loss: 1.966e-05, Loss_0: 7.256e-08, Loss_r: 1.959e-05, lambda_0 : 1.027e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2520, Loss: 2.006e-05, Loss_0: 4.996e-07, Loss_r: 1.956e-05, lambda_0 : 2.478e+02, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2530, Loss: 2.001e-05, Loss_0: 6.569e-07, Loss_r: 1.935e-05, lambda_0 : 1.663e+02, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2540, Loss: 1.934e-05, Loss_0: 2.508e-07, Loss_r: 1.909e-05, lambda_0 : 5.541e+01, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2550, Loss: 1.889e-05, Loss_0: 9.202e-08, Loss_r: 1.880e-05, lambda_0 : 1.060e+02, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2560, Loss: 1.852e-05, Loss_0: 1.791e-08, Loss_r: 1.851e-05, lambda_0 : 2.508e+02, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2570, Loss: 1.821e-05, Loss_0: 1.779e-10, Loss_r: 1.821e-05, lambda_0 : 1.915e+03, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2580, Loss: 1.791e-05, Loss_0: 3.790e-09, Loss_r: 1.791e-05, lambda_0 : 4.547e+02, Time: 0.42, Learning Rate: 0.00012\n",
            "It: 2590, Loss: 1.761e-05, Loss_0: 3.249e-10, Loss_r: 1.761e-05, lambda_0 : 1.353e+03, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2600, Loss: 1.734e-05, Loss_0: 2.311e-10, Loss_r: 1.734e-05, lambda_0 : 1.501e+03, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2610, Loss: 1.708e-05, Loss_0: 3.309e-10, Loss_r: 1.708e-05, lambda_0 : 1.186e+03, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2620, Loss: 1.681e-05, Loss_0: 1.249e-10, Loss_r: 1.681e-05, lambda_0 : 2.657e+03, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2630, Loss: 1.655e-05, Loss_0: 1.258e-10, Loss_r: 1.655e-05, lambda_0 : 2.117e+03, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2640, Loss: 1.629e-05, Loss_0: 1.206e-10, Loss_r: 1.629e-05, lambda_0 : 2.726e+03, Time: 0.43, Learning Rate: 0.00011\n",
            "It: 2650, Loss: 1.603e-05, Loss_0: 1.197e-10, Loss_r: 1.603e-05, lambda_0 : 2.715e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2660, Loss: 1.580e-05, Loss_0: 1.137e-10, Loss_r: 1.580e-05, lambda_0 : 2.599e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2670, Loss: 1.557e-05, Loss_0: 1.139e-10, Loss_r: 1.557e-05, lambda_0 : 2.658e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2680, Loss: 1.534e-05, Loss_0: 1.144e-10, Loss_r: 1.534e-05, lambda_0 : 2.709e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2690, Loss: 1.512e-05, Loss_0: 1.120e-10, Loss_r: 1.512e-05, lambda_0 : 2.634e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2700, Loss: 1.489e-05, Loss_0: 1.116e-10, Loss_r: 1.489e-05, lambda_0 : 2.648e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2710, Loss: 1.467e-05, Loss_0: 1.104e-10, Loss_r: 1.467e-05, lambda_0 : 2.627e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2720, Loss: 1.445e-05, Loss_0: 1.094e-10, Loss_r: 1.445e-05, lambda_0 : 2.618e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2730, Loss: 1.422e-05, Loss_0: 1.083e-10, Loss_r: 1.422e-05, lambda_0 : 2.602e+03, Time: 0.44, Learning Rate: 0.00010\n",
            "It: 2740, Loss: 1.401e-05, Loss_0: 1.073e-10, Loss_r: 1.401e-05, lambda_0 : 2.593e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2750, Loss: 1.379e-05, Loss_0: 1.063e-10, Loss_r: 1.379e-05, lambda_0 : 2.579e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2760, Loss: 1.357e-05, Loss_0: 1.053e-10, Loss_r: 1.357e-05, lambda_0 : 2.567e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2770, Loss: 1.336e-05, Loss_0: 1.043e-10, Loss_r: 1.336e-05, lambda_0 : 2.555e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2780, Loss: 1.315e-05, Loss_0: 1.034e-10, Loss_r: 1.315e-05, lambda_0 : 2.541e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2790, Loss: 1.294e-05, Loss_0: 1.025e-10, Loss_r: 1.294e-05, lambda_0 : 2.527e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2800, Loss: 1.273e-05, Loss_0: 1.015e-10, Loss_r: 1.273e-05, lambda_0 : 2.513e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2810, Loss: 1.252e-05, Loss_0: 1.007e-10, Loss_r: 1.252e-05, lambda_0 : 2.498e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2820, Loss: 1.232e-05, Loss_0: 9.983e-11, Loss_r: 1.232e-05, lambda_0 : 2.483e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2830, Loss: 1.211e-05, Loss_0: 9.902e-11, Loss_r: 1.211e-05, lambda_0 : 2.467e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2840, Loss: 1.191e-05, Loss_0: 9.825e-11, Loss_r: 1.191e-05, lambda_0 : 2.450e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2850, Loss: 1.172e-05, Loss_0: 9.751e-11, Loss_r: 1.172e-05, lambda_0 : 2.433e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2860, Loss: 1.152e-05, Loss_0: 9.678e-11, Loss_r: 1.152e-05, lambda_0 : 2.413e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2870, Loss: 1.132e-05, Loss_0: 9.610e-11, Loss_r: 1.132e-05, lambda_0 : 2.394e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2880, Loss: 1.113e-05, Loss_0: 9.547e-11, Loss_r: 1.113e-05, lambda_0 : 2.374e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2890, Loss: 1.094e-05, Loss_0: 9.483e-11, Loss_r: 1.094e-05, lambda_0 : 2.353e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2900, Loss: 1.075e-05, Loss_0: 9.426e-11, Loss_r: 1.075e-05, lambda_0 : 2.332e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2910, Loss: 1.057e-05, Loss_0: 9.370e-11, Loss_r: 1.057e-05, lambda_0 : 2.309e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2920, Loss: 1.038e-05, Loss_0: 9.316e-11, Loss_r: 1.038e-05, lambda_0 : 2.285e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2930, Loss: 1.020e-05, Loss_0: 9.269e-11, Loss_r: 1.020e-05, lambda_0 : 2.263e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2940, Loss: 1.002e-05, Loss_0: 9.221e-11, Loss_r: 1.002e-05, lambda_0 : 2.236e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2950, Loss: 9.842e-06, Loss_0: 9.177e-11, Loss_r: 9.842e-06, lambda_0 : 2.208e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2960, Loss: 9.667e-06, Loss_0: 9.129e-11, Loss_r: 9.667e-06, lambda_0 : 2.176e+03, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 2970, Loss: 9.494e-06, Loss_0: 9.004e-11, Loss_r: 9.494e-06, lambda_0 : 2.650e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2980, Loss: 9.334e-06, Loss_0: 9.787e-11, Loss_r: 9.334e-06, lambda_0 : 1.539e+04, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 2990, Loss: 1.153e-05, Loss_0: 1.439e-07, Loss_r: 1.139e-05, lambda_0 : 5.633e+03, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 3000, Loss: 7.923e-05, Loss_0: 6.929e-05, Loss_r: 9.947e-06, lambda_0 : 1.366e+02, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 3010, Loss: 1.259e-05, Loss_0: 2.659e-06, Loss_r: 9.929e-06, lambda_0 : 2.455e+02, Time: 0.44, Learning Rate: 0.00010\n",
            "It: 3020, Loss: 1.129e-05, Loss_0: 1.090e-06, Loss_r: 1.020e-05, lambda_0 : 5.903e+01, Time: 0.43, Learning Rate: 0.00010\n",
            "It: 3030, Loss: 1.299e-05, Loss_0: 2.688e-06, Loss_r: 1.030e-05, lambda_0 : 1.164e+02, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 3040, Loss: 1.157e-05, Loss_0: 1.331e-06, Loss_r: 1.024e-05, lambda_0 : 1.421e+02, Time: 0.42, Learning Rate: 0.00009\n",
            "It: 3050, Loss: 1.020e-05, Loss_0: 9.126e-08, Loss_r: 1.011e-05, lambda_0 : 5.446e+01, Time: 0.42, Learning Rate: 0.00009\n",
            "It: 3060, Loss: 1.010e-05, Loss_0: 1.283e-07, Loss_r: 9.972e-06, lambda_0 : 1.695e+02, Time: 0.42, Learning Rate: 0.00009\n",
            "It: 3070, Loss: 9.829e-06, Loss_0: 8.150e-09, Loss_r: 9.821e-06, lambda_0 : 3.475e+02, Time: 0.43, Learning Rate: 0.00009\n",
            "It: 3080, Loss: 9.688e-06, Loss_0: 2.194e-08, Loss_r: 9.666e-06, lambda_0 : 1.147e+02, Time: 0.42, Learning Rate: 0.00009\n",
            "It: 3090, Loss: 9.515e-06, Loss_0: 3.996e-09, Loss_r: 9.511e-06, lambda_0 : 2.857e+02, Time: 0.42, Learning Rate: 0.00009\n",
            "It: 3100, Loss: 9.358e-06, Loss_0: 2.277e-10, Loss_r: 9.358e-06, lambda_0 : 1.035e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3110, Loss: 9.221e-06, Loss_0: 8.823e-10, Loss_r: 9.221e-06, lambda_0 : 4.840e+02, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3120, Loss: 9.085e-06, Loss_0: 1.720e-10, Loss_r: 9.085e-06, lambda_0 : 1.089e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3130, Loss: 8.951e-06, Loss_0: 1.030e-10, Loss_r: 8.951e-06, lambda_0 : 1.492e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3140, Loss: 8.818e-06, Loss_0: 1.094e-10, Loss_r: 8.818e-06, lambda_0 : 1.417e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3150, Loss: 8.686e-06, Loss_0: 9.095e-11, Loss_r: 8.686e-06, lambda_0 : 1.656e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3160, Loss: 8.556e-06, Loss_0: 8.505e-11, Loss_r: 8.556e-06, lambda_0 : 1.849e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3170, Loss: 8.427e-06, Loss_0: 8.400e-11, Loss_r: 8.427e-06, lambda_0 : 1.942e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3180, Loss: 8.300e-06, Loss_0: 8.377e-11, Loss_r: 8.300e-06, lambda_0 : 1.982e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3190, Loss: 8.174e-06, Loss_0: 8.374e-11, Loss_r: 8.174e-06, lambda_0 : 2.008e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3200, Loss: 8.050e-06, Loss_0: 8.404e-11, Loss_r: 8.050e-06, lambda_0 : 2.034e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3210, Loss: 7.927e-06, Loss_0: 8.421e-11, Loss_r: 7.927e-06, lambda_0 : 2.037e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3220, Loss: 7.805e-06, Loss_0: 8.383e-11, Loss_r: 7.805e-06, lambda_0 : 2.010e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3230, Loss: 7.685e-06, Loss_0: 8.351e-11, Loss_r: 7.685e-06, lambda_0 : 1.983e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3240, Loss: 7.567e-06, Loss_0: 8.349e-11, Loss_r: 7.567e-06, lambda_0 : 1.974e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3250, Loss: 7.449e-06, Loss_0: 8.330e-11, Loss_r: 7.449e-06, lambda_0 : 1.955e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3260, Loss: 7.334e-06, Loss_0: 8.317e-11, Loss_r: 7.334e-06, lambda_0 : 1.945e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3270, Loss: 7.219e-06, Loss_0: 8.300e-11, Loss_r: 7.219e-06, lambda_0 : 1.981e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3280, Loss: 7.106e-06, Loss_0: 8.292e-11, Loss_r: 7.106e-06, lambda_0 : 2.025e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3290, Loss: 6.995e-06, Loss_0: 8.279e-11, Loss_r: 6.995e-06, lambda_0 : 2.059e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3300, Loss: 6.885e-06, Loss_0: 8.265e-11, Loss_r: 6.885e-06, lambda_0 : 2.106e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3310, Loss: 6.776e-06, Loss_0: 8.252e-11, Loss_r: 6.776e-06, lambda_0 : 2.146e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3320, Loss: 6.669e-06, Loss_0: 8.240e-11, Loss_r: 6.669e-06, lambda_0 : 2.184e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3330, Loss: 6.563e-06, Loss_0: 8.230e-11, Loss_r: 6.563e-06, lambda_0 : 2.216e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3340, Loss: 6.459e-06, Loss_0: 8.218e-11, Loss_r: 6.459e-06, lambda_0 : 2.262e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3350, Loss: 6.356e-06, Loss_0: 8.207e-11, Loss_r: 6.356e-06, lambda_0 : 2.303e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3360, Loss: 6.255e-06, Loss_0: 8.195e-11, Loss_r: 6.255e-06, lambda_0 : 2.343e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3370, Loss: 6.155e-06, Loss_0: 8.185e-11, Loss_r: 6.155e-06, lambda_0 : 2.379e+03, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3380, Loss: 6.056e-06, Loss_0: 8.168e-11, Loss_r: 6.056e-06, lambda_0 : 2.411e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3390, Loss: 5.959e-06, Loss_0: 8.132e-11, Loss_r: 5.959e-06, lambda_0 : 2.414e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3400, Loss: 5.863e-06, Loss_0: 1.079e-10, Loss_r: 5.863e-06, lambda_0 : 1.583e+03, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3410, Loss: 5.905e-06, Loss_0: 1.358e-07, Loss_r: 5.769e-06, lambda_0 : 5.277e+01, Time: 0.43, Learning Rate: 0.00008\n",
            "It: 3420, Loss: 7.098e-05, Loss_0: 6.515e-05, Loss_r: 5.823e-06, lambda_0 : 2.005e+01, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3430, Loss: 7.690e-06, Loss_0: 1.420e-06, Loss_r: 6.270e-06, lambda_0 : 1.077e+02, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3440, Loss: 6.578e-06, Loss_0: 4.051e-08, Loss_r: 6.537e-06, lambda_0 : 3.147e+02, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3450, Loss: 7.654e-06, Loss_0: 1.035e-06, Loss_r: 6.619e-06, lambda_0 : 4.886e+01, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3460, Loss: 7.714e-06, Loss_0: 1.121e-06, Loss_r: 6.594e-06, lambda_0 : 1.926e+01, Time: 0.42, Learning Rate: 0.00007\n",
            "It: 3470, Loss: 6.539e-06, Loss_0: 1.364e-08, Loss_r: 6.525e-06, lambda_0 : 1.207e+02, Time: 0.42, Learning Rate: 0.00007\n",
            "It: 3480, Loss: 6.601e-06, Loss_0: 1.624e-07, Loss_r: 6.438e-06, lambda_0 : 3.906e+01, Time: 0.43, Learning Rate: 0.00007\n",
            "It: 3490, Loss: 6.350e-06, Loss_0: 4.225e-09, Loss_r: 6.346e-06, lambda_0 : 1.918e+02, Time: 0.42, Learning Rate: 0.00007\n",
            "It: 3500, Loss: 6.265e-06, Loss_0: 1.367e-08, Loss_r: 6.252e-06, lambda_0 : 1.132e+02, Time: 0.42, Learning Rate: 0.00007\n",
            "It: 3510, Loss: 6.163e-06, Loss_0: 6.387e-09, Loss_r: 6.157e-06, lambda_0 : 1.725e+02, Time: 0.42, Learning Rate: 0.00007\n",
            "It: 3520, Loss: 6.063e-06, Loss_0: 4.966e-10, Loss_r: 6.062e-06, lambda_0 : 5.796e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3530, Loss: 5.979e-06, Loss_0: 8.482e-10, Loss_r: 5.979e-06, lambda_0 : 3.915e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3540, Loss: 5.896e-06, Loss_0: 8.476e-11, Loss_r: 5.896e-06, lambda_0 : 1.593e+03, Time: 0.44, Learning Rate: 0.00006\n",
            "It: 3550, Loss: 5.814e-06, Loss_0: 1.504e-10, Loss_r: 5.814e-06, lambda_0 : 9.940e+02, Time: 0.53, Learning Rate: 0.00006\n",
            "It: 3560, Loss: 5.733e-06, Loss_0: 8.103e-11, Loss_r: 5.733e-06, lambda_0 : 1.530e+03, Time: 0.50, Learning Rate: 0.00006\n",
            "It: 3570, Loss: 5.653e-06, Loss_0: 7.919e-11, Loss_r: 5.653e-06, lambda_0 : 1.919e+03, Time: 0.48, Learning Rate: 0.00006\n",
            "It: 3580, Loss: 5.574e-06, Loss_0: 8.170e-11, Loss_r: 5.574e-06, lambda_0 : 1.817e+03, Time: 0.53, Learning Rate: 0.00006\n",
            "It: 3590, Loss: 5.497e-06, Loss_0: 7.787e-11, Loss_r: 5.496e-06, lambda_0 : 2.070e+03, Time: 0.53, Learning Rate: 0.00006\n",
            "It: 3600, Loss: 5.420e-06, Loss_0: 7.582e-11, Loss_r: 5.420e-06, lambda_0 : 2.231e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3610, Loss: 5.344e-06, Loss_0: 7.516e-11, Loss_r: 5.344e-06, lambda_0 : 2.246e+03, Time: 0.47, Learning Rate: 0.00006\n",
            "It: 3620, Loss: 5.269e-06, Loss_0: 7.495e-11, Loss_r: 5.269e-06, lambda_0 : 2.280e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3630, Loss: 5.195e-06, Loss_0: 7.487e-11, Loss_r: 5.195e-06, lambda_0 : 2.316e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3640, Loss: 5.122e-06, Loss_0: 7.480e-11, Loss_r: 5.122e-06, lambda_0 : 2.344e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3650, Loss: 5.050e-06, Loss_0: 7.471e-11, Loss_r: 5.050e-06, lambda_0 : 2.376e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3660, Loss: 4.979e-06, Loss_0: 7.461e-11, Loss_r: 4.979e-06, lambda_0 : 2.413e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3670, Loss: 4.908e-06, Loss_0: 7.455e-11, Loss_r: 4.908e-06, lambda_0 : 2.440e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3680, Loss: 4.839e-06, Loss_0: 7.452e-11, Loss_r: 4.839e-06, lambda_0 : 2.484e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3690, Loss: 4.771e-06, Loss_0: 7.451e-11, Loss_r: 4.771e-06, lambda_0 : 2.521e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3700, Loss: 4.704e-06, Loss_0: 7.447e-11, Loss_r: 4.704e-06, lambda_0 : 2.555e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3710, Loss: 4.637e-06, Loss_0: 7.440e-11, Loss_r: 4.637e-06, lambda_0 : 2.590e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3720, Loss: 4.572e-06, Loss_0: 7.436e-11, Loss_r: 4.572e-06, lambda_0 : 2.627e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3730, Loss: 4.507e-06, Loss_0: 7.432e-11, Loss_r: 4.507e-06, lambda_0 : 2.656e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3740, Loss: 4.444e-06, Loss_0: 7.425e-11, Loss_r: 4.444e-06, lambda_0 : 2.694e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3750, Loss: 4.381e-06, Loss_0: 7.418e-11, Loss_r: 4.381e-06, lambda_0 : 2.723e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3760, Loss: 4.319e-06, Loss_0: 7.412e-11, Loss_r: 4.319e-06, lambda_0 : 2.750e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3770, Loss: 4.259e-06, Loss_0: 7.406e-11, Loss_r: 4.259e-06, lambda_0 : 2.768e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3780, Loss: 4.199e-06, Loss_0: 7.398e-11, Loss_r: 4.199e-06, lambda_0 : 2.807e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3790, Loss: 4.140e-06, Loss_0: 7.389e-11, Loss_r: 4.139e-06, lambda_0 : 2.846e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3800, Loss: 4.081e-06, Loss_0: 7.383e-11, Loss_r: 4.081e-06, lambda_0 : 2.867e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3810, Loss: 4.024e-06, Loss_0: 7.373e-11, Loss_r: 4.024e-06, lambda_0 : 2.902e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3820, Loss: 3.968e-06, Loss_0: 7.363e-11, Loss_r: 3.968e-06, lambda_0 : 2.927e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3830, Loss: 3.912e-06, Loss_0: 7.353e-11, Loss_r: 3.912e-06, lambda_0 : 2.961e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3840, Loss: 3.857e-06, Loss_0: 7.342e-11, Loss_r: 3.857e-06, lambda_0 : 2.987e+03, Time: 0.44, Learning Rate: 0.00006\n",
            "It: 3850, Loss: 3.804e-06, Loss_0: 7.330e-11, Loss_r: 3.803e-06, lambda_0 : 3.020e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3860, Loss: 3.751e-06, Loss_0: 7.319e-11, Loss_r: 3.750e-06, lambda_0 : 3.035e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3870, Loss: 3.698e-06, Loss_0: 7.307e-11, Loss_r: 3.698e-06, lambda_0 : 3.073e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3880, Loss: 3.647e-06, Loss_0: 7.294e-11, Loss_r: 3.647e-06, lambda_0 : 3.068e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3890, Loss: 3.597e-06, Loss_0: 7.281e-11, Loss_r: 3.597e-06, lambda_0 : 3.101e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3900, Loss: 3.547e-06, Loss_0: 7.267e-11, Loss_r: 3.547e-06, lambda_0 : 3.113e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3910, Loss: 3.498e-06, Loss_0: 7.243e-11, Loss_r: 3.498e-06, lambda_0 : 3.089e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3920, Loss: 3.450e-06, Loss_0: 7.449e-11, Loss_r: 3.450e-06, lambda_0 : 2.597e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3930, Loss: 3.410e-06, Loss_0: 7.614e-09, Loss_r: 3.403e-06, lambda_0 : 2.564e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3940, Loss: 4.124e-05, Loss_0: 3.787e-05, Loss_r: 3.372e-06, lambda_0 : 1.810e+01, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3950, Loss: 4.810e-05, Loss_0: 4.430e-05, Loss_r: 3.801e-06, lambda_0 : 1.051e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3960, Loss: 3.912e-06, Loss_0: 9.628e-08, Loss_r: 3.816e-06, lambda_0 : 1.241e+03, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3970, Loss: 6.630e-06, Loss_0: 2.813e-06, Loss_r: 3.818e-06, lambda_0 : 8.775e+01, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3980, Loss: 5.396e-06, Loss_0: 1.583e-06, Loss_r: 3.813e-06, lambda_0 : 1.914e+01, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 3990, Loss: 4.189e-06, Loss_0: 4.040e-07, Loss_r: 3.785e-06, lambda_0 : 6.054e+01, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4000, Loss: 3.922e-06, Loss_0: 1.765e-07, Loss_r: 3.745e-06, lambda_0 : 1.299e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4010, Loss: 3.736e-06, Loss_0: 3.625e-08, Loss_r: 3.700e-06, lambda_0 : 4.636e+01, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4020, Loss: 3.658e-06, Loss_0: 4.982e-09, Loss_r: 3.653e-06, lambda_0 : 1.485e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4030, Loss: 3.607e-06, Loss_0: 7.664e-10, Loss_r: 3.606e-06, lambda_0 : 6.538e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4040, Loss: 3.560e-06, Loss_0: 3.200e-10, Loss_r: 3.559e-06, lambda_0 : 8.894e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4050, Loss: 3.513e-06, Loss_0: 3.007e-10, Loss_r: 3.513e-06, lambda_0 : 6.371e+02, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4060, Loss: 3.472e-06, Loss_0: 1.220e-10, Loss_r: 3.472e-06, lambda_0 : 1.329e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4070, Loss: 3.432e-06, Loss_0: 1.530e-10, Loss_r: 3.432e-06, lambda_0 : 1.027e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4080, Loss: 3.392e-06, Loss_0: 8.619e-11, Loss_r: 3.392e-06, lambda_0 : 1.591e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4090, Loss: 3.353e-06, Loss_0: 6.560e-11, Loss_r: 3.352e-06, lambda_0 : 2.089e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4100, Loss: 3.314e-06, Loss_0: 6.454e-11, Loss_r: 3.314e-06, lambda_0 : 2.515e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4110, Loss: 3.275e-06, Loss_0: 6.505e-11, Loss_r: 3.275e-06, lambda_0 : 2.544e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4120, Loss: 3.238e-06, Loss_0: 6.502e-11, Loss_r: 3.238e-06, lambda_0 : 2.594e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4130, Loss: 3.200e-06, Loss_0: 6.486e-11, Loss_r: 3.200e-06, lambda_0 : 2.596e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4140, Loss: 3.164e-06, Loss_0: 6.472e-11, Loss_r: 3.164e-06, lambda_0 : 2.620e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4150, Loss: 3.128e-06, Loss_0: 6.464e-11, Loss_r: 3.127e-06, lambda_0 : 2.628e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4160, Loss: 3.092e-06, Loss_0: 6.460e-11, Loss_r: 3.092e-06, lambda_0 : 2.664e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4170, Loss: 3.057e-06, Loss_0: 6.462e-11, Loss_r: 3.057e-06, lambda_0 : 2.684e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4180, Loss: 3.022e-06, Loss_0: 6.464e-11, Loss_r: 3.022e-06, lambda_0 : 2.705e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4190, Loss: 2.988e-06, Loss_0: 6.460e-11, Loss_r: 2.988e-06, lambda_0 : 2.738e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4200, Loss: 2.954e-06, Loss_0: 6.453e-11, Loss_r: 2.954e-06, lambda_0 : 2.771e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4210, Loss: 2.921e-06, Loss_0: 6.447e-11, Loss_r: 2.921e-06, lambda_0 : 2.788e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4220, Loss: 2.888e-06, Loss_0: 6.440e-11, Loss_r: 2.888e-06, lambda_0 : 2.807e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4230, Loss: 2.856e-06, Loss_0: 6.433e-11, Loss_r: 2.856e-06, lambda_0 : 2.822e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4240, Loss: 2.825e-06, Loss_0: 6.427e-11, Loss_r: 2.824e-06, lambda_0 : 2.844e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4250, Loss: 2.793e-06, Loss_0: 6.418e-11, Loss_r: 2.793e-06, lambda_0 : 2.858e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4260, Loss: 2.763e-06, Loss_0: 6.410e-11, Loss_r: 2.762e-06, lambda_0 : 2.884e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4270, Loss: 2.732e-06, Loss_0: 6.401e-11, Loss_r: 2.732e-06, lambda_0 : 2.907e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4280, Loss: 2.702e-06, Loss_0: 6.392e-11, Loss_r: 2.702e-06, lambda_0 : 2.917e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4290, Loss: 2.673e-06, Loss_0: 6.383e-11, Loss_r: 2.673e-06, lambda_0 : 2.931e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4300, Loss: 2.644e-06, Loss_0: 6.371e-11, Loss_r: 2.644e-06, lambda_0 : 2.948e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4310, Loss: 2.616e-06, Loss_0: 6.361e-11, Loss_r: 2.616e-06, lambda_0 : 2.961e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4320, Loss: 2.588e-06, Loss_0: 6.351e-11, Loss_r: 2.588e-06, lambda_0 : 2.969e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4330, Loss: 2.560e-06, Loss_0: 6.340e-11, Loss_r: 2.560e-06, lambda_0 : 2.985e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4340, Loss: 2.533e-06, Loss_0: 6.326e-11, Loss_r: 2.533e-06, lambda_0 : 2.992e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4350, Loss: 2.506e-06, Loss_0: 6.314e-11, Loss_r: 2.506e-06, lambda_0 : 3.008e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4360, Loss: 2.480e-06, Loss_0: 6.304e-11, Loss_r: 2.480e-06, lambda_0 : 3.025e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4370, Loss: 2.454e-06, Loss_0: 6.292e-11, Loss_r: 2.454e-06, lambda_0 : 3.025e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4380, Loss: 2.429e-06, Loss_0: 6.278e-11, Loss_r: 2.429e-06, lambda_0 : 3.037e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4390, Loss: 2.404e-06, Loss_0: 6.263e-11, Loss_r: 2.404e-06, lambda_0 : 3.039e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4400, Loss: 2.379e-06, Loss_0: 6.250e-11, Loss_r: 2.379e-06, lambda_0 : 3.049e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4410, Loss: 2.355e-06, Loss_0: 6.236e-11, Loss_r: 2.355e-06, lambda_0 : 3.047e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4420, Loss: 2.331e-06, Loss_0: 6.220e-11, Loss_r: 2.331e-06, lambda_0 : 3.070e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4430, Loss: 2.308e-06, Loss_0: 6.205e-11, Loss_r: 2.308e-06, lambda_0 : 3.067e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4440, Loss: 2.285e-06, Loss_0: 6.193e-11, Loss_r: 2.285e-06, lambda_0 : 3.079e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4450, Loss: 2.262e-06, Loss_0: 6.173e-11, Loss_r: 2.262e-06, lambda_0 : 3.110e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4460, Loss: 2.240e-06, Loss_0: 6.158e-11, Loss_r: 2.240e-06, lambda_0 : 3.079e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4470, Loss: 2.218e-06, Loss_0: 6.144e-11, Loss_r: 2.218e-06, lambda_0 : 3.081e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4480, Loss: 2.197e-06, Loss_0: 6.127e-11, Loss_r: 2.197e-06, lambda_0 : 3.092e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4490, Loss: 2.176e-06, Loss_0: 6.109e-11, Loss_r: 2.176e-06, lambda_0 : 3.093e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4500, Loss: 2.155e-06, Loss_0: 6.096e-11, Loss_r: 2.155e-06, lambda_0 : 3.106e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4510, Loss: 2.135e-06, Loss_0: 6.111e-11, Loss_r: 2.134e-06, lambda_0 : 3.092e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4520, Loss: 2.115e-06, Loss_0: 8.310e-11, Loss_r: 2.114e-06, lambda_0 : 1.545e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4530, Loss: 2.113e-06, Loss_0: 1.518e-08, Loss_r: 2.098e-06, lambda_0 : 5.256e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4540, Loss: 3.174e-05, Loss_0: 2.904e-05, Loss_r: 2.700e-06, lambda_0 : 1.552e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4550, Loss: 1.869e-05, Loss_0: 1.640e-05, Loss_r: 2.289e-06, lambda_0 : 9.952e+01, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4560, Loss: 8.281e-06, Loss_0: 6.008e-06, Loss_r: 2.274e-06, lambda_0 : 1.197e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4570, Loss: 5.068e-06, Loss_0: 2.822e-06, Loss_r: 2.247e-06, lambda_0 : 1.299e+01, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4580, Loss: 2.405e-06, Loss_0: 1.493e-07, Loss_r: 2.255e-06, lambda_0 : 3.044e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4590, Loss: 2.331e-06, Loss_0: 9.326e-08, Loss_r: 2.238e-06, lambda_0 : 2.318e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4600, Loss: 2.220e-06, Loss_0: 1.201e-10, Loss_r: 2.220e-06, lambda_0 : 2.428e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4610, Loss: 2.219e-06, Loss_0: 1.813e-08, Loss_r: 2.201e-06, lambda_0 : 2.201e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4620, Loss: 2.194e-06, Loss_0: 1.218e-08, Loss_r: 2.182e-06, lambda_0 : 9.059e+01, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4630, Loss: 2.163e-06, Loss_0: 5.477e-10, Loss_r: 2.162e-06, lambda_0 : 7.582e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4640, Loss: 2.144e-06, Loss_0: 9.728e-10, Loss_r: 2.143e-06, lambda_0 : 2.582e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4650, Loss: 2.125e-06, Loss_0: 3.401e-10, Loss_r: 2.125e-06, lambda_0 : 8.008e+02, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4660, Loss: 2.108e-06, Loss_0: 1.536e-10, Loss_r: 2.108e-06, lambda_0 : 8.976e+02, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4670, Loss: 2.091e-06, Loss_0: 8.065e-11, Loss_r: 2.091e-06, lambda_0 : 1.277e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4680, Loss: 2.075e-06, Loss_0: 5.789e-11, Loss_r: 2.075e-06, lambda_0 : 1.910e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4690, Loss: 2.059e-06, Loss_0: 5.396e-11, Loss_r: 2.059e-06, lambda_0 : 2.300e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4700, Loss: 2.043e-06, Loss_0: 5.507e-11, Loss_r: 2.043e-06, lambda_0 : 2.429e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4710, Loss: 2.028e-06, Loss_0: 5.574e-11, Loss_r: 2.028e-06, lambda_0 : 2.438e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4720, Loss: 2.012e-06, Loss_0: 5.512e-11, Loss_r: 2.012e-06, lambda_0 : 2.501e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4730, Loss: 1.997e-06, Loss_0: 5.429e-11, Loss_r: 1.997e-06, lambda_0 : 2.459e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4740, Loss: 1.982e-06, Loss_0: 5.393e-11, Loss_r: 1.982e-06, lambda_0 : 2.450e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4750, Loss: 1.968e-06, Loss_0: 5.389e-11, Loss_r: 1.968e-06, lambda_0 : 2.452e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4760, Loss: 1.953e-06, Loss_0: 5.395e-11, Loss_r: 1.953e-06, lambda_0 : 2.486e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4770, Loss: 1.939e-06, Loss_0: 5.393e-11, Loss_r: 1.939e-06, lambda_0 : 2.499e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4780, Loss: 1.925e-06, Loss_0: 5.380e-11, Loss_r: 1.925e-06, lambda_0 : 2.501e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4790, Loss: 1.911e-06, Loss_0: 5.372e-11, Loss_r: 1.911e-06, lambda_0 : 2.508e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4800, Loss: 1.898e-06, Loss_0: 5.366e-11, Loss_r: 1.898e-06, lambda_0 : 2.527e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4810, Loss: 1.884e-06, Loss_0: 5.359e-11, Loss_r: 1.884e-06, lambda_0 : 2.534e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4820, Loss: 1.871e-06, Loss_0: 5.350e-11, Loss_r: 1.871e-06, lambda_0 : 2.548e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4830, Loss: 1.858e-06, Loss_0: 5.342e-11, Loss_r: 1.858e-06, lambda_0 : 2.545e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4840, Loss: 1.845e-06, Loss_0: 5.334e-11, Loss_r: 1.845e-06, lambda_0 : 2.559e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4850, Loss: 1.833e-06, Loss_0: 5.325e-11, Loss_r: 1.833e-06, lambda_0 : 2.560e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4860, Loss: 1.820e-06, Loss_0: 5.315e-11, Loss_r: 1.820e-06, lambda_0 : 2.577e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4870, Loss: 1.808e-06, Loss_0: 5.305e-11, Loss_r: 1.808e-06, lambda_0 : 2.570e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4880, Loss: 1.796e-06, Loss_0: 5.294e-11, Loss_r: 1.796e-06, lambda_0 : 2.588e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4890, Loss: 1.784e-06, Loss_0: 5.285e-11, Loss_r: 1.784e-06, lambda_0 : 2.576e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4900, Loss: 1.772e-06, Loss_0: 5.275e-11, Loss_r: 1.772e-06, lambda_0 : 2.581e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4910, Loss: 1.761e-06, Loss_0: 5.265e-11, Loss_r: 1.761e-06, lambda_0 : 2.592e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4920, Loss: 1.750e-06, Loss_0: 5.254e-11, Loss_r: 1.750e-06, lambda_0 : 2.596e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4930, Loss: 1.739e-06, Loss_0: 5.243e-11, Loss_r: 1.738e-06, lambda_0 : 2.593e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4940, Loss: 1.728e-06, Loss_0: 5.232e-11, Loss_r: 1.728e-06, lambda_0 : 2.591e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4950, Loss: 1.717e-06, Loss_0: 5.220e-11, Loss_r: 1.717e-06, lambda_0 : 2.595e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4960, Loss: 1.706e-06, Loss_0: 5.209e-11, Loss_r: 1.706e-06, lambda_0 : 2.602e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4970, Loss: 1.696e-06, Loss_0: 5.196e-11, Loss_r: 1.696e-06, lambda_0 : 2.595e+03, Time: 0.42, Learning Rate: 0.00004\n",
            "It: 4980, Loss: 1.686e-06, Loss_0: 5.184e-11, Loss_r: 1.685e-06, lambda_0 : 2.591e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "It: 4990, Loss: 1.675e-06, Loss_0: 5.172e-11, Loss_r: 1.675e-06, lambda_0 : 2.594e+03, Time: 0.43, Learning Rate: 0.00004\n",
            "Training time: 215.3949\n",
            "[1, 128, 128, 64, 64, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 2.185e-03, Loss_0: 1.504e-03, Loss_r: 6.811e-04, lambda_0 : 2.715e+01, Time: 1.40, Learning Rate: 0.00100\n",
            "It: 10, Loss: 7.987e-04, Loss_0: 7.140e-04, Loss_r: 8.469e-05, lambda_0 : 5.982e+00, Time: 0.34, Learning Rate: 0.00100\n",
            "It: 20, Loss: 9.964e-05, Loss_0: 1.358e-05, Loss_r: 8.606e-05, lambda_0 : 2.524e+01, Time: 0.30, Learning Rate: 0.00100\n",
            "It: 30, Loss: 1.109e-04, Loss_0: 4.961e-05, Loss_r: 6.124e-05, lambda_0 : 9.461e-01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.932e-05, Loss_0: 5.836e-06, Loss_r: 6.349e-05, lambda_0 : 1.616e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.694e-05, Loss_0: 7.473e-06, Loss_r: 5.946e-05, lambda_0 : 5.886e+00, Time: 0.30, Learning Rate: 0.00100\n",
            "It: 60, Loss: 5.924e-05, Loss_0: 1.775e-07, Loss_r: 5.906e-05, lambda_0 : 1.308e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.952e-05, Loss_0: 2.292e-07, Loss_r: 5.929e-05, lambda_0 : 2.010e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.899e-05, Loss_0: 8.061e-08, Loss_r: 5.890e-05, lambda_0 : 6.846e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.877e-05, Loss_0: 1.267e-08, Loss_r: 5.876e-05, lambda_0 : 1.362e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.865e-05, Loss_0: 1.604e-08, Loss_r: 5.863e-05, lambda_0 : 2.585e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.856e-05, Loss_0: 5.283e-08, Loss_r: 5.851e-05, lambda_0 : 7.579e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.844e-05, Loss_0: 2.027e-08, Loss_r: 5.842e-05, lambda_0 : 1.697e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.833e-05, Loss_0: 3.996e-09, Loss_r: 5.832e-05, lambda_0 : 3.885e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.821e-05, Loss_0: 8.463e-09, Loss_r: 5.820e-05, lambda_0 : 2.403e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.809e-05, Loss_0: 1.396e-08, Loss_r: 5.807e-05, lambda_0 : 2.102e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.796e-05, Loss_0: 9.411e-09, Loss_r: 5.795e-05, lambda_0 : 2.515e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.782e-05, Loss_0: 9.765e-09, Loss_r: 5.781e-05, lambda_0 : 2.487e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.767e-05, Loss_0: 1.099e-08, Loss_r: 5.766e-05, lambda_0 : 2.277e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.751e-05, Loss_0: 9.901e-09, Loss_r: 5.750e-05, lambda_0 : 2.423e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.735e-05, Loss_0: 1.155e-08, Loss_r: 5.734e-05, lambda_0 : 1.931e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 210, Loss: 9.080e-05, Loss_0: 3.368e-05, Loss_r: 5.712e-05, lambda_0 : 3.528e-01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 220, Loss: 6.350e-05, Loss_0: 6.324e-06, Loss_r: 5.717e-05, lambda_0 : 9.256e-01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 230, Loss: 6.066e-05, Loss_0: 3.481e-06, Loss_r: 5.718e-05, lambda_0 : 1.295e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.721e-05, Loss_0: 9.083e-08, Loss_r: 5.712e-05, lambda_0 : 7.026e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.752e-05, Loss_0: 5.128e-07, Loss_r: 5.700e-05, lambda_0 : 3.221e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 260, Loss: 5.687e-05, Loss_0: 2.274e-08, Loss_r: 5.685e-05, lambda_0 : 1.432e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 270, Loss: 5.669e-05, Loss_0: 1.623e-08, Loss_r: 5.668e-05, lambda_0 : 1.643e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 280, Loss: 5.653e-05, Loss_0: 4.085e-08, Loss_r: 5.649e-05, lambda_0 : 9.623e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.630e-05, Loss_0: 2.266e-08, Loss_r: 5.627e-05, lambda_0 : 1.277e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.606e-05, Loss_0: 1.948e-08, Loss_r: 5.604e-05, lambda_0 : 1.345e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.579e-05, Loss_0: 1.707e-08, Loss_r: 5.577e-05, lambda_0 : 1.465e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 320, Loss: 7.161e-05, Loss_0: 1.611e-05, Loss_r: 5.550e-05, lambda_0 : 5.372e-01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.565e-05, Loss_0: 2.332e-07, Loss_r: 5.542e-05, lambda_0 : 4.420e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.651e-05, Loss_0: 1.209e-06, Loss_r: 5.530e-05, lambda_0 : 1.976e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.505e-05, Loss_0: 2.365e-08, Loss_r: 5.503e-05, lambda_0 : 1.341e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.492e-05, Loss_0: 2.527e-07, Loss_r: 5.467e-05, lambda_0 : 4.119e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.436e-05, Loss_0: 1.706e-07, Loss_r: 5.419e-05, lambda_0 : 5.318e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.363e-05, Loss_0: 1.189e-08, Loss_r: 5.362e-05, lambda_0 : 2.095e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 390, Loss: 2.014e-04, Loss_0: 1.483e-04, Loss_r: 5.308e-05, lambda_0 : 4.681e-01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.409e-05, Loss_0: 2.018e-07, Loss_r: 5.389e-05, lambda_0 : 1.246e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.909e-05, Loss_0: 4.982e-06, Loss_r: 5.411e-05, lambda_0 : 2.348e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.373e-05, Loss_0: 1.913e-08, Loss_r: 5.372e-05, lambda_0 : 2.196e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 430, Loss: 5.311e-05, Loss_0: 1.248e-07, Loss_r: 5.298e-05, lambda_0 : 1.195e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 440, Loss: 5.235e-05, Loss_0: 3.546e-07, Loss_r: 5.199e-05, lambda_0 : 8.233e+00, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 450, Loss: 5.089e-05, Loss_0: 3.710e-08, Loss_r: 5.085e-05, lambda_0 : 1.983e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.977e-05, Loss_0: 3.959e-08, Loss_r: 4.973e-05, lambda_0 : 1.995e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.884e-05, Loss_0: 4.132e-09, Loss_r: 4.883e-05, lambda_0 : 6.745e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 480, Loss: 4.825e-05, Loss_0: 1.126e-08, Loss_r: 4.824e-05, lambda_0 : 4.242e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 490, Loss: 4.782e-05, Loss_0: 3.052e-09, Loss_r: 4.781e-05, lambda_0 : 8.993e+01, Time: 0.31, Learning Rate: 0.00100\n",
            "It: 500, Loss: 4.741e-05, Loss_0: 2.258e-09, Loss_r: 4.740e-05, lambda_0 : 1.286e+02, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 510, Loss: 4.701e-05, Loss_0: 4.186e-08, Loss_r: 4.697e-05, lambda_0 : 2.414e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 520, Loss: 4.066e-04, Loss_0: 3.589e-04, Loss_r: 4.762e-05, lambda_0 : 4.804e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 530, Loss: 8.646e-05, Loss_0: 3.399e-05, Loss_r: 5.248e-05, lambda_0 : 6.234e+00, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 540, Loss: 6.106e-05, Loss_0: 7.424e-06, Loss_r: 5.364e-05, lambda_0 : 1.808e+01, Time: 0.33, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.882e-05, Loss_0: 5.442e-06, Loss_r: 5.338e-05, lambda_0 : 3.382e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 560, Loss: 5.506e-05, Loss_0: 2.254e-06, Loss_r: 5.281e-05, lambda_0 : 3.560e+01, Time: 0.32, Learning Rate: 0.00100\n",
            "It: 570, Loss: 5.327e-05, Loss_0: 6.813e-07, Loss_r: 5.258e-05, lambda_0 : 5.452e+01, Time: 0.31, Learning Rate: 0.00090\n",
            "It: 580, Loss: 5.264e-05, Loss_0: 2.313e-07, Loss_r: 5.241e-05, lambda_0 : 1.132e+02, Time: 0.31, Learning Rate: 0.00090\n",
            "It: 590, Loss: 5.227e-05, Loss_0: 3.825e-08, Loss_r: 5.223e-05, lambda_0 : 9.243e+01, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 600, Loss: 5.207e-05, Loss_0: 2.136e-08, Loss_r: 5.205e-05, lambda_0 : 1.178e+02, Time: 0.31, Learning Rate: 0.00090\n",
            "It: 610, Loss: 5.190e-05, Loss_0: 2.933e-09, Loss_r: 5.189e-05, lambda_0 : 3.585e+02, Time: 0.31, Learning Rate: 0.00090\n",
            "It: 620, Loss: 5.173e-05, Loss_0: 6.199e-10, Loss_r: 5.173e-05, lambda_0 : 5.703e+02, Time: 0.32, Learning Rate: 0.00090\n",
            "It: 630, Loss: 5.155e-05, Loss_0: 2.485e-09, Loss_r: 5.155e-05, lambda_0 : 3.158e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 640, Loss: 5.140e-05, Loss_0: 3.246e-09, Loss_r: 5.139e-05, lambda_0 : 2.704e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 650, Loss: 5.123e-05, Loss_0: 2.393e-09, Loss_r: 5.123e-05, lambda_0 : 3.327e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 660, Loss: 5.107e-05, Loss_0: 2.050e-09, Loss_r: 5.107e-05, lambda_0 : 3.535e+02, Time: 0.32, Learning Rate: 0.00081\n",
            "It: 670, Loss: 5.090e-05, Loss_0: 2.497e-09, Loss_r: 5.090e-05, lambda_0 : 3.533e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 680, Loss: 5.073e-05, Loss_0: 2.263e-09, Loss_r: 5.073e-05, lambda_0 : 3.492e+02, Time: 0.31, Learning Rate: 0.00081\n",
            "It: 690, Loss: 5.055e-05, Loss_0: 2.186e-09, Loss_r: 5.055e-05, lambda_0 : 3.714e+02, Time: 0.31, Learning Rate: 0.00073\n",
            "It: 700, Loss: 5.039e-05, Loss_0: 2.224e-09, Loss_r: 5.039e-05, lambda_0 : 3.548e+02, Time: 0.31, Learning Rate: 0.00073\n",
            "It: 710, Loss: 5.022e-05, Loss_0: 2.200e-09, Loss_r: 5.022e-05, lambda_0 : 3.590e+02, Time: 0.31, Learning Rate: 0.00073\n",
            "It: 720, Loss: 5.004e-05, Loss_0: 2.165e-09, Loss_r: 5.004e-05, lambda_0 : 3.581e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 730, Loss: 4.986e-05, Loss_0: 2.151e-09, Loss_r: 4.986e-05, lambda_0 : 3.554e+02, Time: 0.31, Learning Rate: 0.00073\n",
            "It: 740, Loss: 4.968e-05, Loss_0: 2.133e-09, Loss_r: 4.968e-05, lambda_0 : 3.557e+02, Time: 0.32, Learning Rate: 0.00073\n",
            "It: 750, Loss: 4.948e-05, Loss_0: 2.111e-09, Loss_r: 4.948e-05, lambda_0 : 3.553e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 760, Loss: 4.930e-05, Loss_0: 2.091e-09, Loss_r: 4.930e-05, lambda_0 : 3.546e+02, Time: 0.31, Learning Rate: 0.00066\n",
            "It: 770, Loss: 4.912e-05, Loss_0: 2.072e-09, Loss_r: 4.911e-05, lambda_0 : 3.542e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 780, Loss: 4.892e-05, Loss_0: 2.051e-09, Loss_r: 4.892e-05, lambda_0 : 3.541e+02, Time: 0.32, Learning Rate: 0.00066\n",
            "It: 790, Loss: 4.871e-05, Loss_0: 2.031e-09, Loss_r: 4.871e-05, lambda_0 : 3.541e+02, Time: 0.31, Learning Rate: 0.00066\n",
            "It: 800, Loss: 4.850e-05, Loss_0: 2.012e-09, Loss_r: 4.850e-05, lambda_0 : 3.539e+02, Time: 0.31, Learning Rate: 0.00066\n",
            "It: 810, Loss: 4.827e-05, Loss_0: 1.991e-09, Loss_r: 4.827e-05, lambda_0 : 3.538e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 820, Loss: 4.806e-05, Loss_0: 1.972e-09, Loss_r: 4.805e-05, lambda_0 : 3.537e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 830, Loss: 4.783e-05, Loss_0: 1.953e-09, Loss_r: 4.783e-05, lambda_0 : 3.537e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 840, Loss: 4.759e-05, Loss_0: 1.934e-09, Loss_r: 4.758e-05, lambda_0 : 3.538e+02, Time: 0.32, Learning Rate: 0.00059\n",
            "It: 850, Loss: 4.733e-05, Loss_0: 1.916e-09, Loss_r: 4.733e-05, lambda_0 : 3.537e+02, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 860, Loss: 4.706e-05, Loss_0: 1.897e-09, Loss_r: 4.706e-05, lambda_0 : 3.534e+02, Time: 0.31, Learning Rate: 0.00059\n",
            "It: 870, Loss: 4.676e-05, Loss_0: 1.879e-09, Loss_r: 4.676e-05, lambda_0 : 3.531e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 880, Loss: 4.648e-05, Loss_0: 1.863e-09, Loss_r: 4.648e-05, lambda_0 : 3.525e+02, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 890, Loss: 4.618e-05, Loss_0: 1.847e-09, Loss_r: 4.618e-05, lambda_0 : 3.519e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 900, Loss: 4.585e-05, Loss_0: 1.831e-09, Loss_r: 4.585e-05, lambda_0 : 3.511e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 910, Loss: 4.550e-05, Loss_0: 1.817e-09, Loss_r: 4.550e-05, lambda_0 : 3.500e+02, Time: 0.33, Learning Rate: 0.00053\n",
            "It: 920, Loss: 4.511e-05, Loss_0: 1.803e-09, Loss_r: 4.511e-05, lambda_0 : 3.484e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 930, Loss: 4.470e-05, Loss_0: 1.790e-09, Loss_r: 4.469e-05, lambda_0 : 3.466e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 940, Loss: 4.424e-05, Loss_0: 1.777e-09, Loss_r: 4.424e-05, lambda_0 : 3.444e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 950, Loss: 4.375e-05, Loss_0: 1.763e-09, Loss_r: 4.374e-05, lambda_0 : 3.421e+02, Time: 0.33, Learning Rate: 0.00053\n",
            "It: 960, Loss: 4.320e-05, Loss_0: 1.748e-09, Loss_r: 4.320e-05, lambda_0 : 3.391e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 970, Loss: 4.261e-05, Loss_0: 1.731e-09, Loss_r: 4.261e-05, lambda_0 : 3.372e+02, Time: 0.32, Learning Rate: 0.00053\n",
            "It: 980, Loss: 4.208e-05, Loss_0: 1.117e-07, Loss_r: 4.197e-05, lambda_0 : 3.432e+01, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 990, Loss: 2.789e-04, Loss_0: 2.338e-04, Loss_r: 4.505e-05, lambda_0 : 1.590e+00, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1000, Loss: 7.651e-05, Loss_0: 2.626e-05, Loss_r: 5.025e-05, lambda_0 : 4.261e+00, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1010, Loss: 6.415e-05, Loss_0: 1.257e-05, Loss_r: 5.158e-05, lambda_0 : 2.785e+01, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1020, Loss: 5.581e-05, Loss_0: 4.121e-06, Loss_r: 5.169e-05, lambda_0 : 1.535e+01, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1030, Loss: 5.188e-05, Loss_0: 4.053e-07, Loss_r: 5.147e-05, lambda_0 : 1.212e+02, Time: 0.31, Learning Rate: 0.00053\n",
            "It: 1040, Loss: 5.130e-05, Loss_0: 7.173e-09, Loss_r: 5.129e-05, lambda_0 : 2.809e+02, Time: 0.32, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 5.118e-05, Loss_0: 2.627e-08, Loss_r: 5.115e-05, lambda_0 : 1.259e+02, Time: 0.31, Learning Rate: 0.00048\n",
            "It: 1060, Loss: 5.105e-05, Loss_0: 2.267e-08, Loss_r: 5.102e-05, lambda_0 : 1.505e+02, Time: 0.31, Learning Rate: 0.00048\n",
            "It: 1070, Loss: 5.091e-05, Loss_0: 1.656e-08, Loss_r: 5.089e-05, lambda_0 : 1.726e+02, Time: 0.31, Learning Rate: 0.00048\n",
            "It: 1080, Loss: 5.076e-05, Loss_0: 9.653e-09, Loss_r: 5.075e-05, lambda_0 : 2.173e+02, Time: 0.31, Learning Rate: 0.00048\n",
            "It: 1090, Loss: 5.061e-05, Loss_0: 5.233e-09, Loss_r: 5.060e-05, lambda_0 : 2.886e+02, Time: 0.31, Learning Rate: 0.00048\n",
            "It: 1100, Loss: 5.045e-05, Loss_0: 3.230e-09, Loss_r: 5.045e-05, lambda_0 : 3.673e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1110, Loss: 5.031e-05, Loss_0: 2.235e-09, Loss_r: 5.031e-05, lambda_0 : 4.536e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1120, Loss: 5.016e-05, Loss_0: 2.068e-09, Loss_r: 5.016e-05, lambda_0 : 4.995e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1130, Loss: 5.000e-05, Loss_0: 1.866e-09, Loss_r: 5.000e-05, lambda_0 : 5.509e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1140, Loss: 4.984e-05, Loss_0: 1.810e-09, Loss_r: 4.984e-05, lambda_0 : 5.797e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1150, Loss: 4.967e-05, Loss_0: 1.779e-09, Loss_r: 4.967e-05, lambda_0 : 5.859e+02, Time: 0.31, Learning Rate: 0.00043\n",
            "It: 1160, Loss: 4.949e-05, Loss_0: 1.740e-09, Loss_r: 4.949e-05, lambda_0 : 5.821e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1170, Loss: 4.932e-05, Loss_0: 1.730e-09, Loss_r: 4.932e-05, lambda_0 : 5.812e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1180, Loss: 4.914e-05, Loss_0: 1.707e-09, Loss_r: 4.914e-05, lambda_0 : 5.833e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1190, Loss: 4.895e-05, Loss_0: 1.688e-09, Loss_r: 4.895e-05, lambda_0 : 5.852e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1200, Loss: 4.876e-05, Loss_0: 1.669e-09, Loss_r: 4.876e-05, lambda_0 : 5.872e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1210, Loss: 4.855e-05, Loss_0: 1.647e-09, Loss_r: 4.855e-05, lambda_0 : 5.902e+02, Time: 0.31, Learning Rate: 0.00039\n",
            "It: 1220, Loss: 4.833e-05, Loss_0: 1.626e-09, Loss_r: 4.833e-05, lambda_0 : 5.932e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1230, Loss: 4.812e-05, Loss_0: 1.605e-09, Loss_r: 4.812e-05, lambda_0 : 5.963e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1240, Loss: 4.791e-05, Loss_0: 1.582e-09, Loss_r: 4.791e-05, lambda_0 : 5.996e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1250, Loss: 4.768e-05, Loss_0: 1.559e-09, Loss_r: 4.768e-05, lambda_0 : 6.033e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1260, Loss: 4.744e-05, Loss_0: 1.534e-09, Loss_r: 4.744e-05, lambda_0 : 6.073e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1270, Loss: 4.718e-05, Loss_0: 1.507e-09, Loss_r: 4.718e-05, lambda_0 : 6.119e+02, Time: 0.31, Learning Rate: 0.00035\n",
            "It: 1280, Loss: 4.692e-05, Loss_0: 1.478e-09, Loss_r: 4.692e-05, lambda_0 : 6.170e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1290, Loss: 4.667e-05, Loss_0: 1.451e-09, Loss_r: 4.667e-05, lambda_0 : 6.216e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1300, Loss: 4.641e-05, Loss_0: 1.421e-09, Loss_r: 4.641e-05, lambda_0 : 6.267e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1310, Loss: 4.614e-05, Loss_0: 1.390e-09, Loss_r: 4.613e-05, lambda_0 : 6.324e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1320, Loss: 4.585e-05, Loss_0: 1.357e-09, Loss_r: 4.585e-05, lambda_0 : 6.388e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1330, Loss: 4.556e-05, Loss_0: 1.322e-09, Loss_r: 4.556e-05, lambda_0 : 6.459e+02, Time: 0.32, Learning Rate: 0.00031\n",
            "It: 1340, Loss: 4.526e-05, Loss_0: 1.286e-09, Loss_r: 4.526e-05, lambda_0 : 6.533e+02, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1350, Loss: 4.498e-05, Loss_0: 1.251e-09, Loss_r: 4.497e-05, lambda_0 : 6.604e+02, Time: 0.31, Learning Rate: 0.00028\n",
            "It: 1360, Loss: 4.469e-05, Loss_0: 1.215e-09, Loss_r: 4.468e-05, lambda_0 : 6.685e+02, Time: 0.31, Learning Rate: 0.00028\n",
            "It: 1370, Loss: 4.439e-05, Loss_0: 1.178e-09, Loss_r: 4.439e-05, lambda_0 : 6.776e+02, Time: 0.32, Learning Rate: 0.00028\n",
            "It: 1380, Loss: 4.408e-05, Loss_0: 1.140e-09, Loss_r: 4.408e-05, lambda_0 : 6.875e+02, Time: 0.31, Learning Rate: 0.00028\n",
            "It: 1390, Loss: 4.378e-05, Loss_0: 1.101e-09, Loss_r: 4.377e-05, lambda_0 : 6.985e+02, Time: 0.31, Learning Rate: 0.00028\n",
            "It: 1400, Loss: 4.346e-05, Loss_0: 1.061e-09, Loss_r: 4.346e-05, lambda_0 : 7.100e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1410, Loss: 4.317e-05, Loss_0: 1.024e-09, Loss_r: 4.317e-05, lambda_0 : 7.213e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1420, Loss: 4.288e-05, Loss_0: 9.874e-10, Loss_r: 4.288e-05, lambda_0 : 7.337e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1430, Loss: 4.259e-05, Loss_0: 9.504e-10, Loss_r: 4.259e-05, lambda_0 : 7.472e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1440, Loss: 4.229e-05, Loss_0: 9.137e-10, Loss_r: 4.229e-05, lambda_0 : 7.615e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1450, Loss: 4.199e-05, Loss_0: 8.772e-10, Loss_r: 4.199e-05, lambda_0 : 7.769e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1460, Loss: 4.169e-05, Loss_0: 8.411e-10, Loss_r: 4.169e-05, lambda_0 : 7.934e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1470, Loss: 4.139e-05, Loss_0: 8.057e-10, Loss_r: 4.139e-05, lambda_0 : 8.111e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1480, Loss: 4.109e-05, Loss_0: 7.709e-10, Loss_r: 4.109e-05, lambda_0 : 8.300e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1490, Loss: 4.079e-05, Loss_0: 7.370e-10, Loss_r: 4.079e-05, lambda_0 : 8.499e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1500, Loss: 4.049e-05, Loss_0: 7.041e-10, Loss_r: 4.049e-05, lambda_0 : 8.707e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1510, Loss: 4.018e-05, Loss_0: 6.726e-10, Loss_r: 4.018e-05, lambda_0 : 8.925e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1520, Loss: 3.988e-05, Loss_0: 6.422e-10, Loss_r: 3.988e-05, lambda_0 : 9.153e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1530, Loss: 3.957e-05, Loss_0: 6.131e-10, Loss_r: 3.957e-05, lambda_0 : 9.389e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1540, Loss: 3.926e-05, Loss_0: 5.855e-10, Loss_r: 3.926e-05, lambda_0 : 9.631e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1550, Loss: 3.896e-05, Loss_0: 5.593e-10, Loss_r: 3.895e-05, lambda_0 : 9.878e+02, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1560, Loss: 3.864e-05, Loss_0: 5.345e-10, Loss_r: 3.864e-05, lambda_0 : 1.013e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1570, Loss: 3.833e-05, Loss_0: 5.112e-10, Loss_r: 3.833e-05, lambda_0 : 1.038e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1580, Loss: 3.801e-05, Loss_0: 4.894e-10, Loss_r: 3.801e-05, lambda_0 : 1.063e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1590, Loss: 3.769e-05, Loss_0: 4.689e-10, Loss_r: 3.769e-05, lambda_0 : 1.089e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1600, Loss: 3.737e-05, Loss_0: 4.498e-10, Loss_r: 3.737e-05, lambda_0 : 1.114e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1610, Loss: 3.704e-05, Loss_0: 4.322e-10, Loss_r: 3.704e-05, lambda_0 : 1.138e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1620, Loss: 3.671e-05, Loss_0: 4.157e-10, Loss_r: 3.670e-05, lambda_0 : 1.162e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1630, Loss: 3.637e-05, Loss_0: 4.004e-10, Loss_r: 3.637e-05, lambda_0 : 1.185e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1640, Loss: 3.602e-05, Loss_0: 3.864e-10, Loss_r: 3.602e-05, lambda_0 : 1.207e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1650, Loss: 3.567e-05, Loss_0: 3.735e-10, Loss_r: 3.567e-05, lambda_0 : 1.227e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1660, Loss: 3.531e-05, Loss_0: 3.617e-10, Loss_r: 3.531e-05, lambda_0 : 1.246e+03, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1670, Loss: 3.494e-05, Loss_0: 3.508e-10, Loss_r: 3.494e-05, lambda_0 : 1.264e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1680, Loss: 3.457e-05, Loss_0: 3.409e-10, Loss_r: 3.457e-05, lambda_0 : 1.279e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1690, Loss: 3.419e-05, Loss_0: 3.319e-10, Loss_r: 3.419e-05, lambda_0 : 1.293e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1700, Loss: 3.380e-05, Loss_0: 3.238e-10, Loss_r: 3.380e-05, lambda_0 : 1.304e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1710, Loss: 3.340e-05, Loss_0: 3.164e-10, Loss_r: 3.340e-05, lambda_0 : 1.313e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1720, Loss: 3.300e-05, Loss_0: 3.090e-10, Loss_r: 3.300e-05, lambda_0 : 1.315e+03, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1730, Loss: 3.259e-05, Loss_0: 3.996e-09, Loss_r: 3.258e-05, lambda_0 : 3.213e+02, Time: 0.32, Learning Rate: 0.00025\n",
            "It: 1740, Loss: 3.654e-05, Loss_0: 3.670e-06, Loss_r: 3.287e-05, lambda_0 : 1.127e+01, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1750, Loss: 3.747e-05, Loss_0: 1.524e-06, Loss_r: 3.594e-05, lambda_0 : 1.823e+01, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1760, Loss: 4.843e-05, Loss_0: 1.086e-05, Loss_r: 3.758e-05, lambda_0 : 6.088e+00, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1770, Loss: 3.840e-05, Loss_0: 2.657e-07, Loss_r: 3.813e-05, lambda_0 : 4.905e+01, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1780, Loss: 3.858e-05, Loss_0: 3.997e-07, Loss_r: 3.818e-05, lambda_0 : 3.118e+01, Time: 0.31, Learning Rate: 0.00025\n",
            "It: 1790, Loss: 3.854e-05, Loss_0: 5.204e-07, Loss_r: 3.802e-05, lambda_0 : 3.122e+01, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1800, Loss: 3.795e-05, Loss_0: 1.895e-07, Loss_r: 3.776e-05, lambda_0 : 4.905e+01, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1810, Loss: 3.750e-05, Loss_0: 2.358e-08, Loss_r: 3.748e-05, lambda_0 : 1.511e+02, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1820, Loss: 3.717e-05, Loss_0: 2.726e-09, Loss_r: 3.717e-05, lambda_0 : 4.496e+02, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1830, Loss: 3.686e-05, Loss_0: 1.040e-08, Loss_r: 3.685e-05, lambda_0 : 1.989e+02, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1840, Loss: 3.653e-05, Loss_0: 4.097e-10, Loss_r: 3.653e-05, lambda_0 : 1.265e+03, Time: 0.31, Learning Rate: 0.00023\n",
            "It: 1850, Loss: 3.620e-05, Loss_0: 1.178e-09, Loss_r: 3.620e-05, lambda_0 : 6.215e+02, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1860, Loss: 3.590e-05, Loss_0: 3.662e-10, Loss_r: 3.590e-05, lambda_0 : 1.116e+03, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1870, Loss: 3.559e-05, Loss_0: 5.254e-10, Loss_r: 3.559e-05, lambda_0 : 9.253e+02, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1880, Loss: 3.528e-05, Loss_0: 4.475e-10, Loss_r: 3.528e-05, lambda_0 : 1.045e+03, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1890, Loss: 3.496e-05, Loss_0: 3.661e-10, Loss_r: 3.496e-05, lambda_0 : 1.292e+03, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1900, Loss: 3.464e-05, Loss_0: 3.408e-10, Loss_r: 3.464e-05, lambda_0 : 1.414e+03, Time: 0.31, Learning Rate: 0.00021\n",
            "It: 1910, Loss: 3.432e-05, Loss_0: 3.314e-10, Loss_r: 3.432e-05, lambda_0 : 1.419e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1920, Loss: 3.402e-05, Loss_0: 3.215e-10, Loss_r: 3.402e-05, lambda_0 : 1.410e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1930, Loss: 3.371e-05, Loss_0: 3.156e-10, Loss_r: 3.371e-05, lambda_0 : 1.416e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1940, Loss: 3.340e-05, Loss_0: 3.108e-10, Loss_r: 3.340e-05, lambda_0 : 1.425e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1950, Loss: 3.309e-05, Loss_0: 3.060e-10, Loss_r: 3.309e-05, lambda_0 : 1.431e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1960, Loss: 3.278e-05, Loss_0: 3.010e-10, Loss_r: 3.277e-05, lambda_0 : 1.432e+03, Time: 0.31, Learning Rate: 0.00019\n",
            "It: 1970, Loss: 3.245e-05, Loss_0: 2.963e-10, Loss_r: 3.245e-05, lambda_0 : 1.430e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 1980, Loss: 3.216e-05, Loss_0: 2.931e-10, Loss_r: 3.216e-05, lambda_0 : 1.432e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 1990, Loss: 3.186e-05, Loss_0: 2.902e-10, Loss_r: 3.186e-05, lambda_0 : 1.434e+03, Time: 0.30, Learning Rate: 0.00017\n",
            "It: 2000, Loss: 3.156e-05, Loss_0: 2.874e-10, Loss_r: 3.156e-05, lambda_0 : 1.433e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2010, Loss: 3.126e-05, Loss_0: 2.847e-10, Loss_r: 3.126e-05, lambda_0 : 1.431e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2020, Loss: 3.096e-05, Loss_0: 2.823e-10, Loss_r: 3.096e-05, lambda_0 : 1.429e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2030, Loss: 3.065e-05, Loss_0: 2.801e-10, Loss_r: 3.065e-05, lambda_0 : 1.425e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2040, Loss: 3.033e-05, Loss_0: 2.780e-10, Loss_r: 3.033e-05, lambda_0 : 1.420e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2050, Loss: 3.002e-05, Loss_0: 2.761e-10, Loss_r: 3.002e-05, lambda_0 : 1.415e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2060, Loss: 2.970e-05, Loss_0: 2.743e-10, Loss_r: 2.970e-05, lambda_0 : 1.409e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2070, Loss: 2.938e-05, Loss_0: 2.726e-10, Loss_r: 2.938e-05, lambda_0 : 1.403e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2080, Loss: 2.906e-05, Loss_0: 2.710e-10, Loss_r: 2.906e-05, lambda_0 : 1.395e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2090, Loss: 2.874e-05, Loss_0: 2.695e-10, Loss_r: 2.874e-05, lambda_0 : 1.387e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2100, Loss: 2.842e-05, Loss_0: 2.680e-10, Loss_r: 2.842e-05, lambda_0 : 1.379e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2110, Loss: 2.809e-05, Loss_0: 2.665e-10, Loss_r: 2.809e-05, lambda_0 : 1.370e+03, Time: 0.33, Learning Rate: 0.00017\n",
            "It: 2120, Loss: 2.776e-05, Loss_0: 2.650e-10, Loss_r: 2.776e-05, lambda_0 : 1.361e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2130, Loss: 2.744e-05, Loss_0: 2.635e-10, Loss_r: 2.744e-05, lambda_0 : 1.352e+03, Time: 0.32, Learning Rate: 0.00017\n",
            "It: 2140, Loss: 2.711e-05, Loss_0: 2.619e-10, Loss_r: 2.711e-05, lambda_0 : 1.343e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2150, Loss: 2.678e-05, Loss_0: 2.603e-10, Loss_r: 2.678e-05, lambda_0 : 1.334e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2160, Loss: 2.645e-05, Loss_0: 2.586e-10, Loss_r: 2.645e-05, lambda_0 : 1.325e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2170, Loss: 2.612e-05, Loss_0: 2.567e-10, Loss_r: 2.612e-05, lambda_0 : 1.316e+03, Time: 0.30, Learning Rate: 0.00017\n",
            "It: 2180, Loss: 2.579e-05, Loss_0: 2.549e-10, Loss_r: 2.579e-05, lambda_0 : 1.308e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2190, Loss: 2.546e-05, Loss_0: 2.528e-10, Loss_r: 2.546e-05, lambda_0 : 1.299e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2200, Loss: 2.514e-05, Loss_0: 2.485e-10, Loss_r: 2.514e-05, lambda_0 : 1.233e+03, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2210, Loss: 2.509e-05, Loss_0: 2.817e-07, Loss_r: 2.481e-05, lambda_0 : 3.083e+01, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2220, Loss: 4.613e-05, Loss_0: 2.063e-05, Loss_r: 2.551e-05, lambda_0 : 3.533e+00, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2230, Loss: 2.742e-05, Loss_0: 5.157e-08, Loss_r: 2.737e-05, lambda_0 : 1.728e+02, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2240, Loss: 4.018e-05, Loss_0: 1.187e-05, Loss_r: 2.831e-05, lambda_0 : 4.662e+00, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2250, Loss: 3.263e-05, Loss_0: 4.054e-06, Loss_r: 2.857e-05, lambda_0 : 1.015e+01, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2260, Loss: 2.996e-05, Loss_0: 1.452e-06, Loss_r: 2.850e-05, lambda_0 : 1.624e+01, Time: 0.31, Learning Rate: 0.00017\n",
            "It: 2270, Loss: 2.865e-05, Loss_0: 3.672e-07, Loss_r: 2.828e-05, lambda_0 : 3.211e+01, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2280, Loss: 2.815e-05, Loss_0: 1.245e-07, Loss_r: 2.803e-05, lambda_0 : 5.507e+01, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2290, Loss: 2.777e-05, Loss_0: 2.504e-08, Loss_r: 2.775e-05, lambda_0 : 1.266e+02, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2300, Loss: 2.746e-05, Loss_0: 3.388e-10, Loss_r: 2.746e-05, lambda_0 : 9.396e+02, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2310, Loss: 2.717e-05, Loss_0: 5.047e-09, Loss_r: 2.716e-05, lambda_0 : 2.414e+02, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2320, Loss: 2.687e-05, Loss_0: 2.855e-09, Loss_r: 2.687e-05, lambda_0 : 3.115e+02, Time: 0.31, Learning Rate: 0.00015\n",
            "It: 2330, Loss: 2.657e-05, Loss_0: 3.996e-10, Loss_r: 2.657e-05, lambda_0 : 8.355e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2340, Loss: 2.630e-05, Loss_0: 5.272e-10, Loss_r: 2.630e-05, lambda_0 : 7.210e+02, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2350, Loss: 2.604e-05, Loss_0: 2.434e-10, Loss_r: 2.604e-05, lambda_0 : 1.197e+03, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2360, Loss: 2.577e-05, Loss_0: 2.916e-10, Loss_r: 2.577e-05, lambda_0 : 1.086e+03, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2370, Loss: 2.550e-05, Loss_0: 2.343e-10, Loss_r: 2.550e-05, lambda_0 : 1.411e+03, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2380, Loss: 2.523e-05, Loss_0: 2.299e-10, Loss_r: 2.523e-05, lambda_0 : 1.306e+03, Time: 0.31, Learning Rate: 0.00014\n",
            "It: 2390, Loss: 2.497e-05, Loss_0: 2.355e-10, Loss_r: 2.497e-05, lambda_0 : 1.429e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2400, Loss: 2.473e-05, Loss_0: 2.268e-10, Loss_r: 2.473e-05, lambda_0 : 1.379e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2410, Loss: 2.449e-05, Loss_0: 2.248e-10, Loss_r: 2.449e-05, lambda_0 : 1.376e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2420, Loss: 2.425e-05, Loss_0: 2.243e-10, Loss_r: 2.425e-05, lambda_0 : 1.396e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2430, Loss: 2.401e-05, Loss_0: 2.208e-10, Loss_r: 2.401e-05, lambda_0 : 1.375e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2440, Loss: 2.377e-05, Loss_0: 2.194e-10, Loss_r: 2.377e-05, lambda_0 : 1.385e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2450, Loss: 2.353e-05, Loss_0: 2.167e-10, Loss_r: 2.353e-05, lambda_0 : 1.376e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2460, Loss: 2.329e-05, Loss_0: 2.147e-10, Loss_r: 2.329e-05, lambda_0 : 1.378e+03, Time: 0.33, Learning Rate: 0.00012\n",
            "It: 2470, Loss: 2.305e-05, Loss_0: 2.124e-10, Loss_r: 2.305e-05, lambda_0 : 1.376e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2480, Loss: 2.281e-05, Loss_0: 2.100e-10, Loss_r: 2.281e-05, lambda_0 : 1.374e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2490, Loss: 2.257e-05, Loss_0: 2.077e-10, Loss_r: 2.257e-05, lambda_0 : 1.373e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2500, Loss: 2.234e-05, Loss_0: 2.054e-10, Loss_r: 2.234e-05, lambda_0 : 1.372e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2510, Loss: 2.210e-05, Loss_0: 2.030e-10, Loss_r: 2.210e-05, lambda_0 : 1.371e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2520, Loss: 2.186e-05, Loss_0: 2.007e-10, Loss_r: 2.186e-05, lambda_0 : 1.371e+03, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2530, Loss: 2.163e-05, Loss_0: 1.983e-10, Loss_r: 2.163e-05, lambda_0 : 1.370e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2540, Loss: 2.139e-05, Loss_0: 1.959e-10, Loss_r: 2.139e-05, lambda_0 : 1.369e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2550, Loss: 2.116e-05, Loss_0: 1.936e-10, Loss_r: 2.116e-05, lambda_0 : 1.369e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2560, Loss: 2.092e-05, Loss_0: 1.912e-10, Loss_r: 2.092e-05, lambda_0 : 1.368e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2570, Loss: 2.069e-05, Loss_0: 1.890e-10, Loss_r: 2.069e-05, lambda_0 : 1.368e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2580, Loss: 2.046e-05, Loss_0: 1.866e-10, Loss_r: 2.046e-05, lambda_0 : 1.368e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2590, Loss: 2.023e-05, Loss_0: 1.844e-10, Loss_r: 2.023e-05, lambda_0 : 1.367e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2600, Loss: 2.000e-05, Loss_0: 1.821e-10, Loss_r: 2.000e-05, lambda_0 : 1.366e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2610, Loss: 1.977e-05, Loss_0: 1.799e-10, Loss_r: 1.977e-05, lambda_0 : 1.366e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2620, Loss: 1.954e-05, Loss_0: 1.777e-10, Loss_r: 1.954e-05, lambda_0 : 1.364e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2630, Loss: 1.931e-05, Loss_0: 1.755e-10, Loss_r: 1.931e-05, lambda_0 : 1.364e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2640, Loss: 1.908e-05, Loss_0: 1.735e-10, Loss_r: 1.908e-05, lambda_0 : 1.364e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2650, Loss: 1.886e-05, Loss_0: 1.713e-10, Loss_r: 1.885e-05, lambda_0 : 1.362e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2660, Loss: 1.863e-05, Loss_0: 1.689e-10, Loss_r: 1.863e-05, lambda_0 : 1.351e+03, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2670, Loss: 1.841e-05, Loss_0: 3.439e-10, Loss_r: 1.841e-05, lambda_0 : 6.738e+02, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2680, Loss: 2.531e-05, Loss_0: 7.118e-06, Loss_r: 1.819e-05, lambda_0 : 4.766e+00, Time: 0.32, Learning Rate: 0.00012\n",
            "It: 2690, Loss: 2.701e-05, Loss_0: 8.633e-06, Loss_r: 1.838e-05, lambda_0 : 5.140e+01, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2700, Loss: 2.945e-05, Loss_0: 1.083e-05, Loss_r: 1.862e-05, lambda_0 : 3.981e+01, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2710, Loss: 2.184e-05, Loss_0: 3.219e-06, Loss_r: 1.862e-05, lambda_0 : 6.847e+00, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2720, Loss: 1.861e-05, Loss_0: 8.316e-08, Loss_r: 1.853e-05, lambda_0 : 1.800e+02, Time: 0.31, Learning Rate: 0.00012\n",
            "It: 2730, Loss: 1.839e-05, Loss_0: 3.934e-08, Loss_r: 1.835e-05, lambda_0 : 6.872e+01, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2740, Loss: 1.828e-05, Loss_0: 1.086e-07, Loss_r: 1.817e-05, lambda_0 : 3.712e+01, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2750, Loss: 1.802e-05, Loss_0: 3.576e-08, Loss_r: 1.798e-05, lambda_0 : 7.447e+01, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2760, Loss: 1.779e-05, Loss_0: 4.768e-09, Loss_r: 1.778e-05, lambda_0 : 1.849e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2770, Loss: 1.759e-05, Loss_0: 1.931e-09, Loss_r: 1.758e-05, lambda_0 : 3.079e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2780, Loss: 1.739e-05, Loss_0: 2.387e-09, Loss_r: 1.739e-05, lambda_0 : 2.448e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2790, Loss: 1.719e-05, Loss_0: 5.743e-10, Loss_r: 1.719e-05, lambda_0 : 5.092e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2800, Loss: 1.699e-05, Loss_0: 4.387e-10, Loss_r: 1.699e-05, lambda_0 : 5.711e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2810, Loss: 1.680e-05, Loss_0: 1.979e-10, Loss_r: 1.680e-05, lambda_0 : 8.885e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2820, Loss: 1.660e-05, Loss_0: 1.859e-10, Loss_r: 1.660e-05, lambda_0 : 9.249e+02, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2830, Loss: 1.641e-05, Loss_0: 1.536e-10, Loss_r: 1.641e-05, lambda_0 : 1.145e+03, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2840, Loss: 1.622e-05, Loss_0: 1.654e-10, Loss_r: 1.622e-05, lambda_0 : 1.138e+03, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2850, Loss: 1.602e-05, Loss_0: 1.511e-10, Loss_r: 1.602e-05, lambda_0 : 1.341e+03, Time: 0.31, Learning Rate: 0.00011\n",
            "It: 2860, Loss: 1.583e-05, Loss_0: 1.895e-10, Loss_r: 1.583e-05, lambda_0 : 9.350e+02, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2870, Loss: 1.572e-05, Loss_0: 7.824e-08, Loss_r: 1.564e-05, lambda_0 : 4.136e+01, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2880, Loss: 1.161e-04, Loss_0: 1.005e-04, Loss_r: 1.560e-05, lambda_0 : 2.126e+00, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2890, Loss: 5.514e-05, Loss_0: 3.886e-05, Loss_r: 1.628e-05, lambda_0 : 1.761e+00, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2900, Loss: 2.255e-05, Loss_0: 5.859e-06, Loss_r: 1.669e-05, lambda_0 : 5.940e+00, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2910, Loss: 1.852e-05, Loss_0: 1.738e-06, Loss_r: 1.678e-05, lambda_0 : 1.021e+01, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2920, Loss: 1.702e-05, Loss_0: 3.031e-07, Loss_r: 1.671e-05, lambda_0 : 2.649e+01, Time: 0.32, Learning Rate: 0.00011\n",
            "It: 2930, Loss: 1.720e-05, Loss_0: 6.255e-07, Loss_r: 1.657e-05, lambda_0 : 1.667e+01, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2940, Loss: 1.642e-05, Loss_0: 1.653e-09, Loss_r: 1.642e-05, lambda_0 : 3.202e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2950, Loss: 1.634e-05, Loss_0: 8.817e-08, Loss_r: 1.625e-05, lambda_0 : 4.169e+01, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2960, Loss: 1.608e-05, Loss_0: 5.972e-09, Loss_r: 1.608e-05, lambda_0 : 1.524e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2970, Loss: 1.591e-05, Loss_0: 2.744e-09, Loss_r: 1.590e-05, lambda_0 : 2.456e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2980, Loss: 1.573e-05, Loss_0: 3.428e-09, Loss_r: 1.573e-05, lambda_0 : 2.198e+02, Time: 0.31, Learning Rate: 0.00010\n",
            "It: 2990, Loss: 1.556e-05, Loss_0: 1.541e-09, Loss_r: 1.556e-05, lambda_0 : 3.108e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3000, Loss: 1.540e-05, Loss_0: 1.966e-10, Loss_r: 1.540e-05, lambda_0 : 8.394e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3010, Loss: 1.525e-05, Loss_0: 2.241e-10, Loss_r: 1.525e-05, lambda_0 : 7.883e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3020, Loss: 1.509e-05, Loss_0: 2.167e-10, Loss_r: 1.509e-05, lambda_0 : 8.013e+02, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3030, Loss: 1.494e-05, Loss_0: 1.664e-10, Loss_r: 1.494e-05, lambda_0 : 1.021e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3040, Loss: 1.479e-05, Loss_0: 1.476e-10, Loss_r: 1.479e-05, lambda_0 : 1.204e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3050, Loss: 1.464e-05, Loss_0: 1.415e-10, Loss_r: 1.464e-05, lambda_0 : 1.306e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3060, Loss: 1.448e-05, Loss_0: 1.380e-10, Loss_r: 1.448e-05, lambda_0 : 1.384e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3070, Loss: 1.433e-05, Loss_0: 1.354e-10, Loss_r: 1.433e-05, lambda_0 : 1.352e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3080, Loss: 1.418e-05, Loss_0: 1.339e-10, Loss_r: 1.418e-05, lambda_0 : 1.323e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3090, Loss: 1.403e-05, Loss_0: 1.334e-10, Loss_r: 1.403e-05, lambda_0 : 1.319e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3100, Loss: 1.389e-05, Loss_0: 1.333e-10, Loss_r: 1.389e-05, lambda_0 : 1.327e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3110, Loss: 1.374e-05, Loss_0: 1.325e-10, Loss_r: 1.374e-05, lambda_0 : 1.315e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3120, Loss: 1.359e-05, Loss_0: 1.320e-10, Loss_r: 1.359e-05, lambda_0 : 1.309e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3130, Loss: 1.344e-05, Loss_0: 1.314e-10, Loss_r: 1.344e-05, lambda_0 : 1.304e+03, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3140, Loss: 1.330e-05, Loss_0: 1.309e-10, Loss_r: 1.330e-05, lambda_0 : 1.297e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3150, Loss: 1.315e-05, Loss_0: 1.304e-10, Loss_r: 1.315e-05, lambda_0 : 1.289e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3160, Loss: 1.301e-05, Loss_0: 1.300e-10, Loss_r: 1.301e-05, lambda_0 : 1.283e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3170, Loss: 1.287e-05, Loss_0: 1.295e-10, Loss_r: 1.287e-05, lambda_0 : 1.277e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3180, Loss: 1.272e-05, Loss_0: 1.291e-10, Loss_r: 1.272e-05, lambda_0 : 1.270e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3190, Loss: 1.258e-05, Loss_0: 1.286e-10, Loss_r: 1.258e-05, lambda_0 : 1.262e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3200, Loss: 1.244e-05, Loss_0: 1.282e-10, Loss_r: 1.244e-05, lambda_0 : 1.256e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3210, Loss: 1.230e-05, Loss_0: 1.279e-10, Loss_r: 1.230e-05, lambda_0 : 1.249e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3220, Loss: 1.216e-05, Loss_0: 1.274e-10, Loss_r: 1.216e-05, lambda_0 : 1.240e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3230, Loss: 1.203e-05, Loss_0: 1.270e-10, Loss_r: 1.203e-05, lambda_0 : 1.231e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3240, Loss: 1.189e-05, Loss_0: 1.265e-10, Loss_r: 1.189e-05, lambda_0 : 1.218e+03, Time: 0.31, Learning Rate: 0.00009\n",
            "It: 3250, Loss: 1.175e-05, Loss_0: 1.577e-10, Loss_r: 1.175e-05, lambda_0 : 8.039e+02, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3260, Loss: 1.186e-05, Loss_0: 2.388e-07, Loss_r: 1.162e-05, lambda_0 : 2.067e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3270, Loss: 1.533e-05, Loss_0: 3.636e-06, Loss_r: 1.169e-05, lambda_0 : 1.343e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3280, Loss: 4.353e-05, Loss_0: 3.131e-05, Loss_r: 1.222e-05, lambda_0 : 1.268e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3290, Loss: 2.064e-05, Loss_0: 8.161e-06, Loss_r: 1.248e-05, lambda_0 : 9.166e+00, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3300, Loss: 1.263e-05, Loss_0: 9.626e-08, Loss_r: 1.253e-05, lambda_0 : 4.876e+01, Time: 0.32, Learning Rate: 0.00009\n",
            "It: 3310, Loss: 1.300e-05, Loss_0: 5.212e-07, Loss_r: 1.248e-05, lambda_0 : 1.381e+01, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 3320, Loss: 1.245e-05, Loss_0: 6.181e-08, Loss_r: 1.239e-05, lambda_0 : 3.942e+01, Time: 0.31, Learning Rate: 0.00008\n",
            "It: 3330, Loss: 1.242e-05, Loss_0: 1.356e-07, Loss_r: 1.228e-05, lambda_0 : 2.736e+01, Time: 0.31, Learning Rate: 0.00008\n",
            "It: 3340, Loss: 1.216e-05, Loss_0: 1.328e-09, Loss_r: 1.216e-05, lambda_0 : 2.746e+02, Time: 0.32, Learning Rate: 0.00008\n",
            "It: 3350, Loss: 1.205e-05, Loss_0: 1.579e-08, Loss_r: 1.204e-05, lambda_0 : 7.590e+01, Time: 0.31, Learning Rate: 0.00008\n",
            "It: 3360, Loss: 1.192e-05, Loss_0: 3.629e-09, Loss_r: 1.191e-05, lambda_0 : 1.531e+02, Time: 0.31, Learning Rate: 0.00008\n",
            "It: 3370, Loss: 1.179e-05, Loss_0: 1.258e-10, Loss_r: 1.179e-05, lambda_0 : 1.077e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3380, Loss: 1.168e-05, Loss_0: 6.345e-10, Loss_r: 1.168e-05, lambda_0 : 3.717e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3390, Loss: 1.157e-05, Loss_0: 1.857e-10, Loss_r: 1.157e-05, lambda_0 : 7.302e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3400, Loss: 1.146e-05, Loss_0: 2.006e-10, Loss_r: 1.146e-05, lambda_0 : 6.799e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3410, Loss: 1.136e-05, Loss_0: 1.230e-10, Loss_r: 1.136e-05, lambda_0 : 1.190e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3420, Loss: 1.125e-05, Loss_0: 1.281e-10, Loss_r: 1.125e-05, lambda_0 : 9.980e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3430, Loss: 1.114e-05, Loss_0: 1.241e-10, Loss_r: 1.114e-05, lambda_0 : 1.080e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3440, Loss: 1.103e-05, Loss_0: 1.227e-10, Loss_r: 1.103e-05, lambda_0 : 1.159e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3450, Loss: 1.093e-05, Loss_0: 1.227e-10, Loss_r: 1.093e-05, lambda_0 : 1.178e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3460, Loss: 1.082e-05, Loss_0: 1.227e-10, Loss_r: 1.082e-05, lambda_0 : 1.175e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3470, Loss: 1.072e-05, Loss_0: 1.226e-10, Loss_r: 1.072e-05, lambda_0 : 1.168e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3480, Loss: 1.061e-05, Loss_0: 1.224e-10, Loss_r: 1.061e-05, lambda_0 : 1.160e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3490, Loss: 1.051e-05, Loss_0: 1.223e-10, Loss_r: 1.051e-05, lambda_0 : 1.153e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3500, Loss: 1.040e-05, Loss_0: 1.222e-10, Loss_r: 1.040e-05, lambda_0 : 1.147e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3510, Loss: 1.030e-05, Loss_0: 1.222e-10, Loss_r: 1.030e-05, lambda_0 : 1.142e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3520, Loss: 1.020e-05, Loss_0: 1.220e-10, Loss_r: 1.020e-05, lambda_0 : 1.133e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3530, Loss: 1.010e-05, Loss_0: 1.219e-10, Loss_r: 1.010e-05, lambda_0 : 1.126e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3540, Loss: 9.998e-06, Loss_0: 1.218e-10, Loss_r: 9.998e-06, lambda_0 : 1.118e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3550, Loss: 9.898e-06, Loss_0: 1.217e-10, Loss_r: 9.898e-06, lambda_0 : 1.110e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3560, Loss: 9.799e-06, Loss_0: 1.216e-10, Loss_r: 9.799e-06, lambda_0 : 1.103e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3570, Loss: 9.700e-06, Loss_0: 1.215e-10, Loss_r: 9.700e-06, lambda_0 : 1.094e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3580, Loss: 9.602e-06, Loss_0: 1.214e-10, Loss_r: 9.602e-06, lambda_0 : 1.088e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3590, Loss: 9.504e-06, Loss_0: 1.213e-10, Loss_r: 9.504e-06, lambda_0 : 1.080e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3600, Loss: 9.408e-06, Loss_0: 1.212e-10, Loss_r: 9.407e-06, lambda_0 : 1.072e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3610, Loss: 9.312e-06, Loss_0: 1.211e-10, Loss_r: 9.311e-06, lambda_0 : 1.063e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3620, Loss: 9.216e-06, Loss_0: 1.210e-10, Loss_r: 9.216e-06, lambda_0 : 1.056e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3630, Loss: 9.121e-06, Loss_0: 1.209e-10, Loss_r: 9.121e-06, lambda_0 : 1.048e+03, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3640, Loss: 9.027e-06, Loss_0: 1.208e-10, Loss_r: 9.027e-06, lambda_0 : 1.038e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3650, Loss: 8.934e-06, Loss_0: 1.207e-10, Loss_r: 8.934e-06, lambda_0 : 1.031e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3660, Loss: 8.841e-06, Loss_0: 1.206e-10, Loss_r: 8.841e-06, lambda_0 : 1.020e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3670, Loss: 8.749e-06, Loss_0: 1.204e-10, Loss_r: 8.749e-06, lambda_0 : 9.988e+02, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3680, Loss: 8.659e-06, Loss_0: 1.188e-10, Loss_r: 8.658e-06, lambda_0 : 2.134e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3690, Loss: 8.624e-06, Loss_0: 1.026e-10, Loss_r: 8.624e-06, lambda_0 : 2.326e+04, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3700, Loss: 8.487e-06, Loss_0: 1.277e-10, Loss_r: 8.487e-06, lambda_0 : 7.619e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3710, Loss: 8.398e-06, Loss_0: 1.436e-10, Loss_r: 8.398e-06, lambda_0 : 5.466e+03, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3720, Loss: 8.313e-06, Loss_0: 7.643e-09, Loss_r: 8.306e-06, lambda_0 : 3.868e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3730, Loss: 2.390e-05, Loss_0: 1.568e-05, Loss_r: 8.224e-06, lambda_0 : 2.327e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3740, Loss: 9.056e-06, Loss_0: 8.640e-07, Loss_r: 8.192e-06, lambda_0 : 7.498e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3750, Loss: 8.446e-06, Loss_0: 2.913e-07, Loss_r: 8.155e-06, lambda_0 : 1.470e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3760, Loss: 8.518e-06, Loss_0: 4.210e-07, Loss_r: 8.097e-06, lambda_0 : 1.016e+01, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3770, Loss: 8.299e-06, Loss_0: 2.682e-07, Loss_r: 8.030e-06, lambda_0 : 1.362e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3780, Loss: 2.780e-05, Loss_0: 1.983e-05, Loss_r: 7.973e-06, lambda_0 : 1.466e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3790, Loss: 1.229e-05, Loss_0: 4.333e-06, Loss_r: 7.954e-06, lambda_0 : 3.440e+00, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3800, Loss: 9.943e-06, Loss_0: 2.030e-06, Loss_r: 7.913e-06, lambda_0 : 5.069e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3810, Loss: 8.548e-06, Loss_0: 6.919e-07, Loss_r: 7.856e-06, lambda_0 : 8.932e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3820, Loss: 7.968e-06, Loss_0: 1.862e-07, Loss_r: 7.782e-06, lambda_0 : 1.676e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3830, Loss: 7.770e-06, Loss_0: 6.540e-08, Loss_r: 7.705e-06, lambda_0 : 2.632e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3840, Loss: 1.239e-05, Loss_0: 4.762e-06, Loss_r: 7.627e-06, lambda_0 : 3.007e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3850, Loss: 7.751e-06, Loss_0: 1.858e-07, Loss_r: 7.566e-06, lambda_0 : 1.513e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3860, Loss: 1.314e-05, Loss_0: 5.631e-06, Loss_r: 7.514e-06, lambda_0 : 2.820e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3870, Loss: 9.236e-06, Loss_0: 1.757e-06, Loss_r: 7.479e-06, lambda_0 : 5.080e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3880, Loss: 7.471e-06, Loss_0: 4.580e-08, Loss_r: 7.425e-06, lambda_0 : 3.139e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3890, Loss: 1.018e-05, Loss_0: 2.809e-06, Loss_r: 7.374e-06, lambda_0 : 4.079e+00, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3900, Loss: 8.749e-06, Loss_0: 1.423e-06, Loss_r: 7.326e-06, lambda_0 : 1.505e+01, Time: 0.32, Learning Rate: 0.00007\n",
            "It: 3910, Loss: 7.492e-06, Loss_0: 2.222e-07, Loss_r: 7.270e-06, lambda_0 : 1.193e+02, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3920, Loss: 8.348e-06, Loss_0: 1.149e-06, Loss_r: 7.199e-06, lambda_0 : 4.741e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3930, Loss: 1.131e-05, Loss_0: 4.181e-06, Loss_r: 7.132e-06, lambda_0 : 1.842e+01, Time: 0.31, Learning Rate: 0.00007\n",
            "It: 3940, Loss: 9.480e-06, Loss_0: 2.386e-06, Loss_r: 7.094e-06, lambda_0 : 1.631e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 3950, Loss: 7.241e-06, Loss_0: 1.832e-07, Loss_r: 7.058e-06, lambda_0 : 2.568e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 3960, Loss: 7.006e-06, Loss_0: 1.771e-09, Loss_r: 7.004e-06, lambda_0 : 1.442e+02, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 3970, Loss: 6.949e-06, Loss_0: 6.780e-09, Loss_r: 6.942e-06, lambda_0 : 7.903e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 3980, Loss: 6.877e-06, Loss_0: 2.871e-10, Loss_r: 6.877e-06, lambda_0 : 3.486e+02, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 3990, Loss: 6.821e-06, Loss_0: 1.121e-08, Loss_r: 6.810e-06, lambda_0 : 5.470e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4000, Loss: 6.750e-06, Loss_0: 5.873e-09, Loss_r: 6.744e-06, lambda_0 : 7.329e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4010, Loss: 6.680e-06, Loss_0: 2.284e-09, Loss_r: 6.678e-06, lambda_0 : 1.244e+02, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4020, Loss: 6.612e-06, Loss_0: 4.932e-10, Loss_r: 6.612e-06, lambda_0 : 2.396e+02, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4030, Loss: 6.547e-06, Loss_0: 6.542e-10, Loss_r: 6.547e-06, lambda_0 : 2.155e+02, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4040, Loss: 6.482e-06, Loss_0: 2.358e-10, Loss_r: 6.482e-06, lambda_0 : 3.601e+02, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4050, Loss: 6.422e-06, Loss_0: 3.768e-09, Loss_r: 6.418e-06, lambda_0 : 8.524e+01, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4060, Loss: 7.036e-06, Loss_0: 6.821e-07, Loss_r: 6.354e-06, lambda_0 : 6.566e+00, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4070, Loss: 1.042e-05, Loss_0: 4.123e-06, Loss_r: 6.298e-06, lambda_0 : 2.698e+00, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4080, Loss: 7.153e-06, Loss_0: 9.011e-07, Loss_r: 6.252e-06, lambda_0 : 5.892e+00, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4090, Loss: 6.549e-06, Loss_0: 3.494e-07, Loss_r: 6.200e-06, lambda_0 : 9.564e+00, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4100, Loss: 6.178e-06, Loss_0: 3.414e-08, Loss_r: 6.144e-06, lambda_0 : 2.986e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4110, Loss: 9.206e-05, Loss_0: 8.592e-05, Loss_r: 6.136e-06, lambda_0 : 5.475e-01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4120, Loss: 2.550e-05, Loss_0: 1.910e-05, Loss_r: 6.395e-06, lambda_0 : 1.321e+00, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4130, Loss: 1.226e-05, Loss_0: 5.717e-06, Loss_r: 6.542e-06, lambda_0 : 3.289e+00, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4140, Loss: 9.590e-06, Loss_0: 3.006e-06, Loss_r: 6.584e-06, lambda_0 : 3.674e+00, Time: 0.32, Learning Rate: 0.00006\n",
            "It: 4150, Loss: 7.482e-06, Loss_0: 9.217e-07, Loss_r: 6.561e-06, lambda_0 : 6.613e+00, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4160, Loss: 6.794e-06, Loss_0: 2.783e-07, Loss_r: 6.516e-06, lambda_0 : 1.345e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4170, Loss: 6.566e-06, Loss_0: 1.019e-07, Loss_r: 6.464e-06, lambda_0 : 2.004e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4180, Loss: 6.444e-06, Loss_0: 3.451e-08, Loss_r: 6.409e-06, lambda_0 : 3.121e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4190, Loss: 6.365e-06, Loss_0: 1.214e-08, Loss_r: 6.353e-06, lambda_0 : 4.899e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4200, Loss: 6.302e-06, Loss_0: 5.407e-09, Loss_r: 6.296e-06, lambda_0 : 7.213e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4210, Loss: 6.243e-06, Loss_0: 2.759e-09, Loss_r: 6.240e-06, lambda_0 : 9.908e+01, Time: 0.31, Learning Rate: 0.00006\n",
            "It: 4220, Loss: 6.185e-06, Loss_0: 1.143e-09, Loss_r: 6.184e-06, lambda_0 : 1.504e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4230, Loss: 6.134e-06, Loss_0: 3.743e-10, Loss_r: 6.134e-06, lambda_0 : 2.649e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4240, Loss: 6.084e-06, Loss_0: 1.338e-10, Loss_r: 6.084e-06, lambda_0 : 5.701e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4250, Loss: 6.034e-06, Loss_0: 1.100e-10, Loss_r: 6.034e-06, lambda_0 : 7.950e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4260, Loss: 5.985e-06, Loss_0: 1.097e-10, Loss_r: 5.985e-06, lambda_0 : 7.447e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4270, Loss: 5.936e-06, Loss_0: 1.093e-10, Loss_r: 5.936e-06, lambda_0 : 7.555e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4280, Loss: 5.888e-06, Loss_0: 1.094e-10, Loss_r: 5.888e-06, lambda_0 : 7.745e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4290, Loss: 5.840e-06, Loss_0: 1.097e-10, Loss_r: 5.840e-06, lambda_0 : 7.775e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4300, Loss: 5.792e-06, Loss_0: 1.100e-10, Loss_r: 5.792e-06, lambda_0 : 7.748e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4310, Loss: 5.745e-06, Loss_0: 1.101e-10, Loss_r: 5.744e-06, lambda_0 : 7.695e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4320, Loss: 5.697e-06, Loss_0: 1.100e-10, Loss_r: 5.697e-06, lambda_0 : 7.624e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4330, Loss: 5.651e-06, Loss_0: 1.098e-10, Loss_r: 5.651e-06, lambda_0 : 7.536e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4340, Loss: 5.604e-06, Loss_0: 1.097e-10, Loss_r: 5.604e-06, lambda_0 : 7.458e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4350, Loss: 5.558e-06, Loss_0: 1.097e-10, Loss_r: 5.558e-06, lambda_0 : 7.403e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4360, Loss: 5.513e-06, Loss_0: 1.098e-10, Loss_r: 5.512e-06, lambda_0 : 7.350e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4370, Loss: 5.467e-06, Loss_0: 1.097e-10, Loss_r: 5.467e-06, lambda_0 : 7.292e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4380, Loss: 5.422e-06, Loss_0: 1.097e-10, Loss_r: 5.422e-06, lambda_0 : 7.228e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4390, Loss: 5.378e-06, Loss_0: 1.097e-10, Loss_r: 5.377e-06, lambda_0 : 7.164e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4400, Loss: 5.333e-06, Loss_0: 1.096e-10, Loss_r: 5.333e-06, lambda_0 : 7.107e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4410, Loss: 5.289e-06, Loss_0: 1.096e-10, Loss_r: 5.289e-06, lambda_0 : 7.049e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4420, Loss: 5.246e-06, Loss_0: 1.096e-10, Loss_r: 5.246e-06, lambda_0 : 7.000e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4430, Loss: 5.202e-06, Loss_0: 1.095e-10, Loss_r: 5.202e-06, lambda_0 : 6.932e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4440, Loss: 5.159e-06, Loss_0: 1.095e-10, Loss_r: 5.159e-06, lambda_0 : 6.878e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4450, Loss: 5.117e-06, Loss_0: 1.094e-10, Loss_r: 5.117e-06, lambda_0 : 6.818e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4460, Loss: 5.075e-06, Loss_0: 1.094e-10, Loss_r: 5.075e-06, lambda_0 : 6.767e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4470, Loss: 5.033e-06, Loss_0: 1.093e-10, Loss_r: 5.033e-06, lambda_0 : 6.709e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4480, Loss: 4.991e-06, Loss_0: 1.093e-10, Loss_r: 4.991e-06, lambda_0 : 6.653e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4490, Loss: 4.950e-06, Loss_0: 1.092e-10, Loss_r: 4.950e-06, lambda_0 : 6.590e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4500, Loss: 4.909e-06, Loss_0: 1.091e-10, Loss_r: 4.909e-06, lambda_0 : 6.534e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4510, Loss: 4.869e-06, Loss_0: 1.091e-10, Loss_r: 4.869e-06, lambda_0 : 6.474e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4520, Loss: 4.829e-06, Loss_0: 1.090e-10, Loss_r: 4.829e-06, lambda_0 : 6.419e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4530, Loss: 4.789e-06, Loss_0: 1.089e-10, Loss_r: 4.789e-06, lambda_0 : 6.375e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4540, Loss: 4.749e-06, Loss_0: 1.088e-10, Loss_r: 4.749e-06, lambda_0 : 6.319e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4550, Loss: 4.710e-06, Loss_0: 1.088e-10, Loss_r: 4.710e-06, lambda_0 : 6.255e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4560, Loss: 4.672e-06, Loss_0: 1.087e-10, Loss_r: 4.672e-06, lambda_0 : 6.206e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4570, Loss: 4.633e-06, Loss_0: 1.085e-10, Loss_r: 4.633e-06, lambda_0 : 6.140e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4580, Loss: 4.595e-06, Loss_0: 1.085e-10, Loss_r: 4.595e-06, lambda_0 : 6.097e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4590, Loss: 4.558e-06, Loss_0: 1.084e-10, Loss_r: 4.557e-06, lambda_0 : 6.046e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4600, Loss: 4.520e-06, Loss_0: 1.083e-10, Loss_r: 4.520e-06, lambda_0 : 6.012e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4610, Loss: 4.483e-06, Loss_0: 1.081e-10, Loss_r: 4.483e-06, lambda_0 : 5.937e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4620, Loss: 4.447e-06, Loss_0: 1.082e-10, Loss_r: 4.446e-06, lambda_0 : 5.944e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4630, Loss: 4.410e-06, Loss_0: 1.086e-10, Loss_r: 4.410e-06, lambda_0 : 6.086e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4640, Loss: 4.375e-06, Loss_0: 1.286e-10, Loss_r: 4.375e-06, lambda_0 : 1.073e+03, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4650, Loss: 4.369e-06, Loss_0: 8.221e-09, Loss_r: 4.361e-06, lambda_0 : 9.366e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4660, Loss: 1.736e-05, Loss_0: 1.305e-05, Loss_r: 4.313e-06, lambda_0 : 1.774e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4670, Loss: 5.414e-06, Loss_0: 1.038e-06, Loss_r: 4.376e-06, lambda_0 : 2.153e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4680, Loss: 6.500e-06, Loss_0: 2.055e-06, Loss_r: 4.446e-06, lambda_0 : 2.514e+00, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4690, Loss: 7.277e-06, Loss_0: 2.808e-06, Loss_r: 4.469e-06, lambda_0 : 4.808e+00, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4700, Loss: 4.967e-06, Loss_0: 5.061e-07, Loss_r: 4.461e-06, lambda_0 : 9.510e+00, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4710, Loss: 4.439e-06, Loss_0: 1.379e-09, Loss_r: 4.437e-06, lambda_0 : 1.105e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4720, Loss: 4.494e-06, Loss_0: 8.518e-08, Loss_r: 4.409e-06, lambda_0 : 1.325e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4730, Loss: 4.405e-06, Loss_0: 2.661e-08, Loss_r: 4.379e-06, lambda_0 : 2.261e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4740, Loss: 4.350e-06, Loss_0: 3.319e-09, Loss_r: 4.347e-06, lambda_0 : 6.125e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4750, Loss: 4.320e-06, Loss_0: 4.442e-09, Loss_r: 4.316e-06, lambda_0 : 5.324e+01, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4760, Loss: 4.285e-06, Loss_0: 7.575e-10, Loss_r: 4.284e-06, lambda_0 : 1.314e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4770, Loss: 4.253e-06, Loss_0: 2.925e-10, Loss_r: 4.253e-06, lambda_0 : 2.061e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4780, Loss: 4.222e-06, Loss_0: 3.612e-10, Loss_r: 4.222e-06, lambda_0 : 1.782e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4790, Loss: 4.191e-06, Loss_0: 1.330e-10, Loss_r: 4.191e-06, lambda_0 : 3.426e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4800, Loss: 4.161e-06, Loss_0: 1.035e-10, Loss_r: 4.161e-06, lambda_0 : 5.729e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4810, Loss: 4.131e-06, Loss_0: 1.058e-10, Loss_r: 4.130e-06, lambda_0 : 5.489e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4820, Loss: 4.101e-06, Loss_0: 1.032e-10, Loss_r: 4.101e-06, lambda_0 : 5.186e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4830, Loss: 4.071e-06, Loss_0: 1.043e-10, Loss_r: 4.071e-06, lambda_0 : 5.607e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4840, Loss: 4.041e-06, Loss_0: 1.028e-10, Loss_r: 4.041e-06, lambda_0 : 5.448e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4850, Loss: 4.012e-06, Loss_0: 1.030e-10, Loss_r: 4.012e-06, lambda_0 : 5.465e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4860, Loss: 3.983e-06, Loss_0: 1.030e-10, Loss_r: 3.983e-06, lambda_0 : 5.436e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4870, Loss: 3.955e-06, Loss_0: 1.027e-10, Loss_r: 3.955e-06, lambda_0 : 5.365e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4880, Loss: 3.926e-06, Loss_0: 1.028e-10, Loss_r: 3.926e-06, lambda_0 : 5.349e+02, Time: 0.32, Learning Rate: 0.00005\n",
            "It: 4890, Loss: 3.898e-06, Loss_0: 1.028e-10, Loss_r: 3.898e-06, lambda_0 : 5.310e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4900, Loss: 3.870e-06, Loss_0: 1.026e-10, Loss_r: 3.870e-06, lambda_0 : 5.270e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4910, Loss: 3.843e-06, Loss_0: 1.026e-10, Loss_r: 3.843e-06, lambda_0 : 5.236e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4920, Loss: 3.815e-06, Loss_0: 1.025e-10, Loss_r: 3.815e-06, lambda_0 : 5.197e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4930, Loss: 3.788e-06, Loss_0: 1.024e-10, Loss_r: 3.788e-06, lambda_0 : 5.151e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4940, Loss: 3.761e-06, Loss_0: 1.024e-10, Loss_r: 3.761e-06, lambda_0 : 5.114e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4950, Loss: 3.735e-06, Loss_0: 1.023e-10, Loss_r: 3.735e-06, lambda_0 : 5.075e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4960, Loss: 3.708e-06, Loss_0: 1.022e-10, Loss_r: 3.708e-06, lambda_0 : 5.038e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4970, Loss: 3.682e-06, Loss_0: 1.021e-10, Loss_r: 3.682e-06, lambda_0 : 5.001e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4980, Loss: 3.656e-06, Loss_0: 1.020e-10, Loss_r: 3.656e-06, lambda_0 : 4.965e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "It: 4990, Loss: 3.631e-06, Loss_0: 1.019e-10, Loss_r: 3.631e-06, lambda_0 : 4.935e+02, Time: 0.31, Learning Rate: 0.00005\n",
            "Training time: 157.6746\n",
            "[1, 256, 128, 64, 32, 2]\n",
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "It: 0, Loss: 1.435e-03, Loss_0: 1.231e-03, Loss_r: 2.043e-04, lambda_0 : 8.482e+00, Time: 1.89, Learning Rate: 0.00100\n",
            "It: 10, Loss: 3.267e-04, Loss_0: 2.458e-04, Loss_r: 8.099e-05, lambda_0 : 9.453e+00, Time: 0.43, Learning Rate: 0.00100\n",
            "It: 20, Loss: 1.280e-04, Loss_0: 4.050e-05, Loss_r: 8.745e-05, lambda_0 : 3.574e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 30, Loss: 9.433e-05, Loss_0: 8.315e-06, Loss_r: 8.602e-05, lambda_0 : 4.615e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 40, Loss: 6.718e-05, Loss_0: 3.528e-06, Loss_r: 6.366e-05, lambda_0 : 1.901e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 50, Loss: 6.629e-05, Loss_0: 7.279e-06, Loss_r: 5.901e-05, lambda_0 : 3.451e+00, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 60, Loss: 6.021e-05, Loss_0: 1.404e-06, Loss_r: 5.881e-05, lambda_0 : 1.371e+00, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 70, Loss: 5.948e-05, Loss_0: 5.738e-07, Loss_r: 5.890e-05, lambda_0 : 2.347e+00, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 80, Loss: 5.902e-05, Loss_0: 1.665e-07, Loss_r: 5.885e-05, lambda_0 : 1.098e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 90, Loss: 5.868e-05, Loss_0: 3.277e-08, Loss_r: 5.864e-05, lambda_0 : 1.561e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 100, Loss: 5.861e-05, Loss_0: 7.373e-08, Loss_r: 5.853e-05, lambda_0 : 7.788e+00, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 110, Loss: 5.851e-05, Loss_0: 2.879e-09, Loss_r: 5.850e-05, lambda_0 : 2.904e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 120, Loss: 5.841e-05, Loss_0: 2.593e-08, Loss_r: 5.839e-05, lambda_0 : 1.015e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 130, Loss: 5.832e-05, Loss_0: 7.486e-09, Loss_r: 5.831e-05, lambda_0 : 1.281e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 140, Loss: 5.822e-05, Loss_0: 1.327e-08, Loss_r: 5.820e-05, lambda_0 : 1.317e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 150, Loss: 5.811e-05, Loss_0: 9.083e-09, Loss_r: 5.810e-05, lambda_0 : 1.564e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 160, Loss: 5.798e-05, Loss_0: 9.611e-09, Loss_r: 5.797e-05, lambda_0 : 1.564e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 170, Loss: 5.784e-05, Loss_0: 1.083e-08, Loss_r: 5.783e-05, lambda_0 : 1.515e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 180, Loss: 5.768e-05, Loss_0: 9.466e-09, Loss_r: 5.767e-05, lambda_0 : 1.587e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 190, Loss: 5.749e-05, Loss_0: 9.564e-09, Loss_r: 5.748e-05, lambda_0 : 1.639e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 200, Loss: 5.726e-05, Loss_0: 9.784e-09, Loss_r: 5.725e-05, lambda_0 : 1.653e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 210, Loss: 5.697e-05, Loss_0: 9.698e-09, Loss_r: 5.696e-05, lambda_0 : 1.693e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 220, Loss: 5.660e-05, Loss_0: 9.486e-09, Loss_r: 5.659e-05, lambda_0 : 1.786e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 230, Loss: 5.609e-05, Loss_0: 9.271e-09, Loss_r: 5.608e-05, lambda_0 : 2.443e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 240, Loss: 5.538e-05, Loss_0: 8.992e-09, Loss_r: 5.537e-05, lambda_0 : 3.476e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 250, Loss: 5.434e-05, Loss_0: 1.166e-08, Loss_r: 5.433e-05, lambda_0 : 3.361e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 260, Loss: 6.445e-04, Loss_0: 5.924e-04, Loss_r: 5.205e-05, lambda_0 : 1.275e-01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 270, Loss: 7.524e-05, Loss_0: 1.845e-05, Loss_r: 5.679e-05, lambda_0 : 5.970e+00, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 280, Loss: 7.696e-05, Loss_0: 2.096e-05, Loss_r: 5.600e-05, lambda_0 : 4.530e+00, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 290, Loss: 5.792e-05, Loss_0: 2.185e-06, Loss_r: 5.574e-05, lambda_0 : 1.284e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 300, Loss: 5.557e-05, Loss_0: 1.019e-06, Loss_r: 5.456e-05, lambda_0 : 9.072e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 310, Loss: 5.351e-05, Loss_0: 7.410e-07, Loss_r: 5.277e-05, lambda_0 : 1.462e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 320, Loss: 5.248e-05, Loss_0: 3.566e-07, Loss_r: 5.212e-05, lambda_0 : 6.560e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 330, Loss: 5.203e-05, Loss_0: 4.651e-08, Loss_r: 5.199e-05, lambda_0 : 4.896e+01, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 340, Loss: 5.182e-05, Loss_0: 5.457e-09, Loss_r: 5.181e-05, lambda_0 : 2.093e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 350, Loss: 5.160e-05, Loss_0: 5.030e-10, Loss_r: 5.160e-05, lambda_0 : 4.860e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 360, Loss: 5.141e-05, Loss_0: 2.025e-09, Loss_r: 5.141e-05, lambda_0 : 2.440e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 370, Loss: 5.122e-05, Loss_0: 4.782e-09, Loss_r: 5.121e-05, lambda_0 : 1.698e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 380, Loss: 5.101e-05, Loss_0: 4.976e-09, Loss_r: 5.101e-05, lambda_0 : 1.442e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 390, Loss: 5.080e-05, Loss_0: 1.529e-09, Loss_r: 5.080e-05, lambda_0 : 2.812e+02, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 400, Loss: 5.059e-05, Loss_0: 1.839e-09, Loss_r: 5.058e-05, lambda_0 : 2.721e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 410, Loss: 5.035e-05, Loss_0: 1.964e-09, Loss_r: 5.035e-05, lambda_0 : 2.484e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 420, Loss: 5.011e-05, Loss_0: 2.206e-09, Loss_r: 5.011e-05, lambda_0 : 2.424e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 430, Loss: 4.984e-05, Loss_0: 2.099e-09, Loss_r: 4.984e-05, lambda_0 : 2.500e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 440, Loss: 4.955e-05, Loss_0: 1.970e-09, Loss_r: 4.955e-05, lambda_0 : 2.508e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 450, Loss: 4.923e-05, Loss_0: 1.999e-09, Loss_r: 4.923e-05, lambda_0 : 2.557e+02, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 460, Loss: 4.887e-05, Loss_0: 1.940e-09, Loss_r: 4.887e-05, lambda_0 : 2.577e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 470, Loss: 4.845e-05, Loss_0: 3.308e-09, Loss_r: 4.844e-05, lambda_0 : 1.514e+02, Time: 0.40, Learning Rate: 0.00100\n",
            "It: 480, Loss: 2.856e-04, Loss_0: 2.358e-04, Loss_r: 4.979e-05, lambda_0 : 8.296e+00, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 490, Loss: 8.669e-05, Loss_0: 2.937e-05, Loss_r: 5.733e-05, lambda_0 : 5.203e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 500, Loss: 6.842e-05, Loss_0: 1.219e-05, Loss_r: 5.622e-05, lambda_0 : 1.438e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 510, Loss: 5.856e-05, Loss_0: 2.938e-06, Loss_r: 5.563e-05, lambda_0 : 9.027e+00, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 520, Loss: 5.552e-05, Loss_0: 8.999e-07, Loss_r: 5.462e-05, lambda_0 : 3.860e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 530, Loss: 5.398e-05, Loss_0: 2.730e-07, Loss_r: 5.371e-05, lambda_0 : 2.653e+02, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 540, Loss: 5.355e-05, Loss_0: 2.746e-07, Loss_r: 5.328e-05, lambda_0 : 5.460e+01, Time: 0.41, Learning Rate: 0.00100\n",
            "It: 550, Loss: 5.310e-05, Loss_0: 3.522e-08, Loss_r: 5.307e-05, lambda_0 : 1.499e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 560, Loss: 5.283e-05, Loss_0: 8.212e-09, Loss_r: 5.282e-05, lambda_0 : 1.224e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 570, Loss: 5.262e-05, Loss_0: 1.666e-08, Loss_r: 5.261e-05, lambda_0 : 1.208e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 580, Loss: 5.241e-05, Loss_0: 1.163e-08, Loss_r: 5.240e-05, lambda_0 : 1.180e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 590, Loss: 5.219e-05, Loss_0: 3.356e-09, Loss_r: 5.218e-05, lambda_0 : 2.450e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 600, Loss: 5.195e-05, Loss_0: 1.801e-09, Loss_r: 5.195e-05, lambda_0 : 2.927e+02, Time: 0.41, Learning Rate: 0.00090\n",
            "It: 610, Loss: 5.170e-05, Loss_0: 1.825e-09, Loss_r: 5.170e-05, lambda_0 : 2.821e+02, Time: 0.41, Learning Rate: 0.00081\n",
            "It: 620, Loss: 5.145e-05, Loss_0: 1.535e-09, Loss_r: 5.145e-05, lambda_0 : 3.479e+02, Time: 0.41, Learning Rate: 0.00081\n",
            "It: 630, Loss: 5.118e-05, Loss_0: 2.130e-09, Loss_r: 5.118e-05, lambda_0 : 2.874e+02, Time: 0.40, Learning Rate: 0.00081\n",
            "It: 640, Loss: 5.089e-05, Loss_0: 2.199e-09, Loss_r: 5.088e-05, lambda_0 : 2.805e+02, Time: 0.41, Learning Rate: 0.00081\n",
            "It: 650, Loss: 5.055e-05, Loss_0: 2.098e-09, Loss_r: 5.055e-05, lambda_0 : 2.826e+02, Time: 0.41, Learning Rate: 0.00081\n",
            "It: 660, Loss: 5.017e-05, Loss_0: 2.103e-09, Loss_r: 5.017e-05, lambda_0 : 2.783e+02, Time: 0.41, Learning Rate: 0.00081\n",
            "It: 670, Loss: 4.975e-05, Loss_0: 2.024e-09, Loss_r: 4.974e-05, lambda_0 : 2.785e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 680, Loss: 4.931e-05, Loss_0: 1.942e-09, Loss_r: 4.931e-05, lambda_0 : 2.824e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 690, Loss: 4.882e-05, Loss_0: 1.901e-09, Loss_r: 4.882e-05, lambda_0 : 2.824e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 700, Loss: 4.826e-05, Loss_0: 1.819e-09, Loss_r: 4.826e-05, lambda_0 : 2.881e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 710, Loss: 4.764e-05, Loss_0: 1.720e-09, Loss_r: 4.764e-05, lambda_0 : 2.963e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 720, Loss: 4.694e-05, Loss_0: 1.616e-09, Loss_r: 4.694e-05, lambda_0 : 3.063e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 730, Loss: 4.617e-05, Loss_0: 1.497e-09, Loss_r: 4.617e-05, lambda_0 : 3.177e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 740, Loss: 4.533e-05, Loss_0: 1.368e-09, Loss_r: 4.532e-05, lambda_0 : 3.314e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 750, Loss: 4.441e-05, Loss_0: 1.181e-09, Loss_r: 4.441e-05, lambda_0 : 3.138e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 760, Loss: 2.944e-04, Loss_0: 2.423e-04, Loss_r: 5.216e-05, lambda_0 : 3.644e+01, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 770, Loss: 1.408e-04, Loss_0: 8.086e-05, Loss_r: 5.994e-05, lambda_0 : 9.875e+01, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 780, Loss: 7.107e-05, Loss_0: 1.428e-05, Loss_r: 5.679e-05, lambda_0 : 2.181e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 790, Loss: 5.534e-05, Loss_0: 1.085e-06, Loss_r: 5.425e-05, lambda_0 : 6.319e+02, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 800, Loss: 5.646e-05, Loss_0: 3.373e-06, Loss_r: 5.309e-05, lambda_0 : 1.041e+01, Time: 0.41, Learning Rate: 0.00073\n",
            "It: 810, Loss: 5.303e-05, Loss_0: 1.131e-07, Loss_r: 5.291e-05, lambda_0 : 4.696e+02, Time: 0.41, Learning Rate: 0.00066\n",
            "It: 820, Loss: 5.297e-05, Loss_0: 1.803e-07, Loss_r: 5.279e-05, lambda_0 : 3.941e+02, Time: 0.41, Learning Rate: 0.00066\n",
            "It: 830, Loss: 5.276e-05, Loss_0: 1.302e-07, Loss_r: 5.263e-05, lambda_0 : 9.330e+01, Time: 0.41, Learning Rate: 0.00066\n",
            "It: 840, Loss: 5.254e-05, Loss_0: 1.288e-08, Loss_r: 5.253e-05, lambda_0 : 3.073e+02, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 850, Loss: 5.243e-05, Loss_0: 4.339e-09, Loss_r: 5.242e-05, lambda_0 : 3.750e+02, Time: 0.42, Learning Rate: 0.00066\n",
            "It: 860, Loss: 5.232e-05, Loss_0: 7.212e-09, Loss_r: 5.231e-05, lambda_0 : 2.260e+02, Time: 0.41, Learning Rate: 0.00066\n",
            "It: 870, Loss: 5.220e-05, Loss_0: 2.280e-09, Loss_r: 5.220e-05, lambda_0 : 3.862e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 880, Loss: 5.210e-05, Loss_0: 1.841e-09, Loss_r: 5.209e-05, lambda_0 : 4.802e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 890, Loss: 5.199e-05, Loss_0: 2.373e-09, Loss_r: 5.199e-05, lambda_0 : 4.013e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 900, Loss: 5.188e-05, Loss_0: 1.953e-09, Loss_r: 5.187e-05, lambda_0 : 4.317e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 910, Loss: 5.176e-05, Loss_0: 2.016e-09, Loss_r: 5.176e-05, lambda_0 : 4.549e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 920, Loss: 5.164e-05, Loss_0: 1.804e-09, Loss_r: 5.164e-05, lambda_0 : 5.038e+02, Time: 0.41, Learning Rate: 0.00059\n",
            "It: 930, Loss: 5.151e-05, Loss_0: 1.789e-09, Loss_r: 5.151e-05, lambda_0 : 4.949e+02, Time: 0.40, Learning Rate: 0.00053\n",
            "It: 940, Loss: 5.139e-05, Loss_0: 1.798e-09, Loss_r: 5.139e-05, lambda_0 : 4.866e+02, Time: 0.40, Learning Rate: 0.00053\n",
            "It: 950, Loss: 5.126e-05, Loss_0: 1.770e-09, Loss_r: 5.126e-05, lambda_0 : 4.752e+02, Time: 0.41, Learning Rate: 0.00053\n",
            "It: 960, Loss: 5.112e-05, Loss_0: 1.779e-09, Loss_r: 5.112e-05, lambda_0 : 4.639e+02, Time: 0.40, Learning Rate: 0.00053\n",
            "It: 970, Loss: 5.098e-05, Loss_0: 1.745e-09, Loss_r: 5.098e-05, lambda_0 : 4.612e+02, Time: 0.41, Learning Rate: 0.00053\n",
            "It: 980, Loss: 5.082e-05, Loss_0: 1.726e-09, Loss_r: 5.082e-05, lambda_0 : 4.583e+02, Time: 0.41, Learning Rate: 0.00053\n",
            "It: 990, Loss: 5.065e-05, Loss_0: 1.699e-09, Loss_r: 5.065e-05, lambda_0 : 4.563e+02, Time: 0.41, Learning Rate: 0.00048\n",
            "It: 1000, Loss: 5.049e-05, Loss_0: 1.678e-09, Loss_r: 5.049e-05, lambda_0 : 4.533e+02, Time: 0.41, Learning Rate: 0.00048\n",
            "It: 1010, Loss: 5.032e-05, Loss_0: 1.658e-09, Loss_r: 5.031e-05, lambda_0 : 4.505e+02, Time: 0.40, Learning Rate: 0.00048\n",
            "It: 1020, Loss: 5.012e-05, Loss_0: 1.635e-09, Loss_r: 5.012e-05, lambda_0 : 4.484e+02, Time: 0.40, Learning Rate: 0.00048\n",
            "It: 1030, Loss: 4.991e-05, Loss_0: 1.611e-09, Loss_r: 4.991e-05, lambda_0 : 4.471e+02, Time: 0.41, Learning Rate: 0.00048\n",
            "It: 1040, Loss: 4.968e-05, Loss_0: 1.584e-09, Loss_r: 4.968e-05, lambda_0 : 4.466e+02, Time: 0.41, Learning Rate: 0.00048\n",
            "It: 1050, Loss: 4.943e-05, Loss_0: 1.555e-09, Loss_r: 4.943e-05, lambda_0 : 4.460e+02, Time: 0.40, Learning Rate: 0.00043\n",
            "It: 1060, Loss: 4.919e-05, Loss_0: 1.526e-09, Loss_r: 4.919e-05, lambda_0 : 4.458e+02, Time: 0.40, Learning Rate: 0.00043\n",
            "It: 1070, Loss: 4.892e-05, Loss_0: 1.491e-09, Loss_r: 4.892e-05, lambda_0 : 4.469e+02, Time: 0.40, Learning Rate: 0.00043\n",
            "It: 1080, Loss: 4.863e-05, Loss_0: 1.453e-09, Loss_r: 4.862e-05, lambda_0 : 4.489e+02, Time: 0.40, Learning Rate: 0.00043\n",
            "It: 1090, Loss: 4.831e-05, Loss_0: 1.409e-09, Loss_r: 4.831e-05, lambda_0 : 4.518e+02, Time: 0.40, Learning Rate: 0.00043\n",
            "It: 1100, Loss: 4.797e-05, Loss_0: 1.361e-09, Loss_r: 4.797e-05, lambda_0 : 4.565e+02, Time: 0.41, Learning Rate: 0.00043\n",
            "It: 1110, Loss: 4.760e-05, Loss_0: 1.306e-09, Loss_r: 4.760e-05, lambda_0 : 4.635e+02, Time: 0.40, Learning Rate: 0.00039\n",
            "It: 1120, Loss: 4.725e-05, Loss_0: 1.253e-09, Loss_r: 4.725e-05, lambda_0 : 4.720e+02, Time: 0.40, Learning Rate: 0.00039\n",
            "It: 1130, Loss: 4.688e-05, Loss_0: 1.195e-09, Loss_r: 4.688e-05, lambda_0 : 4.825e+02, Time: 0.41, Learning Rate: 0.00039\n",
            "It: 1140, Loss: 4.649e-05, Loss_0: 1.135e-09, Loss_r: 4.649e-05, lambda_0 : 4.951e+02, Time: 0.42, Learning Rate: 0.00039\n",
            "It: 1150, Loss: 4.608e-05, Loss_0: 1.074e-09, Loss_r: 4.608e-05, lambda_0 : 5.097e+02, Time: 0.42, Learning Rate: 0.00039\n",
            "It: 1160, Loss: 4.566e-05, Loss_0: 1.011e-09, Loss_r: 4.566e-05, lambda_0 : 5.263e+02, Time: 0.41, Learning Rate: 0.00039\n",
            "It: 1170, Loss: 4.522e-05, Loss_0: 9.476e-10, Loss_r: 4.522e-05, lambda_0 : 5.444e+02, Time: 0.41, Learning Rate: 0.00035\n",
            "It: 1180, Loss: 4.482e-05, Loss_0: 8.900e-10, Loss_r: 4.482e-05, lambda_0 : 5.627e+02, Time: 0.42, Learning Rate: 0.00035\n",
            "It: 1190, Loss: 4.441e-05, Loss_0: 8.336e-10, Loss_r: 4.441e-05, lambda_0 : 5.814e+02, Time: 0.41, Learning Rate: 0.00035\n",
            "It: 1200, Loss: 4.400e-05, Loss_0: 7.792e-10, Loss_r: 4.400e-05, lambda_0 : 6.012e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1210, Loss: 4.358e-05, Loss_0: 7.269e-10, Loss_r: 4.358e-05, lambda_0 : 6.220e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1220, Loss: 4.316e-05, Loss_0: 6.769e-10, Loss_r: 4.316e-05, lambda_0 : 6.440e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1230, Loss: 4.274e-05, Loss_0: 6.300e-10, Loss_r: 4.274e-05, lambda_0 : 6.671e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1240, Loss: 4.231e-05, Loss_0: 5.864e-10, Loss_r: 4.231e-05, lambda_0 : 6.915e+02, Time: 0.41, Learning Rate: 0.00035\n",
            "It: 1250, Loss: 4.188e-05, Loss_0: 5.464e-10, Loss_r: 4.188e-05, lambda_0 : 7.171e+02, Time: 0.41, Learning Rate: 0.00035\n",
            "It: 1260, Loss: 4.145e-05, Loss_0: 5.101e-10, Loss_r: 4.145e-05, lambda_0 : 7.431e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1270, Loss: 4.100e-05, Loss_0: 4.778e-10, Loss_r: 4.100e-05, lambda_0 : 7.690e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1280, Loss: 4.055e-05, Loss_0: 4.500e-10, Loss_r: 4.055e-05, lambda_0 : 7.937e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1290, Loss: 4.010e-05, Loss_0: 2.542e-09, Loss_r: 4.010e-05, lambda_0 : 2.485e+02, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1300, Loss: 1.132e-04, Loss_0: 7.227e-05, Loss_r: 4.092e-05, lambda_0 : 7.386e+00, Time: 0.41, Learning Rate: 0.00035\n",
            "It: 1310, Loss: 5.554e-05, Loss_0: 1.064e-05, Loss_r: 4.491e-05, lambda_0 : 6.434e+01, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1320, Loss: 4.982e-05, Loss_0: 3.851e-06, Loss_r: 4.597e-05, lambda_0 : 8.794e+01, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1330, Loss: 4.922e-05, Loss_0: 2.802e-06, Loss_r: 4.642e-05, lambda_0 : 1.081e+01, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1340, Loss: 4.710e-05, Loss_0: 8.744e-07, Loss_r: 4.622e-05, lambda_0 : 4.742e+01, Time: 0.40, Learning Rate: 0.00035\n",
            "It: 1350, Loss: 4.611e-05, Loss_0: 6.397e-08, Loss_r: 4.604e-05, lambda_0 : 6.153e+01, Time: 0.41, Learning Rate: 0.00031\n",
            "It: 1360, Loss: 4.582e-05, Loss_0: 3.803e-08, Loss_r: 4.578e-05, lambda_0 : 8.127e+01, Time: 0.40, Learning Rate: 0.00031\n",
            "It: 1370, Loss: 4.553e-05, Loss_0: 5.676e-08, Loss_r: 4.547e-05, lambda_0 : 7.115e+01, Time: 0.40, Learning Rate: 0.00031\n",
            "It: 1380, Loss: 4.520e-05, Loss_0: 5.978e-09, Loss_r: 4.519e-05, lambda_0 : 2.378e+02, Time: 0.40, Learning Rate: 0.00031\n",
            "It: 1390, Loss: 4.489e-05, Loss_0: 1.088e-09, Loss_r: 4.489e-05, lambda_0 : 4.982e+02, Time: 0.41, Learning Rate: 0.00031\n",
            "It: 1400, Loss: 4.459e-05, Loss_0: 1.004e-09, Loss_r: 4.459e-05, lambda_0 : 4.604e+02, Time: 0.40, Learning Rate: 0.00031\n",
            "It: 1410, Loss: 4.429e-05, Loss_0: 1.023e-10, Loss_r: 4.429e-05, lambda_0 : 1.658e+03, Time: 0.41, Learning Rate: 0.00028\n",
            "It: 1420, Loss: 4.401e-05, Loss_0: 9.916e-10, Loss_r: 4.401e-05, lambda_0 : 5.082e+02, Time: 0.40, Learning Rate: 0.00028\n",
            "It: 1430, Loss: 4.373e-05, Loss_0: 4.187e-10, Loss_r: 4.373e-05, lambda_0 : 8.180e+02, Time: 0.40, Learning Rate: 0.00028\n",
            "It: 1440, Loss: 4.344e-05, Loss_0: 7.090e-10, Loss_r: 4.344e-05, lambda_0 : 6.811e+02, Time: 0.42, Learning Rate: 0.00028\n",
            "It: 1450, Loss: 4.315e-05, Loss_0: 4.607e-10, Loss_r: 4.315e-05, lambda_0 : 9.142e+02, Time: 0.41, Learning Rate: 0.00028\n",
            "It: 1460, Loss: 4.286e-05, Loss_0: 4.501e-10, Loss_r: 4.286e-05, lambda_0 : 9.376e+02, Time: 0.41, Learning Rate: 0.00028\n",
            "It: 1470, Loss: 4.256e-05, Loss_0: 4.463e-10, Loss_r: 4.256e-05, lambda_0 : 9.136e+02, Time: 0.41, Learning Rate: 0.00025\n",
            "It: 1480, Loss: 4.228e-05, Loss_0: 4.044e-10, Loss_r: 4.228e-05, lambda_0 : 1.026e+03, Time: 0.41, Learning Rate: 0.00025\n",
            "It: 1490, Loss: 4.200e-05, Loss_0: 4.080e-10, Loss_r: 4.200e-05, lambda_0 : 1.008e+03, Time: 0.41, Learning Rate: 0.00025\n",
            "It: 1500, Loss: 4.171e-05, Loss_0: 4.019e-10, Loss_r: 4.171e-05, lambda_0 : 1.015e+03, Time: 0.40, Learning Rate: 0.00025\n",
            "It: 1510, Loss: 4.141e-05, Loss_0: 3.870e-10, Loss_r: 4.141e-05, lambda_0 : 1.051e+03, Time: 0.40, Learning Rate: 0.00025\n",
            "It: 1520, Loss: 4.111e-05, Loss_0: 3.743e-10, Loss_r: 4.111e-05, lambda_0 : 1.079e+03, Time: 0.40, Learning Rate: 0.00025\n",
            "It: 1530, Loss: 4.080e-05, Loss_0: 3.640e-10, Loss_r: 4.079e-05, lambda_0 : 1.102e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1540, Loss: 4.051e-05, Loss_0: 3.561e-10, Loss_r: 4.051e-05, lambda_0 : 1.120e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1550, Loss: 4.021e-05, Loss_0: 3.491e-10, Loss_r: 4.021e-05, lambda_0 : 1.137e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1560, Loss: 3.991e-05, Loss_0: 3.425e-10, Loss_r: 3.991e-05, lambda_0 : 1.154e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1570, Loss: 3.960e-05, Loss_0: 3.364e-10, Loss_r: 3.960e-05, lambda_0 : 1.170e+03, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1580, Loss: 3.929e-05, Loss_0: 3.307e-10, Loss_r: 3.929e-05, lambda_0 : 1.186e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1590, Loss: 3.897e-05, Loss_0: 3.253e-10, Loss_r: 3.896e-05, lambda_0 : 1.201e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1600, Loss: 3.864e-05, Loss_0: 3.201e-10, Loss_r: 3.864e-05, lambda_0 : 1.217e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1610, Loss: 3.830e-05, Loss_0: 3.153e-10, Loss_r: 3.830e-05, lambda_0 : 1.231e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1620, Loss: 3.796e-05, Loss_0: 3.106e-10, Loss_r: 3.796e-05, lambda_0 : 1.244e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1630, Loss: 3.762e-05, Loss_0: 3.060e-10, Loss_r: 3.762e-05, lambda_0 : 1.250e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1640, Loss: 3.727e-05, Loss_0: 3.013e-10, Loss_r: 3.727e-05, lambda_0 : 1.255e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1650, Loss: 3.692e-05, Loss_0: 2.967e-10, Loss_r: 3.692e-05, lambda_0 : 1.260e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1660, Loss: 3.656e-05, Loss_0: 2.919e-10, Loss_r: 3.656e-05, lambda_0 : 1.265e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1670, Loss: 3.620e-05, Loss_0: 2.870e-10, Loss_r: 3.620e-05, lambda_0 : 1.270e+03, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1680, Loss: 3.583e-05, Loss_0: 2.818e-10, Loss_r: 3.583e-05, lambda_0 : 1.274e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1690, Loss: 3.546e-05, Loss_0: 2.764e-10, Loss_r: 3.546e-05, lambda_0 : 1.279e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1700, Loss: 3.509e-05, Loss_0: 2.708e-10, Loss_r: 3.509e-05, lambda_0 : 1.285e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1710, Loss: 3.472e-05, Loss_0: 2.649e-10, Loss_r: 3.472e-05, lambda_0 : 1.290e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1720, Loss: 3.434e-05, Loss_0: 2.591e-10, Loss_r: 3.434e-05, lambda_0 : 1.296e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1730, Loss: 3.397e-05, Loss_0: 2.659e-10, Loss_r: 3.397e-05, lambda_0 : 1.218e+03, Time: 0.40, Learning Rate: 0.00023\n",
            "It: 1740, Loss: 3.584e-05, Loss_0: 2.256e-06, Loss_r: 3.358e-05, lambda_0 : 1.013e+01, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1750, Loss: 4.778e-05, Loss_0: 1.165e-05, Loss_r: 3.613e-05, lambda_0 : 2.973e+01, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1760, Loss: 5.307e-05, Loss_0: 1.435e-05, Loss_r: 3.873e-05, lambda_0 : 1.174e+01, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1770, Loss: 4.110e-05, Loss_0: 1.353e-06, Loss_r: 3.974e-05, lambda_0 : 4.773e+01, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1780, Loss: 4.107e-05, Loss_0: 9.750e-07, Loss_r: 4.010e-05, lambda_0 : 5.030e+01, Time: 0.41, Learning Rate: 0.00023\n",
            "It: 1790, Loss: 4.066e-05, Loss_0: 6.822e-07, Loss_r: 3.998e-05, lambda_0 : 3.236e+01, Time: 0.41, Learning Rate: 0.00021\n",
            "It: 1800, Loss: 4.008e-05, Loss_0: 3.057e-07, Loss_r: 3.977e-05, lambda_0 : 3.524e+01, Time: 0.41, Learning Rate: 0.00021\n",
            "It: 1810, Loss: 3.954e-05, Loss_0: 7.026e-08, Loss_r: 3.947e-05, lambda_0 : 6.094e+01, Time: 0.40, Learning Rate: 0.00021\n",
            "It: 1820, Loss: 3.917e-05, Loss_0: 9.964e-09, Loss_r: 3.916e-05, lambda_0 : 2.075e+02, Time: 0.40, Learning Rate: 0.00021\n",
            "It: 1830, Loss: 3.883e-05, Loss_0: 4.626e-09, Loss_r: 3.883e-05, lambda_0 : 2.375e+02, Time: 0.40, Learning Rate: 0.00021\n",
            "It: 1840, Loss: 3.850e-05, Loss_0: 2.083e-09, Loss_r: 3.850e-05, lambda_0 : 4.580e+02, Time: 0.40, Learning Rate: 0.00021\n",
            "It: 1850, Loss: 3.817e-05, Loss_0: 2.667e-09, Loss_r: 3.817e-05, lambda_0 : 3.202e+02, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 1860, Loss: 3.787e-05, Loss_0: 7.227e-10, Loss_r: 3.786e-05, lambda_0 : 6.674e+02, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 1870, Loss: 3.756e-05, Loss_0: 7.226e-10, Loss_r: 3.756e-05, lambda_0 : 6.304e+02, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 1880, Loss: 3.725e-05, Loss_0: 2.958e-10, Loss_r: 3.725e-05, lambda_0 : 1.161e+03, Time: 0.41, Learning Rate: 0.00019\n",
            "It: 1890, Loss: 3.694e-05, Loss_0: 2.861e-10, Loss_r: 3.694e-05, lambda_0 : 1.168e+03, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 1900, Loss: 3.663e-05, Loss_0: 3.412e-10, Loss_r: 3.663e-05, lambda_0 : 1.076e+03, Time: 0.40, Learning Rate: 0.00019\n",
            "It: 1910, Loss: 3.631e-05, Loss_0: 3.080e-10, Loss_r: 3.631e-05, lambda_0 : 1.200e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1920, Loss: 3.603e-05, Loss_0: 2.784e-10, Loss_r: 3.603e-05, lambda_0 : 1.292e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1930, Loss: 3.574e-05, Loss_0: 2.769e-10, Loss_r: 3.574e-05, lambda_0 : 1.325e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1940, Loss: 3.545e-05, Loss_0: 2.762e-10, Loss_r: 3.545e-05, lambda_0 : 1.325e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1950, Loss: 3.516e-05, Loss_0: 2.705e-10, Loss_r: 3.516e-05, lambda_0 : 1.335e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1960, Loss: 3.487e-05, Loss_0: 2.637e-10, Loss_r: 3.487e-05, lambda_0 : 1.358e+03, Time: 0.40, Learning Rate: 0.00017\n",
            "It: 1970, Loss: 3.458e-05, Loss_0: 2.576e-10, Loss_r: 3.458e-05, lambda_0 : 1.376e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 1980, Loss: 3.431e-05, Loss_0: 2.523e-10, Loss_r: 3.431e-05, lambda_0 : 1.383e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 1990, Loss: 3.405e-05, Loss_0: 2.473e-10, Loss_r: 3.405e-05, lambda_0 : 1.389e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2000, Loss: 3.378e-05, Loss_0: 2.424e-10, Loss_r: 3.378e-05, lambda_0 : 1.396e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2010, Loss: 3.351e-05, Loss_0: 2.377e-10, Loss_r: 3.351e-05, lambda_0 : 1.403e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2020, Loss: 3.325e-05, Loss_0: 2.330e-10, Loss_r: 3.325e-05, lambda_0 : 1.410e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2030, Loss: 3.298e-05, Loss_0: 2.284e-10, Loss_r: 3.298e-05, lambda_0 : 1.417e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2040, Loss: 3.271e-05, Loss_0: 2.238e-10, Loss_r: 3.271e-05, lambda_0 : 1.424e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2050, Loss: 3.244e-05, Loss_0: 2.193e-10, Loss_r: 3.244e-05, lambda_0 : 1.431e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2060, Loss: 3.217e-05, Loss_0: 2.149e-10, Loss_r: 3.217e-05, lambda_0 : 1.438e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2070, Loss: 3.191e-05, Loss_0: 2.105e-10, Loss_r: 3.191e-05, lambda_0 : 1.445e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2080, Loss: 3.164e-05, Loss_0: 2.062e-10, Loss_r: 3.164e-05, lambda_0 : 1.453e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2090, Loss: 3.137e-05, Loss_0: 2.020e-10, Loss_r: 3.137e-05, lambda_0 : 1.460e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2100, Loss: 3.110e-05, Loss_0: 1.979e-10, Loss_r: 3.110e-05, lambda_0 : 1.467e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2110, Loss: 3.083e-05, Loss_0: 1.938e-10, Loss_r: 3.083e-05, lambda_0 : 1.473e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2120, Loss: 3.057e-05, Loss_0: 1.899e-10, Loss_r: 3.057e-05, lambda_0 : 1.480e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2130, Loss: 3.030e-05, Loss_0: 1.860e-10, Loss_r: 3.030e-05, lambda_0 : 1.487e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2140, Loss: 3.004e-05, Loss_0: 1.822e-10, Loss_r: 3.004e-05, lambda_0 : 1.493e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2150, Loss: 2.977e-05, Loss_0: 1.786e-10, Loss_r: 2.977e-05, lambda_0 : 1.500e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2160, Loss: 2.951e-05, Loss_0: 1.751e-10, Loss_r: 2.951e-05, lambda_0 : 1.504e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2170, Loss: 2.925e-05, Loss_0: 1.716e-10, Loss_r: 2.925e-05, lambda_0 : 1.507e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2180, Loss: 2.898e-05, Loss_0: 1.683e-10, Loss_r: 2.898e-05, lambda_0 : 1.510e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2190, Loss: 2.872e-05, Loss_0: 1.650e-10, Loss_r: 2.872e-05, lambda_0 : 1.513e+03, Time: 0.41, Learning Rate: 0.00015\n",
            "It: 2200, Loss: 2.846e-05, Loss_0: 1.618e-10, Loss_r: 2.846e-05, lambda_0 : 1.516e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2210, Loss: 2.821e-05, Loss_0: 1.587e-10, Loss_r: 2.821e-05, lambda_0 : 1.518e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2220, Loss: 2.795e-05, Loss_0: 1.558e-10, Loss_r: 2.795e-05, lambda_0 : 1.521e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2230, Loss: 2.769e-05, Loss_0: 1.528e-10, Loss_r: 2.769e-05, lambda_0 : 1.521e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2240, Loss: 2.744e-05, Loss_0: 1.495e-10, Loss_r: 2.744e-05, lambda_0 : 1.513e+03, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2250, Loss: 2.719e-05, Loss_0: 2.870e-10, Loss_r: 2.719e-05, lambda_0 : 8.375e+02, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2260, Loss: 5.730e-05, Loss_0: 3.034e-05, Loss_r: 2.696e-05, lambda_0 : 2.991e+00, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2270, Loss: 3.664e-05, Loss_0: 9.179e-06, Loss_r: 2.746e-05, lambda_0 : 2.502e+01, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2280, Loss: 3.663e-05, Loss_0: 8.747e-06, Loss_r: 2.788e-05, lambda_0 : 2.084e+01, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2290, Loss: 3.064e-05, Loss_0: 2.668e-06, Loss_r: 2.797e-05, lambda_0 : 1.350e+01, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2300, Loss: 2.881e-05, Loss_0: 9.437e-07, Loss_r: 2.787e-05, lambda_0 : 1.821e+01, Time: 0.40, Learning Rate: 0.00015\n",
            "It: 2310, Loss: 2.791e-05, Loss_0: 2.375e-07, Loss_r: 2.767e-05, lambda_0 : 3.569e+01, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2320, Loss: 2.759e-05, Loss_0: 1.307e-07, Loss_r: 2.746e-05, lambda_0 : 4.604e+01, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2330, Loss: 2.728e-05, Loss_0: 4.499e-08, Loss_r: 2.723e-05, lambda_0 : 7.977e+01, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2340, Loss: 2.702e-05, Loss_0: 1.754e-08, Loss_r: 2.700e-05, lambda_0 : 1.322e+02, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2350, Loss: 2.677e-05, Loss_0: 1.921e-09, Loss_r: 2.676e-05, lambda_0 : 3.955e+02, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2360, Loss: 2.653e-05, Loss_0: 2.004e-09, Loss_r: 2.653e-05, lambda_0 : 3.178e+02, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2370, Loss: 2.630e-05, Loss_0: 1.382e-10, Loss_r: 2.630e-05, lambda_0 : 1.474e+03, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2380, Loss: 2.607e-05, Loss_0: 1.388e-10, Loss_r: 2.607e-05, lambda_0 : 1.315e+03, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2390, Loss: 2.584e-05, Loss_0: 1.419e-10, Loss_r: 2.584e-05, lambda_0 : 1.238e+03, Time: 0.42, Learning Rate: 0.00014\n",
            "It: 2400, Loss: 2.561e-05, Loss_0: 2.094e-10, Loss_r: 2.561e-05, lambda_0 : 9.797e+02, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2410, Loss: 2.538e-05, Loss_0: 1.648e-10, Loss_r: 2.538e-05, lambda_0 : 1.177e+03, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2420, Loss: 2.516e-05, Loss_0: 1.500e-10, Loss_r: 2.516e-05, lambda_0 : 1.282e+03, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2430, Loss: 2.494e-05, Loss_0: 1.260e-10, Loss_r: 2.494e-05, lambda_0 : 1.486e+03, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2440, Loss: 2.472e-05, Loss_0: 1.415e-10, Loss_r: 2.472e-05, lambda_0 : 1.328e+03, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2450, Loss: 2.450e-05, Loss_0: 1.001e-09, Loss_r: 2.450e-05, lambda_0 : 4.146e+02, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2460, Loss: 3.086e-05, Loss_0: 6.586e-06, Loss_r: 2.427e-05, lambda_0 : 4.987e+00, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2470, Loss: 3.997e-05, Loss_0: 1.555e-05, Loss_r: 2.442e-05, lambda_0 : 1.062e+01, Time: 0.41, Learning Rate: 0.00014\n",
            "It: 2480, Loss: 3.113e-05, Loss_0: 6.555e-06, Loss_r: 2.457e-05, lambda_0 : 6.620e+00, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2490, Loss: 2.632e-05, Loss_0: 1.751e-06, Loss_r: 2.457e-05, lambda_0 : 1.699e+01, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2500, Loss: 2.506e-05, Loss_0: 6.257e-07, Loss_r: 2.444e-05, lambda_0 : 1.735e+01, Time: 0.40, Learning Rate: 0.00014\n",
            "It: 2510, Loss: 2.430e-05, Loss_0: 4.420e-08, Loss_r: 2.426e-05, lambda_0 : 6.732e+01, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2520, Loss: 2.408e-05, Loss_0: 8.157e-09, Loss_r: 2.408e-05, lambda_0 : 1.777e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2530, Loss: 2.390e-05, Loss_0: 1.625e-08, Loss_r: 2.388e-05, lambda_0 : 1.116e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2540, Loss: 2.370e-05, Loss_0: 7.859e-09, Loss_r: 2.369e-05, lambda_0 : 1.796e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2550, Loss: 2.350e-05, Loss_0: 3.236e-09, Loss_r: 2.349e-05, lambda_0 : 2.868e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2560, Loss: 2.330e-05, Loss_0: 1.251e-09, Loss_r: 2.330e-05, lambda_0 : 4.276e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2570, Loss: 2.311e-05, Loss_0: 6.894e-10, Loss_r: 2.311e-05, lambda_0 : 5.362e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2580, Loss: 2.291e-05, Loss_0: 6.303e-10, Loss_r: 2.291e-05, lambda_0 : 4.936e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2590, Loss: 2.272e-05, Loss_0: 1.156e-10, Loss_r: 2.272e-05, lambda_0 : 1.270e+03, Time: 0.41, Learning Rate: 0.00012\n",
            "It: 2600, Loss: 2.254e-05, Loss_0: 9.519e-09, Loss_r: 2.253e-05, lambda_0 : 1.554e+02, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2610, Loss: 8.496e-05, Loss_0: 6.257e-05, Loss_r: 2.239e-05, lambda_0 : 1.870e+00, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2620, Loss: 2.404e-05, Loss_0: 1.417e-06, Loss_r: 2.262e-05, lambda_0 : 3.170e+01, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2630, Loss: 2.712e-05, Loss_0: 4.341e-06, Loss_r: 2.278e-05, lambda_0 : 9.654e+00, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2640, Loss: 2.356e-05, Loss_0: 7.631e-07, Loss_r: 2.279e-05, lambda_0 : 1.839e+01, Time: 0.40, Learning Rate: 0.00012\n",
            "It: 2650, Loss: 2.281e-05, Loss_0: 1.231e-07, Loss_r: 2.269e-05, lambda_0 : 4.112e+01, Time: 0.41, Learning Rate: 0.00012\n",
            "It: 2660, Loss: 2.263e-05, Loss_0: 9.249e-08, Loss_r: 2.253e-05, lambda_0 : 4.855e+01, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2670, Loss: 2.241e-05, Loss_0: 3.129e-08, Loss_r: 2.237e-05, lambda_0 : 8.414e+01, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2680, Loss: 2.224e-05, Loss_0: 3.418e-08, Loss_r: 2.221e-05, lambda_0 : 7.313e+01, Time: 0.42, Learning Rate: 0.00011\n",
            "It: 2690, Loss: 2.205e-05, Loss_0: 1.282e-08, Loss_r: 2.204e-05, lambda_0 : 1.319e+02, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2700, Loss: 2.187e-05, Loss_0: 4.537e-09, Loss_r: 2.187e-05, lambda_0 : 1.918e+02, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2710, Loss: 2.170e-05, Loss_0: 1.277e-10, Loss_r: 2.170e-05, lambda_0 : 1.112e+03, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2720, Loss: 2.153e-05, Loss_0: 7.680e-10, Loss_r: 2.153e-05, lambda_0 : 4.927e+02, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2730, Loss: 2.136e-05, Loss_0: 2.992e-10, Loss_r: 2.136e-05, lambda_0 : 7.152e+02, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2740, Loss: 2.119e-05, Loss_0: 1.980e-10, Loss_r: 2.119e-05, lambda_0 : 8.533e+02, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2750, Loss: 2.103e-05, Loss_0: 1.005e-10, Loss_r: 2.103e-05, lambda_0 : 1.434e+03, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2760, Loss: 2.086e-05, Loss_0: 1.111e-10, Loss_r: 2.086e-05, lambda_0 : 1.360e+03, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2770, Loss: 2.070e-05, Loss_0: 2.141e-10, Loss_r: 2.070e-05, lambda_0 : 7.943e+02, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2780, Loss: 2.056e-05, Loss_0: 2.054e-08, Loss_r: 2.054e-05, lambda_0 : 8.871e+01, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2790, Loss: 9.759e-05, Loss_0: 7.720e-05, Loss_r: 2.039e-05, lambda_0 : 9.343e-01, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2800, Loss: 2.441e-05, Loss_0: 3.751e-06, Loss_r: 2.066e-05, lambda_0 : 1.289e+01, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2810, Loss: 2.635e-05, Loss_0: 5.542e-06, Loss_r: 2.080e-05, lambda_0 : 5.531e+00, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2820, Loss: 2.084e-05, Loss_0: 2.241e-08, Loss_r: 2.082e-05, lambda_0 : 1.017e+02, Time: 0.41, Learning Rate: 0.00011\n",
            "It: 2830, Loss: 2.077e-05, Loss_0: 4.226e-08, Loss_r: 2.073e-05, lambda_0 : 6.860e+01, Time: 0.40, Learning Rate: 0.00011\n",
            "It: 2840, Loss: 2.061e-05, Loss_0: 1.276e-08, Loss_r: 2.060e-05, lambda_0 : 1.117e+02, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2850, Loss: 2.055e-05, Loss_0: 9.119e-08, Loss_r: 2.046e-05, lambda_0 : 4.545e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2860, Loss: 2.032e-05, Loss_0: 6.556e-09, Loss_r: 2.032e-05, lambda_0 : 1.540e+02, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2870, Loss: 2.017e-05, Loss_0: 2.466e-10, Loss_r: 2.017e-05, lambda_0 : 7.224e+02, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2880, Loss: 2.002e-05, Loss_0: 1.529e-10, Loss_r: 2.002e-05, lambda_0 : 9.349e+02, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2890, Loss: 1.988e-05, Loss_0: 3.275e-10, Loss_r: 1.988e-05, lambda_0 : 6.604e+02, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2900, Loss: 1.973e-05, Loss_0: 8.187e-10, Loss_r: 1.973e-05, lambda_0 : 3.953e+02, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2910, Loss: 1.959e-05, Loss_0: 1.055e-10, Loss_r: 1.959e-05, lambda_0 : 1.342e+03, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2920, Loss: 1.944e-05, Loss_0: 1.096e-10, Loss_r: 1.944e-05, lambda_0 : 1.121e+03, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2930, Loss: 1.930e-05, Loss_0: 9.492e-11, Loss_r: 1.930e-05, lambda_0 : 1.306e+03, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2940, Loss: 1.916e-05, Loss_0: 1.046e-10, Loss_r: 1.916e-05, lambda_0 : 1.301e+03, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 2950, Loss: 1.902e-05, Loss_0: 9.486e-11, Loss_r: 1.902e-05, lambda_0 : 1.527e+03, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2960, Loss: 1.888e-05, Loss_0: 9.306e-11, Loss_r: 1.888e-05, lambda_0 : 1.573e+03, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2970, Loss: 1.874e-05, Loss_0: 9.400e-11, Loss_r: 1.874e-05, lambda_0 : 1.246e+03, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2980, Loss: 1.860e-05, Loss_0: 4.754e-09, Loss_r: 1.860e-05, lambda_0 : 1.948e+02, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 2990, Loss: 4.887e-05, Loss_0: 3.040e-05, Loss_r: 1.848e-05, lambda_0 : 2.229e+00, Time: 0.42, Learning Rate: 0.00010\n",
            "It: 3000, Loss: 2.607e-05, Loss_0: 7.520e-06, Loss_r: 1.855e-05, lambda_0 : 9.855e+00, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3010, Loss: 2.053e-05, Loss_0: 1.944e-06, Loss_r: 1.858e-05, lambda_0 : 1.502e+01, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3020, Loss: 1.862e-05, Loss_0: 8.109e-08, Loss_r: 1.853e-05, lambda_0 : 4.667e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3030, Loss: 1.853e-05, Loss_0: 8.758e-08, Loss_r: 1.844e-05, lambda_0 : 4.267e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3040, Loss: 1.855e-05, Loss_0: 2.312e-07, Loss_r: 1.832e-05, lambda_0 : 2.604e+01, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3050, Loss: 1.822e-05, Loss_0: 3.007e-08, Loss_r: 1.819e-05, lambda_0 : 7.586e+01, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3060, Loss: 1.810e-05, Loss_0: 4.315e-08, Loss_r: 1.806e-05, lambda_0 : 5.810e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3070, Loss: 1.793e-05, Loss_0: 1.166e-08, Loss_r: 1.792e-05, lambda_0 : 1.041e+02, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3080, Loss: 1.785e-05, Loss_0: 6.097e-08, Loss_r: 1.778e-05, lambda_0 : 4.667e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3090, Loss: 6.954e-05, Loss_0: 5.188e-05, Loss_r: 1.766e-05, lambda_0 : 1.808e+00, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3100, Loss: 2.461e-05, Loss_0: 6.834e-06, Loss_r: 1.778e-05, lambda_0 : 1.426e+01, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3110, Loss: 1.978e-05, Loss_0: 1.942e-06, Loss_r: 1.784e-05, lambda_0 : 8.932e+00, Time: 0.40, Learning Rate: 0.00010\n",
            "It: 3120, Loss: 1.807e-05, Loss_0: 2.728e-07, Loss_r: 1.779e-05, lambda_0 : 2.570e+01, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3130, Loss: 1.785e-05, Loss_0: 1.490e-07, Loss_r: 1.770e-05, lambda_0 : 2.901e+01, Time: 0.41, Learning Rate: 0.00010\n",
            "It: 3140, Loss: 1.774e-05, Loss_0: 1.584e-07, Loss_r: 1.758e-05, lambda_0 : 2.806e+01, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3150, Loss: 1.747e-05, Loss_0: 1.032e-08, Loss_r: 1.746e-05, lambda_0 : 1.222e+02, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3160, Loss: 1.736e-05, Loss_0: 1.488e-08, Loss_r: 1.734e-05, lambda_0 : 1.009e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3170, Loss: 1.722e-05, Loss_0: 3.657e-09, Loss_r: 1.722e-05, lambda_0 : 1.777e+02, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3180, Loss: 1.710e-05, Loss_0: 4.277e-09, Loss_r: 1.710e-05, lambda_0 : 1.642e+02, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3190, Loss: 1.698e-05, Loss_0: 1.807e-09, Loss_r: 1.698e-05, lambda_0 : 2.450e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3200, Loss: 1.685e-05, Loss_0: 4.963e-10, Loss_r: 1.685e-05, lambda_0 : 4.436e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3210, Loss: 1.673e-05, Loss_0: 1.867e-10, Loss_r: 1.673e-05, lambda_0 : 7.362e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3220, Loss: 1.661e-05, Loss_0: 9.465e-11, Loss_r: 1.661e-05, lambda_0 : 1.225e+03, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3230, Loss: 1.649e-05, Loss_0: 1.847e-10, Loss_r: 1.649e-05, lambda_0 : 7.308e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3240, Loss: 1.637e-05, Loss_0: 1.101e-09, Loss_r: 1.637e-05, lambda_0 : 3.453e+02, Time: 0.40, Learning Rate: 0.00009\n",
            "It: 3250, Loss: 1.682e-05, Loss_0: 5.665e-07, Loss_r: 1.626e-05, lambda_0 : 1.484e+01, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3260, Loss: 1.818e-05, Loss_0: 1.877e-06, Loss_r: 1.631e-05, lambda_0 : 3.313e+01, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3270, Loss: 1.663e-05, Loss_0: 1.586e-09, Loss_r: 1.663e-05, lambda_0 : 8.782e+02, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3280, Loss: 2.212e-05, Loss_0: 5.345e-06, Loss_r: 1.678e-05, lambda_0 : 9.977e+00, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3290, Loss: 1.852e-05, Loss_0: 1.729e-06, Loss_r: 1.679e-05, lambda_0 : 1.089e+01, Time: 0.41, Learning Rate: 0.00009\n",
            "It: 3300, Loss: 1.683e-05, Loss_0: 1.023e-07, Loss_r: 1.672e-05, lambda_0 : 3.273e+01, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3310, Loss: 1.677e-05, Loss_0: 1.343e-07, Loss_r: 1.663e-05, lambda_0 : 3.105e+01, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3320, Loss: 1.659e-05, Loss_0: 6.426e-08, Loss_r: 1.653e-05, lambda_0 : 4.309e+01, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3330, Loss: 1.644e-05, Loss_0: 2.315e-08, Loss_r: 1.642e-05, lambda_0 : 7.108e+01, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3340, Loss: 1.631e-05, Loss_0: 6.072e-09, Loss_r: 1.631e-05, lambda_0 : 1.364e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3350, Loss: 1.620e-05, Loss_0: 4.063e-10, Loss_r: 1.619e-05, lambda_0 : 4.768e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3360, Loss: 1.608e-05, Loss_0: 4.808e-10, Loss_r: 1.608e-05, lambda_0 : 4.851e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3370, Loss: 1.597e-05, Loss_0: 4.388e-10, Loss_r: 1.597e-05, lambda_0 : 5.018e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3380, Loss: 1.586e-05, Loss_0: 1.149e-10, Loss_r: 1.586e-05, lambda_0 : 9.708e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3390, Loss: 1.575e-05, Loss_0: 1.191e-10, Loss_r: 1.575e-05, lambda_0 : 9.342e+02, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3400, Loss: 1.564e-05, Loss_0: 9.526e-11, Loss_r: 1.564e-05, lambda_0 : 1.029e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3410, Loss: 1.553e-05, Loss_0: 9.252e-11, Loss_r: 1.553e-05, lambda_0 : 1.182e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3420, Loss: 1.542e-05, Loss_0: 8.086e-11, Loss_r: 1.542e-05, lambda_0 : 1.257e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3430, Loss: 1.531e-05, Loss_0: 8.318e-11, Loss_r: 1.531e-05, lambda_0 : 1.389e+03, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3440, Loss: 1.520e-05, Loss_0: 7.962e-11, Loss_r: 1.520e-05, lambda_0 : 1.363e+03, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3450, Loss: 1.510e-05, Loss_0: 7.924e-11, Loss_r: 1.510e-05, lambda_0 : 1.363e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3460, Loss: 1.499e-05, Loss_0: 7.933e-11, Loss_r: 1.499e-05, lambda_0 : 1.391e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3470, Loss: 1.488e-05, Loss_0: 7.896e-11, Loss_r: 1.488e-05, lambda_0 : 1.389e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3480, Loss: 1.477e-05, Loss_0: 7.847e-11, Loss_r: 1.477e-05, lambda_0 : 1.380e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3490, Loss: 1.467e-05, Loss_0: 7.797e-11, Loss_r: 1.467e-05, lambda_0 : 1.370e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3500, Loss: 1.456e-05, Loss_0: 7.748e-11, Loss_r: 1.456e-05, lambda_0 : 1.360e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3510, Loss: 1.445e-05, Loss_0: 7.711e-11, Loss_r: 1.445e-05, lambda_0 : 1.357e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3520, Loss: 1.435e-05, Loss_0: 7.669e-11, Loss_r: 1.435e-05, lambda_0 : 1.353e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3530, Loss: 1.424e-05, Loss_0: 7.632e-11, Loss_r: 1.424e-05, lambda_0 : 1.350e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3540, Loss: 1.414e-05, Loss_0: 7.604e-11, Loss_r: 1.414e-05, lambda_0 : 1.352e+03, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3550, Loss: 1.403e-05, Loss_0: 8.679e-11, Loss_r: 1.403e-05, lambda_0 : 1.110e+03, Time: 0.40, Learning Rate: 0.00008\n",
            "It: 3560, Loss: 1.398e-05, Loss_0: 5.508e-08, Loss_r: 1.393e-05, lambda_0 : 4.062e+01, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3570, Loss: 1.370e-04, Loss_0: 1.231e-04, Loss_r: 1.393e-05, lambda_0 : 2.091e+00, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3580, Loss: 4.921e-05, Loss_0: 3.471e-05, Loss_r: 1.450e-05, lambda_0 : 1.501e+00, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3590, Loss: 2.611e-05, Loss_0: 1.123e-05, Loss_r: 1.488e-05, lambda_0 : 5.118e+00, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3600, Loss: 1.511e-05, Loss_0: 1.073e-07, Loss_r: 1.500e-05, lambda_0 : 3.782e+01, Time: 0.42, Learning Rate: 0.00008\n",
            "It: 3610, Loss: 1.646e-05, Loss_0: 1.461e-06, Loss_r: 1.500e-05, lambda_0 : 9.619e+00, Time: 0.41, Learning Rate: 0.00008\n",
            "It: 3620, Loss: 1.543e-05, Loss_0: 5.084e-07, Loss_r: 1.492e-05, lambda_0 : 1.467e+01, Time: 0.40, Learning Rate: 0.00007\n",
            "It: 3630, Loss: 1.502e-05, Loss_0: 1.774e-07, Loss_r: 1.484e-05, lambda_0 : 2.523e+01, Time: 0.40, Learning Rate: 0.00007\n",
            "It: 3640, Loss: 1.479e-05, Loss_0: 5.133e-08, Loss_r: 1.474e-05, lambda_0 : 4.544e+01, Time: 0.40, Learning Rate: 0.00007\n",
            "It: 3650, Loss: 1.465e-05, Loss_0: 7.993e-09, Loss_r: 1.464e-05, lambda_0 : 1.236e+02, Time: 0.40, Learning Rate: 0.00007\n",
            "It: 3660, Loss: 1.453e-05, Loss_0: 8.069e-11, Loss_r: 1.453e-05, lambda_0 : 1.375e+03, Time: 0.40, Learning Rate: 0.00007\n",
            "It: 3670, Loss: 1.443e-05, Loss_0: 1.871e-09, Loss_r: 1.443e-05, lambda_0 : 2.171e+02, Time: 0.41, Learning Rate: 0.00007\n",
            "It: 3680, Loss: 1.433e-05, Loss_0: 1.219e-09, Loss_r: 1.432e-05, lambda_0 : 3.063e+02, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3690, Loss: 1.423e-05, Loss_0: 5.265e-10, Loss_r: 1.423e-05, lambda_0 : 3.863e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3700, Loss: 1.414e-05, Loss_0: 1.801e-10, Loss_r: 1.414e-05, lambda_0 : 6.726e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3710, Loss: 1.404e-05, Loss_0: 1.045e-10, Loss_r: 1.404e-05, lambda_0 : 9.487e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3720, Loss: 1.395e-05, Loss_0: 7.743e-11, Loss_r: 1.395e-05, lambda_0 : 1.349e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3730, Loss: 1.386e-05, Loss_0: 7.772e-11, Loss_r: 1.386e-05, lambda_0 : 1.171e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3740, Loss: 1.376e-05, Loss_0: 8.171e-11, Loss_r: 1.376e-05, lambda_0 : 1.250e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3750, Loss: 1.367e-05, Loss_0: 7.643e-11, Loss_r: 1.367e-05, lambda_0 : 1.328e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3760, Loss: 1.358e-05, Loss_0: 7.562e-11, Loss_r: 1.358e-05, lambda_0 : 1.283e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3770, Loss: 1.349e-05, Loss_0: 7.600e-11, Loss_r: 1.349e-05, lambda_0 : 1.332e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3780, Loss: 1.339e-05, Loss_0: 7.594e-11, Loss_r: 1.339e-05, lambda_0 : 1.339e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3790, Loss: 1.330e-05, Loss_0: 7.537e-11, Loss_r: 1.330e-05, lambda_0 : 1.323e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3800, Loss: 1.321e-05, Loss_0: 7.492e-11, Loss_r: 1.321e-05, lambda_0 : 1.312e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3810, Loss: 1.312e-05, Loss_0: 7.458e-11, Loss_r: 1.312e-05, lambda_0 : 1.306e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3820, Loss: 1.303e-05, Loss_0: 7.428e-11, Loss_r: 1.303e-05, lambda_0 : 1.304e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3830, Loss: 1.294e-05, Loss_0: 7.397e-11, Loss_r: 1.294e-05, lambda_0 : 1.301e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3840, Loss: 1.285e-05, Loss_0: 7.366e-11, Loss_r: 1.285e-05, lambda_0 : 1.297e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3850, Loss: 1.276e-05, Loss_0: 7.336e-11, Loss_r: 1.276e-05, lambda_0 : 1.293e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3860, Loss: 1.267e-05, Loss_0: 7.306e-11, Loss_r: 1.267e-05, lambda_0 : 1.289e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3870, Loss: 1.258e-05, Loss_0: 7.274e-11, Loss_r: 1.258e-05, lambda_0 : 1.285e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3880, Loss: 1.249e-05, Loss_0: 7.244e-11, Loss_r: 1.249e-05, lambda_0 : 1.281e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3890, Loss: 1.240e-05, Loss_0: 7.214e-11, Loss_r: 1.240e-05, lambda_0 : 1.277e+03, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 3900, Loss: 1.231e-05, Loss_0: 7.183e-11, Loss_r: 1.231e-05, lambda_0 : 1.272e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3910, Loss: 1.222e-05, Loss_0: 7.152e-11, Loss_r: 1.222e-05, lambda_0 : 1.268e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 3920, Loss: 1.213e-05, Loss_0: 7.122e-11, Loss_r: 1.213e-05, lambda_0 : 1.263e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3930, Loss: 1.204e-05, Loss_0: 7.094e-11, Loss_r: 1.204e-05, lambda_0 : 1.259e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3940, Loss: 1.196e-05, Loss_0: 7.063e-11, Loss_r: 1.196e-05, lambda_0 : 1.255e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3950, Loss: 1.187e-05, Loss_0: 7.032e-11, Loss_r: 1.187e-05, lambda_0 : 1.251e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3960, Loss: 1.178e-05, Loss_0: 7.004e-11, Loss_r: 1.178e-05, lambda_0 : 1.246e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3970, Loss: 1.170e-05, Loss_0: 6.973e-11, Loss_r: 1.170e-05, lambda_0 : 1.242e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3980, Loss: 1.161e-05, Loss_0: 6.945e-11, Loss_r: 1.161e-05, lambda_0 : 1.237e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 3990, Loss: 1.153e-05, Loss_0: 6.916e-11, Loss_r: 1.153e-05, lambda_0 : 1.233e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4000, Loss: 1.145e-05, Loss_0: 6.886e-11, Loss_r: 1.145e-05, lambda_0 : 1.228e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4010, Loss: 1.136e-05, Loss_0: 6.857e-11, Loss_r: 1.136e-05, lambda_0 : 1.223e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4020, Loss: 1.128e-05, Loss_0: 6.830e-11, Loss_r: 1.128e-05, lambda_0 : 1.219e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4030, Loss: 1.120e-05, Loss_0: 6.801e-11, Loss_r: 1.120e-05, lambda_0 : 1.215e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4040, Loss: 1.112e-05, Loss_0: 6.773e-11, Loss_r: 1.112e-05, lambda_0 : 1.210e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4050, Loss: 1.104e-05, Loss_0: 6.746e-11, Loss_r: 1.104e-05, lambda_0 : 1.206e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4060, Loss: 1.096e-05, Loss_0: 6.719e-11, Loss_r: 1.096e-05, lambda_0 : 1.202e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4070, Loss: 1.088e-05, Loss_0: 6.690e-11, Loss_r: 1.088e-05, lambda_0 : 1.197e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4080, Loss: 1.080e-05, Loss_0: 6.665e-11, Loss_r: 1.080e-05, lambda_0 : 1.193e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4090, Loss: 1.072e-05, Loss_0: 6.648e-11, Loss_r: 1.072e-05, lambda_0 : 1.194e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4100, Loss: 1.065e-05, Loss_0: 6.804e-11, Loss_r: 1.065e-05, lambda_0 : 1.195e+03, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4110, Loss: 1.057e-05, Loss_0: 8.542e-10, Loss_r: 1.057e-05, lambda_0 : 2.430e+02, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4120, Loss: 1.396e-05, Loss_0: 3.463e-06, Loss_r: 1.049e-05, lambda_0 : 4.138e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4130, Loss: 1.081e-05, Loss_0: 3.482e-07, Loss_r: 1.046e-05, lambda_0 : 1.417e+01, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4140, Loss: 1.202e-05, Loss_0: 1.552e-06, Loss_r: 1.047e-05, lambda_0 : 6.113e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4150, Loss: 1.192e-05, Loss_0: 1.455e-06, Loss_r: 1.046e-05, lambda_0 : 1.102e+01, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4160, Loss: 1.107e-05, Loss_0: 6.482e-07, Loss_r: 1.042e-05, lambda_0 : 1.886e+01, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4170, Loss: 1.045e-05, Loss_0: 8.944e-08, Loss_r: 1.036e-05, lambda_0 : 5.467e+01, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4180, Loss: 1.035e-05, Loss_0: 5.061e-08, Loss_r: 1.030e-05, lambda_0 : 3.692e+01, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4190, Loss: 1.026e-05, Loss_0: 2.240e-08, Loss_r: 1.024e-05, lambda_0 : 5.096e+01, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4200, Loss: 1.018e-05, Loss_0: 3.994e-10, Loss_r: 1.018e-05, lambda_0 : 3.338e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4210, Loss: 1.011e-05, Loss_0: 3.488e-10, Loss_r: 1.011e-05, lambda_0 : 4.034e+02, Time: 0.42, Learning Rate: 0.00006\n",
            "It: 4220, Loss: 1.005e-05, Loss_0: 1.614e-10, Loss_r: 1.005e-05, lambda_0 : 5.455e+02, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 4230, Loss: 9.986e-06, Loss_0: 1.003e-10, Loss_r: 9.985e-06, lambda_0 : 7.098e+02, Time: 0.46, Learning Rate: 0.00006\n",
            "It: 4240, Loss: 9.923e-06, Loss_0: 2.945e-10, Loss_r: 9.923e-06, lambda_0 : 3.883e+02, Time: 0.43, Learning Rate: 0.00006\n",
            "It: 4250, Loss: 9.861e-06, Loss_0: 7.514e-11, Loss_r: 9.861e-06, lambda_0 : 9.194e+02, Time: 0.46, Learning Rate: 0.00006\n",
            "It: 4260, Loss: 9.799e-06, Loss_0: 8.996e-11, Loss_r: 9.799e-06, lambda_0 : 7.235e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4270, Loss: 9.738e-06, Loss_0: 8.463e-11, Loss_r: 9.738e-06, lambda_0 : 8.003e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4280, Loss: 9.678e-06, Loss_0: 6.274e-11, Loss_r: 9.678e-06, lambda_0 : 1.092e+03, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4290, Loss: 9.618e-06, Loss_0: 7.865e-11, Loss_r: 9.618e-06, lambda_0 : 7.801e+02, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4300, Loss: 9.560e-06, Loss_0: 1.196e-09, Loss_r: 9.558e-06, lambda_0 : 2.246e+02, Time: 0.41, Learning Rate: 0.00006\n",
            "It: 4310, Loss: 1.002e-05, Loss_0: 5.144e-07, Loss_r: 9.501e-06, lambda_0 : 1.036e+01, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4320, Loss: 2.702e-05, Loss_0: 1.752e-05, Loss_r: 9.503e-06, lambda_0 : 4.652e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4330, Loss: 1.674e-05, Loss_0: 7.142e-06, Loss_r: 9.601e-06, lambda_0 : 2.633e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4340, Loss: 1.390e-05, Loss_0: 4.256e-06, Loss_r: 9.644e-06, lambda_0 : 4.773e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4350, Loss: 1.103e-05, Loss_0: 1.397e-06, Loss_r: 9.634e-06, lambda_0 : 6.508e+00, Time: 0.40, Learning Rate: 0.00006\n",
            "It: 4360, Loss: 1.008e-05, Loss_0: 4.804e-07, Loss_r: 9.597e-06, lambda_0 : 1.102e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4370, Loss: 9.586e-06, Loss_0: 3.326e-08, Loss_r: 9.552e-06, lambda_0 : 4.402e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4380, Loss: 9.566e-06, Loss_0: 6.345e-08, Loss_r: 9.502e-06, lambda_0 : 2.855e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4390, Loss: 9.455e-06, Loss_0: 3.876e-09, Loss_r: 9.451e-06, lambda_0 : 1.278e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4400, Loss: 9.404e-06, Loss_0: 5.175e-09, Loss_r: 9.399e-06, lambda_0 : 1.112e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4410, Loss: 9.350e-06, Loss_0: 3.018e-09, Loss_r: 9.347e-06, lambda_0 : 1.239e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4420, Loss: 9.296e-06, Loss_0: 2.602e-10, Loss_r: 9.295e-06, lambda_0 : 4.254e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4430, Loss: 9.244e-06, Loss_0: 6.264e-11, Loss_r: 9.244e-06, lambda_0 : 9.563e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4440, Loss: 9.193e-06, Loss_0: 8.908e-11, Loss_r: 9.193e-06, lambda_0 : 7.322e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4450, Loss: 9.143e-06, Loss_0: 6.728e-11, Loss_r: 9.142e-06, lambda_0 : 8.689e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4460, Loss: 9.092e-06, Loss_0: 6.322e-11, Loss_r: 9.092e-06, lambda_0 : 1.135e+03, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4470, Loss: 9.043e-06, Loss_0: 6.329e-11, Loss_r: 9.043e-06, lambda_0 : 1.116e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4480, Loss: 8.993e-06, Loss_0: 6.167e-11, Loss_r: 8.993e-06, lambda_0 : 9.791e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4490, Loss: 8.944e-06, Loss_0: 6.264e-11, Loss_r: 8.944e-06, lambda_0 : 1.133e+03, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4500, Loss: 8.896e-06, Loss_0: 6.161e-11, Loss_r: 8.895e-06, lambda_0 : 1.121e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4510, Loss: 8.847e-06, Loss_0: 6.076e-11, Loss_r: 8.847e-06, lambda_0 : 1.072e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4520, Loss: 8.799e-06, Loss_0: 6.067e-11, Loss_r: 8.799e-06, lambda_0 : 1.072e+03, Time: 0.45, Learning Rate: 0.00005\n",
            "It: 4530, Loss: 8.752e-06, Loss_0: 6.067e-11, Loss_r: 8.752e-06, lambda_0 : 1.079e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4540, Loss: 8.705e-06, Loss_0: 6.060e-11, Loss_r: 8.705e-06, lambda_0 : 1.078e+03, Time: 0.45, Learning Rate: 0.00005\n",
            "It: 4550, Loss: 8.658e-06, Loss_0: 6.048e-11, Loss_r: 8.658e-06, lambda_0 : 1.074e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4560, Loss: 8.612e-06, Loss_0: 6.035e-11, Loss_r: 8.611e-06, lambda_0 : 1.070e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4570, Loss: 8.565e-06, Loss_0: 6.023e-11, Loss_r: 8.565e-06, lambda_0 : 1.067e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4580, Loss: 8.520e-06, Loss_0: 6.013e-11, Loss_r: 8.520e-06, lambda_0 : 1.065e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4590, Loss: 8.474e-06, Loss_0: 5.997e-11, Loss_r: 8.474e-06, lambda_0 : 1.059e+03, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4600, Loss: 8.429e-06, Loss_0: 5.985e-11, Loss_r: 8.429e-06, lambda_0 : 1.056e+03, Time: 0.46, Learning Rate: 0.00005\n",
            "It: 4610, Loss: 8.385e-06, Loss_0: 5.974e-11, Loss_r: 8.385e-06, lambda_0 : 1.053e+03, Time: 0.45, Learning Rate: 0.00005\n",
            "It: 4620, Loss: 8.340e-06, Loss_0: 5.956e-11, Loss_r: 8.340e-06, lambda_0 : 1.047e+03, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4630, Loss: 8.296e-06, Loss_0: 5.942e-11, Loss_r: 8.296e-06, lambda_0 : 1.043e+03, Time: 0.46, Learning Rate: 0.00005\n",
            "It: 4640, Loss: 8.253e-06, Loss_0: 5.918e-11, Loss_r: 8.253e-06, lambda_0 : 1.031e+03, Time: 0.43, Learning Rate: 0.00005\n",
            "It: 4650, Loss: 8.209e-06, Loss_0: 6.668e-11, Loss_r: 8.209e-06, lambda_0 : 7.781e+02, Time: 0.44, Learning Rate: 0.00005\n",
            "It: 4660, Loss: 8.190e-06, Loss_0: 2.397e-08, Loss_r: 8.166e-06, lambda_0 : 4.298e+01, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4670, Loss: 9.703e-05, Loss_0: 8.889e-05, Loss_r: 8.147e-06, lambda_0 : 1.819e+00, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4680, Loss: 2.398e-05, Loss_0: 1.574e-05, Loss_r: 8.237e-06, lambda_0 : 7.497e+00, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4690, Loss: 9.447e-06, Loss_0: 1.140e-06, Loss_r: 8.308e-06, lambda_0 : 1.621e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4700, Loss: 8.999e-06, Loss_0: 6.785e-07, Loss_r: 8.320e-06, lambda_0 : 8.669e+00, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4710, Loss: 9.116e-06, Loss_0: 8.159e-07, Loss_r: 8.301e-06, lambda_0 : 8.237e+00, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4720, Loss: 8.273e-06, Loss_0: 4.568e-09, Loss_r: 8.269e-06, lambda_0 : 9.894e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4730, Loss: 8.285e-06, Loss_0: 5.182e-08, Loss_r: 8.233e-06, lambda_0 : 2.950e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4740, Loss: 8.219e-06, Loss_0: 2.371e-08, Loss_r: 8.195e-06, lambda_0 : 4.507e+01, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4750, Loss: 8.157e-06, Loss_0: 1.132e-09, Loss_r: 8.156e-06, lambda_0 : 1.790e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4760, Loss: 8.121e-06, Loss_0: 3.696e-09, Loss_r: 8.117e-06, lambda_0 : 1.006e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4770, Loss: 8.079e-06, Loss_0: 3.824e-10, Loss_r: 8.078e-06, lambda_0 : 3.259e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4780, Loss: 8.040e-06, Loss_0: 2.228e-10, Loss_r: 8.040e-06, lambda_0 : 4.059e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4790, Loss: 8.001e-06, Loss_0: 2.537e-10, Loss_r: 8.001e-06, lambda_0 : 3.544e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4800, Loss: 7.963e-06, Loss_0: 8.605e-11, Loss_r: 7.963e-06, lambda_0 : 6.354e+02, Time: 0.42, Learning Rate: 0.00005\n",
            "It: 4810, Loss: 7.925e-06, Loss_0: 6.209e-11, Loss_r: 7.925e-06, lambda_0 : 9.624e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4820, Loss: 7.887e-06, Loss_0: 5.815e-11, Loss_r: 7.887e-06, lambda_0 : 1.024e+03, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4830, Loss: 7.850e-06, Loss_0: 5.814e-11, Loss_r: 7.849e-06, lambda_0 : 1.021e+03, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4840, Loss: 7.812e-06, Loss_0: 5.796e-11, Loss_r: 7.812e-06, lambda_0 : 1.009e+03, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4850, Loss: 7.775e-06, Loss_0: 5.856e-11, Loss_r: 7.775e-06, lambda_0 : 1.043e+03, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4860, Loss: 7.738e-06, Loss_0: 5.770e-11, Loss_r: 7.738e-06, lambda_0 : 9.863e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4870, Loss: 7.702e-06, Loss_0: 5.812e-11, Loss_r: 7.702e-06, lambda_0 : 1.022e+03, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4880, Loss: 7.666e-06, Loss_0: 5.786e-11, Loss_r: 7.665e-06, lambda_0 : 1.009e+03, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4890, Loss: 7.629e-06, Loss_0: 5.763e-11, Loss_r: 7.629e-06, lambda_0 : 9.964e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4900, Loss: 7.594e-06, Loss_0: 5.760e-11, Loss_r: 7.594e-06, lambda_0 : 9.970e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4910, Loss: 7.558e-06, Loss_0: 5.758e-11, Loss_r: 7.558e-06, lambda_0 : 9.960e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4920, Loss: 7.523e-06, Loss_0: 5.748e-11, Loss_r: 7.522e-06, lambda_0 : 9.922e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4930, Loss: 7.487e-06, Loss_0: 5.737e-11, Loss_r: 7.487e-06, lambda_0 : 9.888e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4940, Loss: 7.452e-06, Loss_0: 5.727e-11, Loss_r: 7.452e-06, lambda_0 : 9.837e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4950, Loss: 7.418e-06, Loss_0: 5.717e-11, Loss_r: 7.418e-06, lambda_0 : 9.803e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4960, Loss: 7.383e-06, Loss_0: 5.706e-11, Loss_r: 7.383e-06, lambda_0 : 9.763e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4970, Loss: 7.349e-06, Loss_0: 5.695e-11, Loss_r: 7.349e-06, lambda_0 : 9.717e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "It: 4980, Loss: 7.315e-06, Loss_0: 5.688e-11, Loss_r: 7.315e-06, lambda_0 : 9.692e+02, Time: 0.40, Learning Rate: 0.00005\n",
            "It: 4990, Loss: 7.281e-06, Loss_0: 5.677e-11, Loss_r: 7.281e-06, lambda_0 : 9.649e+02, Time: 0.41, Learning Rate: 0.00005\n",
            "Training time: 205.2196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare with expected result from matlab model\n",
        "predict_CgSol = np.array(predict_CSol)[:,0:1]\n",
        "predict_CsSol = np.array(predict_CSol)[:,1:2]\n",
        "\n",
        "for idx in range(len(predict_CSol)):\n",
        "  predict_CgSol[idx] = predict_CgSol[idx].reshape(exact_Cg.shape)\n",
        "  predict_CsSol[idx] = predict_CsSol[idx].reshape(exact_Cs.shape)\n",
        "\n",
        "  error_Cg = np.linalg.norm(exact_Cg.flatten()[:,None]-predict_CgSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_Cg.flatten()[:,None],2)\n",
        "  error_Cs = np.linalg.norm(exact_Cs.flatten()[:,None]-predict_CsSol[idx].flatten()[:,None],2)/np.linalg.norm(exact_Cs.flatten()[:,None],2)\n",
        "  print('Error Cg Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_Cg) )\n",
        "  print('Error Cs Sol ' + str(DifferentLayers[idx]) + ' : %e' % (error_Cs) )\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "6ZajY9h_ap5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cf9465-6806-417d-ba75-a7398ee2081d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Cg Sol [1, 64, 64, 64, 2] : 3.541574e-02\n",
            "Error Cs Sol [1, 64, 64, 64, 2] : 3.595853e-02\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 128, 128, 128, 2] : 7.114102e-02\n",
            "Error Cs Sol [1, 128, 128, 128, 2] : 7.411784e-02\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 256, 256, 256, 2] : 1.921617e-01\n",
            "Error Cs Sol [1, 256, 256, 256, 2] : 2.013837e-01\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 64, 64, 64, 64, 2] : 1.196818e-01\n",
            "Error Cs Sol [1, 64, 64, 64, 64, 2] : 1.248989e-01\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 256, 256, 256, 256, 2] : 1.465169e-01\n",
            "Error Cs Sol [1, 256, 256, 256, 256, 2] : 1.538560e-01\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 128, 128, 128, 128, 2] : 9.002507e-02\n",
            "Error Cs Sol [1, 128, 128, 128, 128, 2] : 9.417427e-02\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 128, 128, 64, 64, 2] : 1.884208e-01\n",
            "Error Cs Sol [1, 128, 128, 64, 64, 2] : 1.974845e-01\n",
            "\n",
            "\n",
            "Error Cg Sol [1, 256, 128, 64, 32, 2] : 3.406594e-01\n",
            "Error Cs Sol [1, 256, 128, 64, 32, 2] : 3.571434e-01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color = ['g','y','b','m','r','k']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [0,1,2,3,5,6]:\n",
        "  plt.plot(t, predict_CgSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_Cg.flatten(), 'c', label = 'Cg expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "KEI_9agfap6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "abefc121-87b2-4461-aa76-ad6dd6b2047d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9c1ab48550>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ploting results\n",
        "\n",
        "color1 = ['g','y','b','m','r','k']\n",
        "color_idx = 0\n",
        "\n",
        "plt.figure()\n",
        "for idx in [0,1,2,3,5,6]:\n",
        "  plt.plot(t, predict_CsSol[idx].flatten(),color[color_idx%5], label = str(DifferentLayers[idx]))\n",
        "  color_idx +=1\n",
        "plt.plot(t, exact_Cs.flatten(), 'c', label = 'Cg expected')\n",
        "plt.ylabel(\"C (kmol/m3)\")\n",
        "plt.xlabel('position t (s)')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "oz8zqlLh3QKt",
        "outputId": "fb53f0fd-6d68-4a45-a82d-c7637d30425a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9c1aa2de10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}